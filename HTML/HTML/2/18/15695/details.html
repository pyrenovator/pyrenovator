<html><h3>Pattern ID :15695
</h3><img src='53038553.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        }

        self._device = device
        self._input_dists: nn.ModuleList = <a id="change">nn.ModuleList()</a>
        self._lookups<a id="change">: nn.ModuleList = nn</a><a id="change">.ModuleList()</a>
        self._create_lookups()
        self._output_dists: nn.ModuleList = <a id="change">nn.ModuleList()</a>
        self._create_output_dist()

        self._feature_splits: List[int] = []
        self._features_order: List[int] = []</code></pre><h3>After Change</h3><pre><code class='java'>

        self._device = device
        self._input_dists: List[nn.Module] = []
        <a id="change">self._lookups: List[nn.Module] = </a><a id="change">[]</a>
        self._create_lookups()
        self._output_dists: List[nn.Module] = []
        self._create_output_dist()

        self._feature_splits: List[int] = []
        self._features_order: List[int] = []

        self._has_uninitialized_input_dist: bool = True

        &#47&#47 Get all fused optimizers and combine them.
        optims = []
        for lookup in self._lookups:
            for _, m in lookup.named_modules():
                if isinstance(m, FusedOptimizerModule):
                    &#47&#47 modify param keys to match EmbeddingCollection
                    params: MutableMapping[str, Union[torch.Tensor, ShardedTensor]] = {}
                    for param_key, weight in m.fused_optimizer.params.items():
                        params["embeddings." + param_key] = weight
                    m.fused_optimizer.params = params
                    optims.append(("", m.fused_optimizer))
        self._optim: CombinedOptimizer = CombinedOptimizer(optims)
        self._embedding_dim: int = module.embedding_dim()
        self._embedding_names_per_sharding: List[List[str]] = []
        for sharding in self._sharding_type_to_sharding.values():
            self._embedding_names_per_sharding.append(sharding.embedding_names())
        self._local_embedding_dim: int = self._embedding_dim
        self._features_to_permute_indices: Dict[str, List[int]] = {}
        if ShardingType.COLUMN_WISE.value in self._sharding_type_to_sharding:
            sharding = self._sharding_type_to_sharding[ShardingType.COLUMN_WISE.value]
            &#47&#47 CW partition must be same for all CW sharded parameters
            self._local_embedding_dim = cast(
                ShardMetadata, sharding.embedding_shard_metadata()[0]
            ).shard_sizes[1]
            self._generate_permute_indices_per_feature(
                module.embedding_configs(), table_name_to_parameter_sharding
            )
        self._need_indices: bool = module.need_indices()

        <a id="change">for </a><a id="change">index</a>, (sharding, lookup) in <a id="change">enumerate(
            zip(
                </a><a id="change">self._sharding_type_to_sharding.values()</a>,
                <a id="change">self._lookups</a><a id="change">,
            )</a><a id="change">
        ):
            &#47&#47 TODO: can move this into DpPooledEmbeddingSharding once all modules are composable
            </a><a id="change">if isinstance(</a>sharding, DpSequenceEmbeddingSharding<a id="change">)</a>:
                <a id="change">self._lookups[index] = </a><a id="change">DistributedDataParallel(
                    module=lookup,
                    device_ids=[device]
                    if self._device and self._device.type == "gpu"
                    else None,
                    process_group=env.process_group,
                    gradient_as_bucket_view=True,
                    broadcast_buffers=True,
                    static_graph=True,
                )</a>
        self._initialize_torch_state()

    @staticmethod
    def _pre_load_state_dict_hook(</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 15</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/0ffa1c7616403c416c73118107bec307baa3abfb#diff-ef14f956164adafc139f5a6d4554edaf589f8e92ac501cad851bb9db40d0e315L315' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 53038553</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: 0ffa1c7616403c416c73118107bec307baa3abfb</div><div id='time'> Time: 2022-12-21</div><div id='author'> Author: colin2328@meta.com</div><div id='file'> File Name: torchrec/distributed/embedding.py</div><div id='m_class'> M Class Name: ShardedEmbeddingCollection</div><div id='n_method'> N Class Name: ShardedEmbeddingCollection</div><div id='m_method'> M Method Name: __init__(7)</div><div id='n_method'> N Method Name: __init__(7)</div><div id='m_parent_class'> M Parent Class: FusedOptimizerModule,Subscript</div><div id='n_parent_class'> N Parent Class: FusedOptimizerModule,Subscript</div><div id='m_file'> M File Name: torchrec/distributed/embedding.py</div><div id='n_file'> N File Name: torchrec/distributed/embedding.py</div><div id='m_start'> M Start Line: 336</div><div id='m_end'> M End Line: 339</div><div id='n_start'> N Start Line: 315</div><div id='n_end'> N End Line: 405</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        self._is_weighted: bool = module.is_weighted()
        self._device = device
        self._input_dists = <a id="change">nn.ModuleList()</a>
        self._lookups<a id="change">: nn.ModuleList = </a><a id="change">nn.ModuleList()</a>
        self._create_lookups()
        self._output_dists: nn.ModuleList = <a id="change">nn.ModuleList()</a>
        self._embedding_names: List[str] = []
        self._embedding_dims: List[int] = []
        self._feature_splits: List[int] = []
        self._features_order: List[int] = []</code></pre><h3>After Change</h3><pre><code class='java'>
        self._is_weighted: bool = module.is_weighted()
        self._device = device
        self._input_dists: List[nn.Module] = []
        <a id="change">self._lookups: List[nn.Module] = </a><a id="change">[]</a>
        self._create_lookups()
        self._output_dists: List[nn.Module] = []
        self._embedding_names: List[str] = []
        self._embedding_dims: List[int] = []
        self._feature_splits: List[int] = []
        self._features_order: List[int] = []
        &#47&#47 to support the FP16 hook
        self._create_output_dist()

        &#47&#47 forward pass flow control
        self._has_uninitialized_input_dist: bool = True
        self._has_features_permute: bool = True

        &#47&#47 Get all fused optimizers and combine them.
        optims = []
        for lookup in self._lookups:
            for _, module in lookup.named_modules():
                if isinstance(module, FusedOptimizerModule):
                    &#47&#47 modify param keys to match EmbeddingBagCollection
                    params: Mapping[str, Union[torch.Tensor, ShardedTensor]] = {}
                    for param_key, weight in module.fused_optimizer.params.items():
                        &#47&#47 pyre-fixme[16]: `Mapping` has no attribute `__setitem__`.
                        params["embedding_bags." + param_key] = weight
                    module.fused_optimizer.params = params
                    optims.append(("", module.fused_optimizer))
        self._optim: CombinedOptimizer = CombinedOptimizer(optims)

        <a id="change">for </a><a id="change">index</a>, (sharding, lookup) in <a id="change">enumerate(
            zip(
                </a><a id="change">self._sharding_type_to_sharding.values()</a>,
                self._lookups<a id="change">,
            )</a><a id="change">
        ):
            &#47&#47 TODO: can move this into DpPooledEmbeddingSharding once all modules are composable
            </a><a id="change">if isinstance(</a>sharding, DpPooledEmbeddingSharding<a id="change">)</a>:
                <a id="change">self._lookups[index] = </a><a id="change">DistributedDataParallel(
                    module=lookup,
                    device_ids=[device]
                    if self._device and self._device.type == "gpu"
                    else None,
                    process_group=env.process_group,
                    gradient_as_bucket_view=True,
                    broadcast_buffers=True,
                    static_graph=True,
                )</a>
        self._initialize_torch_state()

    @staticmethod
    def _pre_load_state_dict_hook(</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/c4f251b90040bffc6c6fbd3b8076205287140dc2#diff-fd51ff8537ea74d205ade3b58e7acd1832ae6fc5d8f549719407f53f6ccdfa9cL290' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 53038552</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: c4f251b90040bffc6c6fbd3b8076205287140dc2</div><div id='time'> Time: 2022-12-16</div><div id='author'> Author: colin2328@meta.com</div><div id='file'> File Name: torchrec/distributed/embeddingbag.py</div><div id='m_class'> M Class Name: ShardedEmbeddingBagCollection</div><div id='n_method'> N Class Name: ShardedEmbeddingBagCollection</div><div id='m_method'> M Method Name: __init__(7)</div><div id='n_method'> N Method Name: __init__(7)</div><div id='m_parent_class'> M Parent Class: FusedOptimizerModule,Subscript</div><div id='n_parent_class'> N Parent Class: FusedOptimizerModule,Subscript</div><div id='m_file'> M File Name: torchrec/distributed/embeddingbag.py</div><div id='n_file'> N File Name: torchrec/distributed/embeddingbag.py</div><div id='m_start'> M Start Line: 330</div><div id='m_end'> M End Line: 333</div><div id='n_start'> N Start Line: 305</div><div id='n_end'> N End Line: 396</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        }

        self._device = device
        self._input_dists: nn.ModuleList = <a id="change">nn.ModuleList()</a>
        self._lookups<a id="change">: nn.ModuleList = </a><a id="change">nn.ModuleList()</a>
        self._create_lookups()
        self._output_dists: nn.ModuleList = <a id="change">nn.ModuleList()</a>
        self._create_output_dist()

        self._feature_splits: List[int] = []
        self._features_order: List[int] = []</code></pre><h3>After Change</h3><pre><code class='java'>

        self._device = device
        self._input_dists: List[nn.Module] = []
        <a id="change">self._lookups: List[nn.Module] = </a><a id="change">[]</a>
        self._create_lookups()
        self._output_dists: List[nn.Module] = []
        self._create_output_dist()

        self._feature_splits: List[int] = []
        self._features_order: List[int] = []

        self._has_uninitialized_input_dist: bool = True

        &#47&#47 Get all fused optimizers and combine them.
        optims = []
        for lookup in self._lookups:
            for _, m in lookup.named_modules():
                if isinstance(m, FusedOptimizerModule):
                    &#47&#47 modify param keys to match EmbeddingCollection
                    params: MutableMapping[str, Union[torch.Tensor, ShardedTensor]] = {}
                    for param_key, weight in m.fused_optimizer.params.items():
                        params["embeddings." + param_key] = weight
                    m.fused_optimizer.params = params
                    optims.append(("", m.fused_optimizer))
        self._optim: CombinedOptimizer = CombinedOptimizer(optims)
        self._embedding_dim: int = module.embedding_dim()
        self._embedding_names_per_sharding: List[List[str]] = []
        for sharding in self._sharding_type_to_sharding.values():
            self._embedding_names_per_sharding.append(sharding.embedding_names())
        self._local_embedding_dim: int = self._embedding_dim
        self._features_to_permute_indices: Dict[str, List[int]] = {}
        if ShardingType.COLUMN_WISE.value in self._sharding_type_to_sharding:
            sharding = self._sharding_type_to_sharding[ShardingType.COLUMN_WISE.value]
            &#47&#47 CW partition must be same for all CW sharded parameters
            self._local_embedding_dim = cast(
                ShardMetadata, sharding.embedding_shard_metadata()[0]
            ).shard_sizes[1]
            self._generate_permute_indices_per_feature(
                module.embedding_configs(), table_name_to_parameter_sharding
            )
        self._need_indices: bool = module.need_indices()

        <a id="change">for </a><a id="change">index</a>, (sharding, lookup) in <a id="change">enumerate(
            zip(
                </a><a id="change">self._sharding_type_to_sharding.values()</a>,
                self._lookups<a id="change">,
            )</a><a id="change">
        ):
            &#47&#47 TODO: can move this into DpPooledEmbeddingSharding once all modules are composable
            </a><a id="change">if isinstance(</a>sharding, DpSequenceEmbeddingSharding<a id="change">)</a>:
                <a id="change">self._lookups[index] = </a><a id="change">DistributedDataParallel(
                    module=lookup,
                    device_ids=[device]
                    if self._device and self._device.type == "gpu"
                    else None,
                    process_group=env.process_group,
                    gradient_as_bucket_view=True,
                    broadcast_buffers=True,
                    static_graph=True,
                )</a>
        self._initialize_torch_state()

    @staticmethod
    def _pre_load_state_dict_hook(</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/0ffa1c7616403c416c73118107bec307baa3abfb#diff-ef14f956164adafc139f5a6d4554edaf589f8e92ac501cad851bb9db40d0e315L304' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 53038543</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: 0ffa1c7616403c416c73118107bec307baa3abfb</div><div id='time'> Time: 2022-12-21</div><div id='author'> Author: colin2328@meta.com</div><div id='file'> File Name: torchrec/distributed/embedding.py</div><div id='m_class'> M Class Name: ShardedEmbeddingCollection</div><div id='n_method'> N Class Name: ShardedEmbeddingCollection</div><div id='m_method'> M Method Name: __init__(7)</div><div id='n_method'> N Method Name: __init__(7)</div><div id='m_parent_class'> M Parent Class: FusedOptimizerModule,Subscript</div><div id='n_parent_class'> N Parent Class: FusedOptimizerModule,Subscript</div><div id='m_file'> M File Name: torchrec/distributed/embedding.py</div><div id='n_file'> N File Name: torchrec/distributed/embedding.py</div><div id='m_start'> M Start Line: 336</div><div id='m_end'> M End Line: 339</div><div id='n_start'> N Start Line: 315</div><div id='n_end'> N End Line: 405</div><BR>