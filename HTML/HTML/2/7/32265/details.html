<html><h3>Pattern ID :32265
</h3><img src='94389126.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self._k = k

        if weight:
            self.weight<a id="change"> = nn</a><a id="change">.Parameter(</a>th.Tensor(in_feats, out_feats)<a id="change">)</a>
        else:
            self.register_parameter(&quotweight&quot, None)

        if bias:
            self.bias<a id="change"> = nn</a><a id="change">.Parameter(</a>th.Tensor(out_feats)<a id="change">)</a>
        else:
            self.register_parameter(&quotbias&quot, None)
</code></pre><h3>After Change</h3><pre><code class='java'>
        
        super().__init__()
        if norm not in (&quotnone&quot, &quotboth&quot, &quotright&quot, &quotleft&quot):
            raise DGLError(<a id="change">&quotInvalid norm value. Must be either "none", "both", "right" or "left".&quot
                           &quot But got "{}".&quot.format(</a>norm<a id="change">)</a>)     
        self._in_feats = in_feats
        self._out_feats = out_feats            
        self._cached = cached</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/edisonleeeee/graphwar/commit/c43665fd30401c63acbd50175da1880509a52d21#diff-4e82f29fd5c78eabd4fe1d5867020e786dc0deb6b867e435c11f46885eaadad4L70' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 94389126</div><div id='project'> Project Name: edisonleeeee/graphwar</div><div id='commit'> Commit Name: c43665fd30401c63acbd50175da1880509a52d21</div><div id='time'> Time: 2021-12-06</div><div id='author'> Author: cnljt@outlook.com</div><div id='file'> File Name: graphwar/nn/sgconv.py</div><div id='m_class'> M Class Name: SGConv</div><div id='n_method'> N Class Name: SGConv</div><div id='m_method'> M Method Name: __init__(9)</div><div id='n_method'> N Method Name: __init__(7)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: graphwar/nn/sgconv.py</div><div id='n_file'> N File Name: graphwar/nn/sgconv.py</div><div id='m_start'> M Start Line: 70</div><div id='m_end'> M End Line: 85</div><div id='n_start'> N Start Line: 90</div><div id='n_end'> N End Line: 108</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self._k = k

        if weight:
            self.weight<a id="change"> = </a><a id="change">nn.Parameter(</a>th.Tensor(in_feats, out_feats)<a id="change">)</a>
        else:
            self.register_parameter(&quotweight&quot, None)

        if bias:
            self.bias<a id="change"> = </a><a id="change">nn.Parameter(</a>th.Tensor(out_feats)<a id="change">)</a>
        else:
            self.register_parameter(&quotbias&quot, None)
</code></pre><h3>After Change</h3><pre><code class='java'>
        
        super().__init__()
        if norm not in (&quotnone&quot, &quotboth&quot, &quotright&quot, &quotleft&quot):
            raise DGLError(<a id="change">&quotInvalid norm value. Must be either "none", "both", "right" or "left".&quot
                           &quot But got "{}".&quot.format(</a>norm<a id="change">)</a>)     
        self._in_feats = in_feats
        self._out_feats = out_feats            
        self._cached = cached</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/edisonleeeee/greatx/commit/c43665fd30401c63acbd50175da1880509a52d21#diff-4e82f29fd5c78eabd4fe1d5867020e786dc0deb6b867e435c11f46885eaadad4L63' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 94389127</div><div id='project'> Project Name: edisonleeeee/greatx</div><div id='commit'> Commit Name: c43665fd30401c63acbd50175da1880509a52d21</div><div id='time'> Time: 2021-12-06</div><div id='author'> Author: cnljt@outlook.com</div><div id='file'> File Name: graphwar/nn/sgconv.py</div><div id='m_class'> M Class Name: SGConv</div><div id='n_method'> N Class Name: SGConv</div><div id='m_method'> M Method Name: __init__(9)</div><div id='n_method'> N Method Name: __init__(7)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: graphwar/nn/sgconv.py</div><div id='n_file'> N File Name: graphwar/nn/sgconv.py</div><div id='m_start'> M Start Line: 70</div><div id='m_end'> M End Line: 85</div><div id='n_start'> N Start Line: 90</div><div id='n_end'> N End Line: 108</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            self.classifier = nn.Linear(self.reid_dim, self.nID)
            self.IDLoss = nn.CrossEntropyLoss(ignore_index=-1)
            self.emb_scale = np.math.sqrt(2) * np.math.log(self.nID - 1)
            self.s_det<a id="change"> = </a><a id="change">nn.Parameter(</a>-1.85 * torch.ones(1)<a id="change">, requires_grad=False)</a>
            self.s_id<a id="change"> = </a><a id="change">nn.Parameter(</a>-1.05 * torch.ones(1)<a id="change">, requires_grad=False)</a>

    def forward(self, preds, targets, imgs=None):
        outputs, origin_preds, x_shifts, y_shifts, expanded_strides = [], [], [], [], []
</code></pre><h3>After Change</h3><pre><code class='java'>
            self.classifiers = nn.ModuleList()
            self.emb_scales = []
            for idx, (label, id_num) in enumerate(zip(self.label_name, id_nums)):
                print(<a id="change">"{}, tracking label name: &quot{}&quot, tracking_id number: {}, feat dim: {}".format(</a>idx, label, id_num,
                                                                                                   self.reid_dim<a id="change">)</a>)
                self.emb_scales.append(np.math.sqrt(2) * np.math.log(id_num - 1))
                self.classifiers.append(nn.Linear(self.reid_dim, id_num))
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/zhangming8/yolox-pytorch/commit/e162fc0465b1f5d8b3211cdc81fd8eabb6dd55c7#diff-180d1e08e835c9b52081c2fe023d6b978026584c661357da6f5f7bcab9805901L15' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 94389130</div><div id='project'> Project Name: zhangming8/yolox-pytorch</div><div id='commit'> Commit Name: e162fc0465b1f5d8b3211cdc81fd8eabb6dd55c7</div><div id='time'> Time: 2021-07-26</div><div id='author'> Author: zhangming8@github.com</div><div id='file'> File Name: models/losses/yolox_loss.py</div><div id='m_class'> M Class Name: YOLOXLoss</div><div id='n_method'> N Class Name: YOLOXLoss</div><div id='m_method'> M Method Name: __init__(6)</div><div id='n_method'> N Method Name: __init__(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/losses/yolox_loss.py</div><div id='n_file'> N File Name: models/losses/yolox_loss.py</div><div id='m_start'> M Start Line: 19</div><div id='m_end'> M End Line: 37</div><div id='n_start'> N Start Line: 19</div><div id='n_end'> N End Line: 48</div><BR>