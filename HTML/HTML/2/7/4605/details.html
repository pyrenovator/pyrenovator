<html><h3>Pattern ID :4605
</h3><img src='16589993.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        loss_sum = sum(log.get(&quotloss&quot, 0) for log in logging_outputs)
        ntokens = sum(log.get(&quotntokens&quot, 0) for log in logging_outputs)
        nsentences = sum(log.get(&quotnsentences&quot, 0) for log in logging_outputs)
        sample_size = <a id="change">sum(</a><a id="change">log.get(&quotsample_size&quot, 0) for log in logging_outputs)</a>

        metrics.log_scalar(&quotloss&quot, loss_sum / sample_size / math.log(2), sample_size, round=3)
        if sample_size != ntokens:
            metrics.log_scalar(&quotnll_loss&quot, loss_sum / ntokens / math.log(2), ntokens, round=3)</code></pre><h3>After Change</h3><pre><code class='java'>
        Aggregate logging outputs from data parallel training.
        loss_sum = utils.item(sum(log.get(&quotloss&quot, 0) for log in logging_outputs))
        ntokens = utils.item(sum(log.get(&quotntokens&quot, 0) for log in logging_outputs))
        nsentences = <a id="change">utils.item(</a>sum(log.get(&quotnsentences&quot, 0) for log in logging_outputs)<a id="change">)</a>
        sample_size = <a id="change">utils.item(</a><a id="change">sum(</a><a id="change">log.get(&quotsample_size&quot, 0) for log in logging_outputs))</a>

        metrics.log_scalar(&quotloss&quot, loss_sum / sample_size / math.log(2), sample_size, round=3)
        if sample_size != ntokens:
            metrics.log_scalar(&quotnll_loss&quot, loss_sum / ntokens / math.log(2), ntokens, round=3)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 19</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/kssteven418/i-bert/commit/4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f#diff-509a893450d413604e20fb3a4c81978db373e098b375d632d608a8924e70a854L77' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 16589993</div><div id='project'> Project Name: kssteven418/i-bert</div><div id='commit'> Commit Name: 4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f</div><div id='time'> Time: 2020-02-12</div><div id='author'> Author: myleott@fb.com</div><div id='file'> File Name: fairseq/criterions/sentence_prediction.py</div><div id='m_class'> M Class Name: SentencePredictionCriterion</div><div id='n_method'> N Class Name: SentencePredictionCriterion</div><div id='m_method'> M Method Name: reduce_metrics(1)</div><div id='n_method'> N Method Name: reduce_metrics(1)</div><div id='m_parent_class'> M Parent Class: FairseqCriterion</div><div id='n_parent_class'> N Parent Class: FairseqCriterion</div><div id='m_file'> M File Name: fairseq/criterions/sentence_prediction.py</div><div id='n_file'> N File Name: fairseq/criterions/sentence_prediction.py</div><div id='m_start'> M Start Line: 77</div><div id='m_end'> M End Line: 80</div><div id='n_start'> N Start Line: 77</div><div id='n_end'> N End Line: 80</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        Aggregate logging outputs from data parallel training.
        loss_sum = sum(log.get(&quotloss&quot, 0) for log in logging_outputs)
        ntokens = sum(log.get(&quotntokens&quot, 0) for log in logging_outputs)
        sample_size = <a id="change">sum(</a><a id="change">log.get(&quotsample_size&quot, 0) for log in logging_outputs)</a>

        metrics.log_scalar(&quotloss&quot, loss_sum / sample_size / math.log(2), sample_size, round=3)
        if sample_size != ntokens:
            metrics.log_scalar(&quotnll_loss&quot, loss_sum / ntokens / math.log(2), ntokens, round=3)</code></pre><h3>After Change</h3><pre><code class='java'>
    def reduce_metrics(logging_outputs) -&gt; None:
        Aggregate logging outputs from data parallel training.
        loss_sum = utils.item(sum(log.get(&quotloss&quot, 0) for log in logging_outputs))
        ntokens = <a id="change">utils.item(</a>sum(log.get(&quotntokens&quot, 0) for log in logging_outputs)<a id="change">)</a>
        sample_size = <a id="change">utils.item(</a><a id="change">sum(</a><a id="change">log.get(&quotsample_size&quot, 0) for log in logging_outputs))</a>

        metrics.log_scalar(&quotloss&quot, loss_sum / sample_size / math.log(2), sample_size, round=3)
        if sample_size != ntokens:
            metrics.log_scalar(&quotnll_loss&quot, loss_sum / ntokens / math.log(2), ntokens, round=3)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/mohammadkhalifa/fairseq-tagging/commit/4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f#diff-ecd821f7f237060d793d6c7b1c44bd932aae0a9b2ea46a54ca39b3e9dbb7dc64L77' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 16589977</div><div id='project'> Project Name: mohammadkhalifa/fairseq-tagging</div><div id='commit'> Commit Name: 4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f</div><div id='time'> Time: 2020-02-12</div><div id='author'> Author: myleott@fb.com</div><div id='file'> File Name: fairseq/criterions/adaptive_loss.py</div><div id='m_class'> M Class Name: AdaptiveLoss</div><div id='n_method'> N Class Name: AdaptiveLoss</div><div id='m_method'> M Method Name: reduce_metrics(1)</div><div id='n_method'> N Method Name: reduce_metrics(1)</div><div id='m_parent_class'> M Parent Class: FairseqCriterion</div><div id='n_parent_class'> N Parent Class: FairseqCriterion</div><div id='m_file'> M File Name: fairseq/criterions/adaptive_loss.py</div><div id='n_file'> N File Name: fairseq/criterions/adaptive_loss.py</div><div id='m_start'> M Start Line: 79</div><div id='m_end'> M End Line: 81</div><div id='n_start'> N Start Line: 79</div><div id='n_end'> N End Line: 81</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        loss_sum = sum(log.get(&quotloss&quot, 0) for log in logging_outputs)
        ntokens = sum(log.get(&quotntokens&quot, 0) for log in logging_outputs)
        nsentences = sum(log.get(&quotnsentences&quot, 0) for log in logging_outputs)
        sample_size = <a id="change">sum(</a><a id="change">log.get(&quotsample_size&quot, 0) for log in logging_outputs)</a>

        metrics.log_scalar(&quotloss&quot, loss_sum / sample_size / math.log(2), sample_size, round=3)
        if sample_size != ntokens:
            metrics.log_scalar(&quotnll_loss&quot, loss_sum / ntokens / math.log(2), ntokens, round=3)</code></pre><h3>After Change</h3><pre><code class='java'>
    @staticmethod
    def reduce_metrics(logging_outputs) -&gt; None:
        Aggregate logging outputs from data parallel training.
        loss_sum = <a id="change">utils.item(</a>sum(log.get(&quotloss&quot, 0) for log in logging_outputs)<a id="change">)</a>
        ntokens = utils.item(sum(log.get(&quotntokens&quot, 0) for log in logging_outputs))
        nsentences = utils.item(sum(log.get(&quotnsentences&quot, 0) for log in logging_outputs))
        sample_size = <a id="change">utils.item(</a><a id="change">sum(</a><a id="change">log.get(&quotsample_size&quot, 0) for log in logging_outputs))</a>

        metrics.log_scalar(&quotloss&quot, loss_sum / sample_size / math.log(2), sample_size, round=3)
        if sample_size != ntokens:
            metrics.log_scalar(&quotnll_loss&quot, loss_sum / ntokens / math.log(2), ntokens, round=3)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/kssteven418/i-bert/commit/4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f#diff-509a893450d413604e20fb3a4c81978db373e098b375d632d608a8924e70a854L75' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 16589992</div><div id='project'> Project Name: kssteven418/i-bert</div><div id='commit'> Commit Name: 4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f</div><div id='time'> Time: 2020-02-12</div><div id='author'> Author: myleott@fb.com</div><div id='file'> File Name: fairseq/criterions/sentence_prediction.py</div><div id='m_class'> M Class Name: SentencePredictionCriterion</div><div id='n_method'> N Class Name: SentencePredictionCriterion</div><div id='m_method'> M Method Name: reduce_metrics(1)</div><div id='n_method'> N Method Name: reduce_metrics(1)</div><div id='m_parent_class'> M Parent Class: FairseqCriterion</div><div id='n_parent_class'> N Parent Class: FairseqCriterion</div><div id='m_file'> M File Name: fairseq/criterions/sentence_prediction.py</div><div id='n_file'> N File Name: fairseq/criterions/sentence_prediction.py</div><div id='m_start'> M Start Line: 77</div><div id='m_end'> M End Line: 80</div><div id='n_start'> N Start Line: 77</div><div id='n_end'> N End Line: 80</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def reduce_metrics(logging_outputs) -&gt; None:
        Aggregate logging outputs from data parallel training.
        loss_sum = sum(log.get(&quotloss&quot, 0) for log in logging_outputs)
        ntokens = <a id="change">sum(</a><a id="change">log.get(&quotntokens&quot, 0) for log in logging_outputs)</a>
        sample_size = sum(log.get(&quotsample_size&quot, 0) for log in logging_outputs)

        metrics.log_scalar(&quotloss&quot, loss_sum / sample_size / math.log(2), sample_size, round=3)
        if sample_size != ntokens:</code></pre><h3>After Change</h3><pre><code class='java'>
    @staticmethod
    def reduce_metrics(logging_outputs) -&gt; None:
        Aggregate logging outputs from data parallel training.
        loss_sum = <a id="change">utils.item(</a>sum(log.get(&quotloss&quot, 0) for log in logging_outputs)<a id="change">)</a>
        ntokens = <a id="change">utils.item(</a><a id="change">sum(</a><a id="change">log.get(&quotntokens&quot, 0) for log in logging_outputs))</a>
        sample_size = utils.item(sum(log.get(&quotsample_size&quot, 0) for log in logging_outputs))

        metrics.log_scalar(&quotloss&quot, loss_sum / sample_size / math.log(2), sample_size, round=3)
        if sample_size != ntokens:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/kssteven418/i-bert/commit/4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f#diff-6fb432bfeb5c51147d887f2ef550743aa1842165c119a7c942260afa87d0a431L52' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 16589995</div><div id='project'> Project Name: kssteven418/i-bert</div><div id='commit'> Commit Name: 4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f</div><div id='time'> Time: 2020-02-12</div><div id='author'> Author: myleott@fb.com</div><div id='file'> File Name: fairseq/criterions/cross_entropy.py</div><div id='m_class'> M Class Name: CrossEntropyCriterion</div><div id='n_method'> N Class Name: CrossEntropyCriterion</div><div id='m_method'> M Method Name: reduce_metrics(1)</div><div id='n_method'> N Method Name: reduce_metrics(1)</div><div id='m_parent_class'> M Parent Class: FairseqCriterion</div><div id='n_parent_class'> N Parent Class: FairseqCriterion</div><div id='m_file'> M File Name: fairseq/criterions/cross_entropy.py</div><div id='n_file'> N File Name: fairseq/criterions/cross_entropy.py</div><div id='m_start'> M Start Line: 54</div><div id='m_end'> M End Line: 56</div><div id='n_start'> N Start Line: 54</div><div id='n_end'> N End Line: 56</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def aggregate_logging_outputs(logging_outputs):
        Aggregate logging outputs from data parallel training.
        loss_sum = sum(log.get(&quotloss&quot, 0) for log in logging_outputs)
        ntokens = <a id="change">sum(</a><a id="change">log.get(&quotntokens&quot, 0) for log in logging_outputs)</a>
        nsentences = sum(log.get(&quotnsentences&quot, 0) for log in logging_outputs)
        sample_size = sum(log.get(&quotsample_size&quot, 0) for log in logging_outputs)
        agg_output = {
            &quotloss&quot: loss_sum / sample_size / math.log(2),</code></pre><h3>After Change</h3><pre><code class='java'>
    def aggregate_logging_outputs(logging_outputs):
        Aggregate logging outputs from data parallel training.
        loss_sum = utils.item(sum(log.get(&quotloss&quot, 0) for log in logging_outputs))
        ntokens = <a id="change">utils.item(</a><a id="change">sum(</a><a id="change">log.get(&quotntokens&quot, 0) for log in logging_outputs))</a>
        nsentences = utils.item(sum(log.get(&quotnsentences&quot, 0) for log in logging_outputs))
        sample_size = <a id="change">utils.item(</a>sum(log.get(&quotsample_size&quot, 0) for log in logging_outputs)<a id="change">)</a>
        agg_output = {
            &quotloss&quot: loss_sum / sample_size / math.log(2),
            &quotntokens&quot: ntokens,
            &quotnsentences&quot: nsentences,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/kssteven418/i-bert/commit/4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f#diff-6ef0eef19d0a9e83ea8f5a272d1881d7b1621c38eed5537b928e5e3db58ae0eaL61' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 16589989</div><div id='project'> Project Name: kssteven418/i-bert</div><div id='commit'> Commit Name: 4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f</div><div id='time'> Time: 2020-02-12</div><div id='author'> Author: myleott@fb.com</div><div id='file'> File Name: fairseq/criterions/binary_cross_entropy.py</div><div id='m_class'> M Class Name: BinaryCrossEntropyCriterion</div><div id='n_method'> N Class Name: BinaryCrossEntropyCriterion</div><div id='m_method'> M Method Name: aggregate_logging_outputs(1)</div><div id='n_method'> N Method Name: aggregate_logging_outputs(1)</div><div id='m_parent_class'> M Parent Class: FairseqCriterion</div><div id='n_parent_class'> N Parent Class: FairseqCriterion</div><div id='m_file'> M File Name: fairseq/criterions/binary_cross_entropy.py</div><div id='n_file'> N File Name: fairseq/criterions/binary_cross_entropy.py</div><div id='m_start'> M Start Line: 63</div><div id='m_end'> M End Line: 66</div><div id='n_start'> N Start Line: 63</div><div id='n_end'> N End Line: 66</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        Aggregate logging outputs from data parallel training.
        sample_size = sum(log.get("sample_size", 0) for log in logging_outputs)
        loss = sum(log.get("loss", 0) for log in logging_outputs)
        nll_loss = <a id="change">sum(</a><a id="change">log.get("nll_loss", 0) for log in logging_outputs)</a>

        metrics.log_scalar(&quotloss&quot, loss / sample_size / math.log(2), sample_size, round=3)
        metrics.log_scalar(&quotnll_loss&quot, nll_loss / sample_size / math.log(2), sample_size, round=3)
        metrics.log_derived(&quotppl&quot, lambda meters: round(2**meters[&quotnll_loss&quot].avg, 3))</code></pre><h3>After Change</h3><pre><code class='java'>
    @staticmethod
    def reduce_metrics(logging_outputs) -&gt; None:
        Aggregate logging outputs from data parallel training.
        sample_size = <a id="change">utils.item(</a>sum(log.get("sample_size", 0) for log in logging_outputs)<a id="change">)</a>
        loss = utils.item(sum(log.get("loss", 0) for log in logging_outputs))
        nll_loss = <a id="change">utils.item(</a><a id="change">sum(</a><a id="change">log.get("nll_loss", 0) for log in logging_outputs))</a>

        metrics.log_scalar(&quotloss&quot, loss / sample_size / math.log(2), sample_size, round=3)
        metrics.log_scalar(&quotnll_loss&quot, nll_loss / sample_size / math.log(2), sample_size, round=3)
        metrics.log_derived(&quotppl&quot, lambda meters: utils.get_perplexity(meters[&quotloss&quot].avg))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/kssteven418/i-bert/commit/4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f#diff-3e7c55c128f8bc110fc5114ea5b640e0e5135c9b9115174ea93281cfc550c9fdL142' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 16589973</div><div id='project'> Project Name: kssteven418/i-bert</div><div id='commit'> Commit Name: 4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f</div><div id='time'> Time: 2020-02-12</div><div id='author'> Author: myleott@fb.com</div><div id='file'> File Name: fairseq/criterions/nat_loss.py</div><div id='m_class'> M Class Name: LabelSmoothedDualImitationCriterion</div><div id='n_method'> N Class Name: LabelSmoothedDualImitationCriterion</div><div id='m_method'> M Method Name: reduce_metrics(1)</div><div id='n_method'> N Method Name: reduce_metrics(1)</div><div id='m_parent_class'> M Parent Class: FairseqCriterion</div><div id='n_parent_class'> N Parent Class: FairseqCriterion</div><div id='m_file'> M File Name: fairseq/criterions/nat_loss.py</div><div id='n_file'> N File Name: fairseq/criterions/nat_loss.py</div><div id='m_start'> M Start Line: 144</div><div id='m_end'> M End Line: 146</div><div id='n_start'> N Start Line: 144</div><div id='n_end'> N End Line: 146</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    @staticmethod
    def reduce_metrics(logging_outputs) -&gt; None:
        Aggregate logging outputs from data parallel training.
        loss_sum = <a id="change">sum(</a><a id="change">log.get(&quotloss&quot, 0) for log in logging_outputs)</a>
        nll_loss_sum = sum(log.get(&quotnll_loss&quot, 0) for log in logging_outputs)
        ntokens = sum(log.get(&quotntokens&quot, 0) for log in logging_outputs)
        sample_size = sum(log.get(&quotsample_size&quot, 0) for log in logging_outputs)
</code></pre><h3>After Change</h3><pre><code class='java'>
    @staticmethod
    def reduce_metrics(logging_outputs) -&gt; None:
        Aggregate logging outputs from data parallel training.
        loss_sum = <a id="change">utils.item(</a><a id="change">sum(</a><a id="change">log.get(&quotloss&quot, 0) for log in logging_outputs))</a>
        nll_loss_sum = utils.item(sum(log.get(&quotnll_loss&quot, 0) for log in logging_outputs))
        ntokens = utils.item(sum(log.get(&quotntokens&quot, 0) for log in logging_outputs))
        sample_size = <a id="change">utils.item(</a>sum(log.get(&quotsample_size&quot, 0) for log in logging_outputs)<a id="change">)</a>

        metrics.log_scalar(&quotloss&quot, loss_sum / sample_size / math.log(2), sample_size, round=3)
        metrics.log_scalar(&quotnll_loss&quot, nll_loss_sum / ntokens / math.log(2), ntokens, round=3)
        metrics.log_derived(&quotppl&quot, lambda meters: utils.get_perplexity(meters[&quotnll_loss&quot].avg))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/mohammadkhalifa/fairseq-tagging/commit/4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f#diff-445d8a12d2bf868341e7cc8b158ccabf84cf1ec16dfb503eb1e7d6982d5be669L78' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 16589957</div><div id='project'> Project Name: mohammadkhalifa/fairseq-tagging</div><div id='commit'> Commit Name: 4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f</div><div id='time'> Time: 2020-02-12</div><div id='author'> Author: myleott@fb.com</div><div id='file'> File Name: fairseq/criterions/label_smoothed_cross_entropy.py</div><div id='m_class'> M Class Name: LabelSmoothedCrossEntropyCriterion</div><div id='n_method'> N Class Name: LabelSmoothedCrossEntropyCriterion</div><div id='m_method'> M Method Name: reduce_metrics(1)</div><div id='n_method'> N Method Name: reduce_metrics(1)</div><div id='m_parent_class'> M Parent Class: FairseqCriterion</div><div id='n_parent_class'> N Parent Class: FairseqCriterion</div><div id='m_file'> M File Name: fairseq/criterions/label_smoothed_cross_entropy.py</div><div id='n_file'> N File Name: fairseq/criterions/label_smoothed_cross_entropy.py</div><div id='m_start'> M Start Line: 80</div><div id='m_end'> M End Line: 83</div><div id='n_start'> N Start Line: 80</div><div id='n_end'> N End Line: 83</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if not any(&quotnsentences&quot in log for log in logging_outputs):
            warnings.warn(&quotnsentences not found in Criterion logging outputs, cannot log bsz&quot)
        else:
            nsentences = <a id="change">sum(</a><a id="change">log.get(&quotnsentences&quot, 0) for log in logging_outputs)</a>
            metrics.log_scalar(&quotbsz&quot, nsentences, priority=190, round=1)

        criterion.__class__.reduce_metrics(logging_outputs)
</code></pre><h3>After Change</h3><pre><code class='java'>
        if not any(&quotntokens&quot in log for log in logging_outputs):
            warnings.warn(&quotntokens not found in Criterion logging outputs, cannot log wpb or wps&quot)
        else:
            ntokens = <a id="change">utils.item(</a>sum(log.get(&quotntokens&quot, 0) for log in logging_outputs)<a id="change">)</a>
            metrics.log_scalar(&quotwpb&quot, ntokens, priority=180, round=1)
            metrics.log_speed(&quotwps&quot, ntokens, ignore_first=10, priority=90, round=1)

        if not any(&quotnsentences&quot in log for log in logging_outputs):
            warnings.warn(&quotnsentences not found in Criterion logging outputs, cannot log bsz&quot)
        else:
            nsentences = <a id="change">utils.item(</a><a id="change">sum(</a><a id="change">log.get(&quotnsentences&quot, 0) for log in logging_outputs))</a>
            metrics.log_scalar(&quotbsz&quot, nsentences, priority=190, round=1)

        criterion.__class__.reduce_metrics(logging_outputs)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/mohammadkhalifa/fairseq-tagging/commit/4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f#diff-ee69605d5bb5e920b39cdd771f41e65ea842a5470b90f6cc90f93f463c9f31a2L332' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 16589959</div><div id='project'> Project Name: mohammadkhalifa/fairseq-tagging</div><div id='commit'> Commit Name: 4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f</div><div id='time'> Time: 2020-02-12</div><div id='author'> Author: myleott@fb.com</div><div id='file'> File Name: fairseq/tasks/fairseq_task.py</div><div id='m_class'> M Class Name: FairseqTask</div><div id='n_method'> N Class Name: FairseqTask</div><div id='m_method'> M Method Name: reduce_metrics(3)</div><div id='n_method'> N Method Name: reduce_metrics(3)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: fairseq/tasks/fairseq_task.py</div><div id='n_file'> N File Name: fairseq/tasks/fairseq_task.py</div><div id='m_start'> M Start Line: 350</div><div id='m_end'> M End Line: 357</div><div id='n_start'> N Start Line: 350</div><div id='n_end'> N End Line: 357</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        Aggregate logging outputs from data parallel training.
        loss_sum = sum(log.get(&quotloss&quot, 0) for log in logging_outputs)
        nll_loss_sum = sum(log.get(&quotnll_loss&quot, 0) for log in logging_outputs)
        alignment_loss_sum = <a id="change">sum(</a><a id="change">log.get(&quotalignment_loss&quot, 0) for log in logging_outputs)</a>
        ntokens = sum(log.get(&quotntokens&quot, 0) for log in logging_outputs)
        sample_size = sum(log.get(&quotsample_size&quot, 0) for log in logging_outputs)

        metrics.log_scalar(&quotloss&quot, loss_sum / sample_size / math.log(2), sample_size, round=3)</code></pre><h3>After Change</h3><pre><code class='java'>
        Aggregate logging outputs from data parallel training.
        loss_sum = utils.item(sum(log.get(&quotloss&quot, 0) for log in logging_outputs))
        nll_loss_sum = utils.item(sum(log.get(&quotnll_loss&quot, 0) for log in logging_outputs))
        alignment_loss_sum = <a id="change">utils.item(</a><a id="change">sum(</a><a id="change">log.get(&quotalignment_loss&quot, 0) for log in logging_outputs))</a>
        ntokens = utils.item(sum(log.get(&quotntokens&quot, 0) for log in logging_outputs))
        sample_size = <a id="change">utils.item(</a>sum(log.get(&quotsample_size&quot, 0) for log in logging_outputs)<a id="change">)</a>

        metrics.log_scalar(&quotloss&quot, loss_sum / sample_size / math.log(2), sample_size, round=3)
        metrics.log_scalar(&quotnll_loss&quot, nll_loss_sum / ntokens / math.log(2), ntokens, round=3)
        metrics.log_scalar(&quotalignment_loss&quot, alignment_loss_sum / sample_size / math.log(2), sample_size, round=3)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/kssteven418/i-bert/commit/4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f#diff-1da3f8e87dba178c453fe76a6e7724387043557c6cd33ff8b6ccb6e4babb861aL78' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 16589984</div><div id='project'> Project Name: kssteven418/i-bert</div><div id='commit'> Commit Name: 4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f</div><div id='time'> Time: 2020-02-12</div><div id='author'> Author: myleott@fb.com</div><div id='file'> File Name: fairseq/criterions/label_smoothed_cross_entropy_with_alignment.py</div><div id='m_class'> M Class Name: LabelSmoothedCrossEntropyCriterionWithAlignment</div><div id='n_method'> N Class Name: LabelSmoothedCrossEntropyCriterionWithAlignment</div><div id='m_method'> M Method Name: reduce_metrics(1)</div><div id='n_method'> N Method Name: reduce_metrics(1)</div><div id='m_parent_class'> M Parent Class: LabelSmoothedCrossEntropyCriterion</div><div id='n_parent_class'> N Parent Class: LabelSmoothedCrossEntropyCriterion</div><div id='m_file'> M File Name: fairseq/criterions/label_smoothed_cross_entropy_with_alignment.py</div><div id='n_file'> N File Name: fairseq/criterions/label_smoothed_cross_entropy_with_alignment.py</div><div id='m_start'> M Start Line: 80</div><div id='m_end'> M End Line: 84</div><div id='n_start'> N Start Line: 80</div><div id='n_end'> N End Line: 84</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def reduce_metrics(logging_outputs) -&gt; None:
        Aggregate logging outputs from data parallel training.
        loss_sum = sum(log.get(&quotloss&quot, 0) for log in logging_outputs)
        sample_size = <a id="change">sum(</a><a id="change">log.get(&quotsample_size&quot, 0) for log in logging_outputs)</a>

        metrics.log_scalar(&quotloss&quot, loss_sum / sample_size / math.log(2), sample_size, round=3)
        metrics.log_derived(&quotppl&quot, lambda meters: round(2**meters[&quotloss&quot].avg, 3))
</code></pre><h3>After Change</h3><pre><code class='java'>
    @staticmethod
    def reduce_metrics(logging_outputs) -&gt; None:
        Aggregate logging outputs from data parallel training.
        loss_sum = <a id="change">utils.item(</a>sum(log.get(&quotloss&quot, 0) for log in logging_outputs)<a id="change">)</a>
        sample_size = <a id="change">utils.item(</a><a id="change">sum(</a><a id="change">log.get(&quotsample_size&quot, 0) for log in logging_outputs))</a>

        metrics.log_scalar(&quotloss&quot, loss_sum / sample_size / math.log(2), sample_size, round=3)
        metrics.log_derived(&quotppl&quot, lambda meters: utils.get_perplexity(meters[&quotloss&quot].avg))
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/kssteven418/i-bert/commit/4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f#diff-7d953f4b0fa135da1ac3c7dad3a9de2313a9c57ea301723b9c0d8b4c4e2a6417L63' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 16589952</div><div id='project'> Project Name: kssteven418/i-bert</div><div id='commit'> Commit Name: 4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f</div><div id='time'> Time: 2020-02-12</div><div id='author'> Author: myleott@fb.com</div><div id='file'> File Name: fairseq/criterions/masked_lm.py</div><div id='m_class'> M Class Name: MaskedLmLoss</div><div id='n_method'> N Class Name: MaskedLmLoss</div><div id='m_method'> M Method Name: reduce_metrics(1)</div><div id='n_method'> N Method Name: reduce_metrics(1)</div><div id='m_parent_class'> M Parent Class: FairseqCriterion</div><div id='n_parent_class'> N Parent Class: FairseqCriterion</div><div id='m_file'> M File Name: fairseq/criterions/masked_lm.py</div><div id='n_file'> N File Name: fairseq/criterions/masked_lm.py</div><div id='m_start'> M Start Line: 65</div><div id='m_end'> M End Line: 66</div><div id='n_start'> N Start Line: 65</div><div id='n_end'> N End Line: 66</div><BR>