<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                            )

                        &#47&#47 run request tensors through all requested modules, update caches
                        <a id="change">for </a>backend, backend_cache_handles, prompt in zip(requested_backends, cache_handles, prompts)<a id="change">:
                            </a>if not is_dummy(prompt):
                                hidden_states[:, : prompt.shape[1]] += prompt
                            if hidden_states.numel() == 0:
                                <a id="change">continue</a>  &#47&#47 user passed a tensor with 0 tokens. This is a special case that occurs, e.g.
                                &#47&#47 when user wants to pre-allocate cache or check that server *can* allocate that cache

                            metadata<a id="change"> = </a>InferenceMetadata(prefix_length, <a id="change">tuple(</a>backend_cache_handles<a id="change">)</a>)
                            assert isinstance(
                                hidden_states, torch.Tensor
                            ), f"hidden states must be tensor, got {type(hidden_states)}"</code></pre><h3>After Change</h3><pre><code class='java'>
                            &#47&#47 when user wants to pre-allocate cache or check that server *can* allocate that cache
                        else:
                            assert hidden_states.ndim == 3, f"hidden states must be a single 3d tensor"
                            (hidden_states,) = await self.module_backends[<a id="change">requested_uids[0]</a>].inference_pool.submit_task(
                                hidden_states, hypo_ids, inference_infos, *prompts, priority=priority
                            )
</code></pre>