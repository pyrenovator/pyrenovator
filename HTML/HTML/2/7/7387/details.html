<html><h3>Pattern ID :7387
</h3><img src='24579383.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            &#47&#47 寻找ent的token_span
            ent_token_len = len(ent2token)
            token_start_index = 0 if ent_token_len &gt; 0 else - 1
            <a id="change">while </a>token_start_index != -1<a id="change">:
                </a>try:
                    token_start_index<a id="change"> = </a>text2tokens.index(ent2token[0], token_start_index)
                    <a id="change">if text2tokens[token_start_index:token_start_index + ent_token_len] == ent2token</a>:
                        break
                    else:
                        token_start_index = text2tokens.index(ent2token[0], token_start_index + 1)
                except ValueError:
                    print(f&quot[{ent}] 无法对应到 [{text}] 的token_span，已丢弃&quot)
                    token_start_index<a id="change"> = </a>-1
            
            if token_start_index == -1:
                continue</code></pre><h3>After Change</h3><pre><code class='java'>
            ent2token = self.tokenizer.tokenize(ent, add_special_tokens=False)

            &#47&#47 寻找ent的token_span
            token_start_indexs = [i for i,v in <a id="change">enumerate(</a>text2tokens<a id="change">)</a> if <a id="change">v==ent2token[0]</a>]
            token_end_indexs = [i for i,v in enumerate(text2tokens) if v==ent2token[-1]]
            
            token_start_index = list(filter(lambda x:token2char_span_mapping[x][0] == ent_span[0], token_start_indexs))</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/gaohongkui/globalpointer_pytorch/commit/5146e4d180d865b94c8e0d173320f021df21498d#diff-a9ca103e95ab74ef9bae0986b5cc4c9f3230e50b062e7855c929b211a1d038a5L42' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 24579383</div><div id='project'> Project Name: gaohongkui/globalpointer_pytorch</div><div id='commit'> Commit Name: 5146e4d180d865b94c8e0d173320f021df21498d</div><div id='time'> Time: 2021-07-31</div><div id='author'> Author: 1427224680@qq.com</div><div id='file'> File Name: common/utils.py</div><div id='m_class'> M Class Name: Preprocessor</div><div id='n_method'> N Class Name: Preprocessor</div><div id='m_method'> M Method Name: get_ent2token_spans(3)</div><div id='n_method'> N Method Name: get_ent2token_spans(3)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: common/utils.py</div><div id='n_file'> N File Name: common/utils.py</div><div id='m_start'> M Start Line: 42</div><div id='m_end'> M End Line: 68</div><div id='n_start'> N Start Line: 42</div><div id='n_end'> N End Line: 58</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    num_sentences = 0
    num_errors = 0

    <a id="change">while </a>not query_end<a id="change">:
        </a>results = query_execution.fetchone()
        if results is not None:
            sentence_id<a id="change">, sentence_text = </a>results
            try:
                preprocessed_sentence = model.preprocess(sentence_text)
                embedding = model.embed(preprocessed_sentence)
            except IndexError:
                embedding = np.zeros((model.dim,))
                num_errors += 1
            all_ids.append(sentence_id)
            all_embeddings.append(embedding)
            num_sentences<a id="change"> += </a>1
            <a id="change">if num_sentences % 1000 == 0</a>:
                print(f&quotEmbedded {num_sentences} with {num_errors} errors&quot)
        else:
            query_end = True</code></pre><h3>After Change</h3><pre><code class='java'>
    query_execution = pd.read_sql(sql=query, con=connection, chunksize=1)
    num_errors = 0

    for i_sentence, (sentence_id, sentence_text) in <a id="change">enumerate(</a>query_execution<a id="change">)</a>:
        try:
            preprocessed_sentence = model.preprocess(sentence_text)
            embedding = model.embed(preprocessed_sentence)
        except IndexError:
            embedding = np.zeros((model.dim,))
            num_errors += 1
        all_ids.append(sentence_id)
        all_embeddings.append(embedding)
        <a id="change">if </a>i_sentence % 1000 == 0:
            print(f&quotEmbedded {i_sentence} with {num_errors} errors&quot)

    all_embeddings = np.array(all_embeddings)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/bluebrain/search/commit/fde937247e8021be3bb06189256600622cd6b773#diff-6a370a606e89902d9cea7c7edf876b91c643d1c115291d5912b04e3bd88cf446L265' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 24579350</div><div id='project'> Project Name: bluebrain/search</div><div id='commit'> Commit Name: fde937247e8021be3bb06189256600622cd6b773</div><div id='time'> Time: 2020-07-13</div><div id='author'> Author: Stannislav@users.noreply.github.com</div><div id='file'> File Name: src/bbsearch/embedding_models.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: compute_database_embeddings(2)</div><div id='n_method'> N Method Name: compute_database_embeddings(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/bbsearch/embedding_models.py</div><div id='n_file'> N File Name: src/bbsearch/embedding_models.py</div><div id='m_start'> M Start Line: 281</div><div id='m_end'> M End Line: 309</div><div id='n_start'> N Start Line: 298</div><div id='n_end'> N End Line: 313</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    with bz2.open(wikidata_file, mode="rb") as file:
        line = file.readline()
        cnt = 0
        <a id="change">while </a>line and (not limit or cnt &lt; limit)<a id="change">:
            if cnt % 1000000 == 0</a>:
                print(
                    datetime.datetime.now(), "processed", cnt, "lines of WikiData JSON dump"
                )
            clean_line = line.strip()
            if clean_line.endswith(b","):
                clean_line = clean_line[:-1]
            if len(clean_line) &gt; 1:
                obj = json.loads(clean_line)
                entry_type = obj["type"]

                if entry_type == "item":
                    &#47&#47 filtering records on their properties (currently disabled to get ALL data)
                    &#47&#47 keep = False
                    keep = True

                    claims = obj["claims"]
                    if parse_claims:
                        for prop, value_set in prop_filter.items():
                            claim_property = claims.get(prop, None)
                            if claim_property:
                                for cp in claim_property:
                                    cp_id = (
                                        cp["mainsnak"]
                                        .get("datavalue", {})
                                        .get("value", {})
                                        .get("id")
                                    )
                                    cp_rank = cp["rank"]
                                    if cp_rank != "deprecated" and cp_id in value_set:
                                        keep = True

                    if keep:
                        unique_id = obj["id"]

                        if to_print:
                            print("ID:", unique_id)
                            print("type:", entry_type)

                        &#47&#47 parsing all properties that refer to other entities
                        if parse_properties:
                            for prop, claim_property in claims.items():
                                cp_dicts = [
                                    cp["mainsnak"]["datavalue"].get("value")
                                    for cp in claim_property
                                    if cp["mainsnak"].get("datavalue")
                                ]
                                cp_values = [
                                    cp_dict.get("id")
                                    for cp_dict in cp_dicts
                                    if isinstance(cp_dict, dict)
                                    if cp_dict.get("id") is not None
                                ]
                                if cp_values:
                                    if to_print:
                                        print("prop:", prop, cp_values)

                        found_link = False
                        if parse_sitelinks:
                            site_value = obj["sitelinks"].get(site_filter, None)
                            if site_value:
                                site = site_value["title"]
                                if to_print:
                                    print(site_filter, ":", site)
                                title_to_id[site] = unique_id
                                found_link = True

                        if parse_labels:
                            labels = obj["labels"]
                            if labels:
                                lang_label = labels.get(lang, None)
                                if lang_label:
                                    if to_print:
                                        print(
                                            "label (" + lang + "):", lang_label["value"]
                                        )

                        if found_link and parse_descriptions:
                            descriptions = obj["descriptions"]
                            if descriptions:
                                lang_descr = descriptions.get(lang, None)
                                if lang_descr:
                                    if to_print:
                                        print(
                                            "description (" + lang + "):",
                                            lang_descr["value"],
                                        )
                                    id_to_descr[unique_id] = lang_descr["value"]

                        if parse_aliases:
                            aliases = obj["aliases"]
                            if aliases:
                                lang_aliases = aliases.get(lang, None)
                                if lang_aliases:
                                    for item in lang_aliases:
                                        if to_print:
                                            print(
                                                "alias (" + lang + "):", item["value"]
                                            )

                        if to_print:
                            print()
            line<a id="change"> = </a>file.readline()
            cnt<a id="change"> += </a>1
        print(datetime.datetime.now(), "processed", cnt, "lines of WikiData JSON dump")

    return title_to_id, id_to_descr</code></pre><h3>After Change</h3><pre><code class='java'>
    parse_claims = False

    with gzip.open(wikidata_file, mode=&quotrb&quot) as file:
        for cnt, line in <a id="change">enumerate(</a>file<a id="change">)</a>:
            <a id="change">if </a>limit and cnt &gt;= limit:
                break
            if cnt % 500000 == 0:
                logger.info("processed {} lines of WikiData dump".format(cnt))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/explosion/spaCy/commit/a6830d60e8c4acc6f378a3b4e6c48e851e729408#diff-4ff027b00c6616a42dedaf91f8b1258a197c3f1f471d3615b6309227d0918e9dL9' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 24579423</div><div id='project'> Project Name: explosion/spaCy</div><div id='commit'> Commit Name: a6830d60e8c4acc6f378a3b4e6c48e851e729408</div><div id='time'> Time: 2019-09-13</div><div id='author'> Author: euan.dowers@barcelonagse.eu</div><div id='file'> File Name: bin/wiki_entity_linking/wikidata_processor.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: read_wikidata_entities_json(5)</div><div id='n_method'> N Method Name: read_wikidata_entities_json(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: bin/wiki_entity_linking/wikidata_processor.py</div><div id='n_file'> N File Name: bin/wiki_entity_linking/wikidata_processor.py</div><div id='m_start'> M Start Line: 13</div><div id='m_end'> M End Line: 141</div><div id='n_start'> N Start Line: 12</div><div id='n_end'> N End Line: 137</div><BR>