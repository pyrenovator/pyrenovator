<html><h3>Pattern ID :29270
</h3><img src='85990320.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        k = self.w_ks(mel_encoding)
        v = self.w_vs(mel_encoding)

        src_mask = <a id="change">src_mask.float().unsqueeze(-1</a><a id="change">)</a> &#47&#47 [batch, seq_len, 1]
        mel_mask = mel_mask.float().unsqueeze(-1) &#47&#47 [batch, mel_len, 1]
        attn_mask = torch.bmm(src_mask, mel_mask.transpose(-2, -1)).bool() &#47&#47 [batch, seq_len, mel_len]
</code></pre><h3>After Change</h3><pre><code class='java'>

        src_len, mel_len = src_mask.shape[1], mel_mask.shape[1]
        src_mask_ = src_mask.unsqueeze(-1).expand(-1, -1, mel_len) &#47&#47 [batch, seq_len, mel_len]
        mel_mask_ = <a id="change">mel_mask.unsqueeze(1).expand(-1</a>, src_len, <a id="change">-1</a><a id="change">)</a> &#47&#47 [batch, seq_len, mel_len]

        output, attn = self.attention(q, k, v, src_mask=src_mask_, mel_mask=mel_mask_)
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/keonlee9420/parallel-tacotron2/commit/1f98efe5d09778b2b77e291aff63ef6e497c6bb9#diff-eed8dda7a8e19d25dc983adc6194cdb57bfef7afee02def70160e6694ca3984aL271' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 85990320</div><div id='project'> Project Name: keonlee9420/parallel-tacotron2</div><div id='commit'> Commit Name: 1f98efe5d09778b2b77e291aff63ef6e497c6bb9</div><div id='time'> Time: 2021-07-26</div><div id='author'> Author: 1531820402@qq.com</div><div id='file'> File Name: model/blocks.py</div><div id='m_class'> M Class Name: VariableLengthAttention</div><div id='n_method'> N Class Name: VariableLengthAttention</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: model/blocks.py</div><div id='n_file'> N File Name: model/blocks.py</div><div id='m_start'> M Start Line: 271</div><div id='m_end'> M End Line: 273</div><div id='n_start'> N Start Line: 271</div><div id='n_end'> N End Line: 273</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 combine masks along attended time - first encoder and then decoder
        mask = torch.cat(
            (
                <a id="change">encoder_mask.unsqueeze(1</a><a id="change">)</a>.expand(-1, decoder_length, -1),
                decoder_mask.unsqueeze(0).expand(encoder_lengths.size(0), -1, -1),
            ),
            dim=2,</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47   matter in the future than the past)
            &#47&#47   or alternatively using the same layer but allowing forward attention - i.e. only
            &#47&#47   masking out non-available data and self
            decoder_mask = <a id="change">create_mask(decoder_length, decoder_lengths).unsqueeze(1).expand(-1</a>, decoder_length, <a id="change">-1</a><a id="change">)</a>
        &#47&#47 do not attend to steps where data is padded
        encoder_mask = create_mask(encoder_lengths.max(), encoder_lengths).unsqueeze(1).expand(-1, decoder_length, -1)
        &#47&#47 combine masks along attended time - first encoder and then decoder
        mask = torch.cat(</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/jdb78/pytorch-forecasting/commit/ca4b90615f4bfdeb357d3ad26e492ef8573c7f1c#diff-76ca71ffaeab9ec5eca22b512d1ed2460e8173ce9feb6e881188098900f5f8d4L363' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 85990322</div><div id='project'> Project Name: jdb78/pytorch-forecasting</div><div id='commit'> Commit Name: ca4b90615f4bfdeb357d3ad26e492ef8573c7f1c</div><div id='time'> Time: 2022-04-15</div><div id='author'> Author: beitner.jan@bcg.com</div><div id='file'> File Name: pytorch_forecasting/models/temporal_fusion_transformer/__init__.py</div><div id='m_class'> M Class Name: TemporalFusionTransformer</div><div id='n_method'> N Class Name: TemporalFusionTransformer</div><div id='m_method'> M Method Name: get_attention_mask(3)</div><div id='n_method'> N Method Name: get_attention_mask(3)</div><div id='m_parent_class'> M Parent Class: BaseModelWithCovariates</div><div id='n_parent_class'> N Parent Class: BaseModelWithCovariates</div><div id='m_file'> M File Name: pytorch_forecasting/models/temporal_fusion_transformer/__init__.py</div><div id='n_file'> N File Name: pytorch_forecasting/models/temporal_fusion_transformer/__init__.py</div><div id='m_start'> M Start Line: 371</div><div id='m_end'> M End Line: 391</div><div id='n_start'> N Start Line: 370</div><div id='n_end'> N End Line: 395</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        return xs, masks

    def _integrate_with_utt_embed(self, hs, utt_embeddings):
        speaker_embeddings_projected = <a id="change">self.embedding_projection(utt_embeddings).unsqueeze(1</a><a id="change">)</a>
        hs = hs + speaker_embeddings_projected  &#47&#47 offset phone realization of a speaker
        hs = self.speaker_norm(hs)
        return hs
</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 project embedding into smaller space
        speaker_embeddings_projected = self.embedding_projection(utt_embeddings)
        &#47&#47 concat hidden states with spk embeds and then apply projection
        speaker_embeddings_expanded = <a id="change">F.normalize(speaker_embeddings_projected).unsqueeze(1).expand(-1</a>, hs.size(1), <a id="change">-1</a><a id="change">)</a>
        hs = self.hs_emb_projection(torch.cat([hs, speaker_embeddings_expanded], dim=-1))
        return hs
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/5cde43fc07589b8b65d60187a62d9fabea9fbafd#diff-d635fb003b559f77f551259cb56709a57f9a634f85664ca1d8a45aff44d754d7L127' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 85990316</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 5cde43fc07589b8b65d60187a62d9fabea9fbafd</div><div id='time'> Time: 2022-02-19</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: Layers/Conformer.py</div><div id='m_class'> M Class Name: Conformer</div><div id='n_method'> N Class Name: Conformer</div><div id='m_method'> M Method Name: _integrate_with_utt_embed(3)</div><div id='n_method'> N Method Name: _integrate_with_utt_embed(3)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: Layers/Conformer.py</div><div id='n_file'> N File Name: Layers/Conformer.py</div><div id='m_start'> M Start Line: 128</div><div id='m_end'> M End Line: 130</div><div id='n_start'> N Start Line: 136</div><div id='n_end'> N End Line: 139</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        ], "Cannot use NormalDistributionLoss for positive data"
        assert encoder.transformation not in ["logit"], "Cannot use bound transformation such as &quotlogit&quot"
        loc = encoder(dict(prediction=parameters[..., 0], target_scale=target_scale))
        scale = F.softplus(parameters[..., 1]) * <a id="change">target_scale[..., 1].unsqueeze(1</a><a id="change">)</a>
        return torch.stack([loc, scale], dim=-1)


class MultivariateNormalDistributionLoss(MultivariateDistributionLoss):</code></pre><h3>After Change</h3><pre><code class='java'>
        loc = parameters[..., 0]
        scale = F.softplus(parameters[..., 1])
        return torch.concat(
            [<a id="change">target_scale.unsqueeze(1).expand(-1</a>, loc.size(1), <a id="change">-1</a><a id="change">)</a>, loc.unsqueeze(-1), scale.unsqueeze(-1)], dim=-1
        )

</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/jdb78/pytorch-forecasting/commit/0193b8802943f92d3323715a088b99b6e0d96786#diff-c105df2da19b181ed8de6c83f6a07ed0f8d9c8c712a8172f7f4222ecbcebcdb0L29' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 85990317</div><div id='project'> Project Name: jdb78/pytorch-forecasting</div><div id='commit'> Commit Name: 0193b8802943f92d3323715a088b99b6e0d96786</div><div id='time'> Time: 2022-05-18</div><div id='author'> Author: beitner.jan@bcg.com</div><div id='file'> File Name: pytorch_forecasting/metrics/distributions.py</div><div id='m_class'> M Class Name: NormalDistributionLoss</div><div id='n_method'> N Class Name: NormalDistributionLoss</div><div id='m_method'> M Method Name: rescale_parameters(4)</div><div id='n_method'> N Method Name: rescale_parameters(4)</div><div id='m_parent_class'> M Parent Class: DistributionLoss</div><div id='n_parent_class'> N Parent Class: DistributionLoss</div><div id='m_file'> M File Name: pytorch_forecasting/metrics/distributions.py</div><div id='n_file'> N File Name: pytorch_forecasting/metrics/distributions.py</div><div id='m_start'> M Start Line: 32</div><div id='m_end'> M End Line: 40</div><div id='n_start'> N Start Line: 35</div><div id='n_end'> N End Line: 40</div><BR>