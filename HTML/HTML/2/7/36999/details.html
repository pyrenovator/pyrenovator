<html><h3>Pattern ID :36999
</h3><img src='105248341.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def build(self, query_shape):
        &#47&#47 print(query_shape)
        self.height = self.input_height if self.input_height &gt; 0 else int(<a id="change">tf.math.sqrt(</a><a id="change">float(</a>query_shape[2]<a id="change"> - </a>1<a id="change">))</a>)
        self.width = (query_shape[2] - 1) // self.height
        self.num_heads, self.query_dim = query_shape[1], query_shape[-1]
        self.channel_splits = [ii * self.query_dim for ii in self.head_splits]</code></pre><h3>After Change</h3><pre><code class='java'>
        import tensorflow as tf

        &#47&#47 print(query_shape)
        self.height = self.input_height if self.input_height &gt; 0 else int(<a id="change">float(</a>query_shape[2]<a id="change"> - </a>1<a id="change">) ** 0.5</a>)
        self.width = (query_shape[2] - 1) // self.height
        self.num_heads, self.query_dim = query_shape[1], query_shape[-1]
        self.channel_splits = [ii * self.query_dim for ii in self.head_splits]</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 6</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/2ba27b0132168f3590dd4b3bead9edc15a70ba7d#diff-d77c1598360af0cc3aabc2698058ce15f5c2c73e1d4d820c94c37380c4e0f51eL72' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 105248341</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 2ba27b0132168f3590dd4b3bead9edc15a70ba7d</div><div id='time'> Time: 2023-02-11</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/coat/coat.py</div><div id='m_class'> M Class Name: ConvRelativePositionalEncoding</div><div id='n_method'> N Class Name: ConvRelativePositionalEncoding</div><div id='m_method'> M Method Name: build(2)</div><div id='n_method'> N Method Name: build(2)</div><div id='m_parent_class'> M Parent Class: layers.Layer</div><div id='n_parent_class'> N Parent Class: keras.layers.Layer</div><div id='m_file'> M File Name: keras_cv_attention_models/coat/coat.py</div><div id='n_file'> N File Name: keras_cv_attention_models/coat/coat.py</div><div id='m_start'> M Start Line: 72</div><div id='m_end'> M End Line: 82</div><div id='n_start'> N Start Line: 73</div><div id='n_end'> N End Line: 83</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.supports_masking = False

    def build(self, input_shape):
        self.height = self.input_height if self.input_height &gt; 0 else int(<a id="change">tf.math.sqrt(</a><a id="change">float(</a>input_shape[1]<a id="change"> - </a>1<a id="change">))</a>)
        self.width = (input_shape[1] - 1) // self.height

        self.channel = input_shape[-1]</code></pre><h3>After Change</h3><pre><code class='java'>
        self.supports_masking = False

    def build(self, input_shape):
        self.height = self.input_height if self.input_height &gt; 0 else int(<a id="change">float(</a>input_shape[1]<a id="change"> - </a>1<a id="change">) ** 0.5</a>)
        self.width = (input_shape[1] - 1) // self.height

        self.channel = input_shape[-1]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/2ba27b0132168f3590dd4b3bead9edc15a70ba7d#diff-d77c1598360af0cc3aabc2698058ce15f5c2c73e1d4d820c94c37380c4e0f51eL32' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 105248340</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 2ba27b0132168f3590dd4b3bead9edc15a70ba7d</div><div id='time'> Time: 2023-02-11</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/coat/coat.py</div><div id='m_class'> M Class Name: ConvPositionalEncoding</div><div id='n_method'> N Class Name: ConvPositionalEncoding</div><div id='m_method'> M Method Name: build(2)</div><div id='n_method'> N Method Name: build(2)</div><div id='m_parent_class'> M Parent Class: layers.Layer</div><div id='n_parent_class'> N Parent Class: keras.layers.Layer</div><div id='m_file'> M File Name: keras_cv_attention_models/coat/coat.py</div><div id='n_file'> N File Name: keras_cv_attention_models/coat/coat.py</div><div id='m_start'> M Start Line: 33</div><div id='m_end'> M End Line: 38</div><div id='n_start'> N Start Line: 32</div><div id='n_end'> N End Line: 37</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 input (with_cls_token=False): `[batch, num_heads, attn_blocks, attn_blocks]`. where `attn_blocks = attn_height * attn_width`
        &#47&#47 print(attn_shape)
        if self.attn_height == -1:
            height = width = int(<a id="change">tf.math.sqrt(</a><a id="change">float(</a>attn_shape[2]<a id="change"> - </a>self.cls_token_len<a id="change">))</a>)  &#47&#47 hh == ww, e.g. 14
        else:
            height = self.attn_height
            width = int(float(attn_shape[2] - self.cls_token_len) / height)</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 input (with_cls_token=False): `[batch, num_heads, attn_blocks, attn_blocks]`. where `attn_blocks = attn_height * attn_width`
        &#47&#47 print(attn_shape)
        if self.attn_height == -1:
            height = width = int(<a id="change">float(</a>attn_shape[2]<a id="change"> - </a>self.cls_token_len<a id="change">) ** 0.5</a>)  &#47&#47 hh == ww, e.g. 14
        else:
            height = self.attn_height
            width = int(float(attn_shape[2] - self.cls_token_len) / height)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/1aa29dc686f862bc1ff66a11700fa8ef16bd2b8a#diff-02508ae7c53082aa4dbb63a5c5fd259b84bc5bdbf0b1971ed6cb2060ef9b9779L49' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 105248336</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 1aa29dc686f862bc1ff66a11700fa8ef16bd2b8a</div><div id='time'> Time: 2023-02-03</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/beit/beit.py</div><div id='m_class'> M Class Name: MultiHeadRelativePositionalEmbedding</div><div id='n_method'> N Class Name: MultiHeadRelativePositionalEmbedding</div><div id='m_method'> M Method Name: build(2)</div><div id='n_method'> N Method Name: build(2)</div><div id='m_parent_class'> M Parent Class: layers.Layer</div><div id='n_parent_class'> N Parent Class: keras.layers.Layer</div><div id='m_file'> M File Name: keras_cv_attention_models/beit/beit.py</div><div id='n_file'> N File Name: keras_cv_attention_models/beit/beit.py</div><div id='m_start'> M Start Line: 54</div><div id='m_end'> M End Line: 87</div><div id='n_start'> N Start Line: 53</div><div id='n_end'> N End Line: 87</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        num_heads = self.relative_position_bias_table.shape[0]
        &#47&#47 pos_emb = tf.gather(self.relative_position_bias_table, self.relative_position_index, axis=1).numpy()
        hh = ww = int(<a id="change">tf.math.sqrt(</a><a id="change">float(</a>self.relative_position_bias_table.shape[1]<a id="change"> - </a>self.cls_token_pos_len<a id="change">))</a>)
        pos_emb = tf.reshape(self.relative_position_bias_table[:, : hh * ww], (num_heads, hh, ww)).numpy()
        cols = int(tf.math.ceil(num_heads / rows))
        fig, axes = plt.subplots(rows, cols, figsize=(base_size * cols, base_size * rows))</code></pre><h3>After Change</h3><pre><code class='java'>

        num_heads = self.relative_position_bias_table.shape[0]
        &#47&#47 pos_emb = tf.gather(self.relative_position_bias_table, self.relative_position_index, axis=1).numpy()
        hh = ww = int(<a id="change">float(</a>self.relative_position_bias_table.shape[1]<a id="change"> - </a>self.cls_token_pos_len<a id="change">) ** 0.5</a>)
        pos_emb = self.relative_position_bias_table[:, : hh * ww]
        pos_emb = pos_emb.detach().numpy() if hasattr(pos_emb, "detach") else pos_emb.numpy()
        pos_emb = pos_emb.reshape((num_heads, hh, ww))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/1aa29dc686f862bc1ff66a11700fa8ef16bd2b8a#diff-02508ae7c53082aa4dbb63a5c5fd259b84bc5bdbf0b1971ed6cb2060ef9b9779L123' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 105248333</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 1aa29dc686f862bc1ff66a11700fa8ef16bd2b8a</div><div id='time'> Time: 2023-02-03</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/beit/beit.py</div><div id='m_class'> M Class Name: MultiHeadRelativePositionalEmbedding</div><div id='n_method'> N Class Name: MultiHeadRelativePositionalEmbedding</div><div id='m_method'> M Method Name: show_pos_emb(3)</div><div id='n_method'> N Method Name: show_pos_emb(3)</div><div id='m_parent_class'> M Parent Class: layers.Layer</div><div id='n_parent_class'> N Parent Class: keras.layers.Layer</div><div id='m_file'> M File Name: keras_cv_attention_models/beit/beit.py</div><div id='n_file'> N File Name: keras_cv_attention_models/beit/beit.py</div><div id='m_start'> M Start Line: 128</div><div id='m_end'> M End Line: 130</div><div id='n_start'> N Start Line: 132</div><div id='n_end'> N End Line: 136</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        else:
            source_tt = source_layer.relative_position_bias_table  &#47&#47 layer
        &#47&#47 self.relative_position_bias_table.assign(tf.transpose(source_tt))
        hh = ww = int(<a id="change">tf.math.sqrt(</a><a id="change">float(</a>source_tt.shape[1]<a id="change"> - </a>self.cls_token_pos_len<a id="change">))</a>)  &#47&#47 assume source weights are all square shape
        num_heads = source_tt.shape[0]
        ss = tf.reshape(source_tt[:, : hh * ww], (num_heads, hh, ww))  &#47&#47 [num_heads, hh, ww]
        ss = tf.transpose(ss, [1, 2, 0])  &#47&#47 [hh, ww, num_heads]</code></pre><h3>After Change</h3><pre><code class='java'>
            source_tt = source_layer.get_weights()[0]  &#47&#47 layer
        source_tt = np.array(source_tt)
        &#47&#47 self.relative_position_bias_table.assign(tf.transpose(source_tt))
        hh = ww = int(<a id="change">float(</a>source_tt.shape[1]<a id="change"> - </a>self.cls_token_pos_len<a id="change">) ** 0.5</a>)  &#47&#47 assume source weights are all square shape
        num_heads = source_tt.shape[0]
        ss = source_tt[:, : hh * ww].reshape((num_heads, hh, ww))  &#47&#47 [num_heads, hh, ww]
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/1aa29dc686f862bc1ff66a11700fa8ef16bd2b8a#diff-02508ae7c53082aa4dbb63a5c5fd259b84bc5bdbf0b1971ed6cb2060ef9b9779L99' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 105248348</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 1aa29dc686f862bc1ff66a11700fa8ef16bd2b8a</div><div id='time'> Time: 2023-02-03</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/beit/beit.py</div><div id='m_class'> M Class Name: MultiHeadRelativePositionalEmbedding</div><div id='n_method'> N Class Name: MultiHeadRelativePositionalEmbedding</div><div id='m_method'> M Method Name: load_resized_weights(3)</div><div id='n_method'> N Method Name: load_resized_weights(3)</div><div id='m_parent_class'> M Parent Class: layers.Layer</div><div id='n_parent_class'> N Parent Class: keras.layers.Layer</div><div id='m_file'> M File Name: keras_cv_attention_models/beit/beit.py</div><div id='n_file'> N File Name: keras_cv_attention_models/beit/beit.py</div><div id='m_start'> M Start Line: 101</div><div id='m_end'> M End Line: 121</div><div id='n_start'> N Start Line: 104</div><div id='n_end'> N End Line: 124</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        cpe_out, crpe_out = __cpe_norm_crpe__(xx, shared_cpe, shared_crpe, num_heads, name=cur_name)
        cpe_outs.append(cpe_out)
        crpe_outs.append(crpe_out)
        height = block_heights[id] if len(block_heights) &gt; id else int(<a id="change">tf.math.sqrt(</a><a id="change">float(</a>crpe_out.shape[1]<a id="change"> - </a>1<a id="change">))</a>)
        width = (crpe_out.shape[1] - 1) // height
        crpe_images.append(tf.reshape(crpe_out[:, 1:, :], [-1, height, width, crpe_out.shape[-1]]))
        resample_shapes.append([height, width])</code></pre><h3>After Change</h3><pre><code class='java'>
        cpe_out, crpe_out = __cpe_norm_crpe__(xx, shared_cpe, shared_crpe, num_heads, name=cur_name)
        cpe_outs.append(cpe_out)
        crpe_outs.append(crpe_out)
        height = block_heights[id] if len(block_heights) &gt; id else int(<a id="change">float(</a>crpe_out.shape[1]<a id="change"> - </a>1<a id="change">) ** 0.5</a>)
        width = (crpe_out.shape[1] - 1) // height
        crpe_images.append(functional.reshape(crpe_out[:, 1:, :], [-1, height, width, crpe_out.shape[-1]]))
        resample_shapes.append([height, width])</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/2ba27b0132168f3590dd4b3bead9edc15a70ba7d#diff-d77c1598360af0cc3aabc2698058ce15f5c2c73e1d4d820c94c37380c4e0f51eL201' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 105248345</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 2ba27b0132168f3590dd4b3bead9edc15a70ba7d</div><div id='time'> Time: 2023-02-11</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/coat/coat.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: parallel_block(9)</div><div id='n_method'> N Method Name: parallel_block(9)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/coat/coat.py</div><div id='n_file'> N File Name: keras_cv_attention_models/coat/coat.py</div><div id='m_start'> M Start Line: 211</div><div id='m_end'> M End Line: 211</div><div id='n_start'> N Start Line: 212</div><div id='n_end'> N End Line: 212</div><BR>