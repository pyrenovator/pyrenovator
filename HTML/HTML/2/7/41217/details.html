<html><h3>Pattern ID :41217
</h3><img src='116225613.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self_attns, encoder_attns = list(), list()
        output = None

        inputs = <a id="change">self.embedding(</a>inputs<a id="change">)</a>
        inputs = self.positional_encoding(inputs)

        for layer in self.layers:
            output, self_attn, encoder_attn = layer(inputs, memory, inputs_mask, memory_mask)</code></pre><h3>After Change</h3><pre><code class='java'>
        self_attn_mask = get_attn_pad_mask(targets, self.pad_id) | get_subsequent_mask(targets)
        memory_mask = get_pad_mask(memory, input_lengths).squeeze(-1).unsqueeze(1).expand(-1, targets.size(1), -1)

        output = <a id="change">self.input_dropout(</a><a id="change">self.embedding(targets) * self.logit_scale + self.pos_encoding(</a>targets.size(1)<a id="change">)</a><a id="change">)</a>

        for layer in self.layers:
            output, self_attn, memory_attn = layer(output, memory, non_pad_mask, self_attn_mask, memory_mask)
            self_attns.append(self_attn)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/sooftware/transformer/commit/a0b53d9802b580df564793d1f66e65e286065f00#diff-825f88954874495622d9ca7661409390247cc81d30fc6addd2c4035da5e1e992L136' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116225613</div><div id='project'> Project Name: sooftware/transformer</div><div id='commit'> Commit Name: a0b53d9802b580df564793d1f66e65e286065f00</div><div id='time'> Time: 2020-07-25</div><div id='author'> Author: sh951011@gmail.com</div><div id='file'> File Name: transformer/models/transformer.py</div><div id='m_class'> M Class Name: TransformerDecoder</div><div id='n_method'> N Class Name: TransformerDecoder</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: transformer/models/transformer.py</div><div id='n_file'> N File Name: transformer/models/transformer.py</div><div id='m_start'> M Start Line: 141</div><div id='m_end'> M End Line: 155</div><div id='n_start'> N Start Line: 136</div><div id='n_end'> N End Line: 151</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self_attns = list()
        output = None

        inputs = <a id="change">self.embedding(</a>inputs<a id="change">)</a>
        inputs = self.positional_encoding(inputs)

        for layer in self.layers:
            output, attn = layer(inputs, inputs_mask)</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward(self, inputs: Tensor, input_lengths: Tensor = None) -&gt; Tuple[Tensor, Tensor]:
        self_attns = list()

        output = <a id="change">self.input_dropout(</a><a id="change">self.embedding(inputs) * self.logit_scale + self.pos_encoding(</a>inputs<a id="change">)</a><a id="change">)</a>

        non_pad_mask = get_pad_mask(inputs, input_lengths=input_lengths).eq(False)
        length = inputs.size(1)
        self_attn_mask = get_pad_mask(inputs, input_lengths).squeeze(-1).unsqueeze(1).expand(-1, length, -1)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sooftware/transformer-pytorch/commit/a0b53d9802b580df564793d1f66e65e286065f00#diff-825f88954874495622d9ca7661409390247cc81d30fc6addd2c4035da5e1e992L107' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116225615</div><div id='project'> Project Name: sooftware/transformer-pytorch</div><div id='commit'> Commit Name: a0b53d9802b580df564793d1f66e65e286065f00</div><div id='time'> Time: 2020-07-25</div><div id='author'> Author: sh951011@gmail.com</div><div id='file'> File Name: transformer/models/transformer.py</div><div id='m_class'> M Class Name: TransformerEncoder</div><div id='n_method'> N Class Name: TransformerEncoder</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: transformer/models/transformer.py</div><div id='n_file'> N File Name: transformer/models/transformer.py</div><div id='m_start'> M Start Line: 109</div><div id='m_end'> M End Line: 117</div><div id='n_start'> N Start Line: 101</div><div id='n_end'> N End Line: 108</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self_attns, encoder_attns = list(), list()
        output = None

        inputs = <a id="change">self.embedding(</a>inputs<a id="change">)</a>
        inputs = self.positional_encoding(inputs)

        for layer in self.layers:
            output, self_attn, encoder_attn = layer(inputs, memory, inputs_mask, memory_mask)</code></pre><h3>After Change</h3><pre><code class='java'>
        self_attn_mask = get_attn_pad_mask(targets, self.pad_id) | get_subsequent_mask(targets)
        memory_mask = get_pad_mask(memory, input_lengths).squeeze(-1).unsqueeze(1).expand(-1, targets.size(1), -1)

        output = <a id="change">self.input_dropout(</a><a id="change">self.embedding(targets) * self.logit_scale + self.pos_encoding(</a>targets.size(1)<a id="change">)</a><a id="change">)</a>

        for layer in self.layers:
            output, self_attn, memory_attn = layer(output, memory, non_pad_mask, self_attn_mask, memory_mask)
            self_attns.append(self_attn)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sooftware/transformer-pytorch/commit/a0b53d9802b580df564793d1f66e65e286065f00#diff-825f88954874495622d9ca7661409390247cc81d30fc6addd2c4035da5e1e992L141' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116225620</div><div id='project'> Project Name: sooftware/transformer-pytorch</div><div id='commit'> Commit Name: a0b53d9802b580df564793d1f66e65e286065f00</div><div id='time'> Time: 2020-07-25</div><div id='author'> Author: sh951011@gmail.com</div><div id='file'> File Name: transformer/models/transformer.py</div><div id='m_class'> M Class Name: TransformerDecoder</div><div id='n_method'> N Class Name: TransformerDecoder</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: transformer/models/transformer.py</div><div id='n_file'> N File Name: transformer/models/transformer.py</div><div id='m_start'> M Start Line: 141</div><div id='m_end'> M End Line: 155</div><div id='n_start'> N Start Line: 136</div><div id='n_end'> N End Line: 151</div><BR>