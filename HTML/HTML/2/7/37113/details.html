<html><h3>Pattern ID :37113
</h3><img src='106207892.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def forward(self, pred, target):
        log_prob = F.log_softmax(pred, dim=-1)
        dist = torch.empty_like(pred).fill_(self.smoothing<a id="change"> / </a>(<a id="change">pred.size(-1</a><a id="change">)</a> - 1))
        dist.scatter_(dim=-1, index=target[..., None], value=(1 - self.smoothing))
        loss = F.kl_div(log_prob, dist)
        return loss</code></pre><h3>After Change</h3><pre><code class='java'>
        chunked_target = torch.chunk(target, chunks=self.chunk, dim=0)
        chunked_mask = torch.chunk(mask, chunks=self.chunk, dim=0)
        log_prob = [F.log_softmax(p, dim=-1) for p in chunked_pred]
        loss<a id="change"> = </a><a id="change">[self.smoothed_loss(p, t, m)[None]\
            for p, t, m in zip(log_prob, chunked_target, chunked_mask)]</a>
        loss<a id="change"> = </a>torch.cat(loss, dim=0).sum()
        return loss / mask.sum()

    def smoothed_loss(self, log_prob: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -&gt; torch.Tensor:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/rick-mccoy/reformer-pytorch/commit/3411114d22e0bfcae2e106f5c82a3211da83f409#diff-fe652c109b5da8ce3b578ba9b2009d72fccfc98ec85a96850e18c5af2f882069L11' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 106207892</div><div id='project'> Project Name: rick-mccoy/reformer-pytorch</div><div id='commit'> Commit Name: 3411114d22e0bfcae2e106f5c82a3211da83f409</div><div id='time'> Time: 2020-02-29</div><div id='author'> Author: rickmccoy3141@gmail.com</div><div id='file'> File Name: model/labelsmoothing.py</div><div id='m_class'> M Class Name: LabelSmoothing</div><div id='n_method'> N Class Name: LabelSmoothing</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: model/labelsmoothing.py</div><div id='n_file'> N File Name: model/labelsmoothing.py</div><div id='m_start'> M Start Line: 11</div><div id='m_end'> M End Line: 16</div><div id='n_start'> N Start Line: 13</div><div id='n_end'> N End Line: 24</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                        rec_features.append(rec_feature[:, :inputs[i].size(1)])
                    else:
                        rec_features.append(rec_feature[:, \
                            inputs[i-1].size(1):inputs[i-1].size(1)<a id="change">+</a><a id="change">inputs[i].size(1</a><a id="change">)</a>])
            &quot&quot&quot
            if i == 0:
                rec_features.append(rec_feature[:, :outs[i].size(-1)])</code></pre><h3>After Change</h3><pre><code class='java'>
        
        fuse = self.fuse(outs, training=training)
        logit = self.head(fuse, training=training)
        sizes<a id="change"> = </a><a id="change">[torch.flatten(ii,start_dim=1).size(1) for ii in inputs]</a>
        rec_features = []
        if training:
            rec_feature = self.refiner(fuse, training=training)
            curr=0
            for i in range(input_num):
                if self.has_padding:
                    if i == 0:
                        rec_features.append(rec_feature[:, :inputs[0][i].size(1)])
                    else:
                        rec_features.append(rec_feature[:, \
                            inputs[0][i-1].size(1):inputs[0][i-1].size(1)+inputs[i].size(1)])
                else:
                    if i == 0:
                        rec_features.append(rec_feature[:, :sizes[0]])
                        curr = sizes[0]
                    else:
                        rec_features.append(rec_feature[:, \
                                curr:curr+sizes[i]])
                        curr<a id="change"> += </a>sizes[i]
            &quot&quot&quot
            if i == 0:
                rec_features.append(rec_feature[:, :outs[i].size(-1)])</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pliang279/multibench/commit/72e3344b766884b9160fd383b13945be06819481#diff-d18b23b61842526d6956b06a29d725aed1c3962fa548edca68f628dea2178c2dL25' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 106207940</div><div id='project'> Project Name: pliang279/multibench</div><div id='commit'> Commit Name: 72e3344b766884b9160fd383b13945be06819481</div><div id='time'> Time: 2021-06-05</div><div id='author'> Author: blairc@andrew.cmu.edu</div><div id='file'> File Name: training_structures/Contrastive_Learning.py</div><div id='m_class'> M Class Name: MMDL</div><div id='n_method'> N Class Name: MMDL</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: training_structures/Contrastive_Learning.py</div><div id='n_file'> N File Name: training_structures/Contrastive_Learning.py</div><div id='m_start'> M Start Line: 39</div><div id='m_end'> M End Line: 52</div><div id='n_start'> N Start Line: 37</div><div id='n_end'> N End Line: 56</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        hid = torch.zeros(mels.size(0), 1, self.wav_rnn_dim, device=mels.device)
        wav = torch.full(
            (<a id="change">mels.size(0</a><a id="change">)</a>,), self.quant_dim // 2, dtype=torch.long, device=mels.device,
        )
        wavs = torch.empty(
            mels.size(0),
            mels.size(1) * self.hop_len,
            dtype=torch.long,
            device=mels.device,
        )

        for i, condition in enumerate(torch.unbind(conditions, dim=1)):
            wav_emb = self.embedding(wav)
            _, hid = self.wav_rnn(
                torch.cat((wav_emb, condition), dim=1).unsqueeze(1), hid
            )
            logit = self.affine(hid.squeeze(1))
            posterior = F.softmax(logit, dim=1)
            wav = torch.multinomial(posterior, 1).squeeze(1)
            wavs[:, i] = 2 * wav.item() / (self.quant_dim - 1.0) - 1.0

        mu = self.quant_dim - 1
        wavs = torch.sign(wavs)<a id="change"> / </a>mu * ((1 + mu) ** torch.abs(wavs) - 1)

        return wavs
</code></pre><h3>After Change</h3><pre><code class='java'>
        batch_size = len(mels)
        device = mels[0].device

        mel_lens<a id="change"> = </a><a id="change">[len(mel) for mel in mels]</a>
        wav_lens = [mel_len * self.hop_len for mel_len in mel_lens]
        max_mel_len = max(mel_lens)

        mel_embs = []

        for mel in mels:
            mel = mel.unsqueeze(0)
            mel_emb, _ = self.mel_rnn(mel)
            mel_emb = mel_emb.squeeze(0)
            mel_embs.append(mel_emb)

        mel_embs = pad_sequence(
            mel_embs, batch_first=True, padding_value=float(self.quant_dim // 2)
        )
        mel_embs = mel_embs.transpose(1, 2)

        conditions = F.interpolate(mel_embs, scale_factor=float(self.hop_len))
        conditions = conditions.transpose(1, 2)

        hid = torch.zeros(1, batch_size, self.wav_rnn_dim, device=device)
        wav = torch.full(
            (batch_size,), self.quant_dim // 2, dtype=torch.long, device=device,
        )
        wavs = torch.empty(
            batch_size, max_mel_len * self.hop_len, dtype=torch.long, device=device,
        )

        for i, condition in enumerate(torch.unbind(conditions, dim=1)):
            wav_emb = self.embedding(wav)
            _, hid = self.wav_rnn(
                torch.cat((wav_emb, condition), dim=1).unsqueeze(1), hid
            )
            logit = self.affine(hid.squeeze(0))
            posterior = F.softmax(logit, dim=1)
            wav = torch.multinomial(posterior, 1).squeeze(1)
            wavs[:, i] = 2 * wav / (self.quant_dim - 1.0) - 1.0

        mu = self.quant_dim - 1
        wavs = torch.true_divide(torch.sign(wavs), mu) * (
            (1 + mu) ** torch.abs(wavs) - 1
        )
        wavs<a id="change"> = </a>[
            wav[:length] for wav, length in zip(torch.unbind(wavs, dim=0), wav_lens)
        ]
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/yistlin/universal-vocoder/commit/a2f043170a0335459db6b45b7d8dc692db9a00f5#diff-0b303f21197833211aecf17dfcb415a3d2172623cfededbfc805892f128cd01eL58' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 106207895</div><div id='project'> Project Name: yistlin/universal-vocoder</div><div id='commit'> Commit Name: a2f043170a0335459db6b45b7d8dc692db9a00f5</div><div id='time'> Time: 2020-10-06</div><div id='author'> Author: yishen992@gmail.com</div><div id='file'> File Name: models/universal_vocoder.py</div><div id='m_class'> M Class Name: UniversalVocoder</div><div id='n_method'> N Class Name: UniversalVocoder</div><div id='m_method'> M Method Name: generate(2)</div><div id='n_method'> N Method Name: generate(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/universal_vocoder.py</div><div id='n_file'> N File Name: models/universal_vocoder.py</div><div id='m_start'> M Start Line: 58</div><div id='m_end'> M End Line: 88</div><div id='n_start'> N Start Line: 63</div><div id='n_end'> N End Line: 120</div><BR>