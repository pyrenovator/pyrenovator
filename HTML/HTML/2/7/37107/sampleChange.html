<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.num_patches = self.patch_embed.num_patches

        &#47&#47 Build up each hierarchical level
        self.ls_pos_embed<a id="change"> = </a><a id="change">[]</a>
        self.ls_transformer_encoder = nn.ModuleList([])
        self.ls_block_aggregation = nn.ModuleList([])
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] &#47&#47 drop path rate
        self.feature_info = []
        for level in range(self.num_levels):
            &#47&#47 Positional embedding
            &#47&#47 NOTE: Can&quott use ParameterList for positional embedding as it can&quott be enumerated with TorchScript
            pos_embed = nn.Parameter(
                torch.zeros(1, self.num_blocks[level], self.num_patches // self.num_blocks[0], embed_dims[level]))
            self.register_parameter(f&quotpos_embed_{level}&quot, pos_embed)
            <a id="change">self.ls_pos_embed.append(</a>pos_embed<a id="change">)</a>
            &#47&#47 Transformer encoder
            self.ls_transformer_encoder.append(nn.Sequential(*[
                TransformerLayer(
                    dim=embed_dims[level], num_heads=num_heads[level], mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,
                    drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[sum(depths[:level]) + i],
                    norm_layer=norm_layer, act_layer=act_layer)
                for i in range(depths[level])]))
            
            self.feature_info.append(dict(
                num_chs=embed_dims[level], reduction=2,
                module=f&quotls_transformer_encoder.{level}.{depths[level]-1}.mlp.fc2&quot))

            &#47&#47 Block aggregation (not required for last level)
            if level &lt; self.num_levels - 1:
                self.ls_block_aggregation.append(
                    BlockAggregation(embed_dims[level], embed_dims[level+1], norm_layer, pad_type=pad_type))
            else:
                &#47&#47 NOTE: Required for enumeration over all level components at once
                self.ls_block_aggregation.append(nn.Identity())
        self.ls_pos_embed<a id="change"> = </a>tuple(self.ls_pos_embed) &#47&#47 static length required for torchscript

        
        &#47&#47 Final normalization layer</code></pre><h3>After Change</h3><pre><code class='java'>
        self.block_aggs = nn.ModuleList([])
        drop_path_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]
        for lix in range(self.num_levels):
            dpr = drop_path_rates[sum(depths[:lix]):sum(depths[<a id="change">:lix+1</a>])]
            self.levels.append(NestLevel(
                self.num_blocks[lix], self.block_size, self.seq_length, num_heads[lix], depths[lix],
                embed_dims[lix], mlp_ratio, qkv_bias, drop_rate, attn_drop_rate, dpr, norm_layer,</code></pre>