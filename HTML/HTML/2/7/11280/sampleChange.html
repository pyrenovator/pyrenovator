<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        else:
            n_tokens, batch_size, _ = x.shape &#47&#47 t, b, c

        positions<a id="change"> = </a>repeat(self.pos_scale*torch.arange(n_tokens),
                           &quott -&gt; new_axis t&quot, new_axis=batch_size).to(x)
        positions<a id="change"> = </a>self.augment_positions(positions)

        positions<a id="change"> = </a><a id="change">rearrange(</a>positions, <a id="change">&quotb t -&gt; b t 1&quot</a><a id="change">)</a>
        product = positions * self.freq.to(x)

        pos_emb = torch.sin(product + self.cos_shifts.to(x))

        if not self.batch_first:
            pos_emb = rearrange(pos_emb, &quotb t c -&gt; t b c&quot)

        <a id="change">return </a>pos_emb

    def augment_positions(self, positions: Tensor):
        assert self.max_global_scaling &gt;= 1</code></pre><h3>After Change</h3><pre><code class='java'>
        self.register_buffer(&quotcos_shifts&quot, cos_shifts)

    def forward(self, x: Tensor) -&gt; Tensor:
        <a id="change">return </a>x + self.compute_pos_emb(x)

    def compute_pos_emb(self, x: Tensor) -&gt; Tensor:
        if self.batch_first:</code></pre>