<html><h3>Pattern ID :31801
</h3><img src='92838719.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        rnn_output, context_alignment = None, None
        for t in range(T):
            if t == 0:
                embedding = <a id="change">self.decoder_embeddings(</a>Variable(
                    context[-1].data.new(B).long().fill_(
                        self.field.vocab.stoi[&quot&lt;init&gt;&quot]), volatile=True).unsqueeze(1), [1]<a id="change">*</a>B<a id="change">)</a>
            else:
                embedding = self.decoder_embeddings(outs[:, t - 1].unsqueeze(1), [1]*B)
            decoder_outputs = self.dual_ptr_rnn_decoder(embedding, &#47&#47hiddens[-1][:, t].unsqueeze(1),
                context, </code></pre><h3>After Change</h3><pre><code class='java'>
            if t == 0:
                with torch.no_grad():
                    outs_0 = Variable(context[-1].data.new(B).long().fill_(self.field.vocab.stoi[&quot&lt;init&gt;&quot]))
                embedding<a id="change"> = </a><a id="change">self.decoder_embeddings(outs_0.unsqueeze(1</a><a id="change">)</a>, [1]<a id="change">*</a>B<a id="change">)</a>
            else:
                embedding = self.decoder_embeddings(outs[:, t - 1].unsqueeze(1), [1]*B)
            decoder_outputs = self.dual_ptr_rnn_decoder(embedding, &#47&#47hiddens[-1][:, t].unsqueeze(1),
                context, </code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/stanford-oval/genienlp/commit/9f19cfb09b47ff69b458788a00383ac6d3b071e1#diff-a666c526632b1cdc0ae405d34627213445247898ae8c3f2c372a473428f4574cL114' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 92838719</div><div id='project'> Project Name: stanford-oval/genienlp</div><div id='commit'> Commit Name: 9f19cfb09b47ff69b458788a00383ac6d3b071e1</div><div id='time'> Time: 2018-08-13</div><div id='author'> Author: mehrad@stanford.edu</div><div id='file'> File Name: models/pointer_generator.py</div><div id='m_class'> M Class Name: PointerGenerator</div><div id='n_method'> N Class Name: PointerGenerator</div><div id='m_method'> M Method Name: greedy(5)</div><div id='n_method'> N Method Name: greedy(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/pointer_generator.py</div><div id='n_file'> N File Name: models/pointer_generator.py</div><div id='m_start'> M Start Line: 118</div><div id='m_end'> M End Line: 142</div><div id='n_start'> N Start Line: 114</div><div id='n_end'> N End Line: 141</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        rnn_output, context_alignment = None, None
        for t in range(T):
            if t == 0:
                embedding = <a id="change">self.decoder_embeddings(</a>Variable(
                    self_attended_context[-1].data.new(B).long().fill_(
                        self.field.vocab.stoi[&quot&lt;init&gt;&quot]), volatile=True).unsqueeze(1), [1]<a id="change">*</a>B<a id="change">)</a>
            else:
                embedding = self.decoder_embeddings(outs[:, t - 1].unsqueeze(1), [1]*B)
            hiddens[0][:, t] = hiddens[0][:, t] + (math.sqrt(self.self_attentive_decoder.d_model) * embedding).squeeze(1)
            for l in range(len(self.self_attentive_decoder.layers)):</code></pre><h3>After Change</h3><pre><code class='java'>
        for t in range(T):
            if t == 0:
                outs_0 = Variable(self_attended_context[-1].data.new(B).long().fill_(self.field.vocab.stoi[&quot&lt;init&gt;&quot]), volatile=True)
                embedding<a id="change"> = </a><a id="change">self.decoder_embeddings(outs_0.unsqueeze(1</a><a id="change">)</a>, [1]<a id="change">*</a>B<a id="change">)</a>
            else:
                embedding = self.decoder_embeddings(outs[:, t - 1].unsqueeze(1), [1]*B)
            hiddens[0][:, t] = hiddens[0][:, t] + (math.sqrt(self.self_attentive_decoder.d_model) * embedding).squeeze(1)
            for l in range(len(self.self_attentive_decoder.layers)):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/stanford-oval/genienlp/commit/9f19cfb09b47ff69b458788a00383ac6d3b071e1#diff-ed5b3c3850653ac07b17b1db126163f7a6d160ffc06c6186a8b293fc0297d985L135' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 92838720</div><div id='project'> Project Name: stanford-oval/genienlp</div><div id='commit'> Commit Name: 9f19cfb09b47ff69b458788a00383ac6d3b071e1</div><div id='time'> Time: 2018-08-13</div><div id='author'> Author: mehrad@stanford.edu</div><div id='file'> File Name: models/self_attentive_pointer_generator.py</div><div id='m_class'> M Class Name: SelfAttentivePointerGenerator</div><div id='n_method'> N Class Name: SelfAttentivePointerGenerator</div><div id='m_method'> M Method Name: greedy(6)</div><div id='n_method'> N Method Name: greedy(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/self_attentive_pointer_generator.py</div><div id='n_file'> N File Name: models/self_attentive_pointer_generator.py</div><div id='m_start'> M Start Line: 138</div><div id='m_end'> M End Line: 171</div><div id='n_start'> N Start Line: 135</div><div id='n_end'> N End Line: 170</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        rnn_output, context_alignment = None, None
        for t in range(T):
            if t == 0:
                embedding = <a id="change">self.decoder_embeddings(</a>Variable(
                    self_attended_context[-1].data.new(B).long().fill_(
                        self.field.vocab.stoi[&quot&lt;init&gt;&quot]), volatile=True).unsqueeze(1), [1]<a id="change">*</a>B<a id="change">)</a>
            else:
                embedding = self.decoder_embeddings(outs[:, t - 1].unsqueeze(1), [1]*B)
            hiddens[0][:, t] = hiddens[0][:, t] + (math.sqrt(self.self_attentive_decoder.d_model) * embedding).squeeze(1)
            for l in range(len(self.self_attentive_decoder.layers)):</code></pre><h3>After Change</h3><pre><code class='java'>
            if t == 0:
                with torch.no_grad():
                    outs_0 = Variable(self_attended_context[-1].data.new(B).long().fill_(self.field.vocab.stoi[&quot&lt;init&gt;&quot]))
                embedding<a id="change"> = </a><a id="change">self.decoder_embeddings(outs_0.unsqueeze(1</a><a id="change">)</a>, [1]<a id="change">*</a>B<a id="change">)</a>
            else:
                embedding = self.decoder_embeddings(outs[:, t - 1].unsqueeze(1), [1]*B)
            hiddens[0][:, t] = hiddens[0][:, t] + (math.sqrt(self.self_attentive_decoder.d_model) * embedding).squeeze(1)
            for l in range(len(self.self_attentive_decoder.layers)):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/stanford-oval/genienlp/commit/9f19cfb09b47ff69b458788a00383ac6d3b071e1#diff-132fc63ce5affe92920aa4dc16792a1932aa59397084f8e47fcb4e2b203475a9L142' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 92838721</div><div id='project'> Project Name: stanford-oval/genienlp</div><div id='commit'> Commit Name: 9f19cfb09b47ff69b458788a00383ac6d3b071e1</div><div id='time'> Time: 2018-08-13</div><div id='author'> Author: mehrad@stanford.edu</div><div id='file'> File Name: models/coattentive_pointer_generator.py</div><div id='m_class'> M Class Name: CoattentivePointerGenerator</div><div id='n_method'> N Class Name: CoattentivePointerGenerator</div><div id='m_method'> M Method Name: greedy(6)</div><div id='n_method'> N Method Name: greedy(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/coattentive_pointer_generator.py</div><div id='n_file'> N File Name: models/coattentive_pointer_generator.py</div><div id='m_start'> M Start Line: 145</div><div id='m_end'> M End Line: 178</div><div id='n_start'> N Start Line: 143</div><div id='n_end'> N End Line: 180</div><BR>