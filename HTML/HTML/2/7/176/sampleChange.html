<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
def test_training_lr_scheduler(trainfile, dataroot, tmpdir, device, capsys):
    &#47&#47 Do not shuffle examples randomly when loading the batch
    &#47&#47 This ensures reproducibility
    args<a id="change"> = </a>training.options(
        [
            trainfile,
            "--testfile",
            trainfile,
            "-d",
            dataroot,
            "--no_shuffle",
            "--batch_size",
            "1",
            "--test_every",
            "1",
            "--iterations",
            "5",
            "-o",
            str(tmpdir),
            "-g",
            str(device),
            "--seed",
            "42",
            "--progress_bar",
            "--base_lr",
            "0.1",
            "--lr_dynamic",
            "--lr_patience",
            "1",
            "--lr_min",
            "0.001",
        ]
    )

    <a id="change">training.training(</a>args<a id="change">)</a>

    &#47&#47 Check that the learning rate changes during training
    &#47&#47 TODO: Store learning rate internally and check the cache instead
    captured = capsys.readouterr()</code></pre><h3>After Change</h3><pre><code class='java'>


def test_training_lr_scheduler(trainfile, dataroot, tmpdir, device, capsys):
    <a id="change">with mlflow.start_run()</a><a id="change">:
        &#47&#47 Do not shuffle examples randomly when loading the batch
        &#47&#47 This ensures reproducibility
        </a>args<a id="change"> = </a>training.options(
            [
                trainfile,
                "--testfile",
                trainfile,
                "-d",
                dataroot,
                "--no_shuffle",
                "--batch_size",
                "1",
                "--test_every",
                "1",
                "--iterations",
                "5",
                "-o",
                str(tmpdir),
                "-g",
                str(device),
                "--seed",
                "42",
                "--progress_bar",
                "--base_lr",
                "0.1",
                "--lr_dynamic",
                "--lr_patience",
                "1",
                "--lr_min",
                "0.001",
            ]
        )

        <a id="change">training.training(</a>args<a id="change">)</a>

        &#47&#47 Check that the learning rate changes during training
        &#47&#47 TODO: Store learning rate internally and check the cache instead
        captured = capsys.readouterr()</code></pre>