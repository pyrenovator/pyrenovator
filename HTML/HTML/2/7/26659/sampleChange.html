<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            _DATA_PARALLEL_GROUP = group

    global _TENSOR_MODEL_PARALLEL_GROUP
    <a id="change">assert </a>_TENSOR_MODEL_PARALLEL_GROUP is None, \
        &quottensor model parallel group is already initialized&quot
    &#47&#47 Build the model-parallel groups.
    for i in range(data_parallel_size_):</code></pre><h3>After Change</h3><pre><code class='java'>
def init_dap(tensor_model_parallel_size_=None):
    colossalai.logging.disable_existing_loggers()

    <a id="change">if </a><a id="change">tensor_model_parallel_size_ == None</a>:
        <a id="change">if &quotWORLD_SIZE&quot in os.environ</a>:
            tensor_model_parallel_size_<a id="change"> = </a><a id="change">int(</a>os.environ[&quotWORLD_SIZE&quot]<a id="change">)</a>
        else:
            tensor_model_parallel_size_ = 1

    if torch.torch.distributed.is_initialized():</code></pre>