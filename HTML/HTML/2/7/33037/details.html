<html><h3>Pattern ID :33037
</h3><img src='95548181.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    loop = tqdm(test_loader)
    for batch_idx, frames in enumerate(loop):

        <a id="change">if i &gt;= 10</a>: break

        frames = frames.to(DEVICE)  &#47&#47 [1, T, 3, h, w]
        frames_vis = postprocess_img(frames.squeeze(dim=0))  &#47&#47 [T, 3, h, w]
        input = frames[:, :VIDEO_IN_LENGTH]  &#47&#47 [1, t, 3, h, w]

        pred_rgb = pred_rgb_model.pred_n(input, pred_length=VIDEO_PRED_LENGTH)  &#47&#47 [1, T, 3, h, w]
        pred_rgb_vis = postprocess_img(pred_rgb)  &#47&#47 [T, 3, h, w]

        pred_rgb = torch.cat([input, pred_rgb], dim=1)
        pred_rgb = torch.stack([seg_model(pred_rgb[:, i]) for i in range(pred_rgb.shape[1])], dim=1)
        pred_rgb = pred_rgb.argmax(dim=2).squeeze()  &#47&#47 [T, h, w]
        pred_then_colorized_vis = colorize_semseg(postprocess_mask(pred_rgb), num_classes=SYNPICK_CLASSES) &#47&#47 [T, 3, h, w]

        frames_seg = [seg_model(frames[:, i]).argmax(dim=1) for i in range(frames.shape[1])]
        frames_seg = torch.stack(frames_seg, dim=1)  &#47&#47 [1, 1, h, w]
        input_seg = frames_seg[:, :VIDEO_IN_LENGTH]  &#47&#47 [1, t, 1, h, w]

        pred_mask = pred_mask_model.pred_n(input_seg, pred_length=VIDEO_PRED_LENGTH)
        pred_mask = pred_mask.argmax(dim=2)  &#47&#47 [1, T, 1, h, w]
        pred_mask = postprocess_mask(torch.cat([input_seg, pred_mask], dim=1).squeeze())  &#47&#47 [T, h, w]
        pred_mask_vis = colorize_semseg(pred_mask, num_classes=SYNPICK_CLASSES)  &#47&#47 [T, 3, h, w]

        frames_colorized = colorize_semseg(postprocess_mask(frames_seg.squeeze()), num_classes=SYNPICK_CLASSES).unsqueeze(dim=0) &#47&#47 [1, T, 3, h, w]
        frames_colorized_vis<a id="change"> = </a>postprocess_img(frames_colorized.squeeze(dim=0))  &#47&#47 [T, 3, h, w]
        input_colorized = frames_colorized[:VIDEO_IN_LENGTH]

        colorized_then_pred = pred_colorized_mask_model.pred_n(input_colorized, pred_length=VIDEO_PRED_LENGTH)</code></pre><h3>After Change</h3><pre><code class='java'>
    test_data = SynpickVideoDataset(data_dir=data_dir, vid_type=("rgb", 3), num_frames=VIDEO_TOT_LENGTH,
                                    step=4, allow_overlap=VID_DATA_ALLOW_OVERLAP)
    test_loader = DataLoader(test_data, batch_size=1, shuffle=True, num_workers=4)
    iter_loader<a id="change"> = </a>iter(test_loader)

    with torch.no_grad():
        for i in tqdm(range(10)):

            frames = next(iter_loader).to(DEVICE)  &#47&#47 [1, T, 3, h, w]
            frames_vis = postprocess_img(frames.squeeze(dim=0))  &#47&#47 [T, 3, h, w]
            input = frames[:, :VIDEO_IN_LENGTH]  &#47&#47 [1, t, 3, h, w]

            pred_rgb = pred_rgb_model.pred_n(input, pred_length=VIDEO_PRED_LENGTH)
            pred_rgb = torch.cat([input, pred_rgb], dim=1)  &#47&#47 [1, T, 3, h, w]
            pred_rgb_vis = postprocess_img(pred_rgb.squeeze(dim=0))  &#47&#47 [T, 3, h, w]

            pred_rgb = torch.stack([seg_model(pred_rgb[:, i]) for i in range(pred_rgb.shape[1])], dim=1)
            pred_rgb = pred_rgb.argmax(dim=2).squeeze()  &#47&#47 [T, h, w]
            pred_then_colorized_vis = colorize_semseg(postprocess_mask(pred_rgb), num_classes=SYNPICK_CLASSES).transpose(0, 3, 1, 2) &#47&#47 [T, 3, h, w]

            frames_seg = torch.stack([seg_model(frames[:, i]) for i in range(frames.shape[1])], dim=1).argmax(dim=2)  &#47&#47 [1, T, 1, h, w]
            frames_seg_in<a id="change"> = </a><a id="change">torch.stack([(frames_seg == i) for i in range(SYNPICK_CLASSES)], dim=2).float()</a>  &#47&#47 [1, T, c, h, w] one-hot float
            input_seg = frames_seg_in[:, :VIDEO_IN_LENGTH]  &#47&#47 [1, t, c, h, w]
            pred_mask = pred_mask_model.pred_n(input_seg, pred_length=VIDEO_PRED_LENGTH).argmax(dim=2)  &#47&#47 [1, n, 1, h, w]
            pred_mask = torch.cat([input_seg.argmax(dim=2), pred_mask], dim=1).squeeze()  &#47&#47 [T, h, w]</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/ais-bonn/vp-suite/commit/13016d4ab8ba4f8e7ee087155a6c5171f4d00ba3#diff-e07d70acfca9139cbf2c37b7a0074bdfbb966d41dc6c7edcc2cef05e78c00af0L16' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95548181</div><div id='project'> Project Name: ais-bonn/vp-suite</div><div id='commit'> Commit Name: 13016d4ab8ba4f8e7ee087155a6c5171f4d00ba3</div><div id='time'> Time: 2021-08-02</div><div id='author'> Author: boltres@ais.uni-bonn.de</div><div id='file'> File Name: scripts/visualize_4_way.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: visualize_4_way(1)</div><div id='n_method'> N Method Name: visualize_4_way(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: scripts/visualize_4_way.py</div><div id='n_file'> N File Name: scripts/visualize_4_way.py</div><div id='m_start'> M Start Line: 16</div><div id='m_end'> M End Line: 73</div><div id='n_start'> N Start Line: 17</div><div id='n_end'> N End Line: 74</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        image = np.ascontiguousarray(image, dtype=np.float32)
        image = image
        image /= 255
        <a id="change">if len(image.shape) == 3</a>:
            image<a id="change"> = </a>image[None]  &#47&#47 expand for batch dim

        input_name = self.model.get_inputs()[0].name
        pred = self.model.run([self.model.get_outputs()[0].name],</code></pre><h3>After Change</h3><pre><code class='java'>
            pred = self.model.run([self.model.get_outputs()[0].name], {input_name: processed_image})[0]
            &#47&#47 Run Pytorch model        
        else:
            processed_image<a id="change"> = </a>torch.from_numpy(processed_image).to(self.device)
            &#47&#47 Change image floating point precision if fp16 set to true
            processed_image = processed_image.half() if self.fp16 else <a id="change">processed_image.float()</a> 
            pred = self.model(processed_image, augment=False)[0]
            pred<a id="change"> = </a>pred.detach().cpu().numpy()
        
        if isinstance(pred, np.ndarray):
            pred = torch.tensor(pred, device=self.device)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/augmentedstartups/as-one/commit/79f3ea97d63f873008f3ad548f1428f07f4d9dae#diff-9154a085fa2fce4c0495a4fe364b1cba4be28cc642d14f5b5b8eb80e737ae200L30' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95548154</div><div id='project'> Project Name: augmentedstartups/as-one</div><div id='commit'> Commit Name: 79f3ea97d63f873008f3ad548f1428f07f4d9dae</div><div id='time'> Time: 2022-09-07</div><div id='author'> Author: ajmair.kashif@axcelerate.ai</div><div id='file'> File Name: asone-linux/code/asone/detectors/yolor/yolor_detector.py</div><div id='m_class'> M Class Name: YOLOrDetector</div><div id='n_method'> N Class Name: YOLOrDetector</div><div id='m_method'> M Method Name: detect(8)</div><div id='n_method'> N Method Name: detect(8)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: asone-linux/code/asone/detectors/yolor/yolor_detector.py</div><div id='n_file'> N File Name: asone-linux/code/asone/detectors/yolor/yolor_detector.py</div><div id='m_start'> M Start Line: 38</div><div id='m_end'> M End Line: 72</div><div id='n_start'> N Start Line: 70</div><div id='n_end'> N End Line: 109</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        img = torch.from_numpy(img).to(self.device)
        img = img.float()  &#47&#47 uint8 to fp16/32
        img /= 255.0  &#47&#47 0 - 255 to 0.0 - 1.0
        <a id="change">if img.ndimension() == 3</a>:
            img<a id="change"> = </a>img.unsqueeze(0)
        return img
    
    def _postprocess(self, img, origimg, pred, conf_thres, iou_thres, height, width):</code></pre><h3>After Change</h3><pre><code class='java'>
            img = letterbox(img, new_shape=imgsz)[0]
            pp_imgs.append(img)
        pp_imgs = np.array(pp_imgs)
        pp_imgs<a id="change"> = </a>pp_imgs.transpose(0, 3, 1, 2)
        pp_imgs = torch.from_numpy(pp_imgs).to(self.device)
        pp_imgs = <a id="change">pp_imgs.float()</a>  &#47&#47 uint8 to fp16/32
        pp_imgs<a id="change"> /= </a>255.0  &#47&#47 0 - 255 to 0.0 - 1.0
        return pp_imgs
        
    def _postprocess(self, imgs, origimgs, pred, conf_thres, iou_thres):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/elyha7/yoloface/commit/794e25df420f25fd7937e0af41fefd7bf184fad6#diff-f1b9b6b0dc58eb8986b1bfad93f0267affbcc78f5f1f1a553babeec2b36a8780L63' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95548191</div><div id='project'> Project Name: elyha7/yoloface</div><div id='commit'> Commit Name: 794e25df420f25fd7937e0af41fefd7bf184fad6</div><div id='time'> Time: 2022-01-10</div><div id='author'> Author: artemrebrikov@gmail.com</div><div id='file'> File Name: face_detector.py</div><div id='m_class'> M Class Name: YoloDetector</div><div id='n_method'> N Class Name: YoloDetector</div><div id='m_method'> M Method Name: _preprocess(2)</div><div id='n_method'> N Method Name: _preprocess(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: face_detector.py</div><div id='n_file'> N File Name: face_detector.py</div><div id='m_start'> M Start Line: 67</div><div id='m_end'> M End Line: 83</div><div id='n_start'> N Start Line: 66</div><div id='n_end'> N End Line: 82</div><BR>