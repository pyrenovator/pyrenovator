<html><h3>Pattern ID :1700
</h3><img src='8144508.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def forward(self, feats, coors, mask = None):
        h = self.heads
        <a id="change">q</a><a id="change">, k, v</a> = self.to_qkv(feats).chunk(3, dim = -1)
        q<a id="change">, k, v = </a><a id="change">map(</a>lambda t: rearrange(t, &quotb n (h d) -&gt; b h n d&quot, h = h), (<a id="change">q</a><a id="change">, k, v</a>)<a id="change">)</a>

        if exists(mask):
            mask = rearrange(mask, &quotb n -&gt; b () n ()&quot)
            k.masked_fill_(~mask, -torch.finfo(k.dtype).max)</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward(self, x, queries, mask = None):
        induced = self.attn1(queries, x, mask = mask)
        out     = self.attn2(x, induced)
        return out<a id="change">, 0</a>

class EquivariantAttention(nn.Module):
    def __init__(
        self,</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 6</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/en-transformer/commit/6bd1817d780502d24a2515e850c9cd1600f24642#diff-f288a1692c1c58a186cb9ac763046f632757db1647271b97852e59e3fc5696b9L132' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8144508</div><div id='project'> Project Name: lucidrains/en-transformer</div><div id='commit'> Commit Name: 6bd1817d780502d24a2515e850c9cd1600f24642</div><div id='time'> Time: 2021-06-04</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: en_transformer/en_transformer.py</div><div id='m_class'> M Class Name: GlobalLinearAttention</div><div id='n_method'> N Class Name: GlobalLinearAttention</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: en_transformer/en_transformer.py</div><div id='n_file'> N File Name: en_transformer/en_transformer.py</div><div id='m_start'> M Start Line: 132</div><div id='m_end'> M End Line: 151</div><div id='n_start'> N Start Line: 160</div><div id='n_end'> N End Line: 162</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def forward(self, feats, coors, mask = None):
        h = self.heads
        q, k, v = self.to_qkv(feats).chunk(3, dim = -1)
        q<a id="change">, k, v = </a><a id="change">map(</a>lambda t: rearrange(t, &quotb n (h d) -&gt; b h n d&quot, h = h), (q<a id="change">, k, v</a>)<a id="change">)</a>

        if exists(mask):
            mask = rearrange(mask, &quotb n -&gt; b () n ()&quot)
            k.masked_fill_(~mask, -torch.finfo(k.dtype).max)</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward(self, x, queries, mask = None):
        induced = self.attn1(queries, x, mask = mask)
        out     = self.attn2(x, induced)
        return out<a id="change">, 0</a>

class EquivariantAttention(nn.Module):
    def __init__(
        self,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/en-transformer/commit/6bd1817d780502d24a2515e850c9cd1600f24642#diff-f288a1692c1c58a186cb9ac763046f632757db1647271b97852e59e3fc5696b9L131' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8144509</div><div id='project'> Project Name: lucidrains/en-transformer</div><div id='commit'> Commit Name: 6bd1817d780502d24a2515e850c9cd1600f24642</div><div id='time'> Time: 2021-06-04</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: en_transformer/en_transformer.py</div><div id='m_class'> M Class Name: GlobalLinearAttention</div><div id='n_method'> N Class Name: GlobalLinearAttention</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: en_transformer/en_transformer.py</div><div id='n_file'> N File Name: en_transformer/en_transformer.py</div><div id='m_start'> M Start Line: 132</div><div id='m_end'> M End Line: 151</div><div id='n_start'> N Start Line: 160</div><div id='n_end'> N End Line: 162</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        total_len = mem.shape[2] + lmem.shape[2] + self.seq_len
        pos_emb = self.pos_emb[:, (self.seq_len - t):total_len]

        <a id="change">next_mem</a> = []
        <a id="change">next_lmem</a> = []

        mem_iter, lmem_iter = map(iterate_tensor, (mem, lmem))

        for ind, (attn, ff) in enumerate(zip(self.attn_layers, self.ff_layers)):
            layer_num = ind + 1
            use_memory = layer_num in self.memory_layers

            memories = None
            if use_memory:
                memories = (next(mem_iter), next(lmem_iter))

            x, (mem_out, lmem_out) = attn(x, memories = memories, calc_memory = use_memory, input_mask = mask, pos_emb = pos_emb)
            x, = ff(x)

            if use_memory:
                next_mem.append(mem_out)
                next_lmem.append(lmem_out)

        out = self.to_logits(x)

        next_mem<a id="change">, next_lmem</a> = map(torch.stack, (next_mem, next_lmem))
        next_mem<a id="change">, next_lmem = </a><a id="change">map(</a>torch.detach, (next_mem<a id="change">, next_lmem</a>)<a id="change">)</a>

        return out, Memory(short = next_mem, long = next_lmem)
</code></pre><h3>After Change</h3><pre><code class='java'>
        for ind, (attn, ff) in enumerate(zip(self.attn_layers, self.ff_layers)):
            layer_num = ind + 1
            use_memory = layer_num in self.memory_layers
            memories = map(next, (mem_iter<a id="change">, lmem_iter</a>)) if use_memory else None

            if use_memory:
                hiddens.append(x)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/memory-transformer-xl/commit/cbabe1ae6fa311092a9d0a88116c079a5ad8d790#diff-a0bf60cdef5ad628f9a83f0454c9c66a535268529835e839bb54088805dba051L253' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8144504</div><div id='project'> Project Name: lucidrains/memory-transformer-xl</div><div id='commit'> Commit Name: cbabe1ae6fa311092a9d0a88116c079a5ad8d790</div><div id='time'> Time: 2020-07-22</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='m_class'> M Class Name: MemoryTransformerXL</div><div id='n_method'> N Class Name: MemoryTransformerXL</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='n_file'> N File Name: memory_transformer_xl/memory_transformer_xl.py</div><div id='m_start'> M Start Line: 255</div><div id='m_end'> M End Line: 296</div><div id='n_start'> N Start Line: 306</div><div id='n_end'> N End Line: 345</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 split heads

        q<a id="change">, k, v = </a><a id="change">map(</a>lambda t: rearrange(t, &quotb n (h d) -&gt; (b h) n d&quot, h = h), (q<a id="change">, k, v</a>)<a id="change">)</a>

        &#47&#47 rotary embeddings

        positions = self.rotary_emb(n, device = device)</code></pre><h3>After Change</h3><pre><code class='java'>
        

        n, device, h = x.shape[1], x.device, self.heads
        q, k, v = (self.to_q(x)<a id="change">, *self.to_kv(x).chunk(2, dim = -1)</a>)

        &#47&#47 pre layernorm
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/palm-pytorch/commit/100b3ee7d7209056a2e3ee20def4eecbd16dd4b6#diff-a5ba5faa1b15b08f73db45e9342fcadc130f44195e1a0b082fa3d9b3e025885cL97' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8144488</div><div id='project'> Project Name: lucidrains/palm-pytorch</div><div id='commit'> Commit Name: 100b3ee7d7209056a2e3ee20def4eecbd16dd4b6</div><div id='time'> Time: 2022-04-04</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: palm_pytorch/palm_pytorch.py</div><div id='m_class'> M Class Name: Attention</div><div id='n_method'> N Class Name: Attention</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: palm_pytorch/palm_pytorch.py</div><div id='n_file'> N File Name: palm_pytorch/palm_pytorch.py</div><div id='m_start'> M Start Line: 107</div><div id='m_end'> M End Line: 145</div><div id='n_start'> N Start Line: 108</div><div id='n_end'> N End Line: 149</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        super().__init__()
        self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))

        <a id="change">get_cross_attn</a> = lambda: PreNorm(latent_dim, Attention(latent_dim, dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = dim)
        <a id="change">get_cross_ff</a> = lambda: PreNorm(latent_dim, FeedForward(latent_dim))
        get_latent_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, heads = latent_heads, dim_head = latent_dim_head))
        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim))

        get_cross_attn<a id="change">, get_cross_ff, get_latent_attn, get_latent_ff = </a><a id="change">map(</a>cache_fn, (get_cross_attn<a id="change">, get_cross_ff, get_latent_attn, get_latent_ff</a>)<a id="change">)</a>

        self.layers = nn.ModuleList([])
        for i in range(depth):
            should_cache = i &gt; 0 and weight_tie_layers</code></pre><h3>After Change</h3><pre><code class='java'>

        get_latent_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, heads = latent_heads, dim_head = latent_dim_head))
        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim))
        get_latent_attn<a id="change">, get_latent_ff</a> = map(cache_fn, (get_latent_attn, get_latent_ff))

        self.layers = nn.ModuleList([])
        cache_args = {&quot_cache&quot: weight_tie_layers}</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/perceiver-pytorch/commit/dc530de88e6035a2f08d7e35ce23e57abe8371bd#diff-d109c4942e7a2f6d51cd8e55cbab114efc5eb0b0ce37d8e682449385df84ec13L106' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8144473</div><div id='project'> Project Name: lucidrains/perceiver-pytorch</div><div id='commit'> Commit Name: dc530de88e6035a2f08d7e35ce23e57abe8371bd</div><div id='time'> Time: 2021-08-30</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: perceiver_pytorch/perceiver_io.py</div><div id='m_class'> M Class Name: PerceiverIO</div><div id='n_method'> N Class Name: PerceiverIO</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: perceiver_pytorch/perceiver_io.py</div><div id='n_file'> N File Name: perceiver_pytorch/perceiver_io.py</div><div id='m_start'> M Start Line: 126</div><div id='m_end'> M End Line: 152</div><div id='n_start'> N Start Line: 125</div><div id='n_end'> N End Line: 143</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        k_img, v_img = map(lambda t: F.unfold(t, kernel_size, padding = padding, dilation = dilation), (k_img, v_img))
        k_img, v_img = map(lambda t: rearrange(t, &quotb (d j) i -&gt; b i j d&quot, j = kernel_size ** 2), (k_img, v_img))

        k_text<a id="change">, v_text = </a><a id="change">map(</a>lambda t: repeat(t, &quotb j d -&gt; b i j d&quot, i = img_seq_len), (k_text<a id="change">, v_text</a>)<a id="change">)</a>

        &#47&#47 let image attend to all of text

        k_img = torch.cat((k_text, k_img), dim = 2)</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 aggregate

        attn_image_to_text, attn_image = attn[..., :text_len]<a id="change">, attn[..., text_len:]</a>

        out_image_to_image = einsum(&quotb i j, b i j d -&gt; b i d&quot, attn_image, v_img)
        out_image_to_text = einsum(&quotb i j, b j d -&gt; b i d&quot, attn_image_to_text, v_text)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/dalle-pytorch/commit/f14a313431e9072bef9a8219ea3d99d7683ada06#diff-58807b9968341c05944b3adf3de1bfcd9d33660121a78f3ef013fd0e9bf16fdfL90' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8144480</div><div id='project'> Project Name: lucidrains/dalle-pytorch</div><div id='commit'> Commit Name: f14a313431e9072bef9a8219ea3d99d7683ada06</div><div id='time'> Time: 2021-04-22</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: dalle_pytorch/attention.py</div><div id='m_class'> M Class Name: SparseConvCausalAttention</div><div id='n_method'> N Class Name: SparseConvCausalAttention</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dalle_pytorch/attention.py</div><div id='n_file'> N File Name: dalle_pytorch/attention.py</div><div id='m_start'> M Start Line: 116</div><div id='m_end'> M End Line: 168</div><div id='n_start'> N Start Line: 94</div><div id='n_end'> N End Line: 173</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        heads, r = self.heads, self.reduction_ratio

        q, k, v = self.to_qkv(x).chunk(3, dim = 1)
        k<a id="change">, v = </a><a id="change">map(</a>lambda t: reduce(t, &quotb c (h r1) (w r2) -&gt; b c h w&quot, &quotmean&quot, r1 = r, r2 = r), (k<a id="change">, v</a>)<a id="change">)</a>

        q, k, v = map(lambda t: rearrange(t, &quotb (h c) x y -&gt; (b h) (x y) c&quot, h = heads), (q, k, v))

        sim = einsum(&quotb i d, b j d -&gt; b i j&quot, q, k) * self.scale</code></pre><h3>After Change</h3><pre><code class='java'>
        h, w = x.shape[-2:]
        heads = self.heads

        q, k, v = (self.to_q(x)<a id="change">, *self.to_kv(x).chunk(2, dim = 1)</a>)
        q, k, v = map(lambda t: rearrange(t, &quotb (h c) x y -&gt; (b h) (x y) c&quot, h = heads), (q, k, v))

        sim = einsum(&quotb i d, b j d -&gt; b i j&quot, q, k) * self.scale</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/segformer-pytorch/commit/7ab3c29d960d9dea11417a98a7922b2a624961c1#diff-0f38911db3a61122a663429b4c2da58a59fab732a392fa02e0d929736c6ff309L47' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8144513</div><div id='project'> Project Name: lucidrains/segformer-pytorch</div><div id='commit'> Commit Name: 7ab3c29d960d9dea11417a98a7922b2a624961c1</div><div id='time'> Time: 2021-06-18</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: segformer_pytorch/segformer_pytorch.py</div><div id='m_class'> M Class Name: EfficientSelfAttention</div><div id='n_method'> N Class Name: EfficientSelfAttention</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: segformer_pytorch/segformer_pytorch.py</div><div id='n_file'> N File Name: segformer_pytorch/segformer_pytorch.py</div><div id='m_start'> M Start Line: 49</div><div id='m_end'> M End Line: 52</div><div id='n_start'> N Start Line: 49</div><div id='n_end'> N End Line: 51</div><BR>