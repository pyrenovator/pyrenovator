<html><h3>Pattern ID :24364
</h3><img src='75698609.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    expansion = 1

    def __init__(self, in_planes, planes, stride=1):
        <a id="change">super(</a>BasicBlock, self<a id="change">)</a>.__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)</code></pre><h3>After Change</h3><pre><code class='java'>
        super().__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        <a id="change">if </a><a id="change">groups != 1 or base_width != 64</a>:
            raise ValueError("BasicBlock only supports groups=1 and base_width=64")
        if dilation &gt; 1:
            raise NotImplementedError("Dilation &gt; 1 not supported in BasicBlock")
        &#47&#47 Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = norm_layer(planes)
        self.relu<a id="change"> = </a>nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = norm_layer(planes)
        self.downsample = downsample</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/decile-team/cords/commit/931f2dbbb853ea9beb25e72b69e473c0365e60ca#diff-580e0813097d831f8f05a21aa69ba247d29cf468b66c336c92f2ad30fa9aa7d9L17' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 75698609</div><div id='project'> Project Name: decile-team/cords</div><div id='commit'> Commit Name: 931f2dbbb853ea9beb25e72b69e473c0365e60ca</div><div id='time'> Time: 2023-02-04</div><div id='author'> Author: 61333497+krishnatejakk@users.noreply.github.com</div><div id='file'> File Name: cords/utils/models/resnet.py</div><div id='m_class'> M Class Name: BasicBlock</div><div id='n_method'> N Class Name: BasicBlock</div><div id='m_method'> M Method Name: __init__(9)</div><div id='n_method'> N Method Name: __init__(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: cords/utils/models/resnet.py</div><div id='n_file'> N File Name: cords/utils/models/resnet.py</div><div id='m_start'> M Start Line: 17</div><div id='m_end'> M End Line: 31</div><div id='n_start'> N Start Line: 66</div><div id='n_end'> N End Line: 90</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        return x

    def reorder_incremental_state(self, incremental_state, new_order):
        <a id="change">super()</a>.reorder_incremental_state(incremental_state, new_order)
        cached_state = utils.get_incremental_state(self, incremental_state, &quotcached_state&quot)
        if cached_state is None:
            return</code></pre><h3>After Change</h3><pre><code class='java'>
        return [state_i.index_select(0, new_order) for state_i in state]

    def reorder_incremental_state(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]], new_order):
        <a id="change">if incremental_state is None</a><a id="change"> or len(incremental_state) == 0</a>:
            return
        prev_hiddens, prev_cells, input_feed = self.get_cached_state(incremental_state)
        cached_state = (prev_hiddens, prev_cells, [input_feed])
        new_state = [self.reorder_state(state, new_order) for state in cached_state]
        prev_hiddens_tensor = torch.stack(new_state[0])
        prev_cells_tensor<a id="change"> = </a>torch.stack(new_state[1])
        cached_state_new = torch.jit.annotate(
            Dict[str, Optional[Tensor]],
            {"prev_hiddens": prev_hiddens_tensor, "prev_cells": prev_cells_tensor, "input_feed": new_state[2][0]})</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/kssteven418/i-bert/commit/57526c63433c0b1c997fc91c0881867532567266#diff-6ce7c0d4f0fb1f8f34940039a8bb42a6d902a8894dd18517bee43dac3fc09971L498' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 75698599</div><div id='project'> Project Name: kssteven418/i-bert</div><div id='commit'> Commit Name: 57526c63433c0b1c997fc91c0881867532567266</div><div id='time'> Time: 2020-04-16</div><div id='author'> Author: xfrui@fb.com</div><div id='file'> File Name: fairseq/models/lstm.py</div><div id='m_class'> M Class Name: LSTMDecoder</div><div id='n_method'> N Class Name: LSTMDecoder</div><div id='m_method'> M Method Name: reorder_incremental_state(3)</div><div id='n_method'> N Method Name: reorder_incremental_state(3)</div><div id='m_parent_class'> M Parent Class: FairseqIncrementalDecoder</div><div id='n_parent_class'> N Parent Class: FairseqIncrementalDecoder</div><div id='m_file'> M File Name: fairseq/models/lstm.py</div><div id='n_file'> N File Name: fairseq/models/lstm.py</div><div id='m_start'> M Start Line: 498</div><div id='m_end'> M End Line: 513</div><div id='n_start'> N Start Line: 538</div><div id='n_end'> N End Line: 550</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    
    def __init__(self, *, epsilon, delta, sensitivity):
        <a id="change">super()</a>.__init__(epsilon=epsilon, delta=delta)
        self.sensitivity = self._check_sensitivity(sensitivity)
        self._scale = np.sqrt(2 * np.log(1.25 / self.delta)) * self.sensitivity / self.epsilon
</code></pre><h3>After Change</h3><pre><code class='java'>
        self.delta = float(delta)
        self.sensitivity = float(sensitivity)
        &#47&#47 special requirements for epsilon and delta in Gaussian
        <a id="change">if </a><a id="change">epsilon == 0 or delta == 0</a>:
            raise ValueError("Neither Epsilon nor Delta can be zero")
        self.special_check_for_params()
        self._scale = (
            np.sqrt(2 * np.log(1.25 / self.delta)) * self.sensitivity / self.epsilon
        )
        self._rng<a id="change"> = </a>secrets.SystemRandom()

    def special_check_for_params(self):
        if self.epsilon &gt; 1.0:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/fedml-ai/fedml/commit/b2371aa28ab7bfeffdb0f94705dbb444378faf7e#diff-7a5e94d1e7cbb0f8575dfdcba6bac391852e0e8fd723033975c6eebea4352ba1L60' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 75698603</div><div id='project'> Project Name: fedml-ai/fedml</div><div id='commit'> Commit Name: b2371aa28ab7bfeffdb0f94705dbb444378faf7e</div><div id='time'> Time: 2022-07-24</div><div id='author'> Author: sshan0731@hotmail.com</div><div id='file'> File Name: python/fedml/core/differential_privacy/mechanisms/gaussian.py</div><div id='m_class'> M Class Name: Gaussian</div><div id='n_method'> N Class Name: Gaussian</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: DPMechanism</div><div id='m_file'> M File Name: python/fedml/core/differential_privacy/mechanisms/gaussian.py</div><div id='n_file'> N File Name: python/fedml/core/differential_privacy/mechanisms/gaussian.py</div><div id='m_start'> M Start Line: 61</div><div id='m_end'> M End Line: 62</div><div id='n_start'> N Start Line: 36</div><div id='n_end'> N End Line: 47</div><BR>