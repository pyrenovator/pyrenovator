<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            z_all = torch.cat([z_i, z_j], 0)
            z_j = z_j[:num_contexts]
        else:
            z_all<a id="change"> = </a><a id="change">torch.cat(</a>(z_i<a id="change">, z_j</a>), <a id="change">0</a><a id="change">)</a>

        &#47&#47 zi and zj are both matrices of dim (n x c), since they are z vectors of dim c for every element in the batch
        &#47&#47 This einsum is constructing a vector of size n, where the nth element is a sum over C of the NCth elements
        &#47&#47 That is to say, the nth element is a dot product between the Nth C-dim vector of each matrix</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 I think if we include z_i here, we should use a mask (as in SymmetricContrastiveLoss) to subtract the column
        &#47&#47 on which the similarity is calculated with an image itself?
        mask<a id="change"> = </a><a id="change">torch.eye(num_contexts).to(</a>self.device<a id="change">)</a>

        &#47&#47 Simplify the code
        z_all = torch.cat([z_i, z_j], 0)
        if num_targets &gt; num_contexts:
            z_j = z_j[:num_contexts]

        &#47&#47 zi and zj are both matrices of dim (n x c), since they are z vectors of dim c for every element in the batch
        &#47&#47 This einsum is constructing a vector of size n, where the nth element is a sum over C of the NCth elements
        &#47&#47 That is to say, the nth element is a dot product between the Nth C-dim vector of each matrix
        sim_ij = torch.einsum(&quotnc,nc-&gt;n&quot, [z_i, z_j]).unsqueeze(-1)  &#47&#47 Nx1

        &#47&#47 Calculate similarity of current batch&quots images and other images.
        sim_ik = torch.einsum(&quotnc,ck-&gt;nk&quot, [z_i, z_all.T])
        sim_ik<a id="change"> = </a>sim_ik - mask  &#47&#47 TODO: Check if the diagonal line are all 1s.

        logits = torch.cat((sim_ij, sim_ik), 1)
        logits /= self.temp</code></pre>