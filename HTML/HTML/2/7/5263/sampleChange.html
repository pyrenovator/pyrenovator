<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                                persistent_workers=True)

        loss_record = []
        num_batches_per_epoch<a id="change"> = </a>math.ceil(len(dataset)<a id="change">/</a>self.batch_size)
        assert len(dataloader) == num_batches_per_epoch, \
            "num_batches_per_dataset doesn&quott represent actual length of dataloader"
        assert num_batches_per_epoch &gt; 0, \
            f"num_batches_per_epoch is incorrectly 0: len(ds)={len(dataset)}, bs={self.batch_size}"

        if training_batches is None:
            training_batches = num_batches_per_epoch*training_epochs
        if training_epochs is None:
            training_epochs<a id="change"> = </a><a id="change">math.ceil(</a>training_batches<a id="change">/</a>num_batches_per_epoch<a id="change">)</a>

        if self.scheduler_cls is not None:
            if self.scheduler_cls in [CosineAnnealingLR, LinearWarmupCosine]:
                self.scheduler = self.scheduler_cls(self.optimizer, training_epochs, **to_dict(self.scheduler_kwargs))</code></pre><h3>After Change</h3><pre><code class='java'>
                logger.record(&quotbatches_trained&quot, batches_trained)
                logger.dump(step=batches_trained)
                batches_trained += 1
                if <a id="change">(training_batches is not None and batches_trained &gt;= training_batches)</a>:
                    logging.info(f"Breaking out of training in epoch {epochs_trained} because max batches "
                                 f"value of {training_batches} has been reached")
                    training_complete = True
                    break

            assert batches_trained &gt; 0, \
                "went through training loop with no batches---empty dataset?"
            <a id="change">if </a>epochs_trained == 0:
                &#47&#47 we infer the size of the dataset from the number of
                &#47&#47 iterations through the loop the first time
                logging.debug(f"Dataset size is {batches_trained}")</code></pre>