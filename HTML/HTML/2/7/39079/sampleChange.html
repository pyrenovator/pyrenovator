<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            raise ValueError(
                "&quotmax_grad_norm&quot is not supported as an optimizer parameter, please switch to using the deepspeed parameter &quotgradient_clipping&quot see: https://www.deepspeed.ai/docs/config-json/&#47&#47gradient-clipping for more details"
            )
        if <a id="change">self.optimizer_name() == ADAM_OPTIMIZER</a>:
            if self.zero_cpu_offload():
                optimizer = torch.optim.Adam(model_parameters, **optimizer_parameters)
            else:
                from apex.optimizers.fused_adam import FusedAdam
                optimizer<a id="change"> = </a>FusedAdam(model_parameters, **optimizer_parameters)
        elif self.optimizer_name() == DEEPSPEED_ADAM:
            from deepspeed.ops.adam import DeepSpeedCPUAdam
            optimizer = DeepSpeedCPUAdam(model_parameters, **optimizer_parameters)</code></pre><h3>After Change</h3><pre><code class='java'>
                "&quotmax_grad_norm&quot is not supported as an optimizer parameter, please switch to using the deepspeed parameter &quotgradient_clipping&quot see: https://www.deepspeed.ai/docs/config-json/&#47&#47gradient-clipping for more details"
            )

        if <a id="change">self.optimizer_name() == ADAM_OPTIMIZER</a>:
            torch_adam = optimizer_parameters.pop(TORCH_ADAM_PARAM, False)
            adam_w_mode = optimizer_parameters.pop(ADAM_W_MODE_PARAM, True)

            &#47&#47 zero-offload  torch-adam  adam_w_mode optimizer
            &#47&#47 T|F           T           T           torch.optim.AdamW
            &#47&#47 T|F           T           F           torch.optim.Adam
            &#47&#47 T             F           T|F         DeepSpeedCPUAdam(adam_w_mode)
            &#47&#47 F             F           T|F         FusedAdam(adam_w_mode)
            if torch_adam and adam_w_mode:
                optimizer = torch.optim.AdamW(model_parameters, **optimizer_parameters)
            elif torch_adam and not adam_w_mode:
                optimizer = torch.optim.Adam(model_parameters, **optimizer_parameters)
            elif self.zero_cpu_offload() and not torch_adam:
                from deepspeed.ops.adam import DeepSpeedCPUAdam
                optimizer = DeepSpeedCPUAdam(model_parameters,
                                             **optimizer_parameters,
                                             adamw_mode=adam_w_mode)
            elif not self.zero_cpu_offload() and not torch_adam:
                from apex.optimizers.fused_adam import FusedAdam
                optimizer_parameters[ADAM_W_MODE_PARAM]<a id="change"> = </a>adam_w_mode
                optimizer<a id="change"> = </a>FusedAdam(model_parameters, **optimizer_parameters)

        elif self.optimizer_name() == LAMB_OPTIMIZER:
            from deepspeed.ops.lamb import FusedLamb</code></pre>