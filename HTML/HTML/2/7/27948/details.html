<html><h3>Pattern ID :27948
</h3><img src='82875507.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    if n_aa &lt;= 3:
        new_coords[:, :, :n_aa] = predicted
    &#47&#47 fill backbone (N, C_alpha, C, C_beta)
    elif <a id="change">n_aa == 4</a>:
        new_coords[:, :, :3] = <a id="change">predicted[:, :, :-1]</a>
        new_coords[:, :, 4]<a id="change"> = </a>predicted[:, :, -1]

    &#47&#47 generate sidechain
    for s,seq in enumerate(seqs): </code></pre><h3>After Change</h3><pre><code class='java'>
        * padding: int. padding token. same as in sidechainnet: 20
        Outputs: whole coordinates of shape (batch, L, 14, 3)
    
    atom_mask = <a id="change">atom_mask.bool()</a>.cpu().detach()
    cum_atom_mask<a id="change"> = </a>atom_mask.cumsum(dim=-1).tolist()

    device = backbones.device
    batch, length = backbones.shape[0], backbones.shape[1] // cum_atom_mask[-1]</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/alphafold2/commit/af52b14b8943a19879fb5d9c6829f0d64d4717f9#diff-f9b2e05be1f80257f9a80ec7cc1290e3337a54fcc62aeee31b23a4cb827f12efL659' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 82875507</div><div id='project'> Project Name: lucidrains/alphafold2</div><div id='commit'> Commit Name: af52b14b8943a19879fb5d9c6829f0d64d4717f9</div><div id='time'> Time: 2021-05-25</div><div id='author'> Author: ericalcaide1@gmail.com</div><div id='file'> File Name: alphafold2_pytorch/utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: sidechain_container(5)</div><div id='n_method'> N Method Name: sidechain_container(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: alphafold2_pytorch/utils.py</div><div id='n_file'> N File Name: alphafold2_pytorch/utils.py</div><div id='m_start'> M Start Line: 663</div><div id='m_end'> M End Line: 703</div><div id='n_start'> N Start Line: 659</div><div id='n_end'> N End Line: 697</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        assert self.num_features == H * W

        &#47&#47 left/right shape: [n, h/2, w, c, r] or [n, h, w/2, c, r]
        <a id="change">if self.split_dim == "h"</a>:
            split_val = self.out_shape[0]
            left = x[:, split_val:]
            right<a id="change"> = </a>x[:, :split_val]
        else:
            split_val = self.out_shape[1]
            left = <a id="change">x[:, :, split_val:]</a>
            right = x[:, :, :split_val]

        left_max = torch.max(left, dim=3, keepdim=True)[0]
        left_prob = torch.exp(left - left_max)</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 Apply dropout: Set random sum node children to 0 (-inf in log domain)
        if self.dropout &gt; 0.0 and self.training:
            dropout_indices<a id="change"> = </a><a id="change">self._bernoulli_dist.sample(x.shape).bool()</a>
            x[dropout_indices] = np.NINF

        &#47&#47 Check if padding to next power of 2 is necessary
        if self.in_features != x.shape[1]:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/braun-steven/simple-einet/commit/a4eb63bd492b9a22102ddc82e107efac0bdf04ee#diff-ce4e03668776ccc1157c2698e9c1339324cd0e65a093bded8852f0465c7585e4L68' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 82875428</div><div id='project'> Project Name: braun-steven/simple-einet</div><div id='commit'> Commit Name: a4eb63bd492b9a22102ddc82e107efac0bdf04ee</div><div id='time'> Time: 2022-01-07</div><div id='author'> Author: steven.lang.mz@gmail.com</div><div id='file'> File Name: simple_einet/einsum_layer.py</div><div id='m_class'> M Class Name: EinsumLayer</div><div id='n_method'> N Class Name: EinsumLayer</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: AbstractLayer</div><div id='n_parent_class'> N Parent Class: AbstractLayer</div><div id='m_file'> M File Name: simple_einet/einsum_layer.py</div><div id='n_file'> N File Name: simple_einet/einsum_layer.py</div><div id='m_start'> M Start Line: 85</div><div id='m_end'> M End Line: 111</div><div id='n_start'> N Start Line: 91</div><div id='n_end'> N End Line: 122</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 help auto-solve a frequent area of confusion around input masks in auto-regressive
        &#47&#47 if user supplies a mask that is only off by one from the source sequence, resolve it for them
        mask = kwargs.get(&quotmask&quot, None)
        <a id="change">if </a>mask is not None and <a id="change">mask.shape[1] == x.shape[1]</a>:
            mask = <a id="change">mask[:, :-1]</a>
            kwargs[&quotmask&quot]<a id="change"> = </a>mask

        out = self.net(xi, **kwargs)
        loss = F.cross_entropy(out.transpose(1, 2), xo, ignore_index = self.ignore_index)</code></pre><h3>After Change</h3><pre><code class='java'>
            rand[:, 0] = -torch.finfo(rand.dtype).max &#47&#47 first token should not be masked out
            num_mask = min(int(seq * self.mask_prob), seq - 1)
            indices = rand.topk(num_mask, dim = -1).indices
            mask<a id="change"> = </a>~<a id="change">torch.zeros_like(inp).scatter(1, indices, 1.).bool()</a>
            kwargs.update(context_mask = mask)

        out = self.net(inp, **kwargs)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/x-transformers/commit/595a4745d532c20b8ebd310256c342e946a4cef7#diff-b70a3173d353a448f8f499d8b685672d9bccac7b78bc1d36b77f84afe33be694L106' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 82875418</div><div id='project'> Project Name: lucidrains/x-transformers</div><div id='commit'> Commit Name: 595a4745d532c20b8ebd310256c342e946a4cef7</div><div id='time'> Time: 2022-11-02</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: x_transformers/autoregressive_wrapper.py</div><div id='m_class'> M Class Name: AutoregressiveWrapper</div><div id='n_method'> N Class Name: AutoregressiveWrapper</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: x_transformers/autoregressive_wrapper.py</div><div id='n_file'> N File Name: x_transformers/autoregressive_wrapper.py</div><div id='m_start'> M Start Line: 107</div><div id='m_end'> M End Line: 118</div><div id='n_start'> N Start Line: 122</div><div id='n_end'> N End Line: 142</div><BR>