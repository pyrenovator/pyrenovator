<html><h3>Pattern ID :23387
</h3><img src='73528665.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        parsed_face = torch.sum(parsed_face, dim=1, keepdim=True)

        <a id="change">return </a>parsed_face

    def forward(self, x):
        H, W = x.size()[2:]</code></pre><h3>After Change</h3><pre><code class='java'>

        parsed_face = encode_segmentation_rgb_batch(parsed_face)

        parsed_face<a id="change"> = </a><a id="change">torch.where(torch.sum(parsed_face, dim=[1, 2, 3], keepdim=True) &gt; 5000</a>, parsed_face,
                                  torch.zeros_like(parsed_face)<a id="change">)</a>

        ignore_mask_ids = torch.sum(parsed_face, dim=[1, 2, 3]) == 0

        parsed_face = parsed_face.float().mul_(1 / 255.0)

        parsed_face = F.interpolate(parsed_face, size=(crop_size, crop_size),
                                    mode="bilinear")

        parsed_face = torch.sum(parsed_face, dim=1, keepdim=True)

        <a id="change">return </a>parsed_face<a id="change">, ignore_mask_ids</a>

    def forward(self, x):
        H, W = x.size()[2:]
        feat_res8, feat_cp8, feat_cp16 = self.cp(x)  &#47&#47 here return res3b1 feature</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/mike9251/simswap-inference-pytorch/commit/873c64711778578824a6c72ead2a283938fd9240#diff-ea34d7f700d7324f5cdbfabb680e4b8e80f9cc21c6ce18116147db3355ffc836L250' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 73528665</div><div id='project'> Project Name: mike9251/simswap-inference-pytorch</div><div id='commit'> Commit Name: 873c64711778578824a6c72ead2a283938fd9240</div><div id='time'> Time: 2022-07-12</div><div id='author'> Author: soapbox92@gmail.com</div><div id='file'> File Name: src/PostProcess/ParsingModel/model.py</div><div id='m_class'> M Class Name: BiSeNet</div><div id='n_method'> N Class Name: BiSeNet</div><div id='m_method'> M Method Name: get_mask(3)</div><div id='n_method'> N Method Name: get_mask(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/PostProcess/ParsingModel/model.py</div><div id='n_file'> N File Name: src/PostProcess/ParsingModel/model.py</div><div id='m_start'> M Start Line: 250</div><div id='m_end'> M End Line: 263</div><div id='n_start'> N Start Line: 251</div><div id='n_end'> N End Line: 265</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 add up to 1.
    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  &#47&#47 (..., seq_len_q, seq_len_k)
    output = tf.matmul(attention_weights, v)  &#47&#47 (..., seq_len_q, depth_v)
    <a id="change">return </a>output


class MultiHeadAttention(tf.keras.layers.Layer):</code></pre><h3>After Change</h3><pre><code class='java'>

    scores = tf.matmul(query, tf.transpose(key, perm=[0, 1, 3, 2])) / math.sqrt(query.shape[-1])
    if mask is not None:
        scores<a id="change"> = </a><a id="change">tf.where(mask == 0</a>, -1e9, scores<a id="change">)</a>
    p_attn = tf.nn.softmax(scores, axis=-1)
    <a id="change">return </a>tf.matmul(p_attn, value)<a id="change">, p_attn</a>


class PositionwiseFeedForward(layers.Layer, NestedObject):
     Position-wise Feed-Forward Network </code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/mindee/doctr/commit/9530f81d15395006b4844299236bdadba11c1dde#diff-e1106a3628d5365c5fe2e4fec7304ea8e40c58a5439f23d8f20513e722955178L70' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 73528667</div><div id='project'> Project Name: mindee/doctr</div><div id='commit'> Commit Name: 9530f81d15395006b4844299236bdadba11c1dde</div><div id='time'> Time: 2022-07-01</div><div id='author'> Author: felixdittrich92@gmail.com</div><div id='file'> File Name: doctr/models/recognition/transformer/tensorflow.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: scaled_dot_product_attention(4)</div><div id='n_method'> N Method Name: scaled_dot_product_attention(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: doctr/models/recognition/transformer/tensorflow.py</div><div id='n_file'> N File Name: doctr/models/recognition/transformer/tensorflow.py</div><div id='m_start'> M Start Line: 88</div><div id='m_end'> M End Line: 99</div><div id='n_start'> N Start Line: 63</div><div id='n_end'> N End Line: 67</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def backward(self, ctx, dy):
        x, = ctx.saved_tensors
        zeros = torch.zeros_like(x)
        <a id="change">return </a>torch.where(x &gt; 0, dy * x * 2, zeros)

triton_relu_squared = _relu_squared.apply
</code></pre><h3>After Change</h3><pre><code class='java'>
        x, w, c, mask = ctx.saved_tensors
        zeros = torch.zeros_like(dy)
        dy = torch.where(c &gt; 0, dy * c * 2, zeros)
        db<a id="change"> = </a><a id="change">torch.where(c &gt; 0</a>, dy, zeros<a id="change">)</a>
        dx = dy @ w.t()
        dw = x.transpose(-1, -2) @ dy
        <a id="change">return </a>dx<a id="change">, dw, db</a>

triton_relu_squared = _relu_squared.apply

def fused_relu_squared(x, w, b, use_triton = False):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/triton-transformer/commit/72efae6cb8b3c62b501dd5bcf1ab8cfbf684e0cb#diff-efb464e8953d3171bdd3355d448443781f005f551f7281446030316b4157bb28L93' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 73528666</div><div id='project'> Project Name: lucidrains/triton-transformer</div><div id='commit'> Commit Name: 72efae6cb8b3c62b501dd5bcf1ab8cfbf684e0cb</div><div id='time'> Time: 2021-09-21</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: triton_transformer/triton_transformer.py</div><div id='m_class'> M Class Name: _relu_squared</div><div id='n_method'> N Class Name: _relu_squared</div><div id='m_method'> M Method Name: backward(3)</div><div id='n_method'> N Method Name: backward(3)</div><div id='m_parent_class'> M Parent Class: autograd.Function</div><div id='n_parent_class'> N Parent Class: autograd.Function</div><div id='m_file'> M File Name: triton_transformer/triton_transformer.py</div><div id='n_file'> N File Name: triton_transformer/triton_transformer.py</div><div id='m_start'> M Start Line: 94</div><div id='m_end'> M End Line: 96</div><div id='n_start'> N Start Line: 97</div><div id='n_end'> N End Line: 103</div><BR>