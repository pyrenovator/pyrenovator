<html><h3>Pattern ID :29615
</h3><img src='87750968.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 assert the input shape is : batch_seq_size x entities_size x embeding_size
        &#47&#47 note: because the feature size of entity is not equal to 256, so it can not fed into transformer directly.
        &#47&#47 thus, we add a embedding layer to transfer it to right size.
        <a id="change">print(&quotentity_input is nan:&quot</a>, torch.isnan(x).any()<a id="change">)</a> if debug else None

        &#47&#47 calculate there are how many real entities in each batch
        tmp_x = torch.mean(x, dim=2, keepdim=False)</code></pre><h3>After Change</h3><pre><code class='java'>

    def forward(self, x, debug=True):
        &#47&#47 refactor thanks mostly to the codes from https://github.com/opendilab/DI-star
        batch_size<a id="change"> = </a>x.shape[0]

        &#47&#47 calculate there are how many real entities in each batch
        &#47&#47 tmp_x: [batch_seq_size x entities_size]
        tmp_x = torch.mean(x, dim=2, keepdim=False)

        &#47&#47 tmp_y: [batch_seq_size x entities_size]
        tmp_y = (tmp_x &gt; self.bias_value + 1e3)

        &#47&#47 entity_num: [batch_seq_size]
        entity_num = torch.sum(tmp_y, dim=1, keepdim=False)

        &#47&#47 this means for each batch, there are how many real enetities
        print(&quotentity_num:&quot, entity_num) if debug else None

        &#47&#47 generate the mask for transformer
        mask = torch.arange(0, self.max_entities).float()
        mask<a id="change"> = </a>mask.repeat(batch_size, 1)
        mask<a id="change"> = </a>mask &lt; entity_num.unsqueeze(dim=1)

        print(&quotmask:&quot, mask) if debug else None
        print(&quotmask.shape:&quot, mask.shape) if debug else None

        &#47&#47 mask: [batch_size, max_entities]
        device = next(self.parameters()).device
        mask = <a id="change">mask.to(</a>device<a id="change">)</a>

        &#47&#47 assert the input shape is : batch_seq_size x entities_size x embeding_size
        &#47&#47 note: because the feature size of entity is not equal to 256, so it can not fed into transformer directly.
        &#47&#47 thus, we add a embedding layer to transfer it to right size.
        &#47&#47 x is batch_entities_tensor (dim = 3). Shape: batch_seq_size x entities_size x embeding_size
        x = self.embedd(x)
        print(&quotx.shape:&quot, x.shape) if debug else None

        &#47&#47 mask for transformer need a special format
        mask_seq_len<a id="change"> = </a>mask.shape[-1]
        tran_mask = mask.unsqueeze(1)

        &#47&#47 tran_mask: [batch_seq_size x max_entities x max_entities]</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/liuruoze/mini-alphastar/commit/612d42a7bf5ef827e1e919198d839fce106155cd#diff-71869e70f9cb835282d541d2d3529d25b6ed6795c79bc770e860e9318619f6b1L716' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 87750968</div><div id='project'> Project Name: liuruoze/mini-alphastar</div><div id='commit'> Commit Name: 612d42a7bf5ef827e1e919198d839fce106155cd</div><div id='time'> Time: 2021-11-25</div><div id='author'> Author: liuruoze@163.com</div><div id='file'> File Name: alphastarmini/core/arch/entity_encoder.py</div><div id='m_class'> M Class Name: EntityEncoder</div><div id='n_method'> N Class Name: EntityEncoder</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: alphastarmini/core/arch/entity_encoder.py</div><div id='n_file'> N File Name: alphastarmini/core/arch/entity_encoder.py</div><div id='m_start'> M Start Line: 721</div><div id='m_end'> M End Line: 758</div><div id='n_start'> N Start Line: 716</div><div id='n_end'> N End Line: 778</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            &#47&#47rgb_center_img_tensor = input_center[0:3]
            &#47&#47mask = (input_center[3] + input_first[3] + input_last[3]) / 2

            <a id="change">print (&quotInput size&quot</a>, input.size()<a id="change">)</a>

            center_img = vid_tensor_to_numpy(rgb_center_img_tensor*mask)[0]
            center_img = cv2.cvtColor(center_img, cv2.COLOR_RGB2BGR)</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47mask = torch.round(masks)

            center_img_salient = rgb_center_img_tensor*mask     &#47&#47 3 channels x 1 frame x H x W
            center_img_salient<a id="change"> = </a>center_img_salient.squeeze(1)

            &#47&#47 Put image values into range [0, 1] and then normalize using 
            &#47&#47 mean and std for ImageNet
            &#47&#47 https://pytorch.org/docs/stable/torchvision/models.html

            center_img_salient<a id="change"> = </a>tensor_min_max_normalize(center_img_salient)
            normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
            center_img_salient = normalize(center_img_salient)

            &#47&#47print (&quotInput size&quot, input.size())      
            &#47&#47print (&quotsalient&quot, center_img_salient.size())    
            &#47&#47print(vid_path)

            &#47&#47 Visualize mask region
            &#47&#47center_img = vid_tensor_to_numpy(center_img_salient.unsqueeze(1))[0]
            &#47&#47center_img = cv2.cvtColor(center_img, cv2.COLOR_RGB2BGR)
            &#47&#47center_img = cv_f32_to_u8(center_img)
            &#47&#47cv2.imshow(&quotinput&quot, center_img)
            &#47&#47cv2.waitKey()

            &#47&#47&#47&#47 Visual mask
            &#47&#47&#47&#47mask = vid_tensor_to_numpy(input[3].unsqueeze(0))[0]
            &#47&#47&#47&#47print(mask)
            &#47&#47&#47&#47mask = cv2.merge([mask, mask, mask])

            center_img_salient<a id="change"> = </a>center_img_salient.unsqueeze(0)
            if cuda:
                center_img_salient = <a id="change">center_img_salient.to(</a>device<a id="change">)</a>

            embedd<a id="change"> = </a>model(center_img_salient)
            embeddings.append(embedd.detach().cpu())
            vid_paths.extend(vid_path)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/rvl-lab-utoronto/video_similarity_search/commit/e5eeb446b18f40a7af123ef6d93ea24c32cc0538#diff-98f1ab4b8aa34886d0f0a9d861b630e102aafdb9b6fed0569e138b1facf9d4efL41' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 87751294</div><div id='project'> Project Name: rvl-lab-utoronto/video_similarity_search</div><div id='commit'> Commit Name: e5eeb446b18f40a7af123ef6d93ea24c32cc0538</div><div id='time'> Time: 2020-08-28</div><div id='author'> Author: salar77h@gmail.com</div><div id='file'> File Name: clustering/cluster_masks.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: get_embeddings_mask_regions(3)</div><div id='n_method'> N Method Name: get_embeddings_mask_regions(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: clustering/cluster_masks.py</div><div id='n_file'> N File Name: clustering/cluster_masks.py</div><div id='m_start'> M Start Line: 46</div><div id='m_end'> M End Line: 109</div><div id='n_start'> N Start Line: 51</div><div id='n_end'> N End Line: 127</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    print("Build SRGAN model successfully.")

    &#47&#47 Load the super-resolution model weights
    <a id="change">print(f"Load SRGAN model weights `{os.path.abspath(config.model_path)}`..."</a><a id="change">)</a>
    state_dict = torch.load(config.model_path, map_location=config.device)
    model.load_state_dict(state_dict)
    print(f"Load SRGAN model weights `{os.path.abspath(config.model_path)}` successfully.")
</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 Save image
        sr_image = imgproc.tensor2image(sr_tensor, range_norm=False, half=True)
        sr_image<a id="change"> = </a>cv2.cvtColor(sr_image, cv2.COLOR_RGB2BGR)
        cv2.imwrite(sr_image_path, sr_image)

        &#47&#47 Cal PSNR
        sr_image<a id="change"> = </a>sr_image.astype(np.float32) / 255.
        sr_y_image<a id="change"> = </a>imgproc.bgr2ycbcr(sr_image, use_y_channel=True)
        sr_y_tensor<a id="change"> = </a><a id="change">imgproc.image2tensor(sr_y_image, range_norm=False, half=True).to(</a>config.device<a id="change">)</a>.unsqueeze_(0)

        total_psnr += 10. * torch.log10(1. / torch.mean((sr_y_tensor - hr_y_tensor) ** 2))
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lornatang/srgan-pytorch/commit/231bd74d21d7f532fd746f4a1cb8fb3bc008c933#diff-58b7fc05c0526d9a9b3c0b50dda416fcbf0a654e696da4b6df82cbb5a6b2dcfcL26' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 87751458</div><div id='project'> Project Name: lornatang/srgan-pytorch</div><div id='commit'> Commit Name: 231bd74d21d7f532fd746f4a1cb8fb3bc008c933</div><div id='time'> Time: 2022-03-03</div><div id='author'> Author: liuchangyu1111@gmail.com</div><div id='file'> File Name: validate.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(0)</div><div id='n_method'> N Method Name: main(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: validate.py</div><div id='n_file'> N File Name: validate.py</div><div id='m_start'> M Start Line: 28</div><div id='m_end'> M End Line: 82</div><div id='n_start'> N Start Line: 33</div><div id='n_end'> N End Line: 91</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 Initialize the super-resolution model
    print("Build RRDBNet model...")
    model = Generator().to(config.device)
    <a id="change">print("Build RRDBNet model successfully."</a><a id="change">)</a>

    &#47&#47 Load the super-resolution model weights
    print(f"Load RRDBNet model weights `{os.path.abspath(config.model_path)}`...")
    state_dict = torch.load(config.model_path, map_location=config.device)</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 Save image
        sr_image = imgproc.tensor2image(sr_tensor, range_norm=False, half=True)
        sr_image<a id="change"> = </a>cv2.cvtColor(sr_image, cv2.COLOR_RGB2BGR)
        cv2.imwrite(sr_image_path, sr_image)

        &#47&#47 Cal PSNR
        sr_image<a id="change"> = </a>sr_image.astype(np.float32) / 255.
        sr_y_image<a id="change"> = </a>imgproc.bgr2ycbcr(sr_image, use_y_channel=True)
        sr_y_tensor<a id="change"> = </a><a id="change">imgproc.image2tensor(sr_y_image, range_norm=False, half=True).to(</a>config.device<a id="change">)</a>.unsqueeze_(0)

        total_psnr += 10. * torch.log10(1. / torch.mean((sr_y_tensor - hr_y_tensor) ** 2))
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lornatang/esrgan-pytorch/commit/087e0c9bc621989889918b52b7c0dba9485c5fd6#diff-58b7fc05c0526d9a9b3c0b50dda416fcbf0a654e696da4b6df82cbb5a6b2dcfcL26' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 87751938</div><div id='project'> Project Name: lornatang/esrgan-pytorch</div><div id='commit'> Commit Name: 087e0c9bc621989889918b52b7c0dba9485c5fd6</div><div id='time'> Time: 2022-03-06</div><div id='author'> Author: liuchangyu1111@gmail.com</div><div id='file'> File Name: validate.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(0)</div><div id='n_method'> N Method Name: main(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: validate.py</div><div id='n_file'> N File Name: validate.py</div><div id='m_start'> M Start Line: 28</div><div id='m_end'> M End Line: 82</div><div id='n_start'> N Start Line: 30</div><div id='n_end'> N End Line: 91</div><BR>