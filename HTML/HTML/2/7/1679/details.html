<html><h3>Pattern ID :1679
</h3><img src='7785863.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            raise NotImplementedError
        policy_loss_value = policy_loss.detach().cpu().numpy()
        self.policy_optimizer.zero_grad()
        <a id="change">policy_loss.backward()</a>
        self.policy_optimizer.step()

        self.tot_update_count += 1
        </code></pre><h3>After Change</h3><pre><code class='java'>
        policy_loss_value = policy_loss.detach().cpu().numpy()

        &#47&#47entropy loss
        entropy_loss<a id="change"> = -torch.mean(dist_entropy)</a>
        entropy_loss_value =  entropy_loss.detach().cpu().numpy()
        tot_loss = v_loss<a id="change"> + entropy_loss + </a>policy_loss

        self.policy_optimizer.zero_grad()
        self.v_optimizer.zero_grad()
        <a id="change">tot_loss.backward()</a>
        self.policy_optimizer.step()
        self.v_optimizer.step()

        self.tot_update_count += 1</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/x35f/unstable_baselines/commit/5af9f89c6d399a424b451a524af66c97ab900df8#diff-52f03432c99c716ff4dd3aab99ae6fafde4ebca9abe1cdbec481a8fee78251bcL78' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 7785863</div><div id='project'> Project Name: x35f/unstable_baselines</div><div id='commit'> Commit Name: 5af9f89c6d399a424b451a524af66c97ab900df8</div><div id='time'> Time: 2021-03-30</div><div id='author'> Author: 1621322691@qq.com</div><div id='file'> File Name: ppo/model.py</div><div id='m_class'> M Class Name: PPOAgent</div><div id='n_method'> N Class Name: PPOAgent</div><div id='m_method'> M Method Name: update(2)</div><div id='n_method'> N Method Name: update(2)</div><div id='m_parent_class'> M Parent Class: BaseAgent,torch.nn.Module</div><div id='n_parent_class'> N Parent Class: BaseAgent,torch.nn.Module</div><div id='m_file'> M File Name: ppo/model.py</div><div id='n_file'> N File Name: ppo/model.py</div><div id='m_start'> M Start Line: 78</div><div id='m_end'> M End Line: 118</div><div id='n_start'> N Start Line: 79</div><div id='n_end'> N End Line: 125</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            raise NotImplementedError
        policy_loss_value = policy_loss.detach().cpu().numpy()
        self.policy_optimizer.zero_grad()
        <a id="change">policy_loss.backward()</a>
        self.policy_optimizer.step()

        &#47&#47compute value loss
        v_loss = F.mse_loss(curr_state_v, future_return_batch)</code></pre><h3>After Change</h3><pre><code class='java'>
        

        &#47&#47entropy loss
        entropy_loss<a id="change"> = -</a><a id="change">torch.mean(</a>-new_log_pi<a id="change">)</a> * self.entropy_coeff
        entropy_loss_value = entropy_loss.item()

        &#47&#47compute policy loss
        if self.policy_loss_type == "clipped_surrogate":
            surrogate1 = advantages * ratio_batch
            &#47&#47print(self.clip_range, advantages.shape, ratio_batch.shape)
            surrogate2 =  advantages * torch.clamp(ratio_batch, 1 - self.clip_range, 1 + self.clip_range)
            min_surrogate = - torch.min(surrogate1, surrogate2)
            policy_loss = min_surrogate.mean()
        elif self.policy_loss_type == "naive":
            raise NotImplementedError
        elif self.policy_loss_type == "adaptive_kl":
            raise NotImplementedError
        policy_loss_value = policy_loss.detach().cpu().numpy()
        tot_policy_loss = policy_loss<a id="change"> + </a>self.entropy_coeff<a id="change"> * </a>entropy_loss
        self.policy_optimizer.zero_grad()
        <a id="change">tot_policy_loss.backward()</a>
        self.policy_optimizer.step()

        &#47&#47compute value loss
        v_loss = F.mse_loss(curr_state_v, future_return_batch)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/x35f/unstable_baselines/commit/f41e2cf788d0214add3fb342aee698910c63e651#diff-52f03432c99c716ff4dd3aab99ae6fafde4ebca9abe1cdbec481a8fee78251bcL74' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 7785861</div><div id='project'> Project Name: x35f/unstable_baselines</div><div id='commit'> Commit Name: f41e2cf788d0214add3fb342aee698910c63e651</div><div id='time'> Time: 2021-04-01</div><div id='author'> Author: 1621322691@qq.com</div><div id='file'> File Name: ppo/model.py</div><div id='m_class'> M Class Name: PPOAgent</div><div id='n_method'> N Class Name: PPOAgent</div><div id='m_method'> M Method Name: update(2)</div><div id='n_method'> N Method Name: update(2)</div><div id='m_parent_class'> M Parent Class: BaseAgent,torch.nn.Module</div><div id='n_parent_class'> N Parent Class: BaseAgent,torch.nn.Module</div><div id='m_file'> M File Name: ppo/model.py</div><div id='n_file'> N File Name: ppo/model.py</div><div id='m_start'> M Start Line: 83</div><div id='m_end'> M End Line: 126</div><div id='n_start'> N Start Line: 79</div><div id='n_end'> N End Line: 137</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
  policy_ratio = (trajectories[&quotlog_prob_actions&quot] - trajectories[&quotold_log_prob_actions&quot]).exp()
  policy_loss = -torch.min(policy_ratio * trajectories[&quotadvantages&quot], torch.clamp(policy_ratio, min=1 - ppo_clip, max=1 + ppo_clip) * trajectories[&quotadvantages&quot]).mean()
  actor_optimiser.zero_grad()
  <a id="change">policy_loss.backward()</a>
  actor_optimiser.step()

  &#47&#47 Fit value function by regression on mean squared error</code></pre><h3>After Change</h3><pre><code class='java'>
  &#47&#47 Fit value function by regression on mean squared error
  value_loss = F.mse_loss(trajectories[&quotvalues&quot], trajectories[&quotrewards_to_go&quot])
  &#47&#47 Add entropy regularisation
  entropy_loss<a id="change"> = -trajectories[&quotentropies&quot].mean()</a>  
  
  agent_optimiser.zero_grad()
  <a id="change">(policy_loss + value_loss_coeff * value_loss + entropy_loss_coeff * entropy_loss).backward()</a>
  clip_grad_norm_(agent.parameters(), 1)  &#47&#47 Clamp norm of gradients
  agent_optimiser.step()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/kaixhin/imitation-learning/commit/fd3ee1838359dcc6da9836b6249396e595ff90db#diff-2b28d1dcda2cd70d29d3251359adce0fbfde60edb53108cba1a284ee0d39e256L33' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 7785858</div><div id='project'> Project Name: kaixhin/imitation-learning</div><div id='commit'> Commit Name: fd3ee1838359dcc6da9836b6249396e595ff90db</div><div id='time'> Time: 2020-04-16</div><div id='author'> Author: design@kaixhin.com</div><div id='file'> File Name: training.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: ppo_update(7)</div><div id='n_method'> N Method Name: ppo_update(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: training.py</div><div id='n_file'> N File Name: training.py</div><div id='m_start'> M Start Line: 33</div><div id='m_end'> M End Line: 50</div><div id='n_start'> N Start Line: 34</div><div id='n_end'> N End Line: 51</div><BR>