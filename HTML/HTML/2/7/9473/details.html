<html><h3>Pattern ID :9473
</h3><img src='33849990.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        scale = ctx.scale
        causal = ctx.causal
        q_block_size<a id="change"> = </a>ctx.q_block_size
        k_block_size = ctx.k_block_size
        tile_size = ctx.tile_size
        backward_tile_size = ctx.backward_tile_size

        dq, dk, dv = map(torch.zeros_like, (q, k, v))

        db = torch.zeros((heads, src_seq, tgt_seq), device = device, dtype = dtype) if attn_bias.requires_grad else torch.zeros((heads, 0, 0), device = device, dtype = dtype)
        do_scaled = torch.zeros_like(l)

        <a id="change">backward(</a>do, o, l, q, k, v, dq, dk, dv, db, do_scaled, mask, attn_bias, scale, causal, q_block_size, k_block_size, backward_tile_size<a id="change">)</a>

        db = db if attn_bias.requires_grad else None

        return dq, dk, dv, None, db, None, None, None, None, None, None</code></pre><h3>After Change</h3><pre><code class='java'>
        batch, heads, src_seq, tgt_seq, device, dtype = *q.shape[:3], k.shape[2], q.device, q.dtype

        (
            <a id="change">scale</a><a id="change">,
            causal,
            backward_row_tile_size,
            backward_col_tile_size,
            backward_row_tiles,
            backward_col_tiles</a>
        ) = ctx.params

        dq, dk, dv = map(torch.zeros_like, (q, k, v))

        db = torch.zeros((heads, src_seq, tgt_seq), device = device, dtype = dtype) if attn_bias.requires_grad else torch.zeros((heads, 0, 0), device = device, dtype = dtype)
        do_scaled = torch.zeros_like(l)

        <a id="change">backward(
            </a>do, o, l,
            q, k, v,
            dq, dk, dv, db, do_scaled,
            mask,
            attn_bias,
            <a id="change">scale</a>,
            causal,
            <a id="change">backward_row_tile_size</a>,
            backward_col_tile_size,
            backward_row_tiles,
            <a id="change">backward_col_tiles</a><a id="change">
        )</a>

        db = db if attn_bias.requires_grad else None

        return dq, dk, dv, None, db, None, None, None, None, None, None, None, None, None, None</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/flash-cosine-sim-attention/commit/4eee865e17a85bb19074dc4f9f9be84d113f0f07#diff-9682bb6c6f80a5ac1b8cf3f60ac58f5e21edb3235f4a36b9caed7ba4823b0931L114' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33849990</div><div id='project'> Project Name: lucidrains/flash-cosine-sim-attention</div><div id='commit'> Commit Name: 4eee865e17a85bb19074dc4f9f9be84d113f0f07</div><div id='time'> Time: 2022-09-07</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='m_class'> M Class Name: FlashCosineSimAttention</div><div id='n_method'> N Class Name: FlashCosineSimAttention</div><div id='m_method'> M Method Name: backward(2)</div><div id='n_method'> N Method Name: backward(2)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='n_file'> N File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='m_start'> M Start Line: 114</div><div id='m_end'> M End Line: 130</div><div id='n_start'> N Start Line: 140</div><div id='n_end'> N End Line: 170</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        scale = ctx.scale
        causal = ctx.causal
        q_block_size<a id="change"> = </a>ctx.q_block_size
        k_block_size = ctx.k_block_size
        tile_size = ctx.tile_size
        backward_tile_size = ctx.backward_tile_size

        dq, dk, dv = map(torch.zeros_like, (q, k, v))

        db = torch.zeros((heads, src_seq, tgt_seq), device = device, dtype = dtype) if attn_bias.requires_grad else torch.zeros((heads, 0, 0), device = device, dtype = dtype)
        do_scaled = torch.zeros_like(l)

        <a id="change">backward(</a>do, o, l, q, k, v, dq, dk, dv, db, do_scaled, mask, attn_bias, scale, causal, q_block_size, k_block_size, backward_tile_size<a id="change">)</a>

        db = db if attn_bias.requires_grad else None

        return dq, dk, dv, None, db, None, None, None, None, None, None</code></pre><h3>After Change</h3><pre><code class='java'>
        batch, heads, src_seq, tgt_seq, device, dtype = *q.shape[:3], k.shape[2], q.device, q.dtype

        (
            scale<a id="change">,
            causal,
            backward_row_tile_size,
            backward_col_tile_size,
            backward_row_tiles,
            backward_col_tiles</a>
        ) = ctx.params

        dq, dk, dv = map(torch.zeros_like, (q, k, v))

        db = torch.zeros((heads, src_seq, tgt_seq), device = device, dtype = dtype) if attn_bias.requires_grad else torch.zeros((heads, 0, 0), device = device, dtype = dtype)
        do_scaled = torch.zeros_like(l)

        <a id="change">backward(
            </a>do, o, l,
            q, k, v,
            dq, dk, dv, db, do_scaled,
            mask,
            attn_bias,
            scale,
            causal,
            backward_row_tile_size,
            backward_col_tile_size,
            backward_row_tiles,
            backward_col_tiles<a id="change">
        )</a>

        db = db if attn_bias.requires_grad else None

        return dq, dk, dv, None, db, None, None, None, None, None, None, None, None, None, None</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/flash-cosine-sim-attention/commit/4eee865e17a85bb19074dc4f9f9be84d113f0f07#diff-9682bb6c6f80a5ac1b8cf3f60ac58f5e21edb3235f4a36b9caed7ba4823b0931L109' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33849991</div><div id='project'> Project Name: lucidrains/flash-cosine-sim-attention</div><div id='commit'> Commit Name: 4eee865e17a85bb19074dc4f9f9be84d113f0f07</div><div id='time'> Time: 2022-09-07</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='m_class'> M Class Name: FlashCosineSimAttention</div><div id='n_method'> N Class Name: FlashCosineSimAttention</div><div id='m_method'> M Method Name: backward(2)</div><div id='n_method'> N Method Name: backward(2)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='n_file'> N File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='m_start'> M Start Line: 114</div><div id='m_end'> M End Line: 130</div><div id='n_start'> N Start Line: 140</div><div id='n_end'> N End Line: 170</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        db = torch.zeros((heads, src_seq, tgt_seq), device = device, dtype = dtype) if attn_bias.requires_grad else torch.zeros((heads, 0, 0), device = device, dtype = dtype)
        do_scaled = torch.zeros_like(l)

        <a id="change">backward(</a>do, o, l, q, k, v, dq, dk, dv, db, do_scaled, mask, attn_bias, scale, causal, q_block_size, k_block_size<a id="change">)</a>

        db = db if attn_bias.requires_grad else None

        return dq<a id="change">, dk, dv, None, db, None, None, None, None</a>

&#47&#47 wrapper function

def flash_cosine_sim_attention(</code></pre><h3>After Change</h3><pre><code class='java'>
        q_block_size = ctx.q_block_size
        k_block_size = ctx.k_block_size
        tile_size = ctx.tile_size
        backward_tile_size<a id="change"> = </a>ctx.backward_tile_size

        dq, dk, dv = map(torch.zeros_like, (q, k, v))

        db = torch.zeros((heads, src_seq, tgt_seq), device = device, dtype = dtype) if attn_bias.requires_grad else torch.zeros((heads, 0, 0), device = device, dtype = dtype)
        do_scaled = torch.zeros_like(l)

        <a id="change">backward(</a>do, o, l, q, k, v, dq, dk, dv, db, do_scaled, mask, attn_bias, scale, causal, q_block_size, k_block_size, backward_tile_size<a id="change">)</a>

        db = db if attn_bias.requires_grad else None

        return dq, dk, dv, None, db, None, None, None, None, None, None</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/flash-cosine-sim-attention/commit/1dec3cb2ccf7f31f8c28a130899d3885cd3abcc7#diff-9682bb6c6f80a5ac1b8cf3f60ac58f5e21edb3235f4a36b9caed7ba4823b0931L105' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33850060</div><div id='project'> Project Name: lucidrains/flash-cosine-sim-attention</div><div id='commit'> Commit Name: 1dec3cb2ccf7f31f8c28a130899d3885cd3abcc7</div><div id='time'> Time: 2022-09-04</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='m_class'> M Class Name: FlashCosineSimAttention</div><div id='n_method'> N Class Name: FlashCosineSimAttention</div><div id='m_method'> M Method Name: backward(2)</div><div id='n_method'> N Method Name: backward(2)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='n_file'> N File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='m_start'> M Start Line: 110</div><div id='m_end'> M End Line: 124</div><div id='n_start'> N Start Line: 114</div><div id='n_end'> N End Line: 130</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            backward_col_tiles
        ) = ctx.params

        dq, dk, dv = map(torch.zeros_like, (q<a id="change">, k, v</a>))

        db = torch.zeros((heads, src_seq, tgt_seq), device = device, dtype = dtype) if attn_bias.requires_grad else torch.zeros((heads, 0, 0), device = device, dtype = dtype)
        do_scaled<a id="change"> = </a>torch.zeros_like(l)

        <a id="change">backward(
            </a>do, o, l,
            q, k, v,
            dq, dk, dv, db, do_scaled,
            mask,
            attn_bias,
            scale,
            causal,
            backward_row_tile_size,
            backward_col_tile_size,
            backward_row_tiles,
            backward_col_tiles<a id="change">
        )</a>

        db = db if attn_bias.requires_grad else None

        return dq, dk, dv, None, db, None, None, None, None, None, None, None, None, None, None</code></pre><h3>After Change</h3><pre><code class='java'>

        db = torch.zeros((heads, src_seq, tgt_seq), device = device, dtype = dtype) if attn_bias.requires_grad else torch.zeros((heads, 0, 0), device = device, dtype = dtype)

        dq, dk, dv = <a id="change">backward(
            </a>do, o, l,
            q, k, v,
            db,
            mask,
            attn_bias,
            scale,
            causal,
            backward_row_tile_size,
            backward_col_tile_size,
            backward_row_tiles,
            backward_col_tiles<a id="change">
        )</a>

        db = db if attn_bias.requires_grad else None

        return dq, dk, dv, None, db, None, None, None, None, None, None, None, None, None, None</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/flash-cosine-sim-attention/commit/c0a1c208705410481c1c8e02955b21cbc40e54f0#diff-9682bb6c6f80a5ac1b8cf3f60ac58f5e21edb3235f4a36b9caed7ba4823b0931L135' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33850344</div><div id='project'> Project Name: lucidrains/flash-cosine-sim-attention</div><div id='commit'> Commit Name: c0a1c208705410481c1c8e02955b21cbc40e54f0</div><div id='time'> Time: 2022-09-13</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='m_class'> M Class Name: FlashCosineSimAttention</div><div id='n_method'> N Class Name: FlashCosineSimAttention</div><div id='m_method'> M Method Name: backward(2)</div><div id='n_method'> N Method Name: backward(2)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='n_file'> N File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='m_start'> M Start Line: 149</div><div id='m_end'> M End Line: 166</div><div id='n_start'> N Start Line: 146</div><div id='n_end'> N End Line: 160</div><BR>