<html><h3>Pattern ID :37723
</h3><img src='108333149.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    c[ind2] *= -1

    r = torch.max(t / w, (t - 1) / w).clamp(min=-1e12, max=1e12)
    <a id="change">r[w.abs() &lt; 1e-8] = 1e12</a>
    r[r == -1e12] *= -1
    rs, indr = torch.sort(r, dim=1)
    rs2 = F.pad(rs[:, 1:], (0, 1))
    rs[rs == 1e12] = 0</code></pre><h3>After Change</h3><pre><code class='java'>
    w.mul_(ind2.unsqueeze(1))
    c.mul_(ind2)

    <a id="change">r</a> = torch.max(t / w, (t - 1) / w).clamp(min=-1e12, max=1e12)
    <a id="change">r.masked_fill_(</a>w.abs() &lt; 1e-8, <a id="change">1e12</a><a id="change">)</a>
    r[r == -1e12] *= -1
    rs, indr = torch.sort(r, dim=1)
    rs2 = F.pad(rs[:, 1:], (0, 1))
    rs.masked_fill_(rs == 1e12, 0)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/jeromerony/adversarial-library/commit/1f51f51770105e045bf985ab7553d5480efc4dbe#diff-81c57b5177c60330f4c45eb30ac400e643d1c41217b2ea480d23ebc3b0bd3024L282' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 108333149</div><div id='project'> Project Name: jeromerony/adversarial-library</div><div id='commit'> Commit Name: 1f51f51770105e045bf985ab7553d5480efc4dbe</div><div id='time'> Time: 2020-11-26</div><div id='author'> Author: jerome.rony@gmail.com</div><div id='file'> File Name: adv_lib/attacks/fast_adaptive_boundary/projections.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: projection_l2(3)</div><div id='n_method'> N Method Name: projection_l2(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: adv_lib/attacks/fast_adaptive_boundary/projections.py</div><div id='n_file'> N File Name: adv_lib/attacks/fast_adaptive_boundary/projections.py</div><div id='m_start'> M Start Line: 282</div><div id='m_end'> M End Line: 305</div><div id='n_start'> N Start Line: 283</div><div id='n_end'> N End Line: 305</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    mask = (central &lt;= bins[-2].item()).float()
    &#47&#47 mask diagonal to 0 dist 
    diag = np.arange(shape[-2])
    <a id="change">central</a> = expand_dims_to(central, 3 - len(central.shape))
    <a id="change">central[:, diag, diag] = 0.</a>
    &#47&#47 provide weights
    if wide == "var":
        dispersion = (distogram * (n_bins - central.unsqueeze(-1))**2).sum(dim=-1)
    elif wide == "std":</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 mask diagonal to 0 dist 
    diag_mask = torch.eye(shape[-2], device = device).bool()
    diag = np.arange(shape[-2])
    <a id="change">central</a> = expand_dims_to(central, 3 - len(central.shape))
    <a id="change">central.masked_fill_(</a>diag_mask[None, ...], <a id="change">0.</a><a id="change">)</a>
    &#47&#47 provide weights
    if wide == "var":
        dispersion = (distogram * (n_bins - central.unsqueeze(-1))**2).sum(dim=-1)
    elif wide == "std":</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/alphafold2/commit/63fe134dfb8b6c6b8102def8b0486de0cfb18172#diff-f9b2e05be1f80257f9a80ec7cc1290e3337a54fcc62aeee31b23a4cb827f12efL269' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 108333148</div><div id='project'> Project Name: lucidrains/alphafold2</div><div id='commit'> Commit Name: 63fe134dfb8b6c6b8102def8b0486de0cfb18172</div><div id='time'> Time: 2021-03-01</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: alphafold2_pytorch/utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: center_distogram_torch(5)</div><div id='n_method'> N Method Name: center_distogram_torch(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: alphafold2_pytorch/utils.py</div><div id='n_file'> N File Name: alphafold2_pytorch/utils.py</div><div id='m_start'> M Start Line: 297</div><div id='m_end'> M End Line: 299</div><div id='n_start'> N Start Line: 284</div><div id='n_end'> N End Line: 300</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        bkv_buckets = look_one_back(bkv_buckets)

        &#47&#47 Dot-product attention.
        <a id="change">dots</a> = torch.einsum(&quotbhie,bhje-&gt;bhij&quot, bq, bk) * (bq.shape[-1] ** -0.5)

        &#47&#47 Causal masking
        if self.causal:
            mask = bq_t[:, :, :, None] &lt; bkv_t[:, :, None, :]
            dots[mask] = float(&quot-inf&quot)

        &#47&#47 Mask out attention to self except when no other targets are available.
        self_mask = bq_t[:, :, :, None] == bkv_t[:, :, None, :]
        <a id="change">dots[self_mask] = - 1e5</a>

        &#47&#47 Mask out attention to other hash buckets.
        if not self._attend_across_buckets:
            bucket_mask = bq_buckets[:, :, :, None] != bkv_buckets[:, :, None, :]</code></pre><h3>After Change</h3><pre><code class='java'>
        bkv_buckets = look_one_back(bkv_buckets)

        &#47&#47 Dot-product attention.
        <a id="change">dots</a> = torch.einsum(&quotbhie,bhje-&gt;bhij&quot, bq, bk) * (bq.shape[-1] ** -0.5)

        &#47&#47 Causal masking
        if self.causal:
            mask = bq_t[:, :, :, None] &lt; bkv_t[:, :, None, :]
            dots.masked_fill_(mask, float(&quot-inf&quot))

        &#47&#47 Mask out attention to self except when no other targets are available.
        self_mask = bq_t[:, :, :, None] == bkv_t[:, :, None, :]
        <a id="change">dots.masked_fill_(</a>self_mask, <a id="change">- 1e5</a><a id="change">)</a>

        &#47&#47 Mask out attention to other hash buckets.
        if not self._attend_across_buckets:
            bucket_mask = bq_buckets[:, :, :, None] != bkv_buckets[:, :, None, :]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/reformer-pytorch/commit/fe17355980831aa6e34b10f6407c23d8750ad946#diff-7e740488ff2ce47996cfdd5356981ed3e6ac2bb38739a71b400275ac91d1c3bfL158' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 108333150</div><div id='project'> Project Name: lucidrains/reformer-pytorch</div><div id='commit'> Commit Name: fe17355980831aa6e34b10f6407c23d8750ad946</div><div id='time'> Time: 2020-01-17</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: reformer_pytorch/reformer_pytorch.py</div><div id='m_class'> M Class Name: LSHAttention</div><div id='n_method'> N Class Name: LSHAttention</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: reformer_pytorch/reformer_pytorch.py</div><div id='n_file'> N File Name: reformer_pytorch/reformer_pytorch.py</div><div id='m_start'> M Start Line: 211</div><div id='m_end'> M End Line: 225</div><div id='n_start'> N Start Line: 211</div><div id='n_end'> N End Line: 225</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    r[w.abs() &lt; 1e-8] = 1e12
    r[r == -1e12] *= -1
    rs, indr = torch.sort(r, dim=1)
    <a id="change">rs2</a> = F.pad(rs[:, 1:], (0, 1))
    rs[rs == 1e12] = 0
    <a id="change">rs2[rs2 == 1e12] = 0</a>

    w3s = (w ** 2).gather(1, indr)
    w5 = w3s.sum(dim=1, keepdim=True)
    ws = w5 - torch.cumsum(w3s, dim=1)</code></pre><h3>After Change</h3><pre><code class='java'>
    r.masked_fill_(w.abs() &lt; 1e-8, 1e12)
    r[r == -1e12] *= -1
    rs, indr = torch.sort(r, dim=1)
    <a id="change">rs2</a> = F.pad(rs[:, 1:], (0, 1))
    rs.masked_fill_(rs == 1e12, 0)
    <a id="change">rs2.masked_fill_(</a>rs2 == 1e12, <a id="change">0</a><a id="change">)</a>

    w3s = (w ** 2).gather(1, indr)
    w5 = w3s.sum(dim=1, keepdim=True)
    ws = w5 - torch.cumsum(w3s, dim=1)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/jeromerony/adversarial-library/commit/1f51f51770105e045bf985ab7553d5480efc4dbe#diff-81c57b5177c60330f4c45eb30ac400e643d1c41217b2ea480d23ebc3b0bd3024L278' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 108333147</div><div id='project'> Project Name: jeromerony/adversarial-library</div><div id='commit'> Commit Name: 1f51f51770105e045bf985ab7553d5480efc4dbe</div><div id='time'> Time: 2020-11-26</div><div id='author'> Author: jerome.rony@gmail.com</div><div id='file'> File Name: adv_lib/attacks/fast_adaptive_boundary/projections.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: projection_l2(3)</div><div id='n_method'> N Method Name: projection_l2(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: adv_lib/attacks/fast_adaptive_boundary/projections.py</div><div id='n_file'> N File Name: adv_lib/attacks/fast_adaptive_boundary/projections.py</div><div id='m_start'> M Start Line: 282</div><div id='m_end'> M End Line: 305</div><div id='n_start'> N Start Line: 283</div><div id='n_end'> N End Line: 305</div><BR>