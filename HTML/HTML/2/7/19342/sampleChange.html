<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            request_obj = dist.broadcast(
                tensor, self.local_rank, group=pg, async_op=True)
            request_objects.append(request_obj)
        <a id="change">return </a>request_objects

    def _recv_tensors_bcast(self, x, batch_idx, src, pg):
        &#47&#47 TODO: Double buffering like p2p</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 tensor.detach_()  &#47&#47 RuntimeError: Can&quott detach views in-place. Use detach() instead
            &#47&#47 FIXME: see https://github.com/pytorch/pytorch/issues/25814
            &#47&#47 (its problematic with the tensor.grad, which we plan to avoid anyway.)
            <a id="change">if </a>is_grad:
                <a id="change">with torch.no_grad()</a><a id="change">:
                    </a>tensor<a id="change"> = </a><a id="change">tensor.clone().detach_()</a>
            else:
                tensor.detach_()

            if self.verbose:</code></pre>