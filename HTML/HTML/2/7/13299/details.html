<html><h3>Pattern ID :13299
</h3><img src='45085351.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                state[&quotstep&quot] += 1

                if group[&quotweight_decay&quot] != 0:
                    grad<a id="change"> = </a>grad.add(group[&quotweight_decay&quot], p.data)

                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)

                if group[&quotcentered&quot]:
                    grad_avg = state[&quotgrad_avg&quot]
                    grad_avg.mul_(alpha).add_(1 - alpha, grad)
                    avg = square_avg.addcmul(-1, grad_avg, grad_avg).add(group[&quoteps&quot]).sqrt_()
                else:
                    avg = square_avg.add(group[&quoteps&quot]).sqrt_()

                if group[&quotmomentum&quot] &gt; 0:
                    buf = state[&quotmomentum_buffer&quot]
                    buf.mul_(group[&quotmomentum&quot]).addcdiv_(grad, avg)
                    p.data.add_(-group[&quotlr&quot], buf)
                else:
                    <a id="change">p.data.addcdiv_(</a>-group[&quotlr&quot], grad, avg<a id="change">)</a>

        return loss
</code></pre><h3>After Change</h3><pre><code class='java'>
                        state[&quotgrad_avg&quot] = torch.zeros_like(p.data)

                square_avg = state[&quotsquare_avg&quot]
                one_minus_alpha = 1.<a id="change"> - </a>group[&quotalpha&quot]

                state[&quotstep&quot] += 1

                if group[&quotweight_decay&quot] != 0:
                    if group[&quotdecoupled_decay&quot]:
                        p.data.add_(-group[&quotweight_decay&quot], p.data)
                    else:
                        grad<a id="change"> = </a>grad.add(group[&quotweight_decay&quot], p.data)

                &#47&#47 Tensorflow order of ops for updating squared avg
                square_avg.add_(one_minus_alpha, grad.pow(2) - square_avg)
                &#47&#47 square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)  &#47&#47 PyTorch original

                if group[&quotcentered&quot]:
                    grad_avg = state[&quotgrad_avg&quot]
                    grad_avg.add_(one_minus_alpha, grad - grad_avg)
                    &#47&#47 grad_avg.mul_(alpha).add_(1 - alpha, grad)  &#47&#47 PyTorch original
                    avg = square_avg.addcmul(-1, grad_avg, grad_avg).add(group[&quoteps&quot]).sqrt_()  &#47&#47 eps moved in sqrt
                else:
                    avg = square_avg.add(group[&quoteps&quot]).sqrt_()  &#47&#47 eps moved in sqrt

                if group[&quotmomentum&quot] &gt; 0:
                    buf = state[&quotmomentum_buffer&quot]
                    &#47&#47 Tensorflow accumulates the LR scaling in the momentum buffer
                    if group[&quotlr_in_momentum&quot]:
                        buf.mul_(group[&quotmomentum&quot]).addcdiv_(group[&quotlr&quot], grad, avg)
                        p.data.add_(<a id="change">-buf</a>)
                    else:
                        &#47&#47 PyTorch scales the param update by LR
                        buf.mul_(group[&quotmomentum&quot]).addcdiv_(grad, avg)
                        p.data.add_(-group[&quotlr&quot], buf)
                else:
                    <a id="change">p.data.addcdiv_(</a>-group[&quotlr&quot], grad, avg<a id="change">)</a>

        return loss
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/feng-lab/pytorch-image-models/commit/20d66beead659c82d4bd7358ae3929ad74e5a0d8#diff-f224effd3d45641ee284efd92222dfb47d1385e08648d8f1f949ed1bc3176430L63' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 45085351</div><div id='project'> Project Name: feng-lab/pytorch-image-models</div><div id='commit'> Commit Name: 20d66beead659c82d4bd7358ae3929ad74e5a0d8</div><div id='time'> Time: 2019-05-14</div><div id='author'> Author: rwightman@gmail.com</div><div id='file'> File Name: optim/rmsprop_tf.py</div><div id='m_class'> M Class Name: RMSpropTF</div><div id='n_method'> N Class Name: RMSpropTF</div><div id='m_method'> M Method Name: step(2)</div><div id='n_method'> N Method Name: step(2)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: optim/rmsprop_tf.py</div><div id='n_file'> N File Name: optim/rmsprop_tf.py</div><div id='m_start'> M Start Line: 63</div><div id='m_end'> M End Line: 105</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 122</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                state[&quotstep&quot] += 1

                if group[&quotweight_decay&quot] != 0:
                    grad<a id="change"> = </a>grad.add(group[&quotweight_decay&quot], p.data)

                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)

                if group[&quotcentered&quot]:
                    grad_avg = state[&quotgrad_avg&quot]
                    grad_avg.mul_(alpha).add_(1 - alpha, grad)
                    avg = square_avg.addcmul(-1, grad_avg, grad_avg).add(group[&quoteps&quot]).sqrt_()
                else:
                    avg = square_avg.add(group[&quoteps&quot]).sqrt_()

                if group[&quotmomentum&quot] &gt; 0:
                    buf = state[&quotmomentum_buffer&quot]
                    buf.mul_(group[&quotmomentum&quot]).addcdiv_(grad, avg)
                    p.data.add_(-group[&quotlr&quot], buf)
                else:
                    <a id="change">p.data.addcdiv_(</a>-group[&quotlr&quot], grad, avg<a id="change">)</a>

        return loss
</code></pre><h3>After Change</h3><pre><code class='java'>
            loss = closure()

        for group in self.param_groups:
            for <a id="change">p</a> in group[&quotparams&quot]:
                if p.grad is None:
                    continue
                grad = p.grad.data
                if grad.is_sparse:
                    raise RuntimeError(&quotRMSprop does not support sparse gradients&quot)
                state = self.state[p]

                &#47&#47 State initialization
                if len(state) == 0:
                    state[&quotstep&quot] = 0
                    state[&quotsquare_avg&quot] = torch.ones_like(p.data)  &#47&#47 PyTorch inits to zero
                    if group[&quotmomentum&quot] &gt; 0:
                        state[&quotmomentum_buffer&quot] = torch.zeros_like(p.data)
                    if group[&quotcentered&quot]:
                        state[&quotgrad_avg&quot] = torch.zeros_like(p.data)

                square_avg = state[&quotsquare_avg&quot]
                one_minus_alpha = 1.<a id="change"> - </a>group[&quotalpha&quot]

                state[&quotstep&quot] += 1

                if group[&quotweight_decay&quot] != 0:
                    if group[&quotdecoupled_decay&quot]:
                        p.data.add_(<a id="change">-group[&quotweight_decay&quot]</a>, p.data)
                    else:
                        grad<a id="change"> = </a>grad.add(group[&quotweight_decay&quot], p.data)

                &#47&#47 Tensorflow order of ops for updating squared avg
                square_avg.add_(one_minus_alpha, grad.pow(2) - square_avg)
                &#47&#47 square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)  &#47&#47 PyTorch original

                if group[&quotcentered&quot]:
                    grad_avg = state[&quotgrad_avg&quot]
                    grad_avg.add_(one_minus_alpha, grad - grad_avg)
                    &#47&#47 grad_avg.mul_(alpha).add_(1 - alpha, grad)  &#47&#47 PyTorch original
                    avg = square_avg.addcmul(-1, grad_avg, grad_avg).add(group[&quoteps&quot]).sqrt_()  &#47&#47 eps moved in sqrt
                else:
                    avg = square_avg.add(group[&quoteps&quot]).sqrt_()  &#47&#47 eps moved in sqrt

                if group[&quotmomentum&quot] &gt; 0:
                    buf = state[&quotmomentum_buffer&quot]
                    &#47&#47 Tensorflow accumulates the LR scaling in the momentum buffer
                    if group[&quotlr_in_momentum&quot]:
                        buf.mul_(group[&quotmomentum&quot]).addcdiv_(group[&quotlr&quot], grad, avg)
                        p.data.add_(-buf)
                    else:
                        &#47&#47 PyTorch scales the param update by LR
                        buf.mul_(group[&quotmomentum&quot]).addcdiv_(grad, avg)
                        p.data.add_(-group[&quotlr&quot], buf)
                else:
                    <a id="change">p.data.addcdiv_(</a>-group[&quotlr&quot], grad, avg<a id="change">)</a>

        return loss
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/feng-lab/pytorch-image-models/commit/20d66beead659c82d4bd7358ae3929ad74e5a0d8#diff-f224effd3d45641ee284efd92222dfb47d1385e08648d8f1f949ed1bc3176430L52' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 45085350</div><div id='project'> Project Name: feng-lab/pytorch-image-models</div><div id='commit'> Commit Name: 20d66beead659c82d4bd7358ae3929ad74e5a0d8</div><div id='time'> Time: 2019-05-14</div><div id='author'> Author: rwightman@gmail.com</div><div id='file'> File Name: optim/rmsprop_tf.py</div><div id='m_class'> M Class Name: RMSpropTF</div><div id='n_method'> N Class Name: RMSpropTF</div><div id='m_method'> M Method Name: step(2)</div><div id='n_method'> N Method Name: step(2)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: optim/rmsprop_tf.py</div><div id='n_file'> N File Name: optim/rmsprop_tf.py</div><div id='m_start'> M Start Line: 63</div><div id='m_end'> M End Line: 105</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 122</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        bias_correction2 = 1 - beta2 ** state[&quotstep&quot]

        &#47&#47 $\sqrt{v_t} + \epsilon$
        denominator<a id="change"> = </a>v.sqrt().add_(group[&quoteps&quot])
        &#47&#47 $\alpha \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}$
        step_size = self.get_lr(state, group) * math.sqrt(bias_correction2) / bias_correction1
        &#47&#47 $\theta_t \leftarrow \theta_{t-1} - \alpha \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \cdot
        &#47&#47  \frac{m_t}{\sqrt{v_t} + \hat{\epsilon}}$
        <a id="change">param.data.addcdiv_(</a>m, denominator<a id="change">, value=-step_size)</a>

    def step_param(self, state: Dict[str, any], group: Dict[str, any], grad: torch.Tensor, param: torch.nn.Parameter):
        
        &#47&#47&#47&#47&#47&#47 Take an update step for a given parameter tensor</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 Bias correction term for $\hat{v}_t$, $1 - \beta_2^t$
        bias_correction2 = 1 - beta2 ** state[&quotstep&quot]

        if <a id="change">self.optimized_update</a>:
            &#47&#47 $\sqrt{v_t} + \hat{\epsilon}$
            denominator<a id="change"> = </a>v.sqrt().add_(group[&quoteps&quot])
            &#47&#47 $\alpha \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}$
            step_size = self.get_lr(state, group) * math.sqrt(bias_correction2) / bias_correction1
            &#47&#47 $\theta_t \leftarrow \theta_{t-1} - \alpha \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \cdot
            &#47&#47  \frac{m_t}{\sqrt{v_t} + \hat{\epsilon}}$
            <a id="change">param.data.addcdiv_(</a>m, denominator<a id="change">, value=-step_size)</a>
        else:
            &#47&#47 $\frac{\sqrt{v_t}}{\sqrt{1-\beta_2^t}} + \epsilon$
            denominator = (v.sqrt()<a id="change"> / </a>math.sqrt(bias_correction2)).add_(group[&quoteps&quot])
            &#47&#47 $\frac{\alpha}{1-\beta_1^t}$
            step_size = self.get_lr(state, group)<a id="change"> / </a>bias_correction1
            &#47&#47 $\theta_t \leftarrow \theta_{t-1} - \alpha \cdot
            &#47&#47 \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$
            param.data.addcdiv_(m, denominator, value=-step_size)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lab-ml/nn/commit/4d587576715102dba49e2f42e70bc45079b25bf9#diff-ed1e6301c0efe23af76fb911a3162ee361e74a0b854abb9ae33a53a436ea202dL127' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 45085345</div><div id='project'> Project Name: lab-ml/nn</div><div id='commit'> Commit Name: 4d587576715102dba49e2f42e70bc45079b25bf9</div><div id='time'> Time: 2020-12-09</div><div id='author'> Author: vpjayasiri@gmail.com</div><div id='file'> File Name: labml_nn/optimizers/adam.py</div><div id='m_class'> M Class Name: Adam</div><div id='n_method'> N Class Name: Adam</div><div id='m_method'> M Method Name: adam_update(6)</div><div id='n_method'> N Method Name: adam_update(6)</div><div id='m_parent_class'> M Parent Class: GenericAdaptiveOptimizer</div><div id='n_parent_class'> N Parent Class: GenericAdaptiveOptimizer</div><div id='m_file'> M File Name: labml_nn/optimizers/adam.py</div><div id='n_file'> N File Name: labml_nn/optimizers/adam.py</div><div id='m_start'> M Start Line: 167</div><div id='m_end'> M End Line: 172</div><div id='n_start'> N Start Line: 167</div><div id='n_end'> N End Line: 189</div><BR>