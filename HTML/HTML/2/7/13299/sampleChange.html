<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                state[&quotstep&quot] += 1

                if group[&quotweight_decay&quot] != 0:
                    grad<a id="change"> = </a>grad.add(group[&quotweight_decay&quot], p.data)

                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)

                if group[&quotcentered&quot]:
                    grad_avg = state[&quotgrad_avg&quot]
                    grad_avg.mul_(alpha).add_(1 - alpha, grad)
                    avg = square_avg.addcmul(-1, grad_avg, grad_avg).add(group[&quoteps&quot]).sqrt_()
                else:
                    avg = square_avg.add(group[&quoteps&quot]).sqrt_()

                if group[&quotmomentum&quot] &gt; 0:
                    buf = state[&quotmomentum_buffer&quot]
                    buf.mul_(group[&quotmomentum&quot]).addcdiv_(grad, avg)
                    p.data.add_(-group[&quotlr&quot], buf)
                else:
                    <a id="change">p.data.addcdiv_(</a>-group[&quotlr&quot], grad, avg<a id="change">)</a>

        return loss
</code></pre><h3>After Change</h3><pre><code class='java'>
                        state[&quotgrad_avg&quot] = torch.zeros_like(p.data)

                square_avg = state[&quotsquare_avg&quot]
                one_minus_alpha = 1.<a id="change"> - </a>group[&quotalpha&quot]

                state[&quotstep&quot] += 1

                if group[&quotweight_decay&quot] != 0:
                    if group[&quotdecoupled_decay&quot]:
                        p.data.add_(-group[&quotweight_decay&quot], p.data)
                    else:
                        grad<a id="change"> = </a>grad.add(group[&quotweight_decay&quot], p.data)

                &#47&#47 Tensorflow order of ops for updating squared avg
                square_avg.add_(one_minus_alpha, grad.pow(2) - square_avg)
                &#47&#47 square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)  &#47&#47 PyTorch original

                if group[&quotcentered&quot]:
                    grad_avg = state[&quotgrad_avg&quot]
                    grad_avg.add_(one_minus_alpha, grad - grad_avg)
                    &#47&#47 grad_avg.mul_(alpha).add_(1 - alpha, grad)  &#47&#47 PyTorch original
                    avg = square_avg.addcmul(-1, grad_avg, grad_avg).add(group[&quoteps&quot]).sqrt_()  &#47&#47 eps moved in sqrt
                else:
                    avg = square_avg.add(group[&quoteps&quot]).sqrt_()  &#47&#47 eps moved in sqrt

                if group[&quotmomentum&quot] &gt; 0:
                    buf = state[&quotmomentum_buffer&quot]
                    &#47&#47 Tensorflow accumulates the LR scaling in the momentum buffer
                    if group[&quotlr_in_momentum&quot]:
                        buf.mul_(group[&quotmomentum&quot]).addcdiv_(group[&quotlr&quot], grad, avg)
                        p.data.add_(<a id="change">-buf</a>)
                    else:
                        &#47&#47 PyTorch scales the param update by LR
                        buf.mul_(group[&quotmomentum&quot]).addcdiv_(grad, avg)
                        p.data.add_(-group[&quotlr&quot], buf)
                else:
                    <a id="change">p.data.addcdiv_(</a>-group[&quotlr&quot], grad, avg<a id="change">)</a>

        return loss
</code></pre>