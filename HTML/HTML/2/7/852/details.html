<html><h3>Pattern ID :852
</h3><img src='4196016.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    if isinstance(defaulted, dict):
        if not isinstance(obj, dict):
            return
        common_keys<a id="change"> = </a>set(<a id="change">defaulted.keys()</a>).intersection(set(obj.keys()))
        return {k: strip_runtime_defaultable(obj.get(k), defaulted.get(k)) for k in common_keys}

    &#47&#47 Recurse through lists.</code></pre><h3>After Change</h3><pre><code class='java'>
    Recursively find strings of "*" in defaulted and set corresponding non-None values in obj to
    also be "*", so that equality tests will pass.
    
    <a id="change">if defaulted is None</a><a id="change"> or obj is None</a>:
        return obj

    if isinstance(defaulted, str):
        if defaulted == "*":
            return "*"

    &#47&#47 Recurse through dicts.
    if isinstance(defaulted, dict):
        if not isinstance(obj, dict):
            return obj
        return {k: strip_runtime_defaultable(obj.get(k), defaulted.get(k)) for k in obj}

    if isinstance(defaulted, tuple):
        defaulted<a id="change"> = </a>list(defaulted)

    &#47&#47 Recurse through lists.
    if isinstance(defaulted, list):</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/determined-ai/determined/commit/67de93a1de1c8caeecfd663eabe54d1fc8761ac7#diff-3ee1937a11cbe6b9529f60505cb324ecfc9fbb6497e7e121054b6613e9361516L17' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4196016</div><div id='project'> Project Name: determined-ai/determined</div><div id='commit'> Commit Name: 67de93a1de1c8caeecfd663eabe54d1fc8761ac7</div><div id='time'> Time: 2021-04-14</div><div id='author'> Author: rb@determined.ai</div><div id='file'> File Name: harness/tests/common/test_schemas.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: strip_runtime_defaultable(2)</div><div id='n_method'> N Method Name: strip_runtime_defaultable(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: harness/tests/common/test_schemas.py</div><div id='n_file'> N File Name: harness/tests/common/test_schemas.py</div><div id='m_start'> M Start Line: 18</div><div id='m_end'> M End Line: 33</div><div id='n_start'> N Start Line: 17</div><div id='n_end'> N End Line: 42</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 (weight_name, importance_feature) = list(a_model.items())[-2]
        gradient = []
        protected_keys = []
        all_keys<a id="change"> = </a>list(<a id="change">global_model_of_last_round.keys()</a>)
        if self.protected_layers is not None:
            for protected_layer_idx in self.protected_layers:
                protected_keys.append(all_keys[protected_layer_idx])</code></pre><h3>After Change</h3><pre><code class='java'>

        layer_counter = 0
        for k, _ in global_model_of_last_round.items():
            <a id="change">if "weight" in k</a><a id="change"> or "bias" in k</a>:
                if self.protected_layers is not None and layer_counter in self.protected_layers:
                    gradient.append(torch.from_numpy(np.zeros(global_model_of_last_round[k].size())).float())
                    &#47&#47 if the layer is protected, set to 0
                else:
                    gradient.append(a_model[k] - global_model_of_last_round[k].to(self.device))  &#47&#47 todo: to double check
                layer_counter<a id="change"> += </a>1
        gradient = tuple(gradient)
        dummy_data = torch.randn(self.original_data_size)
        dummy_label = torch.randn(self.original_label_size)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/fedml-ai/fedml/commit/a8f8cebbb7d07c671b270a83c08006501c221f2d#diff-a92997ecfd5ba4b0ec93d15f6288db32b01531d977514bb529e3da431799ae44L54' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4196019</div><div id='project'> Project Name: fedml-ai/fedml</div><div id='commit'> Commit Name: a8f8cebbb7d07c671b270a83c08006501c221f2d</div><div id='time'> Time: 2023-03-07</div><div id='author'> Author: sshan0731@hotmail.com</div><div id='file'> File Name: python/fedml/core/security/attack/dlg_attack.py</div><div id='m_class'> M Class Name: DLGAttack</div><div id='n_method'> N Class Name: DLGAttack</div><div id='m_method'> M Method Name: reconstruct_data(3)</div><div id='n_method'> N Method Name: reconstruct_data(3)</div><div id='m_parent_class'> M Parent Class: BaseAttackMethod</div><div id='n_parent_class'> N Parent Class: BaseAttackMethod</div><div id='m_file'> M File Name: python/fedml/core/security/attack/dlg_attack.py</div><div id='n_file'> N File Name: python/fedml/core/security/attack/dlg_attack.py</div><div id='m_start'> M Start Line: 55</div><div id='m_end'> M End Line: 70</div><div id='n_start'> N Start Line: 59</div><div id='n_end'> N End Line: 71</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 (weight_name, importance_feature) = list(a_model.items())[-2]
        gradient = []
        protected_keys = []
        all_keys<a id="change"> = </a>list(<a id="change">global_model_of_last_round.keys()</a>)
        if self.protected_layers is not None:
            for protected_layer_idx in self.protected_layers:
                protected_keys.append(all_keys[protected_layer_idx])</code></pre><h3>After Change</h3><pre><code class='java'>

        layer_counter = 0
        for k, _ in global_model_of_last_round.items():
            <a id="change">if "weight" in k</a><a id="change"> or "bias" in k</a>:
                if self.protected_layers is not None and layer_counter in self.protected_layers:
                    gradient.append(torch.from_numpy(np.zeros(global_model_of_last_round[k].size())).float())
                    &#47&#47 if the layer is protected, set to 0
                else:
                    gradient.append(a_model[k] - global_model_of_last_round[k].to(self.device))  &#47&#47 todo: to double check
                layer_counter<a id="change"> += </a>1
        gradient = tuple(gradient)
        dummy_data = torch.randn(self.original_data_size)
        dummy_label = torch.randn(self.original_label_size)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/fedml-ai/fedml/commit/901b7f6432b4712acd8a9eebf98365a857db5f53#diff-a92997ecfd5ba4b0ec93d15f6288db32b01531d977514bb529e3da431799ae44L54' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4196023</div><div id='project'> Project Name: fedml-ai/fedml</div><div id='commit'> Commit Name: 901b7f6432b4712acd8a9eebf98365a857db5f53</div><div id='time'> Time: 2023-03-07</div><div id='author'> Author: sshan0731@hotmail.com</div><div id='file'> File Name: python/fedml/core/security/attack/dlg_attack.py</div><div id='m_class'> M Class Name: DLGAttack</div><div id='n_method'> N Class Name: DLGAttack</div><div id='m_method'> M Method Name: reconstruct_data(3)</div><div id='n_method'> N Method Name: reconstruct_data(3)</div><div id='m_parent_class'> M Parent Class: BaseAttackMethod</div><div id='n_parent_class'> N Parent Class: BaseAttackMethod</div><div id='m_file'> M File Name: python/fedml/core/security/attack/dlg_attack.py</div><div id='n_file'> N File Name: python/fedml/core/security/attack/dlg_attack.py</div><div id='m_start'> M Start Line: 55</div><div id='m_end'> M End Line: 70</div><div id='n_start'> N Start Line: 59</div><div id='n_end'> N End Line: 71</div><BR>