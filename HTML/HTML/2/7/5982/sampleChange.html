<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if memories is not None:
            mem, lmem = memories

        mem<a id="change"> = </a><a id="change">default(</a>mem, lambda: torch.empty(b, 0, e, **to(x))<a id="change">)</a>
        lmem = default(lmem, lambda: torch.empty(b, 0, e, **to(x)))

        mem_len = mem.shape[1]
        lmem_len = lmem.shape[1]

        q = self.to_q(x)

        kv_input = torch.cat((lmem, mem, x), dim=1)
        kv_len = kv_input.shape[1]
        k, v = self.to_kv(kv_input).chunk(2, dim=-1)

        merge_heads = lambda x: x.reshape(*x.shape[:2], -1, dim_h).transpose(1, 2)
        q, k, v = map(merge_heads, (q, k, v))

        k, v = map(lambda x: x.expand(-1, h, -1, -1), (k, v))

        dots = torch.einsum(&quotbhid,bhjd-&gt;bhij&quot, q, k) * self.scale
        mask_value = max_neg_value(dots)

        if pos_emb is not None:
            pos_emb = pos_emb[:, -kv_len:].type(q.dtype)
            pos_dots = torch.einsum(&quotbhid,hjd-&gt;bhij&quot, q, pos_emb) * self.scale
            pos_dots = shift(pos_dots)
            dots = dots + pos_dots

        if input_mask is not None:
            mask = input_mask[:, None, :, None] * input_mask[:, None, None, :]
            mask = F.pad(mask, (mem_len + lmem_len, 0), value = True)
            dots.masked_fill_(~mask, mask_value)

        total_mem_len = mem_len + lmem_len
        mask = torch.ones(t, t + total_mem_len, **to(x)).triu_(diagonal = 1 + total_mem_len).bool()
        dots.masked_fill_(mask[None, None, ...], mask_value)

        attn = dots.softmax(dim=-1)
        attn = self.attn_dropout(attn)

        out = torch.einsum(&quotbhij,bhjd-&gt;bhid&quot, attn, v)
        out = out.transpose(1, 2).reshape(b, t, -1)
        logits = self.to_out(out)
        logits = self.dropout(logits)

        new_mem = mem
        new_lmem = lmem

        if self.seq_len &gt; t or not calc_memory:
            return logits, Memory(new_mem, new_lmem)

        &#47&#47 calculate memory and compressed memory

        old_mem<a id="change">, new_mem = </a>split_at_index(1, -self.mem_len, torch.cat((mem<a id="change">, x</a>), dim=1))

        if old_mem.shape[1] != 0 and self.lmem_len &gt; 0:
            pass

        return logits<a id="change">, Memory(new_mem, new_lmem)</a>

&#47&#47 transformer

class MemoryTransformerXL(nn.Module):</code></pre><h3>After Change</h3><pre><code class='java'>
        mem, lmem = memories

        init_mem = lambda: torch.empty(b, 0, e, **to(x))
        mem<a id="change"> = </a><a id="change">default(</a>mem, init_mem<a id="change">)</a>
        lmem = default(lmem, init_mem)

        mem_len = mem.shape[1]
        lmem_len = lmem.shape[1]</code></pre>