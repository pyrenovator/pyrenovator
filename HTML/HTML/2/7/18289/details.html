<html><h3>Pattern ID :18289
</h3><img src='59937250.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 help auto-solve a frequent area of confusion around input masks in auto-regressive
        &#47&#47 if user supplies a mask that is only off by one from the source sequence, resolve it for them
        mask = kwargs.get(&quotmask&quot, None)
        <a id="change">if </a><a id="change">mask is not None and mask.shape[1] == x.shape[1]</a>:
            mask = mask[:, :-1]
            kwargs[&quotmask&quot] = mask
</code></pre><h3>After Change</h3><pre><code class='java'>

        inp, target = x[:, :-1], x[:, 1:]

        <a id="change">if self.mask_prob &gt; 0.</a>:
            rand = torch.randn(inp.shape, device = x.device)
            rand[:, 0] = -torch.finfo(rand.dtype).max &#47&#47 first token should not be masked out
            num_mask = min(int(seq * self.mask_prob), seq - 1)
            indices = rand.topk(num_mask, dim = -1).indices
            mask = ~torch.zeros_like(inp).scatter(1, indices, 1.).bool()
            <a id="change">kwargs.update(context_mask = mask)</a>

        out = self.net(inp, **kwargs)

        out = out.transpose(1, 2)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/x-transformers/commit/595a4745d532c20b8ebd310256c342e946a4cef7#diff-b70a3173d353a448f8f499d8b685672d9bccac7b78bc1d36b77f84afe33be694L107' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 59937250</div><div id='project'> Project Name: lucidrains/x-transformers</div><div id='commit'> Commit Name: 595a4745d532c20b8ebd310256c342e946a4cef7</div><div id='time'> Time: 2022-11-02</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: x_transformers/autoregressive_wrapper.py</div><div id='m_class'> M Class Name: AutoregressiveWrapper</div><div id='n_method'> N Class Name: AutoregressiveWrapper</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: x_transformers/autoregressive_wrapper.py</div><div id='n_file'> N File Name: x_transformers/autoregressive_wrapper.py</div><div id='m_start'> M Start Line: 107</div><div id='m_end'> M End Line: 118</div><div id='n_start'> N Start Line: 122</div><div id='n_end'> N End Line: 142</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        fields_in_same_space = super()._get_fields_in_same_space()
        fields_in_same_space = [
            _ for _ in fields_in_same_space
            if <a id="change">(self.head_entity_field not in _) and
               (self.tail_entity_field not in _) and
               (self.entity_field not in _)</a>
        ]
        return fields_in_same_space

    def _remap_ID_all(self):</code></pre><h3>After Change</h3><pre><code class='java'>
        ]
        ent_fields = self._get_ent_fields_in_same_space()
        for field_set in fields_in_same_space:
            <a id="change">if self.iid_field in field_set</a>:
                <a id="change">field_set.update(</a>ent_fields<a id="change">)</a>
        return fields_in_same_space

    def _contain_ent_field(self, field_set):
        flag = False</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/rucaibox/recbole/commit/d3a6d7309cdc4f04287ae895dcfdeb491e3a05d8#diff-1b5255bd2257e1b85f62533b85b549035713df4082ea4c175034665cf444afc0L110' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 59937248</div><div id='project'> Project Name: rucaibox/recbole</div><div id='commit'> Commit Name: d3a6d7309cdc4f04287ae895dcfdeb491e3a05d8</div><div id='time'> Time: 2020-10-15</div><div id='author'> Author: houyupeng@ruc.edu.cn</div><div id='file'> File Name: recbox/data/dataset/kg_dataset.py</div><div id='m_class'> M Class Name: KnowledgeBasedDataset</div><div id='n_method'> N Class Name: KnowledgeBasedDataset</div><div id='m_method'> M Method Name: _get_fields_in_same_space(1)</div><div id='n_method'> N Method Name: _get_fields_in_same_space(1)</div><div id='m_parent_class'> M Parent Class: Dataset</div><div id='n_parent_class'> N Parent Class: Dataset</div><div id='m_file'> M File Name: recbox/data/dataset/kg_dataset.py</div><div id='n_file'> N File Name: recbox/data/dataset/kg_dataset.py</div><div id='m_start'> M Start Line: 113</div><div id='m_end'> M End Line: 116</div><div id='n_start'> N Start Line: 150</div><div id='n_end'> N End Line: 157</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 in case of conditional generation, if enc_mask is not provided use the correct context_mask
        context = kwargs.pop(&quotcontext&quot, None)
        context_mask = kwargs.pop(&quotcontext_mask&quot, None)
        <a id="change">if context is not None</a><a id="change"> and context_mask is None</a>:
            context_mask = torch.full(context.shape[:2], True, dtype=torch.bool, device=out.device)

        for _ in range(seq_len):</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 in case of conditional generation, if enc_mask is not provided use the correct context_mask
        context_mask = kwargs.pop(&quotcontext_mask&quot, None)

        <a id="change">if &quotcontext&quot in kwargs</a> and not exists(context_mask):
            context = kwargs[&quotcontext&quot]
            context_mask = torch.full(context.shape[:2], True, dtype=torch.bool, device=out.device)
            <a id="change">kwargs.update(context_mask = context_mask)</a>

        for _ in range(seq_len):
            x = out[:, -self.max_seq_len:]
            input_mask = input_mask[:, -self.max_seq_len:]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/performer-pytorch/commit/e10ec182c2aba2eba66b59a175ed767251aa3889#diff-192164ea998deaff038129dced1fc9dbccf034f70eaf2ffa88b31af860963378L35' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 59937239</div><div id='project'> Project Name: lucidrains/performer-pytorch</div><div id='commit'> Commit Name: e10ec182c2aba2eba66b59a175ed767251aa3889</div><div id='time'> Time: 2020-12-08</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: performer_pytorch/autoregressive_wrapper.py</div><div id='m_class'> M Class Name: AutoregressiveWrapper</div><div id='n_method'> N Class Name: AutoregressiveWrapper</div><div id='m_method'> M Method Name: generate(7)</div><div id='n_method'> N Method Name: generate(7)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: performer_pytorch/autoregressive_wrapper.py</div><div id='n_file'> N File Name: performer_pytorch/autoregressive_wrapper.py</div><div id='m_start'> M Start Line: 52</div><div id='m_end'> M End Line: 57</div><div id='n_start'> N Start Line: 55</div><div id='n_end'> N End Line: 62</div><BR>