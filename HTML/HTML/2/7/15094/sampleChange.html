<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        ) / 2

        &#47&#47 ------- classification loss -------
        logits = torch.cat((logits1<a id="change">, logits2</a>))
        target = target.repeat(2)
        class_loss = F.cross_entropy(logits, target, ignore_index=-1)

        &#47&#47 just add together the losses to do only one backward()
        &#47&#47 we have stop gradients on the output y of the model
        loss = nce_loss + class_loss

        &#47&#47 ------- update queue -------
        keys = torch.stack((gather(k1), gather(k2)))
        self._dequeue_and_enqueue(keys)

        &#47&#47 ------- metrics -------
        acc1, acc5 = accuracy_at_k(logits, target, top_k=(1, 5))

        metrics = {
            "train_nce_loss": nce_loss,
            "train_class_loss": class_loss,
            "train_acc1": acc1,
            "train_acc5": acc5,
        }
        self.log_dict(metrics, on_epoch=True, sync_dist=True)
        <a id="change">return </a>loss

    def on_train_batch_end(self, outputs, batch, batch_idx, dataloader_idx):
        &#47&#47 log tau momentum</code></pre><h3>After Change</h3><pre><code class='java'>
        q1 = F.normalize(q1)
        q2 = F.normalize(q2)

        <a id="change">with torch.no_grad()</a><a id="change">:
            </a>k1 = self.momentum_projector(feats1_momentum)
            k2 = self.momentum_projector(feats2_momentum)
            k1<a id="change"> = </a>F.normalize(k1)
            k2<a id="change"> = </a>F.normalize(k2)

        &#47&#47 ------- contrastive loss -------
        &#47&#47 symmetric</code></pre>