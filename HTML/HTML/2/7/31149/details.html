<html><h3>Pattern ID :31149
</h3><img src='91469387.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

                p_data_fp32.add_(-update)

                <a id="change">p.data.copy_(</a>p_data_fp32<a id="change">)</a>

        return loss
</code></pre><h3>After Change</h3><pre><code class='java'>
                p_data_fp32.add_(-update)

                &#47&#47 TODO: remove check once pyTorch avoids a copy for this case
                <a id="change">if p.data_ptr() != p_data_fp32.data_ptr()</a>:
                    <a id="change">p.data.copy_(</a>p_data_fp32<a id="change">)</a>

        return loss
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/mohammadkhalifa/fairseq-tagging/commit/54dacd356d39f13f7d8152de414bb3a2a49e8bb4#diff-c8ff6e9ad677733577ae29b16bab6bcd5f2a83bdb9fb9e5e7f79ee3f42939b1cL155' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 91469387</div><div id='project'> Project Name: mohammadkhalifa/fairseq-tagging</div><div id='commit'> Commit Name: 54dacd356d39f13f7d8152de414bb3a2a49e8bb4</div><div id='time'> Time: 2020-03-10</div><div id='author'> Author: msb@fb.com</div><div id='file'> File Name: fairseq/optim/adafactor.py</div><div id='m_class'> M Class Name: Adafactor</div><div id='n_method'> N Class Name: Adafactor</div><div id='m_method'> M Method Name: step(2)</div><div id='n_method'> N Method Name: step(2)</div><div id='m_parent_class'> M Parent Class: torch.optim.Optimizer</div><div id='n_parent_class'> N Parent Class: torch.optim.Optimizer</div><div id='m_file'> M File Name: fairseq/optim/adafactor.py</div><div id='n_file'> N File Name: fairseq/optim/adafactor.py</div><div id='m_start'> M Start Line: 226</div><div id='m_end'> M End Line: 226</div><div id='n_start'> N Start Line: 155</div><div id='n_end'> N End Line: 230</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

                p_data_fp32.add_(-update)

                <a id="change">p.data.copy_(</a>p_data_fp32<a id="change">)</a>

        return loss
</code></pre><h3>After Change</h3><pre><code class='java'>
            loss = closure()

        for group in self.param_groups:
            for <a id="change">p</a> in group[&quotparams&quot]:
                if p.grad is None:
                    continue
                grad = p.grad.data.float()
                if grad.is_sparse:
                    raise RuntimeError(&quotAdafactor does not support sparse gradients.&quot)

                state = self.state[p]
                grad_shape = grad.shape

                factored, use_first_moment = self._get_options(group, grad_shape)
                &#47&#47 State Initialization
                if len(state) == 0:
                    state[&quotstep&quot] = 0

                    if use_first_moment:
                        &#47&#47 Exponential moving average of gradient values
                        state[&quotexp_avg&quot] = torch.zeros_like(grad)
                    if factored:
                        state[&quotexp_avg_sq_row&quot] = torch.zeros(grad_shape[:-1]).type_as(grad)
                        state[&quotexp_avg_sq_col&quot] = torch.zeros(grad_shape[:-2] + grad_shape[-1:]).type_as(grad)
                    else:
                        state[&quotexp_avg_sq&quot] = torch.zeros_like(grad)

                    state[&quotRMS&quot] = 0
                else:
                    if use_first_moment:
                        state[&quotexp_avg&quot] = state[&quotexp_avg&quot].type_as(grad)
                    if factored:
                        state[&quotexp_avg_sq_row&quot] = state[&quotexp_avg_sq_row&quot].type_as(grad)
                        state[&quotexp_avg_sq_col&quot] = state[&quotexp_avg_sq_col&quot].type_as(grad)
                    else:
                        state[&quotexp_avg_sq&quot] = state[&quotexp_avg_sq&quot].type_as(grad)

                <a id="change">p_data_fp32</a> = p.data.float()

                state[&quotstep&quot] += 1
                state[&quotRMS&quot] = self._rms(p_data_fp32)
                group[&quotlr&quot] = self._get_lr(group, state)

                beta2t = 1.0 - math.pow(state[&quotstep&quot], group[&quotdecay_rate&quot])
                update = (grad**2) + group[&quoteps&quot][0]
                if factored:
                    exp_avg_sq_row = state[&quotexp_avg_sq_row&quot]
                    exp_avg_sq_col = state[&quotexp_avg_sq_col&quot]

                    exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))
                    exp_avg_sq_col.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-2))

                    &#47&#47 Approximation of exponential moving average of square of gradient
                    self._approx_sq_grad(exp_avg_sq_row, exp_avg_sq_col, update)
                    update.mul_(grad)
                else:
                    exp_avg_sq = state[&quotexp_avg_sq&quot]

                    exp_avg_sq.mul_(beta2t).add_(1.0 - beta2t, update)
                    torch.rsqrt(exp_avg_sq, out=update).mul_(grad)

                update.div_(max(1.0, self._rms(update) / group[&quotclip_threshold&quot]))
                update.mul_(group[&quotlr&quot])

                if use_first_moment:
                    exp_avg = state[&quotexp_avg&quot]
                    exp_avg.mul_(group[&quotbeta1&quot]).add_(1 - group[&quotbeta1&quot], update)
                    update = exp_avg

                if group[&quotweight_decay&quot] != 0:
                    p_data_fp32.add_(-group[&quotweight_decay&quot] * group[&quotlr&quot], p_data_fp32)

                p_data_fp32.add_(-update)

                &#47&#47 TODO: remove check once pyTorch avoids a copy for this case
                <a id="change">if p.data_ptr() != p_data_fp32.data_ptr()</a>:
                    <a id="change">p.data.copy_(</a>p_data_fp32<a id="change">)</a>

        return loss
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/kssteven418/i-bert/commit/54dacd356d39f13f7d8152de414bb3a2a49e8bb4#diff-c8ff6e9ad677733577ae29b16bab6bcd5f2a83bdb9fb9e5e7f79ee3f42939b1cL143' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 91469386</div><div id='project'> Project Name: kssteven418/i-bert</div><div id='commit'> Commit Name: 54dacd356d39f13f7d8152de414bb3a2a49e8bb4</div><div id='time'> Time: 2020-03-10</div><div id='author'> Author: msb@fb.com</div><div id='file'> File Name: fairseq/optim/adafactor.py</div><div id='m_class'> M Class Name: Adafactor</div><div id='n_method'> N Class Name: Adafactor</div><div id='m_method'> M Method Name: step(2)</div><div id='n_method'> N Method Name: step(2)</div><div id='m_parent_class'> M Parent Class: torch.optim.Optimizer</div><div id='n_parent_class'> N Parent Class: torch.optim.Optimizer</div><div id='m_file'> M File Name: fairseq/optim/adafactor.py</div><div id='n_file'> N File Name: fairseq/optim/adafactor.py</div><div id='m_start'> M Start Line: 226</div><div id='m_end'> M End Line: 226</div><div id='n_start'> N Start Line: 155</div><div id='n_end'> N End Line: 230</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

                p_data_fp32.addcdiv_(-step_size, exp_avg, denom)

                <a id="change">p.data.copy_(</a>p_data_fp32<a id="change">)</a>

        return loss
</code></pre><h3>After Change</h3><pre><code class='java'>
            loss = closure()

        for group in self.param_groups:
            for <a id="change">p</a> in group[&quotparams&quot]:
                if p.grad is None:
                    continue
                grad = p.grad.data.float()
                if grad.is_sparse:
                    raise RuntimeError(&quotAdam does not support sparse gradients, please consider SparseAdam instead&quot)
                amsgrad = group[&quotamsgrad&quot]

                <a id="change">p_data_fp32</a> = p.data.float()

                state = self.state[p]

                &#47&#47 State initialization
                if len(state) == 0:
                    state[&quotstep&quot] = 0
                    &#47&#47 Exponential moving average of gradient values
                    state[&quotexp_avg&quot] = torch.zeros_like(p_data_fp32)
                    &#47&#47 Exponential moving average of squared gradient values
                    state[&quotexp_avg_sq&quot] = torch.zeros_like(p_data_fp32)
                    if amsgrad:
                        &#47&#47 Maintains max of all exp. moving avg. of sq. grad. values
                        state[&quotmax_exp_avg_sq&quot] = torch.zeros_like(p_data_fp32)
                else:
                    state[&quotexp_avg&quot] = state[&quotexp_avg&quot].type_as(p_data_fp32)
                    state[&quotexp_avg_sq&quot] = state[&quotexp_avg_sq&quot].type_as(p_data_fp32)
                    if amsgrad:
                        state[&quotmax_exp_avg_sq&quot] = state[&quotmax_exp_avg_sq&quot].type_as(p_data_fp32)

                exp_avg, exp_avg_sq = state[&quotexp_avg&quot], state[&quotexp_avg_sq&quot]
                if amsgrad:
                    max_exp_avg_sq = state[&quotmax_exp_avg_sq&quot]
                beta1, beta2 = group[&quotbetas&quot]

                state[&quotstep&quot] += 1

                &#47&#47 Decay the first and second moment running average coefficient
                exp_avg.mul_(beta1).add_(1 - beta1, grad)
                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
                if amsgrad:
                    &#47&#47 Maintains the maximum of all 2nd moment running avg. till now
                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)
                    &#47&#47 Use the max. for normalizing running avg. of gradient
                    denom = max_exp_avg_sq.sqrt().add_(group[&quoteps&quot])
                else:
                    denom = exp_avg_sq.sqrt().add_(group[&quoteps&quot])

                bias_correction1 = 1 - beta1 ** state[&quotstep&quot]
                bias_correction2 = 1 - beta2 ** state[&quotstep&quot]
                step_size = group[&quotlr&quot] * math.sqrt(bias_correction2) / bias_correction1

                if group[&quotweight_decay&quot] != 0:
                    p_data_fp32.add_(-group[&quotweight_decay&quot] * group[&quotlr&quot], p_data_fp32)

                p_data_fp32.addcdiv_(-step_size, exp_avg, denom)

                &#47&#47 TODO: remove check once pyTorch avoids a copy for this case
                <a id="change">if p.data_ptr() != p_data_fp32.data_ptr()</a>:
                    <a id="change">p.data.copy_(</a>p_data_fp32<a id="change">)</a>

        return loss
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/kssteven418/i-bert/commit/54dacd356d39f13f7d8152de414bb3a2a49e8bb4#diff-d0f1814d2f162ba7c67209130e8a071048855102aa528fbdae9b64fe5a0ab649L131' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 91469385</div><div id='project'> Project Name: kssteven418/i-bert</div><div id='commit'> Commit Name: 54dacd356d39f13f7d8152de414bb3a2a49e8bb4</div><div id='time'> Time: 2020-03-10</div><div id='author'> Author: msb@fb.com</div><div id='file'> File Name: fairseq/optim/adam.py</div><div id='m_class'> M Class Name: Adam</div><div id='n_method'> N Class Name: Adam</div><div id='m_method'> M Method Name: step(2)</div><div id='n_method'> N Method Name: step(2)</div><div id='m_parent_class'> M Parent Class: torch.optim.Optimizer</div><div id='n_parent_class'> N Parent Class: torch.optim.Optimizer</div><div id='m_file'> M File Name: fairseq/optim/adam.py</div><div id='n_file'> N File Name: fairseq/optim/adam.py</div><div id='m_start'> M Start Line: 198</div><div id='m_end'> M End Line: 198</div><div id='n_start'> N Start Line: 143</div><div id='n_end'> N End Line: 202</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

                buf.mul_(momentum * lr_correct).add_(-lr, d_p)

                <a id="change">p.data.copy_(</a>p_data_fp32<a id="change">)</a>

            group[&quotlr_old&quot] = lr

        return loss</code></pre><h3>After Change</h3><pre><code class='java'>
            lr_old = group.get(&quotlr_old&quot, lr)
            lr_correct = lr / lr_old

            for <a id="change">p</a> in group[&quotparams&quot]:
                if p.grad is None:
                    continue

                <a id="change">p_data_fp32</a> = p.data.float()

                d_p = p.grad.data.float()
                param_state = self.state[p]
                if &quotmomentum_buffer&quot not in param_state:
                    param_state[&quotmomentum_buffer&quot] = torch.zeros_like(d_p)
                else:
                    param_state[&quotmomentum_buffer&quot] = param_state[&quotmomentum_buffer&quot].type_as(d_p)

                buf = param_state[&quotmomentum_buffer&quot]

                if weight_decay != 0:
                    p_data_fp32.mul_(1 - lr * weight_decay)
                p_data_fp32.add_(momentum * momentum * lr_correct, buf)
                p_data_fp32.add_(-(1 + momentum) * lr, d_p)

                buf.mul_(momentum * lr_correct).add_(-lr, d_p)

                &#47&#47 TODO: remove check once pyTorch avoids a copy for this case
                <a id="change">if p.data_ptr() != p_data_fp32.data_ptr()</a>:
                    <a id="change">p.data.copy_(</a>p_data_fp32<a id="change">)</a>

            group[&quotlr_old&quot] = lr

        return loss</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/kssteven418/i-bert/commit/54dacd356d39f13f7d8152de414bb3a2a49e8bb4#diff-4d9171175a0f1c4dd317d38b7189e54c34cab9fad3ee87b35a4d3388a9d3f212L56' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 91469384</div><div id='project'> Project Name: kssteven418/i-bert</div><div id='commit'> Commit Name: 54dacd356d39f13f7d8152de414bb3a2a49e8bb4</div><div id='time'> Time: 2020-03-10</div><div id='author'> Author: msb@fb.com</div><div id='file'> File Name: fairseq/optim/nag.py</div><div id='m_class'> M Class Name: NAG</div><div id='n_method'> N Class Name: NAG</div><div id='m_method'> M Method Name: step(2)</div><div id='n_method'> N Method Name: step(2)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: fairseq/optim/nag.py</div><div id='n_file'> N File Name: fairseq/optim/nag.py</div><div id='m_start'> M Start Line: 96</div><div id='m_end'> M End Line: 96</div><div id='n_start'> N Start Line: 74</div><div id='n_end'> N End Line: 100</div><BR>