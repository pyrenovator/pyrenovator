<html><h3>Pattern ID :2307
</h3><img src='9792309.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 Knowledge Distillation only
    if config["KD"]["use"]:
        kd_criterion = getattr(module_loss, <a id="change">config["KD"]["kd_loss"]</a>)(config["KD"]["temperature"])
        trainer = TrainerTeacherAssistant(student, teacher, criterion, kd_criterion, metrics, optimizer,
                                          config=config,
                                          train_data_loader=train_data_loader,</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 get function handles of loss and metrics
    supervised_criterion = config.init_obj(&quotsupervised_loss&quot, module_loss)
    div_criterion = config.init_obj(&quotdiv_loss&quot, module_loss)
    kd_criterion = <a id="change">config.init_obj(&quotkd_loss&quot</a>, module_loss<a id="change">)</a>
    criterions<a id="change"> = </a>[supervised_criterion, div_criterion, kd_criterion]
    metrics = [getattr(module_metric, met) for met in config[&quotmetrics&quot]]

    &#47&#47 build optimizer, learning rate scheduler. delete every lines containing lr_scheduler for disabling scheduler</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lehduong/knowledge-distillation-by-replacing-cheap-conv/commit/7a76fa7853083b3d5261cb4aaa7dc5ef33354e0e#diff-ed183d67207df065a11e1289f19d34cc2abbc5448dea952683cfe9728c342b95L22' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 9792309</div><div id='project'> Project Name: lehduong/knowledge-distillation-by-replacing-cheap-conv</div><div id='commit'> Commit Name: 7a76fa7853083b3d5261cb4aaa7dc5ef33354e0e</div><div id='time'> Time: 2020-01-25</div><div id='author'> Author: oopsxilitol@gmail.com</div><div id='file'> File Name: train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: train.py</div><div id='n_file'> N File Name: train.py</div><div id='m_start'> M Start Line: 22</div><div id='m_end'> M End Line: 54</div><div id='n_start'> N Start Line: 29</div><div id='n_end'> N End Line: 58</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
def main(config):
    &#47&#47 setup data_loader instances
    data_loader = getattr(module_data, config[&quotdata_loader&quot][&quottype&quot])(
        <a id="change">config[&quotdata_loader&quot][&quotargs&quot]</a>[&quotdata_dir&quot],
        batch_size=512,
        shuffle=False,
        validation_split=0.0,</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 testset
    testset = config.init_obj(&quotdataset&quot, module_data, mode=&quottest&quot)
    &#47&#47 dataloader
    testloader<a id="change"> = </a><a id="change">config.init_obj(&quotdata_loader&quot</a>, module_data, testset<a id="change">)</a>

    &#47&#47 build model architecture
    model = config.init_obj(&quotmodel&quot, module_model)
    logger.debug(model)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/deeperlearner/pytorch-template/commit/49ac2e7c4e18177db31ae741c8dfd7cdbf5ca0f0#diff-3665d65394f4f58a56a256ad6dd8621c68118d90fe56a19387e251c19cec2d2eL13' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 9792304</div><div id='project'> Project Name: deeperlearner/pytorch-template</div><div id='commit'> Commit Name: 49ac2e7c4e18177db31ae741c8dfd7cdbf5ca0f0</div><div id='time'> Time: 2020-11-11</div><div id='author'> Author: b04202035@g.ntu.edu.tw</div><div id='file'> File Name: test.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: test.py</div><div id='n_file'> N File Name: test.py</div><div id='m_start'> M Start Line: 15</div><div id='m_end'> M End Line: 62</div><div id='n_start'> N Start Line: 20</div><div id='n_end'> N End Line: 76</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.logger.info("Loaded model&quots state dict successfully")

        &#47&#47 load optimizer state from checkpoint only when optimizer type is not changed.
        if <a id="change">checkpoint[&quotconfig&quot][&quotoptimizer&quot][&quottype&quot]</a> != self.config[&quotoptimizer&quot][&quottype&quot]:
            self.logger.warning("Warning: Optimizer type given in config file is different from that of checkpoint. "
                                "Optimizer parameters not being resumed.")
        else:</code></pre><h3>After Change</h3><pre><code class='java'>

            if i == 0 and list(filter(lambda x: x.requires_grad, self.model.parameters())) == 0:
                self.logger.debug(&quotCreating new optimizer...&quot)
                self.optimizer<a id="change"> = </a><a id="change">checkpoint[&quotconfig&quot].init_obj(&quotoptimizer&quot</a>, optim_module, new_layer.parameters<a id="change">)</a>
            else:
                optimizer_arg = checkpoint[&quotconfig&quot][&quotoptimizer&quot][&quotargs&quot]
                self.optimizer.add_param_group({&quotparams&quot: new_layer.parameters(),
                                                **optimizer_arg})</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lehduong/knowledge-distillation-by-replacing-cheap-conv/commit/8aff10acbbb2ad57877dab14f5309cd6628f2c4f#diff-60bd2a7eb048049f580030840b0b7273b0f45a45f557d882b4d695bfeb140c57L82' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 9792301</div><div id='project'> Project Name: lehduong/knowledge-distillation-by-replacing-cheap-conv</div><div id='commit'> Commit Name: 8aff10acbbb2ad57877dab14f5309cd6628f2c4f</div><div id='time'> Time: 2020-02-20</div><div id='author'> Author: oopsxilitol@gmail.com</div><div id='file'> File Name: trainer/takdp_trainer.py</div><div id='m_class'> M Class Name: TAKDPTrainer</div><div id='n_method'> N Class Name: TAKDPTrainer</div><div id='m_method'> M Method Name: _resume_checkpoint(2)</div><div id='n_method'> N Method Name: _resume_checkpoint(2)</div><div id='m_parent_class'> M Parent Class: KDPTrainer</div><div id='n_parent_class'> N Parent Class: KDPTrainer</div><div id='m_file'> M File Name: trainer/takdp_trainer.py</div><div id='n_file'> N File Name: trainer/takdp_trainer.py</div><div id='m_start'> M Start Line: 90</div><div id='m_end'> M End Line: 158</div><div id='n_start'> N Start Line: 90</div><div id='n_end'> N End Line: 150</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            layer_name = checkpoint[&quottrained_ta_layers&quot][idx]
            args.append(DistillationArgs(layer_name, new_layer, layer_name))

            optimizer_arg = <a id="change">checkpoint[&quotconfig&quot][&quotoptimizer&quot][&quotargs&quot]</a>
            self.optimizer.add_param_group({&quotparams&quot: new_layer.parameters(),
                                            **optimizer_arg})
        &#47&#47 TODO: Causing problem if not call assign_blocks, will fix later
        self.model.update_pruned_layers(args)</code></pre><h3>After Change</h3><pre><code class='java'>
        self.model.to_teacher()

        &#47&#47 Prune training TA layers
        self.optimizer<a id="change"> = </a><a id="change">self.config.init_obj(&quotoptimizer&quot</a>, optim_module, self.model.parameters()<a id="change">)</a>
        new_layers = []
        self.logger.info(&quotBegin pruning training TA layers&quot)
        for idx, layer in enumerate(training_ta_layers):
            layer_name = checkpoint[&quottraining_ta_layers&quot][idx]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lehduong/knowledge-distillation-by-replacing-cheap-conv/commit/1031bc372794acfa4fc46c671d180e088bc5def1#diff-60bd2a7eb048049f580030840b0b7273b0f45a45f557d882b4d695bfeb140c57L88' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 9792312</div><div id='project'> Project Name: lehduong/knowledge-distillation-by-replacing-cheap-conv</div><div id='commit'> Commit Name: 1031bc372794acfa4fc46c671d180e088bc5def1</div><div id='time'> Time: 2020-02-20</div><div id='author'> Author: oopsxilitol@gmail.com</div><div id='file'> File Name: trainer/takdp_trainer.py</div><div id='m_class'> M Class Name: TAKDPTrainer</div><div id='n_method'> N Class Name: TAKDPTrainer</div><div id='m_method'> M Method Name: _resume_checkpoint(2)</div><div id='n_method'> N Method Name: _resume_checkpoint(2)</div><div id='m_parent_class'> M Parent Class: KDPTrainer</div><div id='n_parent_class'> N Parent Class: KDPTrainer</div><div id='m_file'> M File Name: trainer/takdp_trainer.py</div><div id='n_file'> N File Name: trainer/takdp_trainer.py</div><div id='m_start'> M Start Line: 96</div><div id='m_end'> M End Line: 127</div><div id='n_start'> N Start Line: 134</div><div id='n_end'> N End Line: 134</div><BR>