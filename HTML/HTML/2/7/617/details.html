<html><h3>Pattern ID :617
</h3><img src='3000525.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        avg_target = target.mean(dim=3)

        bin_size = self.max_pixel_val<a id="change"> / </a>self.output_channel_bits
        channel_bins = torch.arange(bin_size, self.max_pixel_val, bin_size).to(avg_target.device)
        discretized_target = torch.bucketize(avg_target, channel_bins)
        discretized_target<a id="change"> = </a>F.one_hot(discretized_target,
                                       self.output_channel_bits)
        c, bi = self.channels, self.output_channel_bits
        discretized_target = <a id="change">rearrange(</a>discretized_target,
                                       <a id="change">"b n c bi -&gt; b n (c bi)"</a><a id="change">,
                                       c=c,
                                       bi=bi)</a>

        bin_mask = 2**torch.arange(c<a id="change"> * bi - </a>1, -1,
                                   -1).to(discretized_target.device,
                                          discretized_target.dtype)
        target_label = torch.sum(bin_mask * discretized_target, -1)
        predicted_patches = predicted_patches[mask]
        target_label = target_label[mask]
        loss<a id="change"> = </a>F.cross_entropy(predicted_patches, target_label)
        return loss

</code></pre><h3>After Change</h3><pre><code class='java'>
        self.std = torch.tensor(std).view(-1, 1, 1) if std else None

    def forward(self, predicted_patches, target, mask):
        p<a id="change">, c, mpv, bits, device</a> = self.patch_size, self.channels, self.max_pixel_val, self.output_channel_bits, target.device
        bin_size = mpv / (2 ** bits)

        &#47&#47 un-normalize input</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/vit-pytorch/commit/64a2ef6462bde61db4dd8f0887ee71192b273692#diff-6b502c3fca9000d4ac485e72a6b6cb51b335fed8a8e158013b955ffed39c59abL53' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 3000525</div><div id='project'> Project Name: lucidrains/vit-pytorch</div><div id='commit'> Commit Name: 64a2ef6462bde61db4dd8f0887ee71192b273692</div><div id='time'> Time: 2021-06-16</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: vit_pytorch/mpp.py</div><div id='m_class'> M Class Name: MPPLoss</div><div id='n_method'> N Class Name: MPPLoss</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: vit_pytorch/mpp.py</div><div id='n_file'> N File Name: vit_pytorch/mpp.py</div><div id='m_start'> M Start Line: 53</div><div id='m_end'> M End Line: 82</div><div id='n_start'> N Start Line: 53</div><div id='n_end'> N End Line: 72</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        weight = weight.masked_fill(~mask, mask_value)

    if causal and q_start_index &lt; (k_start_index + k_chunk_size - 1):
        q_range<a id="change"> = </a>torch.arange(q_start_index, q_start_index<a id="change"> + </a>q_chunk_size, device = device)
        k_range = torch.arange(k_start_index, k_start_index<a id="change"> + </a>k_chunk_size, device = device)
        causal_mask<a id="change"> = </a><a id="change">rearrange(</a>q_range, <a id="change">&quoti -&gt; i 1&quot</a><a id="change">)</a> &lt; rearrange(k_range, &quotj -&gt; 1 j&quot)
        weight = weight.masked_fill(causal_mask, mask_value)

    exp_weight = weight.exp()</code></pre><h3>After Change</h3><pre><code class='java'>
        weight = weight.masked_fill(~mask, mask_value)

    if causal and q_start_index &lt; (k_start_index + k_chunk_size - 1):
        causal_mask = torch.ones((q_chunk_size<a id="change">, k_chunk_size</a>), dtype = torch.bool, device = device).triu(q_start_index - k_start_index + 1)
        weight = weight.masked_fill(causal_mask, mask_value)

    exp_weight = weight.exp()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/memory-efficient-attention-pytorch/commit/e4d09988df0bc8bdfc32cfd2d7202d061a640052#diff-da6685a180d8c8c636b32d699e32ddc64180690f91b63dc75a91d3846cb1bb37L53' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 3000477</div><div id='project'> Project Name: lucidrains/memory-efficient-attention-pytorch</div><div id='commit'> Commit Name: e4d09988df0bc8bdfc32cfd2d7202d061a640052</div><div id='time'> Time: 2022-03-22</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memory_efficient_attention_pytorch/memory_efficient_cosine_sim_attention.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: summarize_qkv_chunk(7)</div><div id='n_method'> N Method Name: summarize_qkv_chunk(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: memory_efficient_attention_pytorch/memory_efficient_cosine_sim_attention.py</div><div id='n_file'> N File Name: memory_efficient_attention_pytorch/memory_efficient_cosine_sim_attention.py</div><div id='m_start'> M Start Line: 68</div><div id='m_end'> M End Line: 70</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 68</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        weight = weight.masked_fill(~mask, mask_value)

    if causal and q_start_index &lt; (k_start_index + k_chunk_size - 1):
        q_range<a id="change"> = </a>torch.arange(q_start_index, q_start_index<a id="change"> + </a>q_chunk_size, device = device)
        k_range = torch.arange(k_start_index, k_start_index<a id="change"> + </a>k_chunk_size, device = device)
        causal_mask<a id="change"> = </a><a id="change">rearrange(</a>q_range, <a id="change">&quoti -&gt; i 1&quot</a><a id="change">)</a> &lt; rearrange(k_range, &quotj -&gt; 1 j&quot)
        weight = weight.masked_fill(causal_mask, mask_value)

    weight_max = weight.amax(dim = -1, keepdim = True).detach()</code></pre><h3>After Change</h3><pre><code class='java'>
        weight = weight.masked_fill(~mask, mask_value)

    if causal and q_start_index &lt; (k_start_index + k_chunk_size - 1):
        causal_mask = torch.ones((q_chunk_size<a id="change">, k_chunk_size</a>), dtype = torch.bool, device = device).triu(q_start_index - k_start_index + 1)
        weight = weight.masked_fill(causal_mask, mask_value)

    weight_max = weight.amax(dim = -1, keepdim = True).detach()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/memory-efficient-attention-pytorch/commit/e4d09988df0bc8bdfc32cfd2d7202d061a640052#diff-7de1bf8844a9aafc0b616bb3f7f2bc3701cbdab6390a8d8096e5dfea36da1708L52' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 3000500</div><div id='project'> Project Name: lucidrains/memory-efficient-attention-pytorch</div><div id='commit'> Commit Name: e4d09988df0bc8bdfc32cfd2d7202d061a640052</div><div id='time'> Time: 2022-03-22</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memory_efficient_attention_pytorch/memory_efficient_attention.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: summarize_qkv_chunk(7)</div><div id='n_method'> N Method Name: summarize_qkv_chunk(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: memory_efficient_attention_pytorch/memory_efficient_attention.py</div><div id='n_file'> N File Name: memory_efficient_attention_pytorch/memory_efficient_attention.py</div><div id='m_start'> M Start Line: 67</div><div id='m_end'> M End Line: 69</div><div id='n_start'> N Start Line: 67</div><div id='n_end'> N End Line: 67</div><BR>