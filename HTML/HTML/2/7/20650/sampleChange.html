<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 https://www.biorxiv.org/content/10.1101/2020.11.27.401232v1.full.pdf
    if eigen == True:
        if weights is None:
            preds_3d<a id="change"> = </a><a id="change">[]</a>
            for bi in range(pre_dist_mat.shape[0]):
                D = pre_dist_mat[bi]**2
                M = D[:1, :] + D[:, :1] - D 
                u,s,v = torch.svd_lowrank(M/2)</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 do it by eigendecomposition - way faster but not weights
    &#47&#47 https://www.biorxiv.org/content/10.1101/2020.11.27.401232v1.full.pdf
    if eigen == True:
        <a id="change">if </a>weights is None:
            D = pre_dist_mat**2
            M =  0.5 * (D[:, :1, :] + D[:, :, :1] - D) 
            &#47&#47 do loop svd bc it&quots faster: (2-3x in CPU and 1-2x in GPU)
            &#47&#47 https://discuss.pytorch.org/t/batched-svd-lowrank-being-much-slower-than-loop-implementation-both-cpu-and-gpu/119336
            svds = [torch.svd_lowrank(mi) for mi in M]
            u = torch.stack([svd[0] for svd in svds], dim=0)
            s = torch.stack([svd[1] for svd in svds], dim=0)
            v = torch.stack([svd[2] for svd in svds], dim=0)
            preds_3d<a id="change"> = </a><a id="change">torch.transpose( </a>torch.bmm(u, torch.diag_embed(s).sqrt())[..., :3], <a id="change">-1</a>, <a id="change">-2</a><a id="change">)</a>
            
            return preds_3d, torch.zeros_like(torch.stack(his, dim=0))
        else:
            if verbose:</code></pre>