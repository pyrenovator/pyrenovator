<html><h3>Pattern ID :8686
</h3><img src='31560041.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Start training.
        print(&quotStart training...&quot)
        start_time = time.time()
        <a id="change">for </a>i in range(start_iters, self.num_iters)<a id="change">:

            &#47&#47 =================================================================================== &#47&#47
            &#47&#47                             1. Preprocess input data                                &#47&#47
            &#47&#47 =================================================================================== &#47&#47

            &#47&#47 Fetch real images and labels.
            </a>try:
                x_real, label_org = next(data_iter)
            except:
                data_iter = iter(data_loader)</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 Start training.
        print(&quotStart training...&quot)
        start_time = time.time()
        <a id="change">for </a><a id="change">i</a> in range(start_iters, self.num_iters)<a id="change">:

            </a><a id="change">try:
                &#47&#47 =================================================================================== &#47&#47
                &#47&#47                             1. Preprocess input data                                &#47&#47
                &#47&#47 =================================================================================== &#47&#47

                &#47&#47 Fetch real images and labels.
                </a>try:
                    x_real, label_org = next(data_iter)
                    label_org = torch.unsqueeze(label_org, dim=1)
                except:
                    data_iter = iter(self.data_loader)
                    x_real, label_org = next(data_iter)

                &#47&#47 Generate target domain labels randomly.
                rand_idx = torch.randperm(label_org.size(0))
                label_trg = label_org[rand_idx]

                &#47&#47 if self.dataset == &quotCelebA&quot:
                &#47&#47     c_org = label_org.clone()
                &#47&#47     c_trg = label_trg.clone()
                &#47&#47 elif self.dataset == &quotRaFD&quot:
                &#47&#47     c_org = self.label2onehot(label_org, self.c_dim)
                &#47&#47     c_trg = self.label2onehot(label_trg, self.c_dim)

                x_real = x_real.to(self.device)  &#47&#47 Input images.
                &#47&#47 c_org = c_org.to(self.device)  &#47&#47 Original domain labels.
                &#47&#47 c_trg = c_trg.to(self.device)  &#47&#47 Target domain labels.
                label_org = label_org.to(self.device)  &#47&#47 Labels for computing classification loss.
                label_trg = label_trg.to(self.device)  &#47&#47 Labels for computing classification loss.

                &#47&#47 =================================================================================== &#47&#47
                &#47&#47                             2. Train the discriminator                              &#47&#47
                &#47&#47 =================================================================================== &#47&#47
                &#47&#47 self.d_optimizer.zero_grad()
                &#47&#47 Compute loss with real images.
                &#47&#47 out_src, out_cls = self.D(x_real)
                &#47&#47 out_real = self.D(x_real, num_domains=2)

                &#47&#47 d_out_real = torch.gather(out_real, 1, label_org.long())
                &#47&#47 d_loss_real = torch.mean(torch.log(d_out_src))

                &#47&#47 d_loss_real = self.bce_loss(self.zeros_target(self.batch_size), d_out_real)
                &#47&#47 d_loss_cls = self.classification_loss(out_cls, label_org, self.dataset)
                &#47&#47
                &#47&#47 &#47&#47 z:latent code s_tilde:target style code
                &#47&#47 z = self.noise(size=self.batch_size, dimension=16, device=self.device)
                &#47&#47 s_tilde = self.F(z, num_domains=2)
                &#47&#47
                &#47&#47 &#47&#47 target style code s_tilde
                &#47&#47
                &#47&#47 &#47&#47 Compute loss with fake images.
                &#47&#47 &#47&#47 s_tilde_tensor = torch.stack(s_tilde, 1)
                &#47&#47 s_tilde_trg = torch.index_select(torch.stack(s_tilde, 1), 1, label_trg.squeeze().long())[:, 0, :]
                &#47&#47 &#47&#47 s_tilde_trg = torch.gather(s_tilde_tensor, 1, label_trg.expand(s_tilde_tensor.size()).long())
                &#47&#47 &#47&#47 s_tilde_trg = torch.gather(torch.stack(s_tilde, 1), 1, torch.unsqueeze(label_trg, 2).long())
                &#47&#47
                &#47&#47 d_x_fake = self.G(x_real, s_tilde_trg)
                &#47&#47 out_fake = self.D(d_x_fake.detach())
                &#47&#47 d_out_fake = torch.gather(out_fake, 1, label_trg.long())
                &#47&#47 &#47&#47 d_loss_fake = torch.mean(out_fake)
                &#47&#47 d_loss_fake = self.bce_loss(self.ones_target(self.batch_size), d_out_fake)

                &#47&#47 &#47&#47 Compute loss for gradient penalty.
                &#47&#47 alpha = torch.rand(x_real.size(0), 1, 1, 1).to(self.device)
                &#47&#47 x_hat = (alpha * x_real.data + (1 - alpha) * x_fake.data).requires_grad_(True)
                &#47&#47 out_src, _ = self.D(x_hat)
                &#47&#47 d_loss_gp = self.gradient_penalty(out_src, x_hat)

                &#47&#47 &#47&#47 Backward and optimize.
                &#47&#47 d_loss = -(d_loss_real + d_loss_fake)
                &#47&#47
                &#47&#47 &#47&#47 R1 regularization
                &#47&#47 l1_norm = torch.norm(self.D.weight, p=1)
                &#47&#47 d_loss += l1_norm

                s_tilde_trg = self.generate_style_code(label_trg)
                x_fake = self.G(x_real, s_tilde_trg)
                &#47&#47 fake_logits = self.D(x_fake)

                &#47&#47 d_loss, d_loss_real, d_loss_fake = self.compute_adversarial_loss(True, x_real, label_org, d_x_fake, label_trg)
                d_loss, d_loss_real, d_loss_fake = self.train_discriminator(x_real, x_fake, label_org,
                                                                            label_trg)

                &#47&#47 d_loss = d_loss_real + d_loss_fake + self.lambda_cls * d_loss_cls + self.lambda_gp * d_loss_gp
                &#47&#47 d_loss = -d_loss
                &#47&#47 self.reset_grad()
                &#47&#47 d_loss.backward()
                &#47&#47 self.d_optimizer.step()

                &#47&#47 Logging.
                loss = {
                    &quotD/loss&quot: d_loss.item(),
                    &quotD/loss_real&quot: d_loss_real.item(),
                    &quotD/loss_fake&quot: d_loss_fake.item()
                }

                writer.add_scalar(&quotD/loss&quot, d_loss.item(), i)
                writer.add_scalar(&quotD/loss_real&quot, d_loss_real.item(), i)
                writer.add_scalar(&quotD/loss_fake&quot, d_loss_fake.item(), i)

                &#47&#47 =================================================================================== &#47&#47
                &#47&#47                               3. Train the generator                                &#47&#47
                &#47&#47 =================================================================================== &#47&#47

                if (i + 1) % self.n_critic == 0:
                    &#47&#47 &#47&#47 style reconstruction
                    &#47&#47 g_s_tilde_trg = self.generate_style_code(label_trg)
                    &#47&#47 &#47&#47 g_x_fake = self.G(x_real, g_s_tilde_trg)
                    &#47&#47 &#47&#47 s_hat = self.E(d_x_fake)
                    &#47&#47
                    &#47&#47 &#47&#47 s_hat: estimated style code of source image
                    &#47&#47 &#47&#47 loss style reconstruction:style reconstruction
                    &#47&#47 s_hat = self.E(self.G(x_real, g_s_tilde_trg), num_domains=2)
                    &#47&#47 s_hat_trg = torch.index_select(torch.stack(s_hat, 1), 1, label_trg.squeeze().long())[:, 0, :]
                    &#47&#47 g_loss_sty = self.l1_loss(g_s_tilde_trg, s_hat_trg)
                    &#47&#47
                    &#47&#47 &#47&#47 loss cycle: preserving source characteristics
                    &#47&#47 s_hat_org = torch.index_select(torch.stack(s_hat, 1), 1, label_org.squeeze().long())[:, 0, :]
                    &#47&#47 x_fake_cyc = self.G(self.G(x_real, g_s_tilde_trg), s_hat_org)
                    &#47&#47 g_loss_cyc = self.l1_loss(x_real, x_fake_cyc)
                    &#47&#47
                    &#47&#47 &#47&#47 loss style diversification:style diversification
                    &#47&#47 &#47&#47 z1 = self.noise(size=self.batch_size, dimension=16)
                    &#47&#47 &#47&#47 s1_tilde = self.F(z1, num_domains=2)
                    &#47&#47 &#47&#47 s1_tilde_trg = torch.index_select(torch.stack(s1_tilde, 1), 1, label_trg.squeeze().long())[:, 0, :]
                    &#47&#47 s1_tilde_trg = self.generate_style_code(label_trg)
                    &#47&#47 &#47&#47 z2 = self.noise(size=self.batch_size, dimension=16)
                    &#47&#47 &#47&#47 s2_tilde = self.F(z2, num_domains=2)
                    &#47&#47 &#47&#47 s2_tilde_trg = torch.index_select(torch.stack(s2_tilde, 1), 1, label_trg.squeeze().long())[:, 0, :]
                    &#47&#47 s2_tilde_trg = self.generate_style_code(label_trg)
                    &#47&#47 g_loss_ds = self.l1_loss(self.G(x_real, s1_tilde_trg), self.G(x_real, s2_tilde_trg))

                    &#47&#47 &#47&#47 Original-to-target domain.
                    &#47&#47 x_fake = self.G(x_real, c_trg)
                    &#47&#47 out_src, out_cls = self.D(x_fake)
                    &#47&#47 g_loss_fake = - torch.mean(out_src)
                    &#47&#47 g_loss_cls = self.classification_loss(out_cls, label_trg, self.dataset)
                    &#47&#47
                    &#47&#47 &#47&#47 Target-to-original domain.
                    &#47&#47 x_reconst = self.G(x_fake, c_org)
                    &#47&#47 g_loss_rec = torch.mean(torch.abs(x_real - x_reconst))

                    &#47&#47 out_real = self.D(x_real, num_domains=2)
                    &#47&#47 g_out_real = torch.gather(out_real, 1, label_org.long())
                    &#47&#47 g_loss_real = self.bce_loss(self.zeros_target(self.batch_size), g_out_real)
                    &#47&#47 &#47&#47 target style code s_tilde
                    &#47&#47
                    &#47&#47 &#47&#47 Compute loss with fake images.
                    &#47&#47 &#47&#47 x_fake = self.G(x_real, s_tilde[0])
                    &#47&#47 out_fake = self.D(g_x_fake)
                    &#47&#47 g_out_fake = torch.gather(out_fake, 1, label_org.long())
                    &#47&#47 &#47&#47 d_loss_fake = torch.mean(out_fake)
                    &#47&#47 g_loss_fake = self.bce_loss(self.ones_target(self.batch_size), g_out_fake)

                    g_loss, g_adv_loss, g_loss_sty, g_loss_cyc, g_loss_ds = self.train_generator(x_real, x_fake,
                                                                                                 s_tilde_trg, label_org,
                                                                                                 label_trg)
                    &#47&#47 g_loss = self.train_generator(x_real, label_org, label_trg)

                    &#47&#47 g_adv_loss = self.compute_adversarial_loss(False, x_real, label_org, g_s_tilde_trg, label_trg)[0]

                    &#47&#47 Backward and optimize.
                    &#47&#47 g_loss = g_adv_loss + self.lambda_sty * g_loss_sty + self.lambda_cyc * g_loss_cyc + self.lambda_ds * g_loss_ds
                    &#47&#47 self.reset_grad()
                    &#47&#47 g_loss.backward()
                    &#47&#47 self.g_optimizer.step()
                    &#47&#47 self.e_optimizer.step()
                    &#47&#47 self.f_optimizer.step()

                    &#47&#47 Logging.
                    &#47&#47 loss[&quotG/loss_fake&quot] = g_loss_fake.item()
                    &#47&#47 loss[&quotG/loss_sty&quot] = g_loss_sty.item()
                    &#47&#47 loss[&quotG/loss_cyc&quot] = g_loss_cyc.item()
                    &#47&#47 loss[&quotG/loss_ds&quot] = g_loss_ds.item()
                    loss[&quotG/loss&quot] = g_loss.item()
                    loss[&quotG/loss_adv&quot] = g_adv_loss.item()
                    loss[&quotG/loss_sty&quot] = g_loss_sty.item()
                    loss[&quotG/loss_cyc&quot] = g_loss_cyc.item()
                    loss[&quotG/loss_ds&quot] = g_loss_ds.item()

                    &#47&#47 writer.add_scalar(&quotG/loss_cyc&quot, g_loss_cyc.item(), i)
                    &#47&#47 writer.add_scalar(&quotG/loss_ds&quot, g_loss_ds.item(), i)
                    writer.add_scalar(&quotG/loss&quot, g_loss.item(), i)
                    writer.add_scalar(&quotG/loss_adv&quot, g_adv_loss.item(), i)
                    writer.add_scalar(&quotG/loss_sty&quot, g_loss_sty.item(), i)
                    writer.add_scalar(&quotG/loss_cyc&quot, g_loss_cyc.item(), i)
                    writer.add_scalar(&quotG/loss_ds&quot, g_loss_ds.item(), i)

                &#47&#47 =================================================================================== &#47&#47
                &#47&#47                                 4. Miscellaneous                                    &#47&#47
                &#47&#47 =================================================================================== &#47&#47

                &#47&#47 Print out training information.
                if (i + 1) % self.log_step == 0:
                    et = time.time() - start_time
                    et = str(datetime.timedelta(seconds=et))[:-7]
                    log = "Elapsed [{}], Iteration [{}/{}]".format(et, i + 1, self.num_iters)
                    for tag, value in loss.items():
                        log += ", {}: {:.4f}".format(tag, value)
                    print(log)

                    &#47&#47 if self.use_tensorboard:
                    &#47&#47     for tag, value in loss.items():
                    &#47&#47         self.logger.scalar_summary(tag, value, i + 1)

                &#47&#47 Translate fixed images for debugging.
                if (i + 1) % self.sample_step == 0:
                    with torch.no_grad():
                        &#47&#47 random style: source images + generated images
                        g_s_tilde_trg = self.generate_style_code(label_trg)

                        &#47&#47 reference guided style: x_real as the reference image
                        ref_style<a id="change"> = </a>self.get_reference_style(x_real, label_org)

                        x_fake_list = [x_fixed, self.G(x_fixed, g_s_tilde_trg), x_real, self.G(x_fixed, ref_style)]

                        &#47&#47 for c_fixed in label_org:
                        x_concat = torch.cat(x_fake_list, dim=3)
                        sample_path = os.path.join(self.sample_dir, &quot{}-images.jpg&quot.format(i + 1))
                        save_image(self.denorm(x_concat.data.cpu()), sample_path, nrow=1, padding=0)
                        print(&quotSaved real and fake images into {}...&quot.format(sample_path))

                        grid = torchvision.utils.make_grid(x_concat)
                        writer.add_image(&quotimages&quot, grid, 0)
                        &#47&#47 writer.add_graph(model, images)

                &#47&#47 Save model checkpoints.
                if (i + 1) % self.model_save_step == 0:
                    G_path = os.path.join(self.model_save_dir, &quot{}-G.ckpt&quot.format(i + 1))
                    D_path = os.path.join(self.model_save_dir, &quot{}-D.ckpt&quot.format(i + 1))
                    E_path = os.path.join(self.model_save_dir, &quot{}-E.ckpt&quot.format(i + 1))
                    F_path = os.path.join(self.model_save_dir, &quot{}-F.ckpt&quot.format(i + 1))
                    torch.save(self.G.state_dict(), G_path)
                    torch.save(self.D.state_dict(), D_path)
                    torch.save(self.E.state_dict(), E_path)
                    torch.save(self.F.state_dict(), F_path)
                    print(&quotSaved model checkpoints into {}...&quot.format(self.model_save_dir))
                &#47&#47
                &#47&#47 &#47&#47 Decay learning rates.
                &#47&#47 if (i + 1) % self.lr_update_step == 0 and (i + 1) &gt; (self.num_iters - self.num_iters_decay):
                &#47&#47     g_lr -= (self.g_lr / float(self.num_iters_decay))
                &#47&#47     d_lr -= (self.d_lr / float(self.num_iters_decay))
                &#47&#47     self.update_lr(g_lr, d_lr)
                &#47&#47     print(&quotDecayed learning rates, g_lr: {}, d_lr: {}.&quot.format(g_lr, d_lr))

                &#47&#47 Decay weight lambda ds
                if (i + 1) &lt; self.num_iters_decay:
                    self.lambda_ds = 1 - 0.00002 * (i + 1)
                    &#47&#47 print(&quotDecayed weight lambda ds , lambda_ds: {}&quot.format(self.lambda_ds))
            <a id="change">except </a>Exception as e:
                <a id="change">print(</a>str(e)<a id="change">)</a>

        &#47&#47 close the tensorboard summary writter
        writer.close()
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/habout632/stargan2/commit/ee158d86db0ac3b65b72abf65b8b6fe3497fe4ef#diff-a14fbd5481af9a51ad5b33493a027a3921464e32cff58d0dd15bf122c4851e2bL464' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 31560041</div><div id='project'> Project Name: habout632/stargan2</div><div id='commit'> Commit Name: ee158d86db0ac3b65b72abf65b8b6fe3497fe4ef</div><div id='time'> Time: 2020-03-15</div><div id='author'> Author: jifeng.yin@silknets.com</div><div id='file'> File Name: stargan2_solver.py</div><div id='m_class'> M Class Name: Solver</div><div id='n_method'> N Class Name: Solver</div><div id='m_method'> M Method Name: train(1)</div><div id='n_method'> N Method Name: train(1)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: stargan2_solver.py</div><div id='n_file'> N File Name: stargan2_solver.py</div><div id='m_start'> M Start Line: 464</div><div id='m_end'> M End Line: 729</div><div id='n_start'> N Start Line: 493</div><div id='n_end'> N End Line: 766</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            stop_at = start_at + datetime.timedelta(seconds=3600)  &#47&#47 下一个小时整点

            pipelines = dbsession.query(Pipeline).filter(Pipeline.schedule_type==&quotcrontab&quot).all()  &#47&#47 获取model记录
            <a id="change">for </a>pipeline in pipelines<a id="change">:  &#47&#47 循环发起每一个调度

                </a>print(&quotbegin make timerun config %s&quot%pipeline.name)
                &#47&#47 计算start_at和stop_at之间，每一个任务的调度时间，并保障最小周期不超过设定的resolution。
                for eta in next_schedules(pipeline.cron_time, start_at, stop_at, resolution=resolution):  &#47&#47
                    print(&quot执行时间点&quot, eta)</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 stop_at = start_at + datetime.timedelta(seconds=3600)  &#47&#47 下一个小时整点

            pipelines = dbsession.query(Pipeline).filter(Pipeline.schedule_type==&quotcrontab&quot).all()  &#47&#47 获取model记录
            <a id="change">for </a><a id="change">pipeline</a> in pipelines<a id="change">:  &#47&#47 循环发起每一个调度
                </a>start_at = datetime.datetime.strptime(pipeline.cronjob_start_time,&quot%Y-%m-%d %H:%M:%S&quot)

                &#47&#47 缩小指定范围，认为最后一个任务记录之前是调度记录都是已经产生的
                last_run = dbsession.query(RunHistory).filter(RunHistory.pipeline_id==pipeline.id).order_by(RunHistory.id.desc()).first()
                if last_run:
                    last_execution_date = datetime.datetime.strptime(last_run.execution_date,&quot%Y-%m-%d %H:%M:%S&quot)
                    if last_execution_date&gt;start_at:
                        start_at=last_execution_date

                stop_at = datetime.datetime.now() + datetime.timedelta(seconds=300)   &#47&#47 下一个调度时间点，强制5分钟调度一次。这之前的 任务，该调度的都发起或者延迟发起

                print(&quotbegin make timerun config %s&quot%pipeline.name)
                &#47&#47 计算start_at和stop_at之间，每一个任务的调度时间，并保障最小周期不超过设定的resolution。
                <a id="change">try:
                    </a>for eta in next_schedules(pipeline.cron_time, start_at, stop_at, resolution=resolution):  &#47&#47
                        print(&quot执行时间点&quot, eta)
                        execution_date = eta.strftime(&quot%Y-%m-%d %H:%M:%S&quot)
                        if execution_date&gt;pipeline.cronjob_start_time:
                            &#47&#47 要检查是否重复添加记录了
                            exist_timeruns=dbsession.query(RunHistory).filter(RunHistory.pipeline_id==pipeline.id).filter(RunHistory.execution_date==execution_date).all()
                            if not exist_timeruns:
                                pipeline_file = dag_to_pipeline(pipeline=pipeline, dbsession=dbsession,execution_date=execution_date)  &#47&#47 合成workflow
                                &#47&#47 print(&quotmake pipeline file %s&quot % pipeline_file)
                                if pipeline_file:
                                    schedule_history = RunHistory(
                                        created_on=datetime.datetime.now(),
                                        pipeline_id=pipeline.id,
                                        pipeline_argo_id=&quot&quot,
                                        pipeline_file=pipeline_file,
                                        version_id=&quot&quot,
                                        run_id=&quot&quot,
                                        message=&quot&quot,
                                        status=&quotcomed&quot,
                                        execution_date=execution_date
                                    )
                                    dbsession.add(schedule_history)
                                    dbsession.commit()
                                else:
                                    push_message(conf.get(&quotADMIN_USER&quot).split(&quot,&quot),&quotpipeline %s make config fail&quot%pipeline.name)
                            if len(exist_timeruns)&gt;1:
                                for i in range(1,len(exist_timeruns)):
                                    exist_timerun<a id="change"> = </a>exist_timeruns[i]
                                    dbsession.delete(exist_timerun)
                                    dbsession.commit()
                                push_message(conf.get(&quotADMIN_USER&quot).split(&quot,&quot),&quot发现%s 任务流在 %s 时刻存在多个定时记录&quot%(pipeline.name,execution_date))


                    &#47&#47 无论产生任务怎么样，上传都是要执行的，可能会上传之前没有上传的任务
                    &#47&#47 直接触发一次，在5分钟以内的都延迟提交。
                    &#47&#47 upload_timerun(pipeline,stop_at)
                <a id="change">except </a>Exception as e:
                    <a id="change">print(</a>e<a id="change">)</a>

                upload_timerun(pipeline_id=pipeline.id,stop_time=stop_at.strftime(&quot%Y-%m-%d %H:%M:%S&quot))

        except Exception as e:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tencentmusic/cube-studio/commit/842a46a1832379aade515f448e0763d7b1cf0194#diff-cccea5a99fac00532cf9f4a08008066dc95d61078b2c52727ed9a70ef702f2e5L445' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 31560043</div><div id='project'> Project Name: tencentmusic/cube-studio</div><div id='commit'> Commit Name: 842a46a1832379aade515f448e0763d7b1cf0194</div><div id='time'> Time: 2021-10-14</div><div id='author'> Author: pengluan@tencent.com</div><div id='file'> File Name: myapp/tasks/schedules.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: make_timerun_config(1)</div><div id='n_method'> N Method Name: make_timerun_config(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: myapp/tasks/schedules.py</div><div id='n_file'> N File Name: myapp/tasks/schedules.py</div><div id='m_start'> M Start Line: 452</div><div id='m_end'> M End Line: 486</div><div id='n_start'> N Start Line: 436</div><div id='n_end'> N End Line: 500</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if ftype == "fiftyone.core.fields.EmbeddedDocumentField":
            embedded_doc_inds.append(idx)

    <a id="change">for </a>field in dataset_dict.get("frame_fields", [])<a id="change">:
        </a>field["fields"] = []

    dataset_dict["app_sidebar_groups"] = None
</code></pre><h3>After Change</h3><pre><code class='java'>
        else:
            field["fields"] = []

    <a id="change">for </a><a id="change">field</a> in dataset_dict.get("frame_fields", [])<a id="change">:
        </a>ftype = field.get("ftype", None)
        embedded_doc_type = field.get("embedded_doc_type", None)

        if ftype == "fiftyone.core.fields.EmbeddedDocumentField":
            <a id="change">try:
                </a>field["fields"]<a id="change"> = </a>_infer_fields(
                    db[dataset_dict["frame_collection_name"]],
                    name,
                    embedded_doc_type,
                )
            <a id="change">except </a>Exception as e:
                <a id="change">print(
                    </a>"Failed to infer schema of embedded frame field &quot%s&quot "
                    "of type &quot%s&quot: %s" % (name, embedded_doc_type, e)<a id="change">
                )</a>
                field["fields"] = []
        else:
            field["fields"] = []
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/voxel51/fiftyone/commit/f31fb2741432a5337b4ed96f49979fa51ae68041#diff-754d2ab37ba080d18f919795d88978a7031896949642b2fdefeeba1aa2cf5cd5L10' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 31560044</div><div id='project'> Project Name: voxel51/fiftyone</div><div id='commit'> Commit Name: f31fb2741432a5337b4ed96f49979fa51ae68041</div><div id='time'> Time: 2022-04-03</div><div id='author'> Author: brimoor@umich.edu</div><div id='file'> File Name: fiftyone/migrations/revisions/v0_16_0.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: up(2)</div><div id='n_method'> N Method Name: up(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: fiftyone/migrations/revisions/v0_16_0.py</div><div id='n_file'> N File Name: fiftyone/migrations/revisions/v0_16_0.py</div><div id='m_start'> M Start Line: 16</div><div id='m_end'> M End Line: 44</div><div id='n_start'> N Start Line: 13</div><div id='n_end'> N End Line: 71</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if ftype == "fiftyone.core.fields.EmbeddedDocumentField":
            embedded_doc_inds.append(idx)

    <a id="change">for </a>field in dataset_dict.get("frame_fields", [])<a id="change">:
        </a>field["fields"] = []

    dataset_dict["app_sidebar_groups"] = None
</code></pre><h3>After Change</h3><pre><code class='java'>
        else:
            field["fields"] = []

    <a id="change">for </a><a id="change">field</a> in dataset_dict.get("frame_fields", [])<a id="change">:
        </a>ftype = field.get("ftype", None)
        embedded_doc_type = field.get("embedded_doc_type", None)

        if ftype == "fiftyone.core.fields.EmbeddedDocumentField":
            <a id="change">try:
                </a>field["fields"]<a id="change"> = </a>_infer_fields(
                    db[dataset_dict["frame_collection_name"]],
                    name,
                    embedded_doc_type,
                )
            <a id="change">except </a>Exception as e:
                <a id="change">print(
                    </a>"Failed to infer schema of embedded frame field &quot%s&quot "
                    "of type &quot%s&quot: %s" % (name, embedded_doc_type, e)<a id="change">
                )</a>
                field["fields"] = []
        else:
            field["fields"] = []
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/voxel51/fiftyone/commit/700c31ad6f19f9f7a5bc040bbf771fec9ae2db69#diff-754d2ab37ba080d18f919795d88978a7031896949642b2fdefeeba1aa2cf5cd5L10' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 31560046</div><div id='project'> Project Name: voxel51/fiftyone</div><div id='commit'> Commit Name: 700c31ad6f19f9f7a5bc040bbf771fec9ae2db69</div><div id='time'> Time: 2022-04-27</div><div id='author'> Author: brimoor@umich.edu</div><div id='file'> File Name: fiftyone/migrations/revisions/v0_16_0.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: up(2)</div><div id='n_method'> N Method Name: up(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: fiftyone/migrations/revisions/v0_16_0.py</div><div id='n_file'> N File Name: fiftyone/migrations/revisions/v0_16_0.py</div><div id='m_start'> M Start Line: 16</div><div id='m_end'> M End Line: 44</div><div id='n_start'> N Start Line: 13</div><div id='n_end'> N End Line: 71</div><BR>