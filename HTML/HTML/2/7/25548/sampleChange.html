<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            The scaler. It may be wrapped to add additional functionality for use in Determined.
        

        <a id="change">check.true(</a>HAVE_AMP, <a id="change">"Failed to import torch.cuda.amp. PyTorch &gt;= 1.6 required."</a><a id="change">)</a>

        check.false(self._use_apex, "Do not mix APEX with PyTorch AMP.")

        check.is_none(self._scaler, "Please only call wrap_scaler or use_amp once.")</code></pre><h3>After Change</h3><pre><code class='java'>
            The scaler. It may be wrapped to add additional functionality for use in Determined.
        

        <a id="change">if not HAVE_AMP</a>:
            raise det.errors.InvalidExperimentException(
                "Using context.wrap_scaler() requires PyTorch &gt;= 1.6.",
            )

        if self._use_apex:
            raise det.errors.InvalidExperimentException("Do not mix APEX with PyTorch AMP.")

        if self._scaler is not None:
            raise det.errors.InvalidExperimentException(
                "Please only call wrap_scaler or use_amp once.",
            )

        if self.models:
            <a id="change">raise det.errors.InvalidExperimentException(
                "Please call wrap_scaler before wrap_model."</a><a id="change">,
            )</a>

        &#47&#47 We don&quott need to check if CUDA is available because if it is not, a GradScaler is
        &#47&#47  disabled when initialized, and we allow for disabled scalers to exist.
</code></pre>