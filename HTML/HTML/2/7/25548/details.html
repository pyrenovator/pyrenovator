<html><h3>Pattern ID :25548
</h3><img src='77730866.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            The scaler. It may be wrapped to add additional functionality for use in Determined.
        

        <a id="change">check.true(</a>HAVE_AMP, <a id="change">"Failed to import torch.cuda.amp. PyTorch &gt;= 1.6 required."</a><a id="change">)</a>

        check.false(self._use_apex, "Do not mix APEX with PyTorch AMP.")

        check.is_none(self._scaler, "Please only call wrap_scaler or use_amp once.")</code></pre><h3>After Change</h3><pre><code class='java'>
            The scaler. It may be wrapped to add additional functionality for use in Determined.
        

        <a id="change">if not HAVE_AMP</a>:
            raise det.errors.InvalidExperimentException(
                "Using context.wrap_scaler() requires PyTorch &gt;= 1.6.",
            )

        if self._use_apex:
            raise det.errors.InvalidExperimentException("Do not mix APEX with PyTorch AMP.")

        if self._scaler is not None:
            raise det.errors.InvalidExperimentException(
                "Please only call wrap_scaler or use_amp once.",
            )

        if self.models:
            <a id="change">raise det.errors.InvalidExperimentException(
                "Please call wrap_scaler before wrap_model."</a><a id="change">,
            )</a>

        &#47&#47 We don&quott need to check if CUDA is available because if it is not, a GradScaler is
        &#47&#47  disabled when initialized, and we allow for disabled scalers to exist.
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/determined-ai/determined/commit/dd59244bf78d4c17ff5dfeeac1c47b759900f5bf#diff-d0efbe04f5f1ad69350b2a3257125b2bec5dfc90a822e95089384fdf0fbd25d8L371' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 77730866</div><div id='project'> Project Name: determined-ai/determined</div><div id='commit'> Commit Name: dd59244bf78d4c17ff5dfeeac1c47b759900f5bf</div><div id='time'> Time: 2022-09-22</div><div id='author'> Author: 103537968+drh-determined-ai@users.noreply.github.com</div><div id='file'> File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='m_class'> M Class Name: PyTorchTrialContext</div><div id='n_method'> N Class Name: PyTorchTrialContext</div><div id='m_method'> M Method Name: wrap_scaler(2)</div><div id='n_method'> N Method Name: wrap_scaler(2)</div><div id='m_parent_class'> M Parent Class: det.TrialContext,pytorch._PyTorchReducerContext</div><div id='n_parent_class'> N Parent Class: det.TrialContext,pytorch._PyTorchReducerContext</div><div id='m_file'> M File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='n_file'> N File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='m_start'> M Start Line: 371</div><div id='m_end'> M End Line: 382</div><div id='n_start'> N Start Line: 374</div><div id='n_end'> N End Line: 395</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                ``wrap_scaler()`` was called directly.
        

        <a id="change">check.true(
            </a>auto_zero_grads or self._aggregation_frequency == 1,
            <a id="change">"if optimizations.aggregation_frequency is larger than 1, "
            "you can only set auto_zero_grads to be true. "</a><a id="change">,
        )</a>

        if not self._should_communicate_and_update():
            return
</code></pre><h3>After Change</h3><pre><code class='java'>
                ``wrap_scaler()`` was called directly.
        

        <a id="change">if </a>self._aggregation_frequency &gt; 1 and <a id="change">not</a> auto_zero_grads:
            <a id="change">raise det.errors.InvalidExperimentException(
                "if optimizations.aggregation_frequency is larger than 1, "
                "auto_zero_grads must be set to true. "</a><a id="change">,
            )</a>

        if not self._should_communicate_and_update():
            return
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/determined-ai/determined/commit/dd59244bf78d4c17ff5dfeeac1c47b759900f5bf#diff-d0efbe04f5f1ad69350b2a3257125b2bec5dfc90a822e95089384fdf0fbd25d8L654' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 77730864</div><div id='project'> Project Name: determined-ai/determined</div><div id='commit'> Commit Name: dd59244bf78d4c17ff5dfeeac1c47b759900f5bf</div><div id='time'> Time: 2022-09-22</div><div id='author'> Author: 103537968+drh-determined-ai@users.noreply.github.com</div><div id='file'> File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='m_class'> M Class Name: PyTorchTrialContext</div><div id='n_method'> N Class Name: PyTorchTrialContext</div><div id='m_method'> M Method Name: step_optimizer(5)</div><div id='n_method'> N Method Name: step_optimizer(5)</div><div id='m_parent_class'> M Parent Class: det.TrialContext,pytorch._PyTorchReducerContext</div><div id='n_parent_class'> N Parent Class: det.TrialContext,pytorch._PyTorchReducerContext</div><div id='m_file'> M File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='n_file'> N File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='m_start'> M Start Line: 695</div><div id='m_end'> M End Line: 699</div><div id='n_start'> N Start Line: 704</div><div id='n_end'> N End Line: 710</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                "aggregation frequency &gt; 1.",
            )

        <a id="change">check.true(
            </a>torch.cuda.is_available(),
            <a id="change">"Mixed precision training (AMP) is supported only on GPU slots."</a><a id="change">,
        )</a>

        if self._distributed_backend.use_torch():
            &#47&#47 We need to get the pre-wrapped input models to initialize APEX because
            if isinstance(models, list):</code></pre><h3>After Change</h3><pre><code class='java'>
                    "aggregation frequency &gt; 1.",
                )

        <a id="change">if not torch.cuda.is_available()</a>:
            <a id="change">raise det.errors.InvalidExperimentException(
                "context.configure_apex_amp is supported only on GPU slots."</a><a id="change">,
            )</a>

        self._use_apex = True

        if self._distributed_backend.use_torch():</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/determined-ai/determined/commit/dd59244bf78d4c17ff5dfeeac1c47b759900f5bf#diff-d0efbe04f5f1ad69350b2a3257125b2bec5dfc90a822e95089384fdf0fbd25d8L388' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 77730873</div><div id='project'> Project Name: determined-ai/determined</div><div id='commit'> Commit Name: dd59244bf78d4c17ff5dfeeac1c47b759900f5bf</div><div id='time'> Time: 2022-09-22</div><div id='author'> Author: 103537968+drh-determined-ai@users.noreply.github.com</div><div id='file'> File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='m_class'> M Class Name: PyTorchTrialContext</div><div id='n_method'> N Class Name: PyTorchTrialContext</div><div id='m_method'> M Method Name: configure_apex_amp(15)</div><div id='n_method'> N Method Name: configure_apex_amp(15)</div><div id='m_parent_class'> M Parent Class: det.TrialContext,pytorch._PyTorchReducerContext</div><div id='n_parent_class'> N Parent Class: det.TrialContext,pytorch._PyTorchReducerContext</div><div id='m_file'> M File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='n_file'> N File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='m_start'> M Start Line: 462</div><div id='m_end'> M End Line: 488</div><div id='n_start'> N Start Line: 473</div><div id='n_end'> N End Line: 497</div><BR>