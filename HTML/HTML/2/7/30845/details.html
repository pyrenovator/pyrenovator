<html><h3>Pattern ID :30845
</h3><img src='90804651.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        
&#47&#47         v_sem_h = self.__dropout(v_sem_h, .8, &quotv_sem_h&quot)
&#47&#47         v_syn_h = self.__dropout(v_syn_h, .8, &quotv_syn_h&quot)
        sem_syn_h<a id="change"> = </a>self.__dropout(sem_syn_h, .8, &quotsem_syn_h&quot)
        h<a id="change"> = </a>torch.cat((h, sem_syn_h), dim=1)
        <a id="change">return </a>torch.relu(self.merge4(h))
        
&#47&#47         v_sem_h = (torch.relu(self.v_sem_fc(v_sem_h)) * v_syn_h) + (beta1 * v_sem_h)
&#47&#47         v_syn_h = (torch.relu(self.v_syn_fc(v_syn_h)) * v_sem_h) + ((1-beta1) * v_syn_h)</code></pre><h3>After Change</h3><pre><code class='java'>
        
    def __adaptive_merge(self, rnn_h, v_attn, v_sem_h, v_syn_h, sem_syn_h):
        h = torch.cat((rnn_h, v_attn), dim=1)
        beta1 = <a id="change">torch.sigmoid(</a>self.merge1(h)<a id="change">)</a>
        beta2 = torch.sigmoid(self.merge2(h))
        aa1<a id="change"> = </a>beta1<a id="change"> * v_sem_h + </a>(1 - beta1) * v_syn_h
        <a id="change">return </a>beta2 * aa1 + (1 - beta2) * sem_syn_h
    
    def forward_fn(self, v_pool, s_pool, pos_emb, enc_hidden, v_feats, captions, teacher_forcing_ratio=0.5):
        &#47&#47 Determine whether it is an inferred mode based on whether it is passed into caption</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/jssprz/visual_syntactic_embedding_video_captioning/commit/08bb3aae9ddc9fd1d5affa73c3fd11db7cad6bd5#diff-e2ac364650a0efa77613ac5b008d3966a8869d3e77297dd5190e62b70735f379L349' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 90804651</div><div id='project'> Project Name: jssprz/visual_syntactic_embedding_video_captioning</div><div id='commit'> Commit Name: 08bb3aae9ddc9fd1d5affa73c3fd11db7cad6bd5</div><div id='time'> Time: 2021-03-05</div><div id='author'> Author: jperezmartin90@gmail.com</div><div id='file'> File Name: model/decoder.py</div><div id='m_class'> M Class Name: SemSynCNDecoder</div><div id='n_method'> N Class Name: SemSynCNDecoder</div><div id='m_method'> M Method Name: __adaptive_merge(6)</div><div id='n_method'> N Method Name: __adaptive_merge(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: model/decoder.py</div><div id='n_file'> N File Name: model/decoder.py</div><div id='m_start'> M Start Line: 349</div><div id='m_end'> M End Line: 360</div><div id='n_start'> N Start Line: 420</div><div id='n_end'> N End Line: 424</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        super().__init__()

    def forward(self, x):
        <a id="change">return </a>x

class Teacher(nn.Module):
    def __init__(</code></pre><h3>After Change</h3><pre><code class='java'>

        gru_input = torch.cat((proprio, latent_extero), dim = -1)

        next_hiddens<a id="change"> = </a>[]
        for gru_cell, prev_hidden in zip(self.gru_cells, hiddens):
            gru_input = gru_cell(gru_input, prev_hidden)
            next_hiddens.append(gru_input)

        gru_output = gru_input

        &#47&#47 attention gating of exteroception

        attention_gate = self.to_extero_attn_gate(gru_output)
        gated_extero = latent_extero<a id="change"> * </a><a id="change">attention_gate.sigmoid()</a>

        &#47&#47 belief state and add gated exteroception

        belief_state<a id="change"> = </a>self.belief_state_encoder(gru_output)
        belief_state<a id="change"> = </a>sum_with_zeropad(belief_state, gated_extero)

        &#47&#47 to action logits

        action_logits = self.to_action_logits(belief_state)
        <a id="change">return </a>action_logits, next_hiddens

class Teacher(nn.Module):
    def __init__(</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/anymal-belief-state-encoder-decoder-pytorch/commit/31d37d8d81db1d32cbfae83f1e43a669e4c8d5ea#diff-73be6483f8f8caa2ae70e99ad4ab6af1d1c938d94af55a7444fe6a52139b298bL74' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 90804569</div><div id='project'> Project Name: lucidrains/anymal-belief-state-encoder-decoder-pytorch</div><div id='commit'> Commit Name: 31d37d8d81db1d32cbfae83f1e43a669e4c8d5ea</div><div id='time'> Time: 2022-04-17</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: anymal_belief_state_encoder_decoder_pytorch/networks.py</div><div id='m_class'> M Class Name: Student</div><div id='n_method'> N Class Name: Student</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: anymal_belief_state_encoder_decoder_pytorch/networks.py</div><div id='n_file'> N File Name: anymal_belief_state_encoder_decoder_pytorch/networks.py</div><div id='m_start'> M Start Line: 74</div><div id='m_end'> M End Line: 75</div><div id='n_start'> N Start Line: 119</div><div id='n_end'> N End Line: 157</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            nn.Linear(dim * mult, dim))

    def forward(self, x):
        <a id="change">return </a>self.net(x)

class PreNorm(nn.Module):
    def __init__(self, norm_class, dim, fn):</code></pre><h3>After Change</h3><pre><code class='java'>

    def forward(self, x):
        if not self.glu:
            x<a id="change"> = </a>self.w1(x)
            x<a id="change"> = </a>self.act(x)
        else:
            x, v = self.w1(x).chunk(2, dim=-1)
            x<a id="change"> = </a>self.act(x)<a id="change"> * </a><a id="change">F.sigmoid(</a>v<a id="change">)</a>

        x = self.dropout(x)
        x = self.w2(x)
        <a id="change">return </a>x

class PreNorm(nn.Module):
    def __init__(self, norm_class, dim, fn):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/sinkhorn-transformer/commit/3e638458d4bc5c6df8037e1b4f460bfa044288e8#diff-2ed19ae7feb552dc52b4c7a7c89a078c0fd371c922789f894af09ff01f35eccbL121' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 90804574</div><div id='project'> Project Name: lucidrains/sinkhorn-transformer</div><div id='commit'> Commit Name: 3e638458d4bc5c6df8037e1b4f460bfa044288e8</div><div id='time'> Time: 2020-04-09</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='m_class'> M Class Name: FeedForward</div><div id='n_method'> N Class Name: FeedForward</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='n_file'> N File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='m_start'> M Start Line: 122</div><div id='m_end'> M End Line: 122</div><div id='n_start'> N Start Line: 122</div><div id='n_end'> N End Line: 131</div><BR>