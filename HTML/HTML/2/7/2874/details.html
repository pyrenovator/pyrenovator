<html><h3>Pattern ID :2874
</h3><img src='11317805.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        tokens_replaced = logits_masked.multinomial(num_samples=1, replacement=True).view(-1)
        input_ids_disc = labels.clone()
        input_ids_disc[masked_bool] = tokens_replaced
        labels_disc<a id="change"> = </a><a id="change">(input_ids_disc != labels).to(</a>torch.long<a id="change">)</a>

        outputs_disc = self.discriminator(
            input_ids=input_ids_disc, attention_mask=attention_mask, token_type_ids=token_type_ids, 
            position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, </code></pre><h3>After Change</h3><pre><code class='java'>

        loss_gen = outputs_gen.loss &#47&#47 (1,)
        logits_gen = outputs_gen.logits &#47&#47 (batch_size, seq_length, config.vocab_size)
        <a id="change">with torch</a><a id="change">.no_grad():
            </a>masked_bool = (labels == -100)
            &#47&#47 logits_masked: (batch_size*masked_length, config.vocab_size)
            &#47&#47 logits_masked = F.softmax(logits_gen[masked_bool].reshape(-1, self.discriminator.electra.config.vocab_size), dim=1)
            &#47&#47 replaced tokens are set with logits
            &#47&#47 tokens_replaced = logits_masked.multinomial(num_samples=1, replacement=True).reshape(-1)
            logits = logits_gen[masked_bool]&#47&#47.reshape(-1, self.discriminator.electra.config.vocab_size)
            gumbel = self.gumbel_dist.sample(logits.shape)&#47&#47.to(logits.device)
            tokens_replaced<a id="change"> = </a>(logits + gumbel).argmax(dim=-1)
            input_ids_disc = labels.clone()
            input_ids_disc[masked_bool] = tokens_replaced
            labels_disc<a id="change"> = </a>(input_ids_disc != labels)&#47&#47.to(torch.long)

        outputs_disc = self.discriminator(
            input_ids=input_ids_disc, attention_mask=attention_mask, token_type_ids=token_type_ids, </code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/retarfi/language-pretraining/commit/ea8063480044fc1c8e844831355b7feb7edee091#diff-2ab458989f9a1c4e0c8dfb20cfb9dcbfbb0c9671651bd5df7c188d99658d99ddL60' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 11317805</div><div id='project'> Project Name: retarfi/language-pretraining</div><div id='commit'> Commit Name: ea8063480044fc1c8e844831355b7feb7edee091</div><div id='time'> Time: 2021-07-15</div><div id='author'> Author: valerososoccer@gmail.com</div><div id='file'> File Name: utils/model.py</div><div id='m_class'> M Class Name: ElectraForPretrainingModel</div><div id='n_method'> N Class Name: ElectraForPretrainingModel</div><div id='m_method'> M Method Name: forward(11)</div><div id='n_method'> N Method Name: forward(11)</div><div id='m_parent_class'> M Parent Class: PreTrainedModel</div><div id='n_parent_class'> N Parent Class: PreTrainedModel</div><div id='m_file'> M File Name: utils/model.py</div><div id='n_file'> N File Name: utils/model.py</div><div id='m_start'> M Start Line: 60</div><div id='m_end'> M End Line: 68</div><div id='n_start'> N Start Line: 69</div><div id='n_end'> N End Line: 83</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.replay_buffer.store(
            self._states, self._actions, reward, states)
        self._states = states
        actions = self.policy.eval(<a id="change">states.to(</a>self.device<a id="change">)</a>)
        actions<a id="change"> = </a>actions + self._noise_policy.sample([actions.shape[0]])
        self._actions = Action(actions).to("cpu")
        return self._actions
</code></pre><h3>After Change</h3><pre><code class='java'>
        self._replay_buffer.store(
            self._states, self._actions, reward, states)
        self._states = states
        <a id="change">with torch</a><a id="change">.no_grad():
            </a>actions<a id="change"> = </a>self._policy_model(states)
            actions<a id="change"> = </a>actions + self._noise_policy.sample([actions.shape[0]])
        self._actions = Action(actions)
        return self._actions
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/syuntoku14/pytorch-rl-il/commit/76defe6cb7c6e9bab65823acd587275d83d1ea9c#diff-9f0056bb754f3c4cad05f0f077874206d410e8b870fc1df576c0c8559c0737dcL134' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 11317801</div><div id='project'> Project Name: syuntoku14/pytorch-rl-il</div><div id='commit'> Commit Name: 76defe6cb7c6e9bab65823acd587275d83d1ea9c</div><div id='time'> Time: 2020-04-08</div><div id='author'> Author: syuntoku14@gmail.com</div><div id='file'> File Name: rlil/agents/td3.py</div><div id='m_class'> M Class Name: TD3LazyAgent</div><div id='n_method'> N Class Name: TD3LazyAgent</div><div id='m_method'> M Method Name: act(3)</div><div id='n_method'> N Method Name: act(3)</div><div id='m_parent_class'> M Parent Class: LazyAgent</div><div id='n_parent_class'> N Parent Class: LazyAgent</div><div id='m_file'> M File Name: rlil/agents/td3.py</div><div id='n_file'> N File Name: rlil/agents/td3.py</div><div id='m_start'> M Start Line: 138</div><div id='m_end'> M End Line: 140</div><div id='n_start'> N Start Line: 149</div><div id='n_end'> N End Line: 152</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        if i &gt;= 10: break

        frames = <a id="change">frames.to(</a>DEVICE<a id="change">)</a>  &#47&#47 [1, T, 3, h, w]
        frames_vis = postprocess_img(frames.squeeze(dim=0))  &#47&#47 [T, 3, h, w]
        input = frames[:, :VIDEO_IN_LENGTH]  &#47&#47 [1, t, 3, h, w]

        pred_rgb = pred_rgb_model.pred_n(input, pred_length=VIDEO_PRED_LENGTH)  &#47&#47 [1, T, 3, h, w]
        pred_rgb_vis = postprocess_img(pred_rgb)  &#47&#47 [T, 3, h, w]

        pred_rgb<a id="change"> = </a>torch.cat([input, pred_rgb], dim=1)
        pred_rgb = torch.stack([seg_model(pred_rgb[:, i]) for i in range(pred_rgb.shape[1])], dim=1)
        pred_rgb = pred_rgb.argmax(dim=2).squeeze()  &#47&#47 [T, h, w]
        pred_then_colorized_vis = colorize_semseg(postprocess_mask(pred_rgb), num_classes=SYNPICK_CLASSES) &#47&#47 [T, 3, h, w]</code></pre><h3>After Change</h3><pre><code class='java'>
    test_loader = DataLoader(test_data, batch_size=1, shuffle=True, num_workers=4)
    iter_loader = iter(test_loader)

    <a id="change">with torch</a><a id="change">.no_grad():
        </a>for i in tqdm(range(10)):

            frames<a id="change"> = </a>next(iter_loader).to(DEVICE)  &#47&#47 [1, T, 3, h, w]
            frames_vis = postprocess_img(frames.squeeze(dim=0))  &#47&#47 [T, 3, h, w]
            input = frames[:, :VIDEO_IN_LENGTH]  &#47&#47 [1, t, 3, h, w]

            pred_rgb = pred_rgb_model.pred_n(input, pred_length=VIDEO_PRED_LENGTH)
            pred_rgb<a id="change"> = </a>torch.cat([input, pred_rgb], dim=1)  &#47&#47 [1, T, 3, h, w]
            pred_rgb_vis = postprocess_img(pred_rgb.squeeze(dim=0))  &#47&#47 [T, 3, h, w]

            pred_rgb = torch.stack([seg_model(pred_rgb[:, i]) for i in range(pred_rgb.shape[1])], dim=1)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/ais-bonn/vp-suite/commit/13016d4ab8ba4f8e7ee087155a6c5171f4d00ba3#diff-e07d70acfca9139cbf2c37b7a0074bdfbb966d41dc6c7edcc2cef05e78c00af0L13' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 11317819</div><div id='project'> Project Name: ais-bonn/vp-suite</div><div id='commit'> Commit Name: 13016d4ab8ba4f8e7ee087155a6c5171f4d00ba3</div><div id='time'> Time: 2021-08-02</div><div id='author'> Author: boltres@ais.uni-bonn.de</div><div id='file'> File Name: scripts/visualize_4_way.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: visualize_4_way(1)</div><div id='n_method'> N Method Name: visualize_4_way(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: scripts/visualize_4_way.py</div><div id='n_file'> N File Name: scripts/visualize_4_way.py</div><div id='m_start'> M Start Line: 16</div><div id='m_end'> M End Line: 73</div><div id='n_start'> N Start Line: 17</div><div id='n_end'> N End Line: 74</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
  num_softmax = output.shape[1]
  lower_edge = torch.argmax((output &gt;= lam).to(int), dim=1)/num_softmax
  prediction = torch.argmax(output, dim=1)/num_softmax
  upper_edge = 1-torch.argmax(<a id="change">(output.flip(dims=(1,)) &gt;= lam).to(</a>int<a id="change">)</a>, dim=1)/num_softmax

  upper_edge<a id="change"> = </a>torch.maximum(upper_edge, prediction + 1e-6) &#47&#47 set a lower bound on the size.
  lower_edge = torch.minimum(lower_edge, prediction - 1e-6)
  upper_edge[upper_edge &gt; 1] = 1.0
  lower_edge[lower_edge &lt; 0] = 0.0</code></pre><h3>After Change</h3><pre><code class='java'>
  return loss

def softmax_nested_sets_from_output(model, output, lam=None):
  <a id="change">with torch</a><a id="change">.no_grad():
    </a>if lam == None:
        if model.lhat == None:
            raise Exception("You have to specify lambda unless your model is already calibrated.")
        lam = model.lhat
    
    output = output.softmax(dim=1)
    num_softmax = output.shape[1]

    &#47&#47 The Romano et al. version of classification
    vals, idxs = output.sort(dim=1,descending=True)
    torch.cumsum(vals,dim=1,out=vals)
    idxs[vals &gt; lam] = -num_softmax
    lower_edge = idxs.abs().min(dim=1,keepdim=True)[0]/num_softmax
    prediction = torch.argmax(output, dim=1)/num_softmax
    upper_edge<a id="change"> = </a>idxs.max(dim=1,keepdim=True)[0]/num_softmax

    upper_edge<a id="change"> = </a>torch.maximum(upper_edge, prediction + 1e-6) &#47&#47 set a lower bound on the size.
    lower_edge = torch.minimum(lower_edge, prediction - 1e-6)
    torch.cuda.empty_cache()
    upper_edge[upper_edge &gt; 1] = 1.0</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/aangelopoulos/im2im-uq/commit/da2dbe136e9c04611c95c976550aa712ff4518a5#diff-e5b05b0d825f0258e16e505acf449fe9f7cf17899ec9015de5bac46d6e05afb4L26' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 11317799</div><div id='project'> Project Name: aangelopoulos/im2im-uq</div><div id='commit'> Commit Name: da2dbe136e9c04611c95c976550aa712ff4518a5</div><div id='time'> Time: 2021-06-22</div><div id='author'> Author: angelopoulos@n0024.abc0</div><div id='file'> File Name: core/models/finallayers/softmax_layer.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: softmax_nested_sets_from_output(3)</div><div id='n_method'> N Method Name: softmax_nested_sets_from_output(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: core/models/finallayers/softmax_layer.py</div><div id='n_file'> N File Name: core/models/finallayers/softmax_layer.py</div><div id='m_start'> M Start Line: 27</div><div id='m_end'> M End Line: 42</div><div id='n_start'> N Start Line: 27</div><div id='n_end'> N End Line: 49</div><BR>