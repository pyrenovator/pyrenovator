<html><h3>Pattern ID :20000
</h3><img src='65091617.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())

    def forward(self, x):
        <a id="change">return </a>self.act(self.bn(self.conv(x)))

    def fuseforward(self, x):
        return self.act(self.conv(x))</code></pre><h3>After Change</h3><pre><code class='java'>
        if self.norm is not None:
            if self.norm_type == &quotHIN&quot:
                out1, out2 = torch.chunk(out, 2, dim = 1)
                out2<a id="change"> = </a>self.norm(out2)
                out<a id="change"> = </a>torch.cat((out1, out2), dim = 1)
            else:
                out<a id="change"> = </a><a id="change">self.norm(</a>out<a id="change">)</a>
        out<a id="change"> = </a>self.act(out)
        <a id="change">return </a>out

    def fuseforward(self, x):
        return self.act(self.conv(x))</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/liudakai2/unsupdis-pytorch/commit/e4802740d986fb6830ac0b78d31eea7ce8e6afd4#diff-cfb1ff087a99a34369673c9f34bdcd22f2d429ab3599a89f386c5de1fd9a2566L36' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 65091617</div><div id='project'> Project Name: liudakai2/unsupdis-pytorch</div><div id='commit'> Commit Name: e4802740d986fb6830ac0b78d31eea7ce8e6afd4</div><div id='time'> Time: 2022-06-02</div><div id='author'> Author: 35397867+tgjjj@users.noreply.github.com</div><div id='file'> File Name: models/common.py</div><div id='m_class'> M Class Name: Conv</div><div id='n_method'> N Class Name: Conv</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/common.py</div><div id='n_file'> N File Name: models/common.py</div><div id='m_start'> M Start Line: 36</div><div id='m_end'> M End Line: 36</div><div id='n_start'> N Start Line: 44</div><div id='n_end'> N End Line: 53</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        super().__init__()

    def forward(self, x_categ, x_cont):
        <a id="change">return </a>0.
</code></pre><h3>After Change</h3><pre><code class='java'>
        x = self.categorical_embeds(x_categ)

        for attn, ff in self.layers:
            x<a id="change"> = </a>attn(x)
            x<a id="change"> = </a>ff(x)

        flat_categ = x.flatten(1)
        normed_cont<a id="change"> = </a><a id="change">self.norm(</a>x_cont<a id="change">)</a>

        x<a id="change"> = </a>torch.cat((flat_categ, normed_cont), dim = -1)
        <a id="change">return </a>self.mlp(x)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/tab-transformer-pytorch/commit/68a07cc4f6301efde2159e6d179c672b6a3bd7ec#diff-d87dc12538c5ed85349330c1ea686aed85875355e3b485c6d2ab4ae1d54231c6L17' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 65091555</div><div id='project'> Project Name: lucidrains/tab-transformer-pytorch</div><div id='commit'> Commit Name: 68a07cc4f6301efde2159e6d179c672b6a3bd7ec</div><div id='time'> Time: 2020-12-15</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: tab_transformer_pytorch/tab_transformer_pytorch.py</div><div id='m_class'> M Class Name: TabTransformer</div><div id='n_method'> N Class Name: TabTransformer</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: tab_transformer_pytorch/tab_transformer_pytorch.py</div><div id='n_file'> N File Name: tab_transformer_pytorch/tab_transformer_pytorch.py</div><div id='m_start'> M Start Line: 18</div><div id='m_end'> M End Line: 18</div><div id='n_start'> N Start Line: 115</div><div id='n_end'> N End Line: 125</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        xrcheck = dxnorm &lt; self.x_rtol * xnorm
        ytcheck = df &lt; self.f_tol
        yrcheck = df &lt; self.f_rtol * f
        <a id="change">return </a>xtcheck or xrcheck or ytcheck or yrcheck
</code></pre><h3>After Change</h3><pre><code class='java'>
    def to_stop(self, i: int, x: torch.Tensor, xprev: torch.Tensor,
                f: torch.Tensor, fprev: torch.Tensor) -&gt; bool:
        xnorm = float(x.detach().norm())
        dxnorm<a id="change"> = </a>float(<a id="change">(xprev - x).detach().norm()</a>)
        f<a id="change"> = </a>float(f.detach())
        df<a id="change"> = </a>float((fprev - f).detach().abs())

        xtcheck = dxnorm &lt; self.x_tol
        xrcheck = dxnorm &lt; self.x_rtol * xnorm
        ytcheck = df &lt; self.f_tol
        yrcheck = df &lt; self.f_rtol * f
        converge<a id="change"> = </a>xtcheck or xrcheck or ytcheck or yrcheck
        if self.verbose:
            if i == 0:
                print("   &#47&#47:             f |        dx,        df")
            if converge:
                print("Finish with convergence")
            if i == 0 or ((i + 1) % 10) == 0 or converge:
                print("%4d: %.6e | %.3e, %.3e" % (i + 1, f, dxnorm, df))
        <a id="change">return </a>i &gt; 0 and converge
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/xitorch/xitorch/commit/26cdd597461255510eb08439793eef296aaaf3d1#diff-a0b5277eb8a62d78ea19d79d9d6988d9ff4cc8e54c480801df93fb461ebdaf66L176' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 65091570</div><div id='project'> Project Name: xitorch/xitorch</div><div id='commit'> Commit Name: 26cdd597461255510eb08439793eef296aaaf3d1</div><div id='time'> Time: 2021-05-06</div><div id='author'> Author: firman.kasim@gmail.com</div><div id='file'> File Name: xitorch/_impls/optimize/minimizer.py</div><div id='m_class'> M Class Name: TerminationCondition</div><div id='n_method'> N Class Name: TerminationCondition</div><div id='m_method'> M Method Name: to_stop(6)</div><div id='n_method'> N Method Name: to_stop(5)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: xitorch/_impls/optimize/minimizer.py</div><div id='n_file'> N File Name: xitorch/_impls/optimize/minimizer.py</div><div id='m_start'> M Start Line: 177</div><div id='m_end'> M End Line: 181</div><div id='n_start'> N Start Line: 156</div><div id='n_end'> N End Line: 174</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    for i in range(1, len(path)-1):
        implicit_grad = darts_helper(implicit_grad, path[i], path[i+1], config)

    <a id="change">return </a>[add_with_none(dg, ig) for dg, ig in zip(direct_grad, implicit_grad)]


def darts_helper(vector, curr, prev, config):</code></pre><h3>After Change</h3><pre><code class='java'>
    re-interpreting it from the implicit differentiation perspective. Empirically, this method
    achieves better memory efficiency, training wall time, and test accuracy that other methods.
    
    config<a id="change"> = </a>curr.config
    R = config.darts_alpha
    eps<a id="change"> = </a>R / <a id="change">to_vec(vector).norm()</a>

    &#47&#47 positive
    for p, v in zip(curr.trainable_parameters(), vector):
        p.data.add_(v.data, alpha=eps)
    loss_p<a id="change"> = </a>curr.training_step(curr.cur_batch)
    grad_p = torch.autograd.grad(loss_p, prev.trainable_parameters())

    &#47&#47 negative
    for p, v in zip(curr.trainable_parameters(), vector):
        p.data.add_(v.data, alpha=-2*eps)
    loss_n = curr.training_step(curr.cur_batch)
    grad_n = torch.autograd.grad(loss_n, prev.trainable_parameters())

    &#47&#47 reverse weight change
    for p, v in zip(curr.trainable_parameters(), vector):
        p.data.add(v.data, alpha=eps)

    implicit_grad<a id="change"> = </a>[(x - y).div_(2 * eps) for x, y in zip(grad_n, grad_p)]

    <a id="change">return </a>implicit_grad
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leopard-ai/betty/commit/5542eb6dd30fc3b008a0626760a339f3fe31088e#diff-fee71c6e11eca0a047fb000e6ce9c0986274faf27ef8f81ac06ee49870b9ef48L6' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 65091577</div><div id='project'> Project Name: leopard-ai/betty</div><div id='commit'> Commit Name: 5542eb6dd30fc3b008a0626760a339f3fe31088e</div><div id='time'> Time: 2022-06-03</div><div id='author'> Author: sangkeuc@andrew.cmu.edu</div><div id='file'> File Name: betty/hypergradient/darts.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: darts(3)</div><div id='n_method'> N Method Name: darts(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: betty/hypergradient/darts.py</div><div id='n_file'> N File Name: betty/hypergradient/darts.py</div><div id='m_start'> M Start Line: 6</div><div id='m_end'> M End Line: 49</div><div id='n_start'> N Start Line: 6</div><div id='n_end'> N End Line: 36</div><BR>