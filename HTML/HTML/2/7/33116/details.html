<html><h3>Pattern ID :33116
</h3><img src='95767153.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 update shards and devices after ensuring that devices
        &#47&#47 have enough storage for all sharding_options
        <a id="change">for </a>sharding_option in sharding_options<a id="change">:
            for </a>shard, device in <a id="change">zip(</a>sharding_option.shards, devices<a id="change">):
                </a>shard.rank = device.rank
                device.storage -= cast(Storage, shard.storage)
                device.perf += cast(float, shard.perf)
</code></pre><h3>After Change</h3><pre><code class='java'>
                else:
                    sharding_option.shards[i].rank = devices[i].rank
                    devices[i].storage -= storage_needed
                    devices[i].perf<a id="change"> += </a><a id="change">cast(</a>float, sharding_option.shards[i].perf<a id="change">)</a>
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/5947662583f63371e78fd03033f67dee0ffe8d83#diff-6896fef48f71711c5dc772b09352769766468c8f89ac4506aca764a9451aec65L256' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95767153</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: 5947662583f63371e78fd03033f67dee0ffe8d83</div><div id='time'> Time: 2022-04-13</div><div id='author'> Author: joshuadeng@fb.com</div><div id='file'> File Name: torchrec/distributed/planner/partitioners.py</div><div id='m_class'> M Class Name: GreedyPerfPartitioner</div><div id='n_method'> N Class Name: GreedyPerfPartitioner</div><div id='m_method'> M Method Name: _uniform_partition(3)</div><div id='n_method'> N Method Name: _uniform_partition(3)</div><div id='m_parent_class'> M Parent Class: Partitioner</div><div id='n_parent_class'> N Parent Class: Partitioner</div><div id='m_file'> M File Name: torchrec/distributed/planner/partitioners.py</div><div id='n_file'> N File Name: torchrec/distributed/planner/partitioners.py</div><div id='m_start'> M Start Line: 271</div><div id='m_end'> M End Line: 275</div><div id='n_start'> N Start Line: 256</div><div id='n_end'> N End Line: 270</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if context.task_type == ModelType.MULTICLASS:
            x = [&quotClass&quot, &quotModel&quot]
            results = []
            <a id="change">for </a>_, test, model, <a id="change">model_name</a> in context<a id="change">:
                </a>for scorer in scorers:
                    score_result = scorer(model, test)
                    &#47&#47 Multiclass scorers return numpy array of result per class
                    <a id="change">for </a>class_score, class_name in <a id="change">zip(</a>score_result, test.classes<a id="change">):
                        </a>results.append([model_name, class_score, scorer.name, class_name])
            results_df = pd.DataFrame(results, columns=[&quotModel&quot, &quotValue&quot, &quotMetric&quot, &quotClass&quot])
        else:
            x = &quotModel&quot</code></pre><h3>After Change</h3><pre><code class='java'>

        else:
            plot_x_axis = &quotModel&quot
            results<a id="change"> = </a>[
                [model_name, scorer(model, test_dataset), scorer.name, <a id="change">cast(</a>pd.Series, test_dataset.label_col<a id="change">)</a>.count()]
                for _, test_dataset, model, <a id="change">model_name</a> in context
                for scorer in scorers
            ]
            results_df = pd.DataFrame(results, columns=[&quotModel&quot, &quotValue&quot, &quotMetric&quot, &quotNumber of samples&quot])</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/deepchecks/deepchecks/commit/09e6fba6d2fb4f185d4e7f50b39afefa8ba28e67#diff-1fcb2e258a84c483b53d2a88c820f4d59e752abc1df5322ea30620976a64c823L280' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95767155</div><div id='project'> Project Name: deepchecks/deepchecks</div><div id='commit'> Commit Name: 09e6fba6d2fb4f185d4e7f50b39afefa8ba28e67</div><div id='time'> Time: 2022-01-04</div><div id='author'> Author: 71635444+yromanyshyn@users.noreply.github.com</div><div id='file'> File Name: deepchecks/checks/performance/performance_report.py</div><div id='m_class'> M Class Name: MultiModelPerformanceReport</div><div id='n_method'> N Class Name: MultiModelPerformanceReport</div><div id='m_method'> M Method Name: run_logic(2)</div><div id='n_method'> N Method Name: run_logic(2)</div><div id='m_parent_class'> M Parent Class: ModelComparisonBaseCheck</div><div id='n_parent_class'> N Parent Class: ModelComparisonBaseCheck</div><div id='m_file'> M File Name: deepchecks/checks/performance/performance_report.py</div><div id='n_file'> N File Name: deepchecks/checks/performance/performance_report.py</div><div id='m_start'> M Start Line: 284</div><div id='m_end'> M End Line: 314</div><div id='n_start'> N Start Line: 309</div><div id='n_end'> N End Line: 356</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 update shards and devices after ensuring that devices
        &#47&#47 have enough storage for all sharding_options
        <a id="change">for sharding_option</a> in sharding_options<a id="change">:
            for </a>shard, device in <a id="change">zip(</a>sharding_option.shards, devices<a id="change">):
                </a>shard.rank = device.rank
                device.storage -= cast(Storage, shard.storage)
                device.perf += cast(float, shard.perf)
</code></pre><h3>After Change</h3><pre><code class='java'>
    def _uniform_partition(
        self, sharding_options: List[ShardingOption], devices: List[DeviceHardware]
    ) -&gt; None:
        for <a id="change">sharding_option</a> in sharding_options:
            if sharding_option.num_shards != len(devices):
                raise RuntimeError(
                    f"For a uniform partition, the number of shards ({sharding_option.num_shards}) must equal the number of devices ({len(devices)})"
                )
            for i in range(len(devices)):
                storage_needed = cast(Storage, sharding_option.shards[i].storage)
                if storage_needed &gt; devices[i].storage:
                    raise PlannerError(
                        f"Shard of size {storage_needed} bytes does not fit on any rank. Device memory cap: {devices[i].storage}."
                    )
                else:
                    sharding_option.shards[i].rank = devices[i].rank
                    devices[i].storage -= storage_needed
                    devices[i].perf<a id="change"> += </a><a id="change">cast(</a>float, sharding_option.shards[i].perf<a id="change">)</a>
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/5947662583f63371e78fd03033f67dee0ffe8d83#diff-6896fef48f71711c5dc772b09352769766468c8f89ac4506aca764a9451aec65L254' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95767154</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: 5947662583f63371e78fd03033f67dee0ffe8d83</div><div id='time'> Time: 2022-04-13</div><div id='author'> Author: joshuadeng@fb.com</div><div id='file'> File Name: torchrec/distributed/planner/partitioners.py</div><div id='m_class'> M Class Name: GreedyPerfPartitioner</div><div id='n_method'> N Class Name: GreedyPerfPartitioner</div><div id='m_method'> M Method Name: _uniform_partition(3)</div><div id='n_method'> N Method Name: _uniform_partition(3)</div><div id='m_parent_class'> M Parent Class: Partitioner</div><div id='n_parent_class'> N Parent Class: Partitioner</div><div id='m_file'> M File Name: torchrec/distributed/planner/partitioners.py</div><div id='n_file'> N File Name: torchrec/distributed/planner/partitioners.py</div><div id='m_start'> M Start Line: 271</div><div id='m_end'> M End Line: 275</div><div id='n_start'> N Start Line: 256</div><div id='n_end'> N End Line: 270</div><BR>