<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
      &quotood&quot: ood_dataset_builder,
      &quotood_identity&quot: ood_identity_dataset_builder,
  }
  train_dataset<a id="change"> = </a><a id="change">train_dataset_builder.build(split=base.Split.TRAIN)</a>

  class_weight = utils.create_class_weight(
      train_dataset_builders, test_dataset_builders)
  logging.info(&quotclass_weight: %s&quot, str(class_weight))

  ds_info = train_dataset_builder.info
  num_classes = ds_info[&quotnum_classes&quot]  &#47&#47 Positive and negative classes.

  steps_per_epoch<a id="change"> = </a>ds_info[&quotnum_train_examples&quot] // batch_size
  train_datasets = {}
  for dataset_name, dataset_builder in train_dataset_builders.items():
    train_datasets[dataset_name] = dataset_builder.build(split=base.Split.TRAIN)

  test_datasets = {}
  steps_per_eval = {}
  for dataset_name, dataset_builder in test_dataset_builders.items():
    test_datasets[dataset_name] = dataset_builder.build(split=base.Split.TEST)
    steps_per_eval[dataset_name] = (
        dataset_builder.info[&quotnum_test_examples&quot] // test_batch_size)

  if FLAGS.use_bfloat16:
    policy = tf.keras.mixed_precision.experimental.Policy(&quotmixed_bfloat16&quot)
    tf.keras.mixed_precision.experimental.set_policy(policy)

  summary_writer = tf.summary.create_file_writer(
      os.path.join(FLAGS.output_dir, &quotsummaries&quot))

  with strategy.scope():
    logging.info(&quotBuilding BERT %s model&quot, FLAGS.bert_model_type)
    logging.info(&quotuse_gp_layer=%s&quot, FLAGS.use_gp_layer)
    logging.info(&quotuse_spec_norm_att=%s&quot, FLAGS.use_spec_norm_att)
    logging.info(&quotuse_spec_norm_ffn=%s&quot, FLAGS.use_spec_norm_ffn)
    logging.info(&quotuse_layer_norm_att=%s&quot, FLAGS.use_layer_norm_att)
    logging.info(&quotuse_layer_norm_ffn=%s&quot, FLAGS.use_layer_norm_ffn)

    bert_config_dir, bert_ckpt_dir = utils.resolve_bert_ckpt_and_config_dir(
        FLAGS.bert_model_type, FLAGS.bert_dir, FLAGS.bert_config_dir,
        FLAGS.bert_ckpt_dir)
    bert_config = utils.create_config(bert_config_dir)

    gp_layer_kwargs = dict(
        num_inducing=FLAGS.gp_hidden_dim,
        gp_kernel_scale=FLAGS.gp_scale,
        gp_output_bias=FLAGS.gp_bias,
        normalize_input=FLAGS.gp_input_normalization,
        gp_cov_momentum=FLAGS.gp_cov_discount_factor,
        gp_cov_ridge_penalty=FLAGS.gp_cov_ridge_penalty)
    spec_norm_kwargs = dict(
        iteration=FLAGS.spec_norm_iteration,
        norm_multiplier=FLAGS.spec_norm_bound)

    model, bert_encoder = ub.models.SngpBertBuilder(
        num_classes=num_classes,
        bert_config=bert_config,
        gp_layer_kwargs=gp_layer_kwargs,
        spec_norm_kwargs=spec_norm_kwargs,
        use_gp_layer=FLAGS.use_gp_layer,
        use_spec_norm_att=FLAGS.use_spec_norm_att,
        use_spec_norm_ffn=FLAGS.use_spec_norm_ffn,
        use_layer_norm_att=FLAGS.use_layer_norm_att,
        use_layer_norm_ffn=FLAGS.use_layer_norm_ffn,
        use_spec_norm_plr=FLAGS.use_spec_norm_plr)
    optimizer = utils.create_optimizer(
        FLAGS.base_learning_rate,
        steps_per_epoch=steps_per_epoch,
        epochs=FLAGS.train_epochs,
        warmup_proportion=FLAGS.warmup_proportion)

    logging.info(&quotModel input shape: %s&quot, model.input_shape)
    logging.info(&quotModel output shape: %s&quot, model.output_shape)
    logging.info(&quotModel number of weights: %s&quot, model.count_params())

    metrics = {
        &quottrain/negative_log_likelihood&quot: tf.keras.metrics.Mean(),
        &quottrain/accuracy&quot: tf.keras.metrics.Accuracy(),
        &quottrain/accuracy_weighted&quot: tf.keras.metrics.Accuracy(),
        &quottrain/auroc&quot: tf.keras.metrics.AUC(),
        &quottrain/loss&quot: tf.keras.metrics.Mean(),
        &quottrain/ece&quot: um.ExpectedCalibrationError(num_bins=FLAGS.num_bins),
    }

    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)
    if FLAGS.prediction_mode:
      latest_checkpoint = tf.train.latest_checkpoint(FLAGS.eval_checkpoint_dir)
    else:
      latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir)
    initial_epoch = 0
    if latest_checkpoint:
      &#47&#47 checkpoint.restore must be within a strategy.scope() so that optimizer
      &#47&#47 slot variables are mirrored.
      checkpoint.restore(latest_checkpoint)
      logging.info(&quotLoaded checkpoint %s&quot, latest_checkpoint)
      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch
    else:
      &#47&#47 load BERT from initial checkpoint
      bert_encoder, _, _ = utils.load_bert_weight_from_ckpt(
          bert_model=bert_encoder,
          bert_ckpt_dir=bert_ckpt_dir,
          repl_patterns=ub.models.bert_sngp.CHECKPOINT_REPL_PATTERNS)
      logging.info(&quotLoaded BERT checkpoint %s&quot, bert_ckpt_dir)

    &#47&#47 Finally, define test metrics outside the accelerator scope for CPU eval.
    metrics.update({
        &quottest/negative_log_likelihood&quot: tf.keras.metrics.Mean(),
        &quottest/auroc&quot: tf.keras.metrics.AUC(curve=&quotROC&quot),
        &quottest/aupr&quot: tf.keras.metrics.AUC(curve=&quotPR&quot),
        &quottest/brier&quot: tf.keras.metrics.MeanSquaredError(),
        &quottest/brier_weighted&quot: tf.keras.metrics.MeanSquaredError(),
        &quottest/ece&quot: um.ExpectedCalibrationError(num_bins=FLAGS.num_bins),
        &quottest/acc&quot: tf.keras.metrics.Accuracy(),
        &quottest/acc_weighted&quot: tf.keras.metrics.Accuracy(),
        &quottest/eval_time&quot: tf.keras.metrics.Mean(),
        &quottest/stddev&quot: tf.keras.metrics.Mean(),
    })
    for fraction in FLAGS.fractions:
      metrics.update({
          &quottest_collab_acc/collab_acc_{}&quot.format(fraction):
              um.OracleCollaborativeAccuracy(
                  fraction=float(fraction), num_bins=FLAGS.num_bins)
      })
    for dataset_name, test_dataset in test_datasets.items():
      if dataset_name != &quotind&quot:
        metrics.update({
            &quottest/nll_{}&quot.format(dataset_name):
                tf.keras.metrics.Mean(),
            &quottest/auroc_{}&quot.format(dataset_name):
                tf.keras.metrics.AUC(curve=&quotROC&quot),
            &quottest/aupr_{}&quot.format(dataset_name):
                tf.keras.metrics.AUC(curve=&quotPR&quot),
            &quottest/brier_{}&quot.format(dataset_name):
                tf.keras.metrics.MeanSquaredError(),
            &quottest/brier_weighted_{}&quot.format(dataset_name):
                tf.keras.metrics.MeanSquaredError(),
            &quottest/ece_{}&quot.format(dataset_name):
                um.ExpectedCalibrationError(num_bins=FLAGS.num_bins),
            &quottest/acc_{}&quot.format(dataset_name):
                tf.keras.metrics.Accuracy(),
            &quottest/acc_weighted_{}&quot.format(dataset_name):
                tf.keras.metrics.Accuracy(),
            &quottest/eval_time_{}&quot.format(dataset_name):
                tf.keras.metrics.Mean(),
            &quottest/stddev_{}&quot.format(dataset_name):
                tf.keras.metrics.Mean(),
        })
        for fraction in FLAGS.fractions:
          metrics.update({
              &quottest_collab_acc/collab_acc_{}_{}&quot.format(fraction, dataset_name):
                  um.OracleCollaborativeAccuracy(
                      fraction=float(fraction), num_bins=FLAGS.num_bins)
          })

  @tf.function
  def generate_sample_weight(labels, class_weight, label_threshold=0.7):
    Generate sample weight for weighted accuracy calculation.
    if label_threshold != 0.7:
      logging.warning(&quotThe class weight was based on `label_threshold` = 0.7, &quot
                      &quotand weighted accuracy/brier will be meaningless if &quot
                      &quot`label_threshold` is not equal to this value, which is &quot
                      &quotrecommended by Jigsaw Conversation AI team.&quot)
    labels_int = tf.cast(labels &gt; label_threshold, tf.int32)
    sample_weight = tf.gather(class_weight, labels_int)
    return sample_weight

  @tf.function
  def train_step(iterator, dataset_name=&quotwikipedia_toxicity_subtypes&quot):
    Training StepFn.

    def step_fn(inputs):
      Per-Replica StepFn.
      features, labels, _ = utils.create_feature_and_label(inputs)

      with tf.GradientTape() as tape:
        logits = model(features, training=True)

        if isinstance(logits, tuple):
          &#47&#47 If model returns a tuple of (logits, covmat), extract logits
          logits, _ = logits
        if FLAGS.use_bfloat16:
          logits = tf.cast(logits, tf.float32)

        loss_logits = tf.squeeze(logits, axis=1)
        if FLAGS.loss_type == &quotcross_entropy&quot:
          logging.info(&quotUsing cross entropy loss&quot)
          negative_log_likelihood = tf.nn.sigmoid_cross_entropy_with_logits(
              labels, loss_logits)
        elif FLAGS.loss_type == &quotfocal_cross_entropy&quot:
          logging.info(&quotUsing focal cross entropy loss&quot)
          negative_log_likelihood = tfa_losses.sigmoid_focal_crossentropy(
              labels,
              loss_logits,
              alpha=FLAGS.focal_loss_alpha,
              gamma=FLAGS.focal_loss_gamma,
              from_logits=True)
        elif FLAGS.loss_type == &quotmse&quot:
          logging.info(&quotUsing mean squared error loss&quot)
          loss_probs = tf.nn.sigmoid(loss_logits)
          negative_log_likelihood = tf.keras.losses.mean_squared_error(
              labels, loss_probs)
        elif FLAGS.loss_type == &quotmae&quot:
          logging.info(&quotUsing mean absolute error loss&quot)
          loss_probs = tf.nn.sigmoid(loss_logits)
          negative_log_likelihood = tf.keras.losses.mean_absolute_error(
              labels, loss_probs)

        negative_log_likelihood = tf.reduce_mean(negative_log_likelihood)

        l2_loss = sum(model.losses)
        loss = negative_log_likelihood + l2_loss
        &#47&#47 Scale the loss given the TPUStrategy will reduce sum all gradients.
        scaled_loss = loss / strategy.num_replicas_in_sync

      grads = tape.gradient(scaled_loss, model.trainable_variables)
      optimizer.apply_gradients(zip(grads, model.trainable_variables))

      probs = tf.nn.sigmoid(logits)
      &#47&#47 Cast labels to discrete for ECE computation.
      ece_labels = tf.cast(labels &gt; FLAGS.ece_label_threshold, tf.float32)
      ece_probs = tf.concat([1. - probs, probs], axis=1)
      auc_probs = tf.squeeze(probs, axis=1)
      pred_labels = tf.math.argmax(ece_probs, axis=-1)

      sample_weight = generate_sample_weight(
          labels, class_weight[&quottrain/{}&quot.format(dataset_name)],
          FLAGS.ece_label_threshold)
      metrics[&quottrain/negative_log_likelihood&quot].update_state(
          negative_log_likelihood)
      metrics[&quottrain/accuracy&quot].update_state(labels, pred_labels)
      metrics[&quottrain/accuracy_weighted&quot].update_state(
          ece_labels, pred_labels, sample_weight=sample_weight)
      metrics[&quottrain/auroc&quot].update_state(labels, auc_probs)
      metrics[&quottrain/loss&quot].update_state(loss)
      metrics[&quottrain/ece&quot].update_state(ece_labels, ece_probs)

    strategy.run(step_fn, args=(next(iterator),))

  @tf.function
  def test_step(iterator, dataset_name):
    Evaluation StepFn.

    def step_fn(inputs):
      Per-Replica StepFn.
      features, labels, _ = utils.create_feature_and_label(inputs)

      eval_start_time = time.time()
      &#47&#47 Compute ensemble prediction over Monte Carlo forward-pass samples.
      logits_list = []
      stddev_list = []
      for _ in range(FLAGS.num_mc_samples):
        logits = model(features, training=False)

        if isinstance(logits, tuple):
          &#47&#47 If model returns a tuple of (logits, covmat), extract both.
          logits, covmat = logits
        else:
          covmat = tf.eye(test_batch_size)

        if FLAGS.use_bfloat16:
          logits = tf.cast(logits, tf.float32)
          covmat = tf.cast(covmat, tf.float32)

        logits = ed.layers.utils.mean_field_logits(
            logits, covmat, mean_field_factor=FLAGS.gp_mean_field_factor)
        stddev = tf.sqrt(tf.linalg.diag_part(covmat))

        logits_list.append(logits)
        stddev_list.append(stddev)

      eval_time = (time.time() - eval_start_time) / FLAGS.per_core_batch_size
      &#47&#47 Logits dimension is (num_samples, batch_size, num_classes).
      logits_list = tf.stack(logits_list, axis=0)
      stddev_list = tf.stack(stddev_list, axis=0)

      stddev = tf.reduce_mean(stddev_list, axis=0)
      probs_list = tf.nn.sigmoid(logits_list)
      probs = tf.reduce_mean(probs_list, axis=0)
      &#47&#47 Cast labels to discrete for ECE computation.
      ece_labels = tf.cast(labels &gt; FLAGS.ece_label_threshold, tf.float32)
      ece_probs = tf.concat([1. - probs, probs], axis=1)
      pred_labels = tf.math.argmax(ece_probs, axis=-1)
      auc_probs = tf.squeeze(probs, axis=1)

      ce = tf.nn.sigmoid_cross_entropy_with_logits(
          labels=tf.broadcast_to(
              labels, [FLAGS.num_mc_samples, labels.shape[0]]),
          logits=tf.squeeze(logits_list, axis=-1)
      )
      negative_log_likelihood = -tf.reduce_logsumexp(
          -ce, axis=0) + tf.math.log(float(FLAGS.num_mc_samples))
      negative_log_likelihood = tf.reduce_mean(negative_log_likelihood)

      sample_weight = generate_sample_weight(
          labels, class_weight[&quottest/{}&quot.format(dataset_name)],
          FLAGS.ece_label_threshold)
      if dataset_name == &quotind&quot:
        metrics[&quottest/negative_log_likelihood&quot].update_state(
            negative_log_likelihood)
        metrics[&quottest/auroc&quot].update_state(labels, auc_probs)
        metrics[&quottest/aupr&quot].update_state(labels, auc_probs)
        metrics[&quottest/brier&quot].update_state(labels, auc_probs)
        metrics[&quottest/brier_weighted&quot].update_state(
            tf.expand_dims(labels, -1), probs, sample_weight=sample_weight)
        metrics[&quottest/ece&quot].update_state(ece_labels, ece_probs)
        metrics[&quottest/acc&quot].update_state(ece_labels, pred_labels)
        metrics[&quottest/acc_weighted&quot].update_state(
            ece_labels, pred_labels, sample_weight=sample_weight)
        metrics[&quottest/eval_time&quot].update_state(eval_time)
        metrics[&quottest/stddev&quot].update_state(stddev)
        for fraction in FLAGS.fractions:
          metrics[&quottest_collab_acc/collab_acc_{}&quot.format(
              fraction)].update_state(ece_labels, ece_probs)
      else:
        metrics[&quottest/nll_{}&quot.format(dataset_name)].update_state(
            negative_log_likelihood)
        metrics[&quottest/auroc_{}&quot.format(dataset_name)].update_state(
            labels, auc_probs)
        metrics[&quottest/aupr_{}&quot.format(dataset_name)].update_state(
            labels, auc_probs)
        metrics[&quottest/brier_{}&quot.format(dataset_name)].update_state(
            labels, auc_probs)
        metrics[&quottest/brier_weighted_{}&quot.format(dataset_name)].update_state(
            tf.expand_dims(labels, -1), probs, sample_weight=sample_weight)
        metrics[&quottest/ece_{}&quot.format(dataset_name)].update_state(
            ece_labels, ece_probs)
        metrics[&quottest/acc_{}&quot.format(dataset_name)].update_state(
            ece_labels, pred_labels)
        metrics[&quottest/acc_weighted_{}&quot.format(dataset_name)].update_state(
            ece_labels, pred_labels, sample_weight=sample_weight)
        metrics[&quottest/eval_time_{}&quot.format(dataset_name)].update_state(
            eval_time)
        metrics[&quottest/stddev_{}&quot.format(dataset_name)].update_state(stddev)
        for fraction in FLAGS.fractions:
          metrics[&quottest_collab_acc/collab_acc_{}_{}&quot.format(
              fraction, dataset_name)].update_state(ece_labels, ece_probs)

    strategy.run(step_fn, args=(next(iterator),))

  @tf.function
  def final_eval_step(iterator):
    Final Evaluation StepFn to save prediction to directory.

    def step_fn(inputs):
      bert_features, labels, additional_labels = utils.create_feature_and_label(
          inputs)
      logits = model(bert_features, training=False)
      if isinstance(logits, tuple):
        &#47&#47 If model returns a tuple of (logits, covmat), extract both.
        logits, covmat = logits
      else:
        covmat = tf.eye(test_batch_size)

      if FLAGS.use_bfloat16:
        logits = tf.cast(logits, tf.float32)
        covmat = tf.cast(covmat, tf.float32)

      logits = ed.layers.utils.mean_field_logits(
          logits, covmat, mean_field_factor=FLAGS.gp_mean_field_factor)
      features = inputs[&quotinput_ids&quot]
      return features, logits, labels, additional_labels

    (per_replica_texts, per_replica_logits, per_replica_labels,
     per_replica_additional_labels) = (
         strategy.run(step_fn, args=(next(iterator),)))

    if strategy.num_replicas_in_sync &gt; 1:
      texts_list = tf.concat(per_replica_texts.values, axis=0)
      logits_list = tf.concat(per_replica_logits.values, axis=0)
      labels_list = tf.concat(per_replica_labels.values, axis=0)
      additional_labels_dict = {}
      for additional_label in utils.IDENTITY_LABELS:
        if additional_label in per_replica_additional_labels:
          additional_labels_dict[additional_label] = tf.concat(
              per_replica_additional_labels[additional_label], axis=0)
    else:
      texts_list = per_replica_texts
      logits_list = per_replica_logits
      labels_list = per_replica_labels
      additional_labels_dict = {}
      for additional_label in utils.IDENTITY_LABELS:
        if additional_label in per_replica_additional_labels:
          additional_labels_dict[
              additional_label] = per_replica_additional_labels[
                  additional_label]

    return texts_list, logits_list, labels_list, additional_labels_dict

  if FLAGS.prediction_mode:
    &#47&#47 Prediction and exit.
    for dataset_name, test_dataset in test_datasets.items():
      test_iterator = iter(test_dataset)  &#47&#47 pytype: disable=wrong-arg-types
      message = &quotFinal eval on dataset {}&quot.format(dataset_name)
      logging.info(message)

      texts_all = []
      logits_all = []
      labels_all = []
      additional_labels_all_dict = {}
      if &quotidentity&quot in dataset_name:
        for identity_label_name in utils.IDENTITY_LABELS:
          additional_labels_all_dict[identity_label_name] = []

      for step in range(steps_per_eval[dataset_name]):
        if step % 20 == 0:
          message = &quotStarting to run eval step {}/{} of dataset: {}&quot.format(
              step, steps_per_eval[dataset_name], dataset_name)
          logging.info(message)

        try:
          (text_step, logits_step, labels_step,
           additional_labels_dict_step) = final_eval_step(test_iterator)
        except tf.errors.OutOfRangeError:
          continue

        texts_all.append(text_step)
        logits_all.append(logits_step)
        labels_all.append(labels_step)
        if &quotidentity&quot in dataset_name:
          for identity_label_name in utils.IDENTITY_LABELS:
            additional_labels_all_dict[identity_label_name].append(
                additional_labels_dict_step[identity_label_name])

      texts_all = tf.concat(texts_all, axis=0)
      logits_all = tf.concat(logits_all, axis=0)
      labels_all = tf.concat(labels_all, axis=0)
      additional_labels_all = []
      if additional_labels_all_dict:
        for identity_label_name in utils.IDENTITY_LABELS:
          additional_labels_all.append(
              tf.concat(
                  additional_labels_all_dict[identity_label_name], axis=0))
      additional_labels_all = tf.convert_to_tensor(additional_labels_all)

      utils.save_prediction(
          texts_all.numpy(),
          path=os.path.join(FLAGS.output_dir, &quottexts_{}&quot.format(dataset_name)))
      utils.save_prediction(
          labels_all.numpy(),
          path=os.path.join(FLAGS.output_dir, &quotlabels_{}&quot.format(dataset_name)))
      utils.save_prediction(
          logits_all.numpy(),
          path=os.path.join(FLAGS.output_dir, &quotlogits_{}&quot.format(dataset_name)))
      if &quotidentity&quot in dataset_name:
        utils.save_prediction(
            additional_labels_all.numpy(),
            path=os.path.join(FLAGS.output_dir,
                              &quotadditional_labels_{}&quot.format(dataset_name)))
      logging.info(&quotDone with testing on %s&quot, dataset_name)

  else:
    train_iterator<a id="change"> = </a>iter(train_dataset)
    start_time = time.time()
    for epoch in range(initial_epoch, FLAGS.train_epochs):
      logging.info(&quotStarting to run epoch: %s&quot, epoch)
      <a id="change">for </a><a id="change">step</a> in range(steps_per_epoch)<a id="change">:
        </a><a id="change">train_step(</a>train_iterator<a id="change">)</a>

        current_step<a id="change"> = epoch</a><a id="change"> * steps_per_epoch + </a>(<a id="change">step</a><a id="change"> + 1</a>)
        max_steps = steps_per_epoch * FLAGS.train_epochs
        time_elapsed = time.time() - start_time
        steps_per_sec = float(current_step) / time_elapsed</code></pre><h3>After Change</h3><pre><code class='java'>
  num_classes = ds_info[&quotnum_classes&quot]  &#47&#47 Positive and negative classes.

  train_datasets = {}
  <a id="change">dataset_steps_per_epoch = </a><a id="change">{}</a>
  total_steps_per_epoch<a id="change"> = 0</a>
  for dataset_name, dataset_builder in train_dataset_builders.items():
    train_datasets[dataset_name] = dataset_builder.build(split=base.Split.TRAIN)
    <a id="change">dataset_steps_per_epoch[dataset_name] = </a>(
        dataset_builder.info[&quotnum_train_examples&quot] // batch_size)
    total_steps_per_epoch<a id="change"> += dataset_steps_per_epoch</a><a id="change">[dataset_name]</a>

  test_datasets = {}
  steps_per_eval = {}
  for dataset_name, dataset_builder in test_dataset_builders.items():
    test_datasets[dataset_name] = dataset_builder.build(split=base.Split.TEST)
    steps_per_eval[dataset_name] = (
        dataset_builder.info[&quotnum_test_examples&quot] // test_batch_size)

  if FLAGS.use_bfloat16:
    policy = tf.keras.mixed_precision.experimental.Policy(&quotmixed_bfloat16&quot)
    tf.keras.mixed_precision.experimental.set_policy(policy)

  summary_writer = tf.summary.create_file_writer(
      os.path.join(FLAGS.output_dir, &quotsummaries&quot))

  with strategy.scope():
    logging.info(&quotBuilding BERT %s model&quot, FLAGS.bert_model_type)
    logging.info(&quotuse_gp_layer=%s&quot, FLAGS.use_gp_layer)
    logging.info(&quotuse_spec_norm_att=%s&quot, FLAGS.use_spec_norm_att)
    logging.info(&quotuse_spec_norm_ffn=%s&quot, FLAGS.use_spec_norm_ffn)
    logging.info(&quotuse_layer_norm_att=%s&quot, FLAGS.use_layer_norm_att)
    logging.info(&quotuse_layer_norm_ffn=%s&quot, FLAGS.use_layer_norm_ffn)

    bert_config_dir, bert_ckpt_dir = utils.resolve_bert_ckpt_and_config_dir(
        FLAGS.bert_model_type, FLAGS.bert_dir, FLAGS.bert_config_dir,
        FLAGS.bert_ckpt_dir)
    bert_config = utils.create_config(bert_config_dir)

    gp_layer_kwargs = dict(
        num_inducing=FLAGS.gp_hidden_dim,
        gp_kernel_scale=FLAGS.gp_scale,
        gp_output_bias=FLAGS.gp_bias,
        normalize_input=FLAGS.gp_input_normalization,
        gp_cov_momentum=FLAGS.gp_cov_discount_factor,
        gp_cov_ridge_penalty=FLAGS.gp_cov_ridge_penalty)
    spec_norm_kwargs = dict(
        iteration=FLAGS.spec_norm_iteration,
        norm_multiplier=FLAGS.spec_norm_bound)

    model, bert_encoder = ub.models.SngpBertBuilder(
        num_classes=num_classes,
        bert_config=bert_config,
        gp_layer_kwargs=gp_layer_kwargs,
        spec_norm_kwargs=spec_norm_kwargs,
        use_gp_layer=FLAGS.use_gp_layer,
        use_spec_norm_att=FLAGS.use_spec_norm_att,
        use_spec_norm_ffn=FLAGS.use_spec_norm_ffn,
        use_layer_norm_att=FLAGS.use_layer_norm_att,
        use_layer_norm_ffn=FLAGS.use_layer_norm_ffn,
        use_spec_norm_plr=FLAGS.use_spec_norm_plr)
    optimizer = utils.create_optimizer(
        FLAGS.base_learning_rate,
        steps_per_epoch=total_steps_per_epoch,
        epochs=FLAGS.train_epochs,
        warmup_proportion=FLAGS.warmup_proportion)

    logging.info(&quotModel input shape: %s&quot, model.input_shape)
    logging.info(&quotModel output shape: %s&quot, model.output_shape)
    logging.info(&quotModel number of weights: %s&quot, model.count_params())

    metrics = {
        &quottrain/negative_log_likelihood&quot: tf.keras.metrics.Mean(),
        &quottrain/accuracy&quot: tf.keras.metrics.Accuracy(),
        &quottrain/accuracy_weighted&quot: tf.keras.metrics.Accuracy(),
        &quottrain/auroc&quot: tf.keras.metrics.AUC(),
        &quottrain/loss&quot: tf.keras.metrics.Mean(),
        &quottrain/ece&quot: um.ExpectedCalibrationError(num_bins=FLAGS.num_bins),
    }

    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)
    if FLAGS.prediction_mode:
      latest_checkpoint = tf.train.latest_checkpoint(FLAGS.eval_checkpoint_dir)
    else:
      latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir)
    initial_epoch = 0
    if latest_checkpoint:
      &#47&#47 checkpoint.restore must be within a strategy.scope() so that optimizer
      &#47&#47 slot variables are mirrored.
      checkpoint.restore(latest_checkpoint)
      logging.info(&quotLoaded checkpoint %s&quot, latest_checkpoint)
      initial_epoch = optimizer.iterations.numpy() // total_steps_per_epoch
    else:
      &#47&#47 load BERT from initial checkpoint
      bert_encoder, _, _ = utils.load_bert_weight_from_ckpt(
          bert_model=bert_encoder,
          bert_ckpt_dir=bert_ckpt_dir,
          repl_patterns=ub.models.bert_sngp.CHECKPOINT_REPL_PATTERNS)
      logging.info(&quotLoaded BERT checkpoint %s&quot, bert_ckpt_dir)

    &#47&#47 Finally, define test metrics outside the accelerator scope for CPU eval.
    metrics.update({
        &quottest/negative_log_likelihood&quot: tf.keras.metrics.Mean(),
        &quottest/auroc&quot: tf.keras.metrics.AUC(curve=&quotROC&quot),
        &quottest/aupr&quot: tf.keras.metrics.AUC(curve=&quotPR&quot),
        &quottest/brier&quot: tf.keras.metrics.MeanSquaredError(),
        &quottest/brier_weighted&quot: tf.keras.metrics.MeanSquaredError(),
        &quottest/ece&quot: um.ExpectedCalibrationError(num_bins=FLAGS.num_bins),
        &quottest/acc&quot: tf.keras.metrics.Accuracy(),
        &quottest/acc_weighted&quot: tf.keras.metrics.Accuracy(),
        &quottest/eval_time&quot: tf.keras.metrics.Mean(),
        &quottest/stddev&quot: tf.keras.metrics.Mean(),
    })
    for fraction in FLAGS.fractions:
      metrics.update({
          &quottest_collab_acc/collab_acc_{}&quot.format(fraction):
              um.OracleCollaborativeAccuracy(
                  fraction=float(fraction), num_bins=FLAGS.num_bins)
      })
    for dataset_name, test_dataset in test_datasets.items():
      if dataset_name != &quotind&quot:
        metrics.update({
            &quottest/nll_{}&quot.format(dataset_name):
                tf.keras.metrics.Mean(),
            &quottest/auroc_{}&quot.format(dataset_name):
                tf.keras.metrics.AUC(curve=&quotROC&quot),
            &quottest/aupr_{}&quot.format(dataset_name):
                tf.keras.metrics.AUC(curve=&quotPR&quot),
            &quottest/brier_{}&quot.format(dataset_name):
                tf.keras.metrics.MeanSquaredError(),
            &quottest/brier_weighted_{}&quot.format(dataset_name):
                tf.keras.metrics.MeanSquaredError(),
            &quottest/ece_{}&quot.format(dataset_name):
                um.ExpectedCalibrationError(num_bins=FLAGS.num_bins),
            &quottest/acc_{}&quot.format(dataset_name):
                tf.keras.metrics.Accuracy(),
            &quottest/acc_weighted_{}&quot.format(dataset_name):
                tf.keras.metrics.Accuracy(),
            &quottest/eval_time_{}&quot.format(dataset_name):
                tf.keras.metrics.Mean(),
            &quottest/stddev_{}&quot.format(dataset_name):
                tf.keras.metrics.Mean(),
        })
        for fraction in FLAGS.fractions:
          metrics.update({
              &quottest_collab_acc/collab_acc_{}_{}&quot.format(fraction, dataset_name):
                  um.OracleCollaborativeAccuracy(
                      fraction=float(fraction), num_bins=FLAGS.num_bins)
          })

  @tf.function
  def generate_sample_weight(labels, class_weight, label_threshold=0.7):
    Generate sample weight for weighted accuracy calculation.
    if label_threshold != 0.7:
      logging.warning(&quotThe class weight was based on `label_threshold` = 0.7, &quot
                      &quotand weighted accuracy/brier will be meaningless if &quot
                      &quot`label_threshold` is not equal to this value, which is &quot
                      &quotrecommended by Jigsaw Conversation AI team.&quot)
    labels_int = tf.cast(labels &gt; label_threshold, tf.int32)
    sample_weight = tf.gather(class_weight, labels_int)
    return sample_weight

  @tf.function
  def train_step(iterator, dataset_name):
    Training StepFn.

    def step_fn(inputs):
      Per-Replica StepFn.
      features, labels, _ = utils.create_feature_and_label(inputs)

      with tf.GradientTape() as tape:
        logits = model(features, training=True)

        if isinstance(logits, tuple):
          &#47&#47 If model returns a tuple of (logits, covmat), extract logits
          logits, _ = logits
        if FLAGS.use_bfloat16:
          logits = tf.cast(logits, tf.float32)

        loss_logits = tf.squeeze(logits, axis=1)
        if FLAGS.loss_type == &quotcross_entropy&quot:
          logging.info(&quotUsing cross entropy loss&quot)
          negative_log_likelihood = tf.nn.sigmoid_cross_entropy_with_logits(
              labels, loss_logits)
        elif FLAGS.loss_type == &quotfocal_cross_entropy&quot:
          logging.info(&quotUsing focal cross entropy loss&quot)
          negative_log_likelihood = tfa_losses.sigmoid_focal_crossentropy(
              labels,
              loss_logits,
              alpha=FLAGS.focal_loss_alpha,
              gamma=FLAGS.focal_loss_gamma,
              from_logits=True)
        elif FLAGS.loss_type == &quotmse&quot:
          logging.info(&quotUsing mean squared error loss&quot)
          loss_probs = tf.nn.sigmoid(loss_logits)
          negative_log_likelihood = tf.keras.losses.mean_squared_error(
              labels, loss_probs)
        elif FLAGS.loss_type == &quotmae&quot:
          logging.info(&quotUsing mean absolute error loss&quot)
          loss_probs = tf.nn.sigmoid(loss_logits)
          negative_log_likelihood = tf.keras.losses.mean_absolute_error(
              labels, loss_probs)

        negative_log_likelihood = tf.reduce_mean(negative_log_likelihood)

        l2_loss = sum(model.losses)
        loss = negative_log_likelihood + l2_loss
        &#47&#47 Scale the loss given the TPUStrategy will reduce sum all gradients.
        scaled_loss = loss / strategy.num_replicas_in_sync

      grads = tape.gradient(scaled_loss, model.trainable_variables)
      optimizer.apply_gradients(zip(grads, model.trainable_variables))

      probs = tf.nn.sigmoid(logits)
      &#47&#47 Cast labels to discrete for ECE computation.
      ece_labels = tf.cast(labels &gt; FLAGS.ece_label_threshold, tf.float32)
      ece_probs = tf.concat([1. - probs, probs], axis=1)
      auc_probs = tf.squeeze(probs, axis=1)
      pred_labels = tf.math.argmax(ece_probs, axis=-1)

      sample_weight = generate_sample_weight(
          labels, class_weight[&quottrain/{}&quot.format(dataset_name)],
          FLAGS.ece_label_threshold)
      metrics[&quottrain/negative_log_likelihood&quot].update_state(
          negative_log_likelihood)
      metrics[&quottrain/accuracy&quot].update_state(labels, pred_labels)
      metrics[&quottrain/accuracy_weighted&quot].update_state(
          ece_labels, pred_labels, sample_weight=sample_weight)
      metrics[&quottrain/auroc&quot].update_state(labels, auc_probs)
      metrics[&quottrain/loss&quot].update_state(loss)
      metrics[&quottrain/ece&quot].update_state(ece_labels, ece_probs)

    strategy.run(step_fn, args=(next(iterator),))

  @tf.function
  def test_step(iterator, dataset_name):
    Evaluation StepFn.

    def step_fn(inputs):
      Per-Replica StepFn.
      features, labels, _ = utils.create_feature_and_label(inputs)

      eval_start_time = time.time()
      &#47&#47 Compute ensemble prediction over Monte Carlo forward-pass samples.
      logits_list = []
      stddev_list = []
      for _ in range(FLAGS.num_mc_samples):
        logits = model(features, training=False)

        if isinstance(logits, tuple):
          &#47&#47 If model returns a tuple of (logits, covmat), extract both.
          logits, covmat = logits
        else:
          covmat = tf.eye(test_batch_size)

        if FLAGS.use_bfloat16:
          logits = tf.cast(logits, tf.float32)
          covmat = tf.cast(covmat, tf.float32)

        logits = ed.layers.utils.mean_field_logits(
            logits, covmat, mean_field_factor=FLAGS.gp_mean_field_factor)
        stddev = tf.sqrt(tf.linalg.diag_part(covmat))

        logits_list.append(logits)
        stddev_list.append(stddev)

      eval_time = (time.time() - eval_start_time) / FLAGS.per_core_batch_size
      &#47&#47 Logits dimension is (num_samples, batch_size, num_classes).
      logits_list = tf.stack(logits_list, axis=0)
      stddev_list = tf.stack(stddev_list, axis=0)

      stddev = tf.reduce_mean(stddev_list, axis=0)
      probs_list = tf.nn.sigmoid(logits_list)
      probs = tf.reduce_mean(probs_list, axis=0)
      &#47&#47 Cast labels to discrete for ECE computation.
      ece_labels = tf.cast(labels &gt; FLAGS.ece_label_threshold, tf.float32)
      ece_probs = tf.concat([1. - probs, probs], axis=1)
      pred_labels = tf.math.argmax(ece_probs, axis=-1)
      auc_probs = tf.squeeze(probs, axis=1)

      ce = tf.nn.sigmoid_cross_entropy_with_logits(
          labels=tf.broadcast_to(
              labels, [FLAGS.num_mc_samples, labels.shape[0]]),
          logits=tf.squeeze(logits_list, axis=-1)
      )
      negative_log_likelihood = -tf.reduce_logsumexp(
          -ce, axis=0) + tf.math.log(float(FLAGS.num_mc_samples))
      negative_log_likelihood = tf.reduce_mean(negative_log_likelihood)

      sample_weight = generate_sample_weight(
          labels, class_weight[&quottest/{}&quot.format(dataset_name)],
          FLAGS.ece_label_threshold)
      if dataset_name == &quotind&quot:
        metrics[&quottest/negative_log_likelihood&quot].update_state(
            negative_log_likelihood)
        metrics[&quottest/auroc&quot].update_state(labels, auc_probs)
        metrics[&quottest/aupr&quot].update_state(labels, auc_probs)
        metrics[&quottest/brier&quot].update_state(labels, auc_probs)
        metrics[&quottest/brier_weighted&quot].update_state(
            tf.expand_dims(labels, -1), probs, sample_weight=sample_weight)
        metrics[&quottest/ece&quot].update_state(ece_labels, ece_probs)
        metrics[&quottest/acc&quot].update_state(ece_labels, pred_labels)
        metrics[&quottest/acc_weighted&quot].update_state(
            ece_labels, pred_labels, sample_weight=sample_weight)
        metrics[&quottest/eval_time&quot].update_state(eval_time)
        metrics[&quottest/stddev&quot].update_state(stddev)
        for fraction in FLAGS.fractions:
          metrics[&quottest_collab_acc/collab_acc_{}&quot.format(
              fraction)].update_state(ece_labels, ece_probs)
      else:
        metrics[&quottest/nll_{}&quot.format(dataset_name)].update_state(
            negative_log_likelihood)
        metrics[&quottest/auroc_{}&quot.format(dataset_name)].update_state(
            labels, auc_probs)
        metrics[&quottest/aupr_{}&quot.format(dataset_name)].update_state(
            labels, auc_probs)
        metrics[&quottest/brier_{}&quot.format(dataset_name)].update_state(
            labels, auc_probs)
        metrics[&quottest/brier_weighted_{}&quot.format(dataset_name)].update_state(
            tf.expand_dims(labels, -1), probs, sample_weight=sample_weight)
        metrics[&quottest/ece_{}&quot.format(dataset_name)].update_state(
            ece_labels, ece_probs)
        metrics[&quottest/acc_{}&quot.format(dataset_name)].update_state(
            ece_labels, pred_labels)
        metrics[&quottest/acc_weighted_{}&quot.format(dataset_name)].update_state(
            ece_labels, pred_labels, sample_weight=sample_weight)
        metrics[&quottest/eval_time_{}&quot.format(dataset_name)].update_state(
            eval_time)
        metrics[&quottest/stddev_{}&quot.format(dataset_name)].update_state(stddev)
        for fraction in FLAGS.fractions:
          metrics[&quottest_collab_acc/collab_acc_{}_{}&quot.format(
              fraction, dataset_name)].update_state(ece_labels, ece_probs)

    strategy.run(step_fn, args=(next(iterator),))

  @tf.function
  def final_eval_step(iterator):
    Final Evaluation StepFn to save prediction to directory.

    def step_fn(inputs):
      bert_features, labels, additional_labels = utils.create_feature_and_label(
          inputs)
      logits = model(bert_features, training=False)
      if isinstance(logits, tuple):
        &#47&#47 If model returns a tuple of (logits, covmat), extract both.
        logits, covmat = logits
      else:
        covmat = tf.eye(test_batch_size)

      if FLAGS.use_bfloat16:
        logits = tf.cast(logits, tf.float32)
        covmat = tf.cast(covmat, tf.float32)

      logits = ed.layers.utils.mean_field_logits(
          logits, covmat, mean_field_factor=FLAGS.gp_mean_field_factor)
      features = inputs[&quotinput_ids&quot]
      return features, logits, labels, additional_labels

    (per_replica_texts, per_replica_logits, per_replica_labels,
     per_replica_additional_labels) = (
         strategy.run(step_fn, args=(next(iterator),)))

    if strategy.num_replicas_in_sync &gt; 1:
      texts_list = tf.concat(per_replica_texts.values, axis=0)
      logits_list = tf.concat(per_replica_logits.values, axis=0)
      labels_list = tf.concat(per_replica_labels.values, axis=0)
      additional_labels_dict = {}
      for additional_label in utils.IDENTITY_LABELS:
        if additional_label in per_replica_additional_labels:
          additional_labels_dict[additional_label] = tf.concat(
              per_replica_additional_labels[additional_label], axis=0)
    else:
      texts_list = per_replica_texts
      logits_list = per_replica_logits
      labels_list = per_replica_labels
      additional_labels_dict = {}
      for additional_label in utils.IDENTITY_LABELS:
        if additional_label in per_replica_additional_labels:
          additional_labels_dict[
              additional_label] = per_replica_additional_labels[
                  additional_label]

    return texts_list, logits_list, labels_list, additional_labels_dict

  if FLAGS.prediction_mode:
    &#47&#47 Prediction and exit.
    for dataset_name, test_dataset in test_datasets.items():
      test_iterator = iter(test_dataset)  &#47&#47 pytype: disable=wrong-arg-types
      message = &quotFinal eval on dataset {}&quot.format(dataset_name)
      logging.info(message)

      texts_all = []
      logits_all = []
      labels_all = []
      additional_labels_all_dict = {}
      if &quotidentity&quot in dataset_name:
        for identity_label_name in utils.IDENTITY_LABELS:
          additional_labels_all_dict[identity_label_name] = []

      for step in range(steps_per_eval[dataset_name]):
        if step % 20 == 0:
          message = &quotStarting to run eval step {}/{} of dataset: {}&quot.format(
              step, steps_per_eval[dataset_name], dataset_name)
          logging.info(message)

        try:
          (text_step, logits_step, labels_step,
           additional_labels_dict_step) = final_eval_step(test_iterator)
        except tf.errors.OutOfRangeError:
          continue

        texts_all.append(text_step)
        logits_all.append(logits_step)
        labels_all.append(labels_step)
        if &quotidentity&quot in dataset_name:
          for identity_label_name in utils.IDENTITY_LABELS:
            additional_labels_all_dict[identity_label_name].append(
                additional_labels_dict_step[identity_label_name])

      texts_all = tf.concat(texts_all, axis=0)
      logits_all = tf.concat(logits_all, axis=0)
      labels_all = tf.concat(labels_all, axis=0)
      additional_labels_all = []
      if additional_labels_all_dict:
        for identity_label_name in utils.IDENTITY_LABELS:
          additional_labels_all.append(
              tf.concat(
                  additional_labels_all_dict[identity_label_name], axis=0))
      additional_labels_all = tf.convert_to_tensor(additional_labels_all)

      utils.save_prediction(
          texts_all.numpy(),
          path=os.path.join(FLAGS.output_dir, &quottexts_{}&quot.format(dataset_name)))
      utils.save_prediction(
          labels_all.numpy(),
          path=os.path.join(FLAGS.output_dir, &quotlabels_{}&quot.format(dataset_name)))
      utils.save_prediction(
          logits_all.numpy(),
          path=os.path.join(FLAGS.output_dir, &quotlogits_{}&quot.format(dataset_name)))
      if &quotidentity&quot in dataset_name:
        utils.save_prediction(
            additional_labels_all.numpy(),
            path=os.path.join(FLAGS.output_dir,
                              &quotadditional_labels_{}&quot.format(dataset_name)))
      logging.info(&quotDone with testing on %s&quot, dataset_name)

  else:
    &#47&#47 Execute train / eval loop.
    start_time = time.time()
    for epoch in range(initial_epoch, FLAGS.train_epochs):
      logging.info(&quotStarting to run epoch: %s&quot, epoch)
      current_step<a id="change"> = epoch</a><a id="change"> * </a>total_steps_per_epoch
      <a id="change">for </a>dataset_name, <a id="change">train_dataset</a> in <a id="change">train_datasets.items():
        for </a>step in range(<a id="change">dataset_steps_per_epoch[dataset_name]</a>)<a id="change">:
          </a>train_iterator<a id="change"> = </a>iter(train_dataset)
          <a id="change">train_step(</a>train_iterator, dataset_name<a id="change">)</a>

          current_step<a id="change"> += </a>1
          max_steps = total_steps_per_epoch * FLAGS.train_epochs
          time_elapsed = time.time() - start_time
          steps_per_sec = float(current_step) / time_elapsed</code></pre>