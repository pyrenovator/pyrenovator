<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.block_dim = self.lbmgr.get_block_dim(self.rank)
        self.comm_func = all_reduce

        <a id="change">if blk_embed is not None</a>:
            weights = blk_embed.get_weights(detach=True)
            base_embedding_dim = blk_embed.get_base_embedding_dim()
            assert weights[0].size() == (sum([self.field_dims[i] for i in self.group]), self.block_dim), \
                &quotpassed embedding layer dimensions are wrong: {x1} vs {x2} \
                    &quot.format(x1=weights[0].size(), x2=(sum([self.field_dims[i] for i in self.group]), self.block_dim))
            if self.block_dim != self.embedding_dim:
                assert weights[1].size() == (self.embedding_dim, self.block_dim), \
                    &quotpassed linear layer dimensions are wrong: {x1} vs {x2} \
                    &quot.format(x1=weights[1].size(), x2=(self.embedding_dim, self.block_dim))
            if base_embedding_dim != self.embedding_dim:
                logger.warning(&quotBase embedding dimension provided by blk_embed is different from \
                    default or manually passed. Will overwrite by blk_embed.base_embedding_dim&quot, ranks=[0])
                self.embedding_dim = base_embedding_dim
            self.embed = BlockEmbeddingBag.from_pretrained(
                                                weights=weights,
                                                base_embedding_dim=base_embedding_dim,
                                                freeze=freeze)
        else:
            self.embed<a id="change"> = </a>BlockEmbeddingBag(
                                    <a id="change">sum(</a><a id="change">[self.field_dims[i] for i in self.group])</a>, 
                                    block_embedding_dim=self.block_dim,
                                    base_embedding_dim=self.embedding_dim,
                                    mode=mode,</code></pre><h3>After Change</h3><pre><code class='java'>

        if lbmgr is not None:
            self.lbmgr = lbmgr
            <a id="change">self.field_dims</a> = lbmgr.get_field_dims()
            self.embedding_dim = lbmgr.get_base_dim()
        else:
            self.lbmgr = LoadBalanceManager(field_dims, self.num_groups, embedding_dim)

        self.group = self.lbmgr.get_group(self.rank)
        self.block_dim = self.lbmgr.get_block_dim(self.rank)
        self.qr_bucket_size = self.lbmgr.get_qr_bucket_size(self.rank)
        self.offsets = torch.tensor((0,*np.cumsum(np.array( \
            self.field_dims, dtype=np.long)[self.group])[:-1]), device=self.device)
        
        self.comm_func = all_reduce
        cls = BlockEmbeddingBag<a id="change"> if </a><a id="change">not enable_qr else </a>QREmbeddingBag

        <a id="change">if pretrain_embed is not None</a>:
            assert isinstance(pretrain_embed, cls), f"{type(pretrain_embed)} isn&quott {cls}"
            <a id="change">if </a><a id="change">not enable_qr</a>:
                weights = pretrain_embed.get_weights(detach=True)
                base_embedding_dim = pretrain_embed.get_base_embedding_dim()
                assert weights[0].size() == (sum([self.field_dims[i] for i in self.group]), self.block_dim), \
                    &quotpassed embedding layer dimensions are wrong: {x1} vs {x2} \
                        &quot.format(x1=weights[0].size(), x2=(sum([self.field_dims[i] for i in self.group]), self.block_dim))
                if self.block_dim != self.embedding_dim:
                    assert weights[1].size() == (self.embedding_dim, self.block_dim), \
                        &quotpassed linear layer dimensions are wrong: {x1} vs {x2} \
                        &quot.format(x1=weights[1].size(), x2=(self.embedding_dim, self.block_dim))
                if base_embedding_dim != self.embedding_dim:
                    logger.warning(&quotBase embedding dimension provided by blk_embed is different from \
                        default or manually passed. Will overwrite by blk_embed.base_embedding_dim&quot)
                    self.embedding_dim = base_embedding_dim
                self.embed = cls.from_pretrained(weights=weights,
                                            base_embedding_dim=base_embedding_dim,
                                            freeze=freeze)
            else:
                weights<a id="change"> = pretrain_embed</a><a id="change">.get_weights(detach=True)</a>
                num_embeddings<a id="change"> = pretrain_embed</a><a id="change">.get_num_embeddings()</a>
                <a id="change">assert </a>num_embeddings == sum([self.field_dims[i] for i in self.group]), \
                    f&quotpassed embedding layer have wrong number of embeddings: {num_embeddings} vs \
                        {sum([self.field_dims[i] for i in self.group])}&quot
                <a id="change">assert </a>weights[0].size() == weights[1].size() == (self.qr_bucket_size, self.embedding_dim), \
                    &quoteither q- or r-embedding layer has wrong input dimensions: {x1} vs {x2} vs {x3} \
                        &quot.format(x1=weights[0].size(), x2=weights[0].size(), 
                                x3=(self.qr_bucket_size, self.embedding_dim))
                self.embed<a id="change"> = </a><a id="change">cls.from_pretrained(weights=weights,
                                            num_embeddings=num_embeddings,
                                            freeze=freeze)</a>
        else:
            <a id="change">if not enable_qr</a>:
                self.embed<a id="change"> = </a>cls(<a id="change">sum(</a><a id="change">[self.field_dims[i] for i in self.group])</a>, 
                                block_embedding_dim=self.block_dim,
                                base_embedding_dim=self.embedding_dim,
                                mode=mode,
                                *args,
                                **kwargs)
            else:
                self.embed<a id="change"> = </a><a id="change">cls(sum(</a><a id="change">[self.field_dims[i] for i in self.group])</a>, 
                                num_buckets=self.qr_bucket_size,
                                embedding_dim=self.embedding_dim,
                                mode=mode,
                                *<a id="change">args,
                                **kwargs)</a>

    def _shard_tensor(self, _input: Tensor) -&gt; Tensor:
        assert min(self.group) &gt;= 0 and max(self.group) &lt; _input.size(1)
        return _input[:, self.group].to(self.device)</code></pre>