<html><h3>Pattern ID :1087
</h3><img src='5544532.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.block_dim = self.lbmgr.get_block_dim(self.rank)
        self.comm_func = all_reduce

        <a id="change">if blk_embed is not None</a>:
            weights = blk_embed.get_weights(detach=True)
            base_embedding_dim = blk_embed.get_base_embedding_dim()
            assert weights[0].size() == (sum([self.field_dims[i] for i in self.group]), self.block_dim), \
                &quotpassed embedding layer dimensions are wrong: {x1} vs {x2} \
                    &quot.format(x1=weights[0].size(), x2=(sum([self.field_dims[i] for i in self.group]), self.block_dim))
            if self.block_dim != self.embedding_dim:
                assert weights[1].size() == (self.embedding_dim, self.block_dim), \
                    &quotpassed linear layer dimensions are wrong: {x1} vs {x2} \
                    &quot.format(x1=weights[1].size(), x2=(self.embedding_dim, self.block_dim))
            if base_embedding_dim != self.embedding_dim:
                logger.warning(&quotBase embedding dimension provided by blk_embed is different from \
                    default or manually passed. Will overwrite by blk_embed.base_embedding_dim&quot, ranks=[0])
                self.embedding_dim = base_embedding_dim
            self.embed = BlockEmbeddingBag.from_pretrained(
                                                weights=weights,
                                                base_embedding_dim=base_embedding_dim,
                                                freeze=freeze)
        else:
            self.embed<a id="change"> = </a>BlockEmbeddingBag(
                                    <a id="change">sum(</a><a id="change">[self.field_dims[i] for i in self.group])</a>, 
                                    block_embedding_dim=self.block_dim,
                                    base_embedding_dim=self.embedding_dim,
                                    mode=mode,</code></pre><h3>After Change</h3><pre><code class='java'>

        if lbmgr is not None:
            self.lbmgr = lbmgr
            <a id="change">self.field_dims</a> = lbmgr.get_field_dims()
            self.embedding_dim = lbmgr.get_base_dim()
        else:
            self.lbmgr = LoadBalanceManager(field_dims, self.num_groups, embedding_dim)

        self.group = self.lbmgr.get_group(self.rank)
        self.block_dim = self.lbmgr.get_block_dim(self.rank)
        self.qr_bucket_size = self.lbmgr.get_qr_bucket_size(self.rank)
        self.offsets = torch.tensor((0,*np.cumsum(np.array( \
            self.field_dims, dtype=np.long)[self.group])[:-1]), device=self.device)
        
        self.comm_func = all_reduce
        cls = BlockEmbeddingBag<a id="change"> if </a><a id="change">not enable_qr else </a>QREmbeddingBag

        <a id="change">if pretrain_embed is not None</a>:
            assert isinstance(pretrain_embed, cls), f"{type(pretrain_embed)} isn&quott {cls}"
            <a id="change">if </a><a id="change">not enable_qr</a>:
                weights = pretrain_embed.get_weights(detach=True)
                base_embedding_dim = pretrain_embed.get_base_embedding_dim()
                assert weights[0].size() == (sum([self.field_dims[i] for i in self.group]), self.block_dim), \
                    &quotpassed embedding layer dimensions are wrong: {x1} vs {x2} \
                        &quot.format(x1=weights[0].size(), x2=(sum([self.field_dims[i] for i in self.group]), self.block_dim))
                if self.block_dim != self.embedding_dim:
                    assert weights[1].size() == (self.embedding_dim, self.block_dim), \
                        &quotpassed linear layer dimensions are wrong: {x1} vs {x2} \
                        &quot.format(x1=weights[1].size(), x2=(self.embedding_dim, self.block_dim))
                if base_embedding_dim != self.embedding_dim:
                    logger.warning(&quotBase embedding dimension provided by blk_embed is different from \
                        default or manually passed. Will overwrite by blk_embed.base_embedding_dim&quot)
                    self.embedding_dim = base_embedding_dim
                self.embed = cls.from_pretrained(weights=weights,
                                            base_embedding_dim=base_embedding_dim,
                                            freeze=freeze)
            else:
                weights<a id="change"> = pretrain_embed</a><a id="change">.get_weights(detach=True)</a>
                num_embeddings<a id="change"> = pretrain_embed</a><a id="change">.get_num_embeddings()</a>
                <a id="change">assert </a>num_embeddings == sum([self.field_dims[i] for i in self.group]), \
                    f&quotpassed embedding layer have wrong number of embeddings: {num_embeddings} vs \
                        {sum([self.field_dims[i] for i in self.group])}&quot
                <a id="change">assert </a>weights[0].size() == weights[1].size() == (self.qr_bucket_size, self.embedding_dim), \
                    &quoteither q- or r-embedding layer has wrong input dimensions: {x1} vs {x2} vs {x3} \
                        &quot.format(x1=weights[0].size(), x2=weights[0].size(), 
                                x3=(self.qr_bucket_size, self.embedding_dim))
                self.embed<a id="change"> = </a><a id="change">cls.from_pretrained(weights=weights,
                                            num_embeddings=num_embeddings,
                                            freeze=freeze)</a>
        else:
            <a id="change">if not enable_qr</a>:
                self.embed<a id="change"> = </a>cls(<a id="change">sum(</a><a id="change">[self.field_dims[i] for i in self.group])</a>, 
                                block_embedding_dim=self.block_dim,
                                base_embedding_dim=self.embedding_dim,
                                mode=mode,
                                *args,
                                **kwargs)
            else:
                self.embed<a id="change"> = </a><a id="change">cls(sum(</a><a id="change">[self.field_dims[i] for i in self.group])</a>, 
                                num_buckets=self.qr_bucket_size,
                                embedding_dim=self.embedding_dim,
                                mode=mode,
                                *<a id="change">args,
                                **kwargs)</a>

    def _shard_tensor(self, _input: Tensor) -&gt; Tensor:
        assert min(self.group) &gt;= 0 and max(self.group) &lt; _input.size(1)
        return _input[:, self.group].to(self.device)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 36</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/hpcaitech/freqcacheembedding/commit/a43c647bec9dfa2af8e9d3adca2093bc9648023b#diff-5c9aa898945c80eb3501856ca336e7c14ee654517e43434196eab7cf2bbf74a8L235' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5544532</div><div id='project'> Project Name: hpcaitech/freqcacheembedding</div><div id='commit'> Commit Name: a43c647bec9dfa2af8e9d3adca2093bc9648023b</div><div id='time'> Time: 2022-07-14</div><div id='author'> Author: jiatong.han@u.nus.edu</div><div id='file'> File Name: colo_recsys/modules/embeddings/parallel_mix_vocab_embedding.py</div><div id='m_class'> M Class Name: ParallelMixVocabEmbeddingBag</div><div id='n_method'> N Class Name: ParallelMixVocabEmbeddingBag</div><div id='m_method'> M Method Name: __init__(10)</div><div id='n_method'> N Method Name: __init__(9)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: colo_recsys/modules/embeddings/parallel_mix_vocab_embedding.py</div><div id='n_file'> N File Name: colo_recsys/modules/embeddings/parallel_mix_vocab_embedding.py</div><div id='m_start'> M Start Line: 235</div><div id='m_end'> M End Line: 265</div><div id='n_start'> N Start Line: 334</div><div id='n_end'> N End Line: 416</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.block_dim = self.lbmgr.get_block_dim(self.rank)
        self.comm_func = reduce_forward

        <a id="change">if blk_embed is not None</a>:
            weights = blk_embed.get_weights(detach=True)
            base_embedding_dim = blk_embed.get_base_embedding_dim()
            assert weights[0].size() == (sum([self.field_dims[i] for i in self.group]), self.block_dim), \
                &quotpassed embedding layer dimensions are wrong: {x1} vs {x2} \
                    &quot.format(x1=weights[0].size(), x2=(sum([self.field_dims[i] for i in self.group]), self.block_dim))
            if self.block_dim != self.embedding_dim:
                assert weights[1].size() == (self.embedding_dim, self.block_dim), \
                    &quotpassed linear layer dimensions are wrong: {x1} vs {x2} \
                    &quot.format(x1=weights[1].size(), x2=(self.embedding_dim, self.block_dim))
            if base_embedding_dim != self.embedding_dim:
                DISTLogger.warning(&quotBase embedding dimension provided by blk_embed is different from \
                    default or manually passed. Will overwrite by blk_embed.base_embedding_dim&quot)
                self.embedding_dim = base_embedding_dim
            self.embed = BlockEmbeddingBag.from_pretrained(
                                                weights=weights,
                                                base_embedding_dim=base_embedding_dim,
                                                freeze=freeze)
        else:
            self.embed<a id="change"> = </a>BlockEmbeddingBag(
                                    <a id="change">sum(</a><a id="change">[self.field_dims[i] for i in self.group])</a>, 
                                    block_embedding_dim=self.block_dim,
                                    base_embedding_dim=self.embedding_dim,
                                    mode=mode,</code></pre><h3>After Change</h3><pre><code class='java'>
                *args,
                **kwargs):
        super().__init__()
        <a id="change">self.field_dims</a> = field_dims
        self.embedding_dim = embedding_dim
        self.mode = mode
        self.device = device if device is not None else torch.device(&quotcuda&quot, torch.cuda.current_device())

        &#47&#47 Decide number of nodes
        self.parallel_mode = ParallelMode.DEFAULT if parallel_mode is None else parallel_mode
        self.world_size = DISTMGR.get_world_size(self.parallel_mode)
        self.rank = DISTMGR.get_rank(self.parallel_mode)
        self.num_groups = self.world_size &#47&#47 default setting

        if lbmgr is not None:
            self.lbmgr = lbmgr
            <a id="change">self.field_dims</a> = lbmgr.get_field_dims()
            self.embedding_dim = lbmgr.get_base_dim()
        else:
            self.lbmgr = LoadBalanceManager(field_dims, self.num_groups, embedding_dim)

        self.group = self.lbmgr.get_group(self.rank)
        self.block_dim = self.lbmgr.get_block_dim(self.rank)
        self.qr_bucket_size = self.lbmgr.get_qr_bucket_size(self.rank)
        self.offsets = torch.tensor((0,*np.cumsum(np.array( \
            self.field_dims, dtype=np.long)[self.group])[:-1]), device=self.device)
        
        self.comm_func = reduce_forward
        cls = BlockEmbeddingBag<a id="change"> if </a><a id="change">not enable_qr else </a>QREmbeddingBag

        <a id="change">if pretrain_embed is not None</a>:
            <a id="change">if </a><a id="change">not enable_qr</a>:
                assert isinstance(pretrain_embed, cls)
                weights = pretrain_embed.get_weights(detach=True)
                base_embedding_dim = pretrain_embed.get_base_embedding_dim()
                assert weights[0].size() == (sum([self.field_dims[i] for i in self.group]), self.block_dim), \
                    &quotpassed embedding layer dimensions are wrong: {x1} vs {x2} \
                        &quot.format(x1=weights[0].size(), x2=(sum([self.field_dims[i] for i in self.group]), self.block_dim))
                if self.block_dim != self.embedding_dim:
                    assert weights[1].size() == (self.embedding_dim, self.block_dim), \
                        &quotpassed linear layer dimensions are wrong: {x1} vs {x2} \
                        &quot.format(x1=weights[1].size(), x2=(self.embedding_dim, self.block_dim))
                if base_embedding_dim != self.embedding_dim:
                    DISTLogger.warning(&quotBase embedding dimension provided by blk_embed is different from \
                        default or manually passed. Will overwrite by blk_embed.base_embedding_dim&quot)
                    self.embedding_dim = base_embedding_dim
                self.embed = cls.from_pretrained(weights=weights,
                                            base_embedding_dim=base_embedding_dim,
                                            freeze=freeze)
            else:
                <a id="change">assert </a>isinstance(pretrain_embed, cls)
                weights<a id="change"> = </a><a id="change">pretrain_embed.get_weights(detach=True)</a>
                num_embeddings<a id="change"> = </a><a id="change">pretrain_embed.get_num_embeddings()</a>
                assert num_embeddings == sum([self.field_dims[i] for i in self.group]), \
                    f&quotpassed embedding layer have wrong number of embeddings: {num_embeddings} vs \
                        {sum([self.field_dims[i] for i in self.group])}&quot
                <a id="change">assert </a>weights[0].size() == weights[1].size() == (self.qr_bucket_size, self.embedding_dim), \
                    &quoteither q- or r-embedding layer has wrong input dimensions: {x1} vs {x2} vs {x3} \
                        &quot.format(x1=weights[0].size(), x2=weights[0].size(), 
                                x3=(self.qr_bucket_size, self.embedding_dim))
                self.embed<a id="change"> = </a><a id="change">cls.from_pretrained(weights=weights,
                                            num_embeddings=num_embeddings,
                                            freeze=freeze)</a>
        else:
            <a id="change">if not enable_qr</a>:
                self.embed<a id="change"> = </a>cls(<a id="change">sum(</a><a id="change">[self.field_dims[i] for i in self.group])</a>, 
                                block_embedding_dim=self.block_dim,
                                embedding_dim=self.embedding_dim,
                                mode=mode,
                                *args,
                                **kwargs)
            else:
                self.embed<a id="change"> = </a><a id="change">cls(sum(</a><a id="change">[self.field_dims[i] for i in self.group])</a>, 
                                num_buckets=self.qr_bucket_size,
                                embedding_dim=self.embedding_dim,
                                mode=mode,
                                *<a id="change">args,
                                **kwargs)</a>

    def _shard_tensor(self, _input: Tensor) -&gt; Tensor:
        assert min(self.group) &gt;= 0 and max(self.group) &lt; _input.size(1)
        return _input[:, self.group].to(self.device)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/hpcaitech/freqcacheembedding/commit/8d5208244075b5fe13e566d3b1de07f3d9cb570b#diff-5eb5715d975b7975822b38af2e5bd3c8cd410d833addf110e32fc3ca3e425e81L196' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5544533</div><div id='project'> Project Name: hpcaitech/freqcacheembedding</div><div id='commit'> Commit Name: 8d5208244075b5fe13e566d3b1de07f3d9cb570b</div><div id='time'> Time: 2022-07-13</div><div id='author'> Author: jiatong.han@u.nus.edu</div><div id='file'> File Name: recsys/modules/embeddings/parallel_mix_vocab_embedding.py</div><div id='m_class'> M Class Name: ParallelMixVocabEmbeddingBag</div><div id='n_method'> N Class Name: ParallelMixVocabEmbeddingBag</div><div id='m_method'> M Method Name: __init__(10)</div><div id='n_method'> N Method Name: __init__(9)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: recsys/modules/embeddings/parallel_mix_vocab_embedding.py</div><div id='n_file'> N File Name: recsys/modules/embeddings/parallel_mix_vocab_embedding.py</div><div id='m_start'> M Start Line: 229</div><div id='m_end'> M End Line: 259</div><div id='n_start'> N Start Line: 333</div><div id='n_end'> N End Line: 416</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.block_dim = self.lbmgr.get_block_dim(self.rank)
        self.comm_func = all_reduce

        <a id="change">if blk_embed is not None</a>:
            weights = blk_embed.get_weights(detach=True)
            base_embedding_dim = blk_embed.get_base_embedding_dim()
            assert weights[0].size() == (sum([self.field_dims[i] for i in self.group]), self.block_dim), \
                &quotpassed embedding layer dimensions are wrong: {x1} vs {x2} \
                    &quot.format(x1=weights[0].size(), x2=(sum([self.field_dims[i] for i in self.group]), self.block_dim))
            if self.block_dim != self.embedding_dim:
                assert weights[1].size() == (self.embedding_dim, self.block_dim), \
                    &quotpassed linear layer dimensions are wrong: {x1} vs {x2} \
                    &quot.format(x1=weights[1].size(), x2=(self.embedding_dim, self.block_dim))
            if base_embedding_dim != self.embedding_dim:
                logger.warning(&quotBase embedding dimension provided by blk_embed is different from \
                    default or manually passed. Will overwrite by blk_embed.base_embedding_dim&quot, ranks=[0])
                self.embedding_dim = base_embedding_dim
            self.embed = BlockEmbeddingBag.from_pretrained(
                                                weights=weights,
                                                base_embedding_dim=base_embedding_dim,
                                                freeze=freeze)
        else:
            self.embed<a id="change"> = </a>BlockEmbeddingBag(
                                    <a id="change">sum(</a><a id="change">[self.field_dims[i] for i in self.group])</a>, 
                                    block_embedding_dim=self.block_dim,
                                    base_embedding_dim=self.embedding_dim,
                                    mode=mode,</code></pre><h3>After Change</h3><pre><code class='java'>
                *args,
                **kwargs):
        super().__init__()
        <a id="change">self.field_dims</a> = field_dims
        self.embedding_dim = embedding_dim
        self.mode = mode
        self.device = device if device is not None else get_current_device()

        &#47&#47 Decide number of nodes
        self.parallel_mode = ParallelMode.GLOBAL if parallel_mode is None else parallel_mode
        self.world_size = gpc.get_world_size(self.parallel_mode)
        self.rank = gpc.get_local_rank(self.parallel_mode)
        self.num_groups = self.world_size &#47&#47 default setting

        if lbmgr is not None:
            self.lbmgr = lbmgr
            <a id="change">self.field_dims</a> = lbmgr.get_field_dims()
            self.embedding_dim = lbmgr.get_base_dim()
        else:
            self.lbmgr = LoadBalanceManager(field_dims, self.num_groups, embedding_dim)

        self.group = self.lbmgr.get_group(self.rank)
        self.block_dim = self.lbmgr.get_block_dim(self.rank)
        self.qr_bucket_size = self.lbmgr.get_qr_bucket_size(self.rank)
        self.offsets = torch.tensor((0,*np.cumsum(np.array( \
            self.field_dims, dtype=np.long)[self.group])[:-1]), device=self.device)
        
        self.comm_func = all_reduce
        cls = BlockEmbeddingBag<a id="change"> if </a><a id="change">not enable_qr else </a>QREmbeddingBag

        <a id="change">if pretrain_embed is not None</a>:
            assert isinstance(pretrain_embed, cls), f"{type(pretrain_embed)} isn&quott {cls}"
            <a id="change">if </a><a id="change">not enable_qr</a>:
                weights = pretrain_embed.get_weights(detach=True)
                base_embedding_dim = pretrain_embed.get_base_embedding_dim()
                assert weights[0].size() == (sum([self.field_dims[i] for i in self.group]), self.block_dim), \
                    &quotpassed embedding layer dimensions are wrong: {x1} vs {x2} \
                        &quot.format(x1=weights[0].size(), x2=(sum([self.field_dims[i] for i in self.group]), self.block_dim))
                if self.block_dim != self.embedding_dim:
                    assert weights[1].size() == (self.embedding_dim, self.block_dim), \
                        &quotpassed linear layer dimensions are wrong: {x1} vs {x2} \
                        &quot.format(x1=weights[1].size(), x2=(self.embedding_dim, self.block_dim))
                if base_embedding_dim != self.embedding_dim:
                    logger.warning(&quotBase embedding dimension provided by blk_embed is different from \
                        default or manually passed. Will overwrite by blk_embed.base_embedding_dim&quot)
                    self.embedding_dim = base_embedding_dim
                self.embed = cls.from_pretrained(weights=weights,
                                            base_embedding_dim=base_embedding_dim,
                                            freeze=freeze)
            else:
                weights<a id="change"> = </a><a id="change">pretrain_embed.get_weights(detach=True)</a>
                num_embeddings<a id="change"> = </a><a id="change">pretrain_embed.get_num_embeddings()</a>
                <a id="change">assert </a>num_embeddings == sum([self.field_dims[i] for i in self.group]), \
                    f&quotpassed embedding layer have wrong number of embeddings: {num_embeddings} vs \
                        {sum([self.field_dims[i] for i in self.group])}&quot
                <a id="change">assert </a>weights[0].size() == weights[1].size() == (self.qr_bucket_size, self.embedding_dim), \
                    &quoteither q- or r-embedding layer has wrong input dimensions: {x1} vs {x2} vs {x3} \
                        &quot.format(x1=weights[0].size(), x2=weights[0].size(), 
                                x3=(self.qr_bucket_size, self.embedding_dim))
                self.embed<a id="change"> = </a><a id="change">cls.from_pretrained(weights=weights,
                                            num_embeddings=num_embeddings,
                                            freeze=freeze)</a>
        else:
            <a id="change">if not enable_qr</a>:
                self.embed<a id="change"> = </a>cls(<a id="change">sum(</a><a id="change">[self.field_dims[i] for i in self.group])</a>, 
                                block_embedding_dim=self.block_dim,
                                base_embedding_dim=self.embedding_dim,
                                mode=mode,
                                *args,
                                **kwargs)
            else:
                self.embed<a id="change"> = </a><a id="change">cls(sum(</a><a id="change">[self.field_dims[i] for i in self.group])</a>, 
                                num_buckets=self.qr_bucket_size,
                                embedding_dim=self.embedding_dim,
                                mode=mode,
                                *<a id="change">args,
                                **kwargs)</a>

    def _shard_tensor(self, _input: Tensor) -&gt; Tensor:
        assert min(self.group) &gt;= 0 and max(self.group) &lt; _input.size(1)
        return _input[:, self.group].to(self.device)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/hpcaitech/freqcacheembedding/commit/a43c647bec9dfa2af8e9d3adca2093bc9648023b#diff-5c9aa898945c80eb3501856ca336e7c14ee654517e43434196eab7cf2bbf74a8L202' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5544542</div><div id='project'> Project Name: hpcaitech/freqcacheembedding</div><div id='commit'> Commit Name: a43c647bec9dfa2af8e9d3adca2093bc9648023b</div><div id='time'> Time: 2022-07-14</div><div id='author'> Author: jiatong.han@u.nus.edu</div><div id='file'> File Name: colo_recsys/modules/embeddings/parallel_mix_vocab_embedding.py</div><div id='m_class'> M Class Name: ParallelMixVocabEmbeddingBag</div><div id='n_method'> N Class Name: ParallelMixVocabEmbeddingBag</div><div id='m_method'> M Method Name: __init__(10)</div><div id='n_method'> N Method Name: __init__(9)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: colo_recsys/modules/embeddings/parallel_mix_vocab_embedding.py</div><div id='n_file'> N File Name: colo_recsys/modules/embeddings/parallel_mix_vocab_embedding.py</div><div id='m_start'> M Start Line: 235</div><div id='m_end'> M End Line: 265</div><div id='n_start'> N Start Line: 334</div><div id='n_end'> N End Line: 416</div><BR>