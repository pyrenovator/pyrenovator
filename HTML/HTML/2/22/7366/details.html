<html><h3>Pattern ID :7366
</h3><img src='24481077.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def forward(self, text, view=False):
        with torch.no_grad():
            phones = <a id="change">self.text2phone.string_to_tensor(text).squeeze(0).long()</a>.to(torch.device(self.device))
            mel, durations, pitch, energy = self.phone2mel(phones, speaker_embedding=self.speaker_embedding, return_duration_pitch_energy=True)
            mel = mel.transpose(0, 1)
            wave = self.mel2wav(mel)
        if view:
            from Utility.utils import cumsum_durations
            fig, ax = plt.subplots(nrows=2, ncols=1)
            ax[0].plot(wave.cpu().numpy())
            lbd.specshow(mel.cpu().numpy(),
                         ax=ax[1],
                         sr=16000,
                         cmap=&quotGnBu&quot,
                         y_axis=&quotmel&quot,
                         x_axis=None,
                         hop_length=256)
            ax[0].yaxis.set_visible(False)
            ax[1].yaxis.set_visible(False)
            <a id="change">duration_splits</a><a id="change">, label_positions</a> = cumsum_durations(durations.cpu().numpy())
            <a id="change">ax[1].set_xticks(duration_splits</a><a id="change">, minor=True)</a>
            <a id="change">ax[1]</a>.xaxis.grid(True, which=&quotminor&quot)
            <a id="change">ax[1].set_xticks(label_positions</a><a id="change">, minor=False)</a>
            <a id="change">ax[1].set_xticklabels(</a><a id="change">self.text2phone.get_phone_string(text</a><a id="change">))</a>
            ax[0].set_title(text)
            plt.subplots_adjust(left=0.05, bottom=0.1, right=0.95, top=.9, wspace=0.0, hspace=0.0)
            plt.show()
        return wave</code></pre><h3>After Change</h3><pre><code class='java'>

    def forward(self, text, view=False):
        with torch.no_grad():
            phones = <a id="change">self.text2phone.string_to_tensor(</a>text<a id="change">)</a>.to(torch.device(self.device))
            mel, durations, pitch, energy = self.phone2mel(phones, speaker_embedding=self.speaker_embedding, return_duration_pitch_energy=True)
            mel = mel.transpose(0, 1)
            wave = self.mel2wav(mel)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 13</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/e4dcb8977dff5be8c8632dccd084be0ecce361a5#diff-1f63ab95c9347b87795c1ff3835921e8bf53468a0fd2686ec56a913abdde3321L29' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 24481077</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: e4dcb8977dff5be8c8632dccd084be0ecce361a5</div><div id='time'> Time: 2021-08-12</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: InferenceInterfaces/Nancy_FastSpeech2.py</div><div id='m_class'> M Class Name: Nancy_FastSpeech2</div><div id='n_method'> N Class Name: Nancy_FastSpeech2</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: InferenceInterfaces/Nancy_FastSpeech2.py</div><div id='n_file'> N File Name: InferenceInterfaces/Nancy_FastSpeech2.py</div><div id='m_start'> M Start Line: 35</div><div id='m_end'> M End Line: 56</div><div id='n_start'> N Start Line: 29</div><div id='n_end'> N End Line: 29</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def forward(self, text, view=False):
        with torch.no_grad():
            phones = <a id="change">self.text2phone.string_to_tensor(text).squeeze(0).long()</a>.to(torch.device(self.device))
            mel, durations, pitch, energy = self.phone2mel(phones, speaker_embedding=self.speaker_embedding, return_duration_pitch_energy=True)
            mel = mel.transpose(0, 1)
            wave = self.mel2wav(mel)
        if view:
            from Utility.utils import cumsum_durations
            fig, ax = plt.subplots(nrows=2, ncols=1)
            ax[0].plot(wave.cpu().numpy())
            lbd.specshow(mel.cpu().numpy(),
                         ax=ax[1],
                         sr=16000,
                         cmap=&quotGnBu&quot,
                         y_axis=&quotmel&quot,
                         x_axis=None,
                         hop_length=256)
            ax[0].yaxis.set_visible(False)
            ax[1].yaxis.set_visible(False)
            duration_splits<a id="change">, label_positions</a> = cumsum_durations(durations.cpu().numpy())
            <a id="change">ax[1].set_xticks(</a>duration_splits<a id="change">, minor=True)</a>
            <a id="change">ax[1]</a>.xaxis.grid(True, which=&quotminor&quot)
            <a id="change">ax[1].set_xticks(</a>label_positions<a id="change">, minor=False)</a>
            <a id="change">ax[1].set_xticklabels(</a><a id="change">self.text2phone.get_phone_string(</a>text<a id="change">))</a>
            ax[0].set_title(text)
            plt.subplots_adjust(left=0.05, bottom=0.1, right=0.95, top=.9, wspace=0.0, hspace=0.0)
            plt.show()
        return wave</code></pre><h3>After Change</h3><pre><code class='java'>

    def forward(self, text, view=False):
        with torch.no_grad():
            phones = <a id="change">self.text2phone.string_to_tensor(</a>text<a id="change">)</a>.to(torch.device(self.device))
            mel, durations, pitch, energy = self.phone2mel(phones, speaker_embedding=self.speaker_embedding, return_duration_pitch_energy=True)
            mel = mel.transpose(0, 1)
            wave = self.mel2wav(mel)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/e4dcb8977dff5be8c8632dccd084be0ecce361a5#diff-d6349bc4fe66e0815860524d44ffe680c786183cb3b1452eb1a1fe89b583bd53L34' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 24481076</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: e4dcb8977dff5be8c8632dccd084be0ecce361a5</div><div id='time'> Time: 2021-08-12</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: InferenceInterfaces/LibriTTS_FastSpeech2.py</div><div id='m_class'> M Class Name: LibriTTS_FastSpeech2</div><div id='n_method'> N Class Name: LibriTTS_FastSpeech2</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: InferenceInterfaces/LibriTTS_FastSpeech2.py</div><div id='n_file'> N File Name: InferenceInterfaces/LibriTTS_FastSpeech2.py</div><div id='m_start'> M Start Line: 36</div><div id='m_end'> M End Line: 57</div><div id='n_start'> N Start Line: 30</div><div id='n_end'> N End Line: 30</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def forward(self, text, view=False):
        with torch.no_grad():
            phones = <a id="change">self.text2phone.string_to_tensor(text).squeeze(0).long()</a>.to(torch.device(self.device))
            mel, durations, pitch, energy = self.phone2mel(phones, speaker_embedding=self.speaker_embedding, return_duration_pitch_energy=True)
            mel = mel.transpose(0, 1)
            wave = self.mel2wav(mel)
        if view:
            from Utility.utils import cumsum_durations
            fig, ax = plt.subplots(nrows=2, ncols=1)
            ax[0].plot(wave.cpu().numpy())
            lbd.specshow(mel.cpu().numpy(),
                         ax=ax[1],
                         sr=16000,
                         cmap=&quotGnBu&quot,
                         y_axis=&quotmel&quot,
                         x_axis=None,
                         hop_length=256)
            ax[0].yaxis.set_visible(False)
            ax[1].yaxis.set_visible(False)
            duration_splits<a id="change">, label_positions</a> = cumsum_durations(durations.cpu().numpy())
            <a id="change">ax[1].set_xticks(</a>duration_splits<a id="change">, minor=True)</a>
            <a id="change">ax[1]</a>.xaxis.grid(True, which=&quotminor&quot)
            <a id="change">ax[1].set_xticks(</a>label_positions<a id="change">, minor=False)</a>
            <a id="change">ax[1].set_xticklabels(</a><a id="change">self.text2phone.get_phone_string(</a>text<a id="change">))</a>
            ax[0].set_title(text)
            plt.subplots_adjust(left=0.05, bottom=0.1, right=0.95, top=.9, wspace=0.0, hspace=0.0)
            plt.show()
        return wave</code></pre><h3>After Change</h3><pre><code class='java'>

    def forward(self, text, view=False):
        with torch.no_grad():
            phones = <a id="change">self.text2phone.string_to_tensor(</a>text<a id="change">)</a>.to(torch.device(self.device))
            mel, durations, pitch, energy = self.phone2mel(phones, speaker_embedding=self.speaker_embedding, return_duration_pitch_energy=True)
            mel = mel.transpose(0, 1)
            wave = self.mel2wav(mel)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/e4dcb8977dff5be8c8632dccd084be0ecce361a5#diff-35a7e7c3ad111bf67fe8f37667065dfd135e3ee8dbb86ccf6bfd92b5bcb1de10L32' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 24481060</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: e4dcb8977dff5be8c8632dccd084be0ecce361a5</div><div id='time'> Time: 2021-08-12</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: InferenceInterfaces/Thorsten_FastSpeech2.py</div><div id='m_class'> M Class Name: Thorsten_FastSpeech2</div><div id='n_method'> N Class Name: Thorsten_FastSpeech2</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: InferenceInterfaces/Thorsten_FastSpeech2.py</div><div id='n_file'> N File Name: InferenceInterfaces/Thorsten_FastSpeech2.py</div><div id='m_start'> M Start Line: 34</div><div id='m_end'> M End Line: 55</div><div id='n_start'> N Start Line: 29</div><div id='n_end'> N End Line: 29</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def forward(self, text, view=False):
        with torch.no_grad():
            phones = <a id="change">self.text2phone.string_to_tensor(text).squeeze(0).long()</a>.to(torch.device(self.device))
            mel, durations, pitch, energy = self.phone2mel(phones, speaker_embedding=self.speaker_embedding, return_duration_pitch_energy=True)
            mel = mel.transpose(0, 1)
            wave = self.mel2wav(mel)
        if view:
            from Utility.utils import cumsum_durations
            fig, ax = plt.subplots(nrows=2, ncols=1)
            ax[0].plot(wave.cpu().numpy())
            lbd.specshow(mel.cpu().numpy(),
                         ax=ax[1],
                         sr=16000,
                         cmap=&quotGnBu&quot,
                         y_axis=&quotmel&quot,
                         x_axis=None,
                         hop_length=256)
            ax[0].yaxis.set_visible(False)
            ax[1].yaxis.set_visible(False)
            duration_splits<a id="change">, label_positions</a> = cumsum_durations(durations.cpu().numpy())
            <a id="change">ax[1].set_xticks(</a>duration_splits<a id="change">, minor=True)</a>
            <a id="change">ax[1]</a>.xaxis.grid(True, which=&quotminor&quot)
            <a id="change">ax[1].set_xticks(</a>label_positions<a id="change">, minor=False)</a>
            <a id="change">ax[1].set_xticklabels(</a><a id="change">self.text2phone.get_phone_string(</a>text<a id="change">))</a>
            ax[0].set_title(text)
            plt.subplots_adjust(left=0.05, bottom=0.1, right=0.95, top=.9, wspace=0.0, hspace=0.0)
            plt.show()
        return wave</code></pre><h3>After Change</h3><pre><code class='java'>

    def forward(self, text, view=False):
        with torch.no_grad():
            phones = <a id="change">self.text2phone.string_to_tensor(</a>text<a id="change">)</a>.to(torch.device(self.device))
            mel, durations, pitch, energy = self.phone2mel(phones, speaker_embedding=self.speaker_embedding, return_duration_pitch_energy=True)
            mel = mel.transpose(0, 1)
            wave = self.mel2wav(mel)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/e4dcb8977dff5be8c8632dccd084be0ecce361a5#diff-1f63ab95c9347b87795c1ff3835921e8bf53468a0fd2686ec56a913abdde3321L33' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 24481079</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: e4dcb8977dff5be8c8632dccd084be0ecce361a5</div><div id='time'> Time: 2021-08-12</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: InferenceInterfaces/Nancy_FastSpeech2.py</div><div id='m_class'> M Class Name: Nancy_FastSpeech2</div><div id='n_method'> N Class Name: Nancy_FastSpeech2</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: InferenceInterfaces/Nancy_FastSpeech2.py</div><div id='n_file'> N File Name: InferenceInterfaces/Nancy_FastSpeech2.py</div><div id='m_start'> M Start Line: 35</div><div id='m_end'> M End Line: 56</div><div id='n_start'> N Start Line: 29</div><div id='n_end'> N End Line: 29</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def forward(self, text, view=False):
        with torch.no_grad():
            phones = <a id="change">self.text2phone.string_to_tensor(text).squeeze(0).long()</a>.to(torch.device(self.device))
            mel, durations, pitch, energy = self.phone2mel(phones, speaker_embedding=self.speaker_embedding, return_duration_pitch_energy=True)
            mel = mel.transpose(0, 1)
            wave = self.mel2wav(mel)
        if view:
            from Utility.utils import cumsum_durations
            fig, ax = plt.subplots(nrows=2, ncols=1)
            ax[0].plot(wave.cpu().numpy())
            lbd.specshow(mel.cpu().numpy(),
                         ax=ax[1],
                         sr=16000,
                         cmap=&quotGnBu&quot,
                         y_axis=&quotmel&quot,
                         x_axis=None,
                         hop_length=256)
            ax[0].yaxis.set_visible(False)
            ax[1].yaxis.set_visible(False)
            duration_splits<a id="change">, label_positions</a> = cumsum_durations(durations.cpu().numpy())
            <a id="change">ax[1].set_xticks(</a>duration_splits<a id="change">, minor=True)</a>
            <a id="change">ax[1]</a>.xaxis.grid(True, which=&quotminor&quot)
            <a id="change">ax[1].set_xticks(</a>label_positions<a id="change">, minor=False)</a>
            <a id="change">ax[1].set_xticklabels(</a><a id="change">self.text2phone.get_phone_string(</a>text<a id="change">))</a>
            ax[0].set_title(text)
            plt.subplots_adjust(left=0.05, bottom=0.1, right=0.95, top=.9, wspace=0.0, hspace=0.0)
            plt.show()
        return wave</code></pre><h3>After Change</h3><pre><code class='java'>

    def forward(self, text, view=False):
        with torch.no_grad():
            phones = <a id="change">self.text2phone.string_to_tensor(</a>text<a id="change">)</a>.to(torch.device(self.device))
            mel, durations, pitch, energy = self.phone2mel(phones, speaker_embedding=self.speaker_embedding, return_duration_pitch_energy=True)
            mel = mel.transpose(0, 1)
            wave = self.mel2wav(mel)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/e4dcb8977dff5be8c8632dccd084be0ecce361a5#diff-fa9b964a51baaa8308650d43e48b873abc13a67e1583252e13d4d9c3b40614f8L33' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 24481078</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: e4dcb8977dff5be8c8632dccd084be0ecce361a5</div><div id='time'> Time: 2021-08-12</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: InferenceInterfaces/LJSpeech_FastSpeech2.py</div><div id='m_class'> M Class Name: LJSpeech_FastSpeech2</div><div id='n_method'> N Class Name: LJSpeech_FastSpeech2</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: InferenceInterfaces/LJSpeech_FastSpeech2.py</div><div id='n_file'> N File Name: InferenceInterfaces/LJSpeech_FastSpeech2.py</div><div id='m_start'> M Start Line: 35</div><div id='m_end'> M End Line: 56</div><div id='n_start'> N Start Line: 29</div><div id='n_end'> N End Line: 29</div><BR>