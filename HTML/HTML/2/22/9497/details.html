<html><h3>Pattern ID :9497
</h3><img src='33913941.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        env_fn=[partial(wrapped_env, cfg.env, cfg.env_wrapper.train, *tcp_list[i]) for i in range(env_num)],
        cfg=cfg.env_manager,
    )
    evaluate_env = ContinuousBenchmarkEnvWrapper(<a id="change">SimpleCarlaEnv(</a>cfg.env, *<a id="change">tcp_list[env_num])</a>, cfg.env_wrapper.eval)
    collector_env.seed(seed)
    evaluate_env.seed(seed)
    set_pkg_seed(seed)

    model = DDPGRLModel()
    policy = DDPGPolicy(cfg.policy, model=model)

    tb_logger = SummaryWriter(os.path.join(&quot./log/&quot, cfg.exp_name))
    learner = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger)
    collector = SampleCollector(cfg.policy.collect.collector, collector_env, policy.collect_mode, tb_logger)
    evaluator = SingleCarlaEvaluator(cfg.eval, evaluate_env, policy.eval_mode)
    replay_buffer = NaiveReplayBuffer(cfg.policy.other.replay_buffer, tb_logger)

    learner._instance_name = cfg.exp_name + &quot_&quot + time.ctime().replace(&quot &quot, &quot_&quot).replace(&quot:&quot, &quot_&quot)
    learner.call_hook(&quotbefore_run&quot)
    new_data = collector.collect(n_sample=10000, train_iter=learner.train_iter)
    replay_buffer.push(new_data, cur_collector_envstep=collector.envstep)

    while True:
        if evaluator.should_eval(learner.train_iter):
            <a id="change">results_list = </a><a id="change">[]</a>
            <a id="change">for </a><a id="change">_</a> in <a id="change">range(</a>cfg.eval.eval_num<a id="change">):
                results_list</a><a id="change">.append(evaluator.eval()</a><a id="change">)</a>
            <a id="change">success_rate</a><a id="change"> = </a><a id="change">sum(results_list</a><a id="change">) / len(results_list</a><a id="change">)</a>
            if <a id="change">success_rate &gt; cfg.eval.success_rate</a>:
                break
            <a id="change">print("Evaluate success rate: {:.2f}%".format(success_rate</a><a id="change">*100</a><a id="change">)</a><a id="change">)</a>
        &#47&#47 Sampling data from environments
        new_data = collector.collect(n_sample=3000, train_iter=learner.train_iter)
        update_per_collect = len(new_data) // 32
        replay_buffer.push(new_data, cur_collector_envstep=collector.envstep)</code></pre><h3>After Change</h3><pre><code class='java'>

    while True:
        if evaluator.should_eval(learner.train_iter):
            stop<a id="change">, rate = </a><a id="change">evaluator.eval(</a>learner.save_checkpoint, learner.train_iter, collector.envstep<a id="change">)</a>
            if stop:
                break
        &#47&#47 Sampling data from environments
        new_data = collector.collect(train_iter=learner.train_iter)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 17</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/opendilab/di-drive/commit/140d1a26bbba0681936f8f15167f38b1a06288ea#diff-db9a7c8caabf825b8915ba39182f24937b95c86894acad7f768b706233be08dcL117' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33913941</div><div id='project'> Project Name: opendilab/di-drive</div><div id='commit'> Commit Name: 140d1a26bbba0681936f8f15167f38b1a06288ea</div><div id='time'> Time: 2021-09-19</div><div id='author'> Author: sissure@qq.com</div><div id='file'> File Name: demo/simple_rl/ddpg_train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(2)</div><div id='n_method'> N Method Name: main(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: demo/simple_rl/ddpg_train.py</div><div id='n_file'> N File Name: demo/simple_rl/ddpg_train.py</div><div id='m_start'> M Start Line: 117</div><div id='m_end'> M End Line: 162</div><div id='n_start'> N Start Line: 123</div><div id='n_end'> N End Line: 170</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        env_fn=[partial(wrapped_env, cfg.env, cfg.env_wrapper.train, *tcp_list[i]) for i in range(env_num)],
        cfg=cfg.env_manager,
    )
    evaluate_env = ContinuousBenchmarkEnvWrapper(<a id="change">SimpleCarlaEnv(</a>cfg.env, *<a id="change">tcp_list[env_num])</a>, cfg.env_wrapper.eval)
    collector_env.seed(seed)
    evaluate_env.seed(seed)
    set_pkg_seed(seed)

    model = SACRLModel(**cfg.policy.model)
    policy = SACPolicy(cfg.policy, model=model)

    tb_logger = SummaryWriter(os.path.join(&quot./log/&quot, cfg.exp_name))
    learner = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger)
    collector = SampleCollector(cfg.policy.collect.collector, collector_env, policy.collect_mode, tb_logger)
    evaluator = SingleCarlaEvaluator(cfg.eval, evaluate_env, policy.eval_mode)
    replay_buffer = NaiveReplayBuffer(cfg.policy.other.replay_buffer, tb_logger)

    learner._instance_name = cfg.exp_name + &quot_&quot + time.ctime().replace(&quot &quot, &quot_&quot).replace(&quot:&quot, &quot_&quot)
    learner.call_hook(&quotbefore_run&quot)

    new_data = collector.collect(n_sample=10000, train_iter=learner.train_iter)
    replay_buffer.push(new_data, cur_collector_envstep=collector.envstep)

    while True:
        if evaluator.should_eval(learner.train_iter):
            <a id="change">results_list = </a><a id="change">[]</a>
            <a id="change">for </a><a id="change">_</a> in <a id="change">range(</a>cfg.eval.eval_num<a id="change">):
                </a><a id="change">results_list.append(evaluator.eval()</a><a id="change">)</a>
            <a id="change">success_rate</a><a id="change"> = </a><a id="change">sum(</a>results_list<a id="change">) / len(</a>results_list<a id="change">)</a>
            if <a id="change">success_rate &gt; cfg.eval.success_rate</a>:
                break
            <a id="change">print("Evaluate success rate: {:.2f}%".format(</a>success_rate<a id="change">*100</a><a id="change">)</a><a id="change">)</a>
        &#47&#47 Sampling data from environments
        new_data = collector.collect(n_sample=3000, train_iter=learner.train_iter)
        update_per_collect = len(new_data) // 32
        replay_buffer.push(new_data, cur_collector_envstep=collector.envstep)</code></pre><h3>After Change</h3><pre><code class='java'>

    while True:
        if evaluator.should_eval(learner.train_iter):
            stop<a id="change">, rate = </a><a id="change">evaluator.eval(</a>learner.save_checkpoint, learner.train_iter, collector.envstep<a id="change">)</a>
            if stop:
                break
        &#47&#47 Sampling data from environments
        new_data = collector.collect(train_iter=learner.train_iter)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/opendilab/di-drive/commit/140d1a26bbba0681936f8f15167f38b1a06288ea#diff-25e88a1417c8839f312f23a0252c366ee1e751c44bcf835801e2521c60c71f6cL121' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33913943</div><div id='project'> Project Name: opendilab/di-drive</div><div id='commit'> Commit Name: 140d1a26bbba0681936f8f15167f38b1a06288ea</div><div id='time'> Time: 2021-09-19</div><div id='author'> Author: sissure@qq.com</div><div id='file'> File Name: demo/simple_rl/sac_train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(2)</div><div id='n_method'> N Method Name: main(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: demo/simple_rl/sac_train.py</div><div id='n_file'> N File Name: demo/simple_rl/sac_train.py</div><div id='m_start'> M Start Line: 122</div><div id='m_end'> M End Line: 168</div><div id='n_start'> N Start Line: 129</div><div id='n_end'> N End Line: 177</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        env_fn=[partial(wrapped_env, cfg.env, cfg.env_wrapper.train, *tcp_list[i]) for i in range(env_num)],
        cfg=cfg.env_manager,
    )
    evaluate_env = ContinuousBenchmarkEnvWrapper(<a id="change">SimpleCarlaEnv(</a>cfg.env, *<a id="change">tcp_list[env_num])</a>, cfg.env_wrapper.eval)
    collector_env.seed(seed)
    evaluate_env.seed(seed)
    set_pkg_seed(seed)

    model = TD3RLModel()
    policy = TD3Policy(cfg.policy, model=model)

    tb_logger = SummaryWriter(os.path.join(&quot./log/&quot, cfg.exp_name))
    learner = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger)
    collector = SampleCollector(cfg.policy.collect.collector, collector_env, policy.collect_mode, tb_logger)
    evaluator = SingleCarlaEvaluator(cfg.eval, evaluate_env, policy.eval_mode)
    replay_buffer = NaiveReplayBuffer(cfg.policy.other.replay_buffer, tb_logger)

    learner._instance_name = cfg.exp_name + &quot_&quot + time.ctime().replace(&quot &quot, &quot_&quot).replace(&quot:&quot, &quot_&quot)
    learner.call_hook(&quotbefore_run&quot)

    new_data = collector.collect(n_sample=10000, train_iter=learner.train_iter)
    replay_buffer.push(new_data, cur_collector_envstep=collector.envstep)

    while True:
        if evaluator.should_eval(learner.train_iter):
            <a id="change">results_list = </a><a id="change">[]</a>
            <a id="change">for </a><a id="change">_</a> in <a id="change">range(</a>cfg.eval.eval_num<a id="change">):
                </a><a id="change">results_list.append(evaluator.eval()</a><a id="change">)</a>
            <a id="change">success_rate</a><a id="change"> = </a><a id="change">sum(</a>results_list<a id="change">) / len(</a>results_list<a id="change">)</a>
            if <a id="change">success_rate &gt; cfg.eval.success_rate</a>:
                break
            <a id="change">print("Evaluate success rate: {:.2f}%".format(</a>success_rate<a id="change">*100</a><a id="change">)</a><a id="change">)</a>
        &#47&#47 Sampling data from environments
        new_data = collector.collect(n_sample=3000, train_iter=learner.train_iter)
        update_per_collect = len(new_data) // 32
        replay_buffer.push(new_data, cur_collector_envstep=collector.envstep)</code></pre><h3>After Change</h3><pre><code class='java'>

    while True:
        if evaluator.should_eval(learner.train_iter):
            stop<a id="change">, rate = </a><a id="change">evaluator.eval(</a>learner.save_checkpoint, learner.train_iter, collector.envstep<a id="change">)</a>
            if stop:
                break
        &#47&#47 Sampling data from environments
        new_data = collector.collect(train_iter=learner.train_iter)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/opendilab/di-drive/commit/140d1a26bbba0681936f8f15167f38b1a06288ea#diff-deaba84a0f039a1bf09e142ddfa189fbc340e05faaf37e85f7e1e3744e9960e7L115' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33913942</div><div id='project'> Project Name: opendilab/di-drive</div><div id='commit'> Commit Name: 140d1a26bbba0681936f8f15167f38b1a06288ea</div><div id='time'> Time: 2021-09-19</div><div id='author'> Author: sissure@qq.com</div><div id='file'> File Name: demo/simple_rl/td3_train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(2)</div><div id='n_method'> N Method Name: main(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: demo/simple_rl/td3_train.py</div><div id='n_file'> N File Name: demo/simple_rl/td3_train.py</div><div id='m_start'> M Start Line: 116</div><div id='m_end'> M End Line: 162</div><div id='n_start'> N Start Line: 121</div><div id='n_end'> N End Line: 169</div><BR>