<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    loss_Dr1 = r1_penalty * (self.r1_gamma / 2)
                    loss_numpy[&quotloss_Dr1&quot] = loss_Dr1.cpu().detach().numpy().mean()

                loss4 = <a id="change">(loss_Dreal + loss_Dr1).mean()</a> * float(gain)
                &#47&#47 if do_Dmain:
                &#47&#47     loss4 += loss3
            with torch.autograd.profiler.record_function(name + &quot_backward&quot):</code></pre><h3>After Change</h3><pre><code class='java'>
                &#47&#47     loss4 += loss3
            with torch.autograd.profiler.record_function(name + &quot_backward&quot):
                &#47&#47 loss4.backward()  &#47&#47 咩酱：gain即上文提到的这个阶段的训练间隔。
                <a id="change">(real_logits * 0 + loss_Dreal + loss_Dr1).mean().mul(gain).backward()</a>
                if self.align_grad:
                    mapping = self.mapping.module if self.is_distributed else self.mapping
                    synthesis = self.synthesis.module if self.is_distributed else self.synthesis
                    discriminator = self.discriminator.module if self.is_distributed else self.discriminator</code></pre>