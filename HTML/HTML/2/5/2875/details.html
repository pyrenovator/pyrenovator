<html><h3>Pattern ID :2875
</h3><img src='11318140.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        output_not_checkpointed = out.data.clone()
        grad_not_checkpointed = {}
        for name, param in model.named_parameters():
            grad_not_checkpointed[name]<a id="change"> = </a><a id="change">param.grad.data.clone()</a>

        model.enable_gradient_checkpointing()
        out = model(**inputs_dict).sample
        &#47&#47 run the backwards pass on the model. For backwards pass, for simplicity purpose,</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 run the backwards pass on the model. For backwards pass, for simplicity purpose,
        &#47&#47 we won&quott calculate the loss and rather backprop on out.sum()
        model_2.zero_grad()
        loss_2<a id="change"> = </a><a id="change">(out_2 - labels).mean()</a>
        loss_2.backward()

        &#47&#47 compare the output and parameters gradients
        self.assertTrue((loss - loss_2).abs() &lt; 1e-5)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/huggingface/diffusers/commit/22963ed82682465b5fdfd1bd474e1b0f2579b4db#diff-e65cd8679b47855997f7a1e7b0bc1db8231acd2168d2cb6616d03d1fc201c1d7L273' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 11318140</div><div id='project'> Project Name: huggingface/diffusers</div><div id='commit'> Commit Name: 22963ed82682465b5fdfd1bd474e1b0f2579b4db</div><div id='time'> Time: 2022-10-10</div><div id='author'> Author: patrick.v.platen@gmail.com</div><div id='file'> File Name: tests/test_models_unet.py</div><div id='m_class'> M Class Name: UNet2DConditionModelTests</div><div id='n_method'> N Class Name: UNet2DConditionModelTests</div><div id='m_method'> M Method Name: test_gradient_checkpointing(1)</div><div id='n_method'> N Method Name: test_gradient_checkpointing(1)</div><div id='m_parent_class'> M Parent Class: unittest.TestCase,ModelTesterMixin</div><div id='n_parent_class'> N Parent Class: unittest.TestCase,ModelTesterMixin</div><div id='m_file'> M File Name: tests/test_models_unet.py</div><div id='n_file'> N File Name: tests/test_models_unet.py</div><div id='m_start'> M Start Line: 273</div><div id='m_end'> M End Line: 333</div><div id='n_start'> N Start Line: 273</div><div id='n_end'> N End Line: 308</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

            &#47&#47 resample the data
            if (n % self.resample.resample_every == 0) or (n == nepoch-1):
                if <a id="change">self.resample.resample_from_last</a>:
                    pos<a id="change"> = </a><a id="change">pos.clone()</a>.detach().to(self.device)
                else:
                    pos = None
                pos = self.sample(</code></pre><h3>After Change</h3><pre><code class='java'>

                    self.opt.step(lpos)
                    eloc = self.opt.eloc
                    cumulative_loss<a id="change"> += </a><a id="change">torch.mean(</a>eloc<a id="change">)</a>

            if cumulative_loss &lt; min_loss:
                min_loss = self.save_checkpoint(
                    n, cumulative_loss, self.save_model)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/nlesc-jcer/qmctorch/commit/9f40f526749f6a91afacd7fa260c5e0c7e934715#diff-350136797dbb04b53442e1c61df2f76bb9eb26ead557b886172f70d8f487df6dL81' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 11318136</div><div id='project'> Project Name: nlesc-jcer/qmctorch</div><div id='commit'> Commit Name: 9f40f526749f6a91afacd7fa260c5e0c7e934715</div><div id='time'> Time: 2020-02-07</div><div id='author'> Author: nicolas.gm.renaud@gmail.com</div><div id='file'> File Name: deepqmc/solver/solver_orbital.py</div><div id='m_class'> M Class Name: SolverOrbital</div><div id='n_method'> N Class Name: SolverOrbital</div><div id='m_method'> M Method Name: run(5)</div><div id='n_method'> N Method Name: run(5)</div><div id='m_parent_class'> M Parent Class: SolverBase</div><div id='n_parent_class'> N Parent Class: SolverBase</div><div id='m_file'> M File Name: deepqmc/solver/solver_orbital.py</div><div id='n_file'> N File Name: deepqmc/solver/solver_orbital.py</div><div id='m_start'> M Start Line: 94</div><div id='m_end'> M End Line: 182</div><div id='n_start'> N Start Line: 94</div><div id='n_end'> N End Line: 179</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        segment_ids = segment_ids.to(args.device)

        with torch.no_grad():
            if <a id="change">args.mlm</a>:
                outputs = model(inputs, masked_lm_labels=labels, position_ids=position_ids, token_type_ids=segment_ids)
            else:
                if args.model_type == &quotbart&quot:
                    decoder_input_ids = labels[:, :-1].contiguous()
                    decoder_input_ids[decoder_input_ids == args.mlm_ignore_index] = tokenizer.pad_token_id
                    lm_labels<a id="change"> = </a><a id="change">labels[:, 1:].clone()</a>
                    outputs = model(inputs, labels=labels, lm_labels=lm_labels, decoder_input_ids=decoder_input_ids, position_ids=position_ids, token_type_ids=segment_ids)
                else:
                    outputs = model(inputs, labels=labels, position_ids=position_ids, token_type_ids=segment_ids)
            lm_loss = outputs[0]</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 Same behavior as modeling_bart.py, besides ignoring pad_token_id
            ce_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=args.mlm_ignore_index)
            loss = ce_loss_fct(lm_logits.view(-1, lm_logits.shape[-1]), labels.view(-1))
            eval_loss<a id="change"> += </a><a id="change">loss.mean()</a>.item()
            
        nb_eval_steps += 1
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/stanford-oval/genienlp/commit/b84a6548a69fd9f62652eed1c74fd4b1fdb8b65b#diff-90e760d3065758cd3fed35d36be801cfed8d2418d888df295dbecdc933432ad7L293' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 11318139</div><div id='project'> Project Name: stanford-oval/genienlp</div><div id='commit'> Commit Name: b84a6548a69fd9f62652eed1c74fd4b1fdb8b65b</div><div id='time'> Time: 2020-11-15</div><div id='author'> Author: mehrad@stanford.edu</div><div id='file'> File Name: genienlp/paraphrase/run_lm_finetuning.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: evaluate(5)</div><div id='n_method'> N Method Name: evaluate(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: genienlp/paraphrase/run_lm_finetuning.py</div><div id='n_file'> N File Name: genienlp/paraphrase/run_lm_finetuning.py</div><div id='m_start'> M Start Line: 318</div><div id='m_end'> M End Line: 344</div><div id='n_start'> N Start Line: 330</div><div id='n_end'> N End Line: 369</div><BR>