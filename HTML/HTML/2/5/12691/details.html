<html><h3>Pattern ID :12691
</h3><img src='43012852.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        backpointers = -99 * torch.ones([batch_size, U_max, fb_max_length])

        &#47&#47 for cropping v_matrix later
        phn_len_mask<a id="change"> = </a><a id="change">torch.arange(</a>U_max<a id="change">)</a>[None, :].to(device) &lt; phn_lens_abs[
            :, None
        ].to(device)

        &#47&#47 initialise
        v_matrix[:, :, 0] = pi_prob + emiss_pred_useful[:, :, 0]

        for t in range(2, fb_max_length + 1):  &#47&#47 note: t here is 1+ indexing
            x, argmax = batch_log_maxvecmul(
                trans_prob.permute(0, 2, 1), v_matrix[:, :, t - 2]
            )
            v_matrix[:, :, t - 1] = x + emiss_pred_useful[:, :, t - 1]

            &#47&#47 crop v_matrix
            v_matrix<a id="change"> = </a>torch.where(
                phn_len_mask[:, :, None],
                v_matrix,
                torch.tensor(-999999.0).to(device),</code></pre><h3>After Change</h3><pre><code class='java'>
            x, argmax = batch_log_maxvecmul(
                trans_prob.permute(0, 2, 1), v_matrix[:, :, t - 1]
            )
            <a id="change">v_matrix[:, :, t]</a> = x + emiss_pred_useful[:, :, t]

            backpointers[:, :, t] = argmax.type(torch.FloatTensor)
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/speechbrain/speechbrain/commit/49516fde8bc485f4beac1ea8452e6c8e615a6e6f#diff-5998bf9afa99b8b276f0e74798ed1f028f51712bbe36e0601d8362e6909aeb9cL708' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43012852</div><div id='project'> Project Name: speechbrain/speechbrain</div><div id='commit'> Commit Name: 49516fde8bc485f4beac1ea8452e6c8e615a6e6f</div><div id='time'> Time: 2020-07-10</div><div id='author'> Author: elenaras@yahoo.co.uk</div><div id='file'> File Name: speechbrain/alignment/aligner.py</div><div id='m_class'> M Class Name: HMMAligner</div><div id='n_method'> N Class Name: HMMAligner</div><div id='m_method'> M Method Name: _dp_viterbi(8)</div><div id='n_method'> N Method Name: _dp_viterbi(8)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: speechbrain/alignment/aligner.py</div><div id='n_file'> N File Name: speechbrain/alignment/aligner.py</div><div id='m_start'> M Start Line: 715</div><div id='m_end'> M End Line: 748</div><div id='n_start'> N Start Line: 708</div><div id='n_end'> N End Line: 728</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        seq_length = input_shape[1]
        device = input_ids.device if input_ids is not None else inputs_embeds.device
        if position_ids is None:
            position_ids = <a id="change">torch.arange(</a>seq_length<a id="change">, dtype=torch.long, device=device)</a>
            position_ids<a id="change"> = </a>position_ids.unsqueeze(0).expand(input_shape)
        if token_type_ids is None:
            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)

        if entity_masking is not None and mask_entities:
            input_ids = input_ids * ~(entity_masking.max(-1)[0].bool())

        if inputs_embeds is None:
            inputs_embeds = self.word_embeddings(input_ids)
        position_embeddings = self.position_embeddings(position_ids)
        token_type_embeddings = self.token_type_embeddings(token_type_ids)

        embeddings<a id="change"> = </a>inputs_embeds + position_embeddings + token_type_embeddings
        
        if self.num_db_types &gt; 0 and entity_ids is not None:
            &#47&#47 average embedding of different types</code></pre><h3>After Change</h3><pre><code class='java'>
        seq_length = input_shape[1]
        
        if position_ids is None:
            position_ids = <a id="change">self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]</a>
            
        if token_type_ids is None:
            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/stanford-oval/genienlp/commit/ff10f0db1ebabf529f14812c255f318cd8e774eb#diff-8a8209748f984504f04bf50ecfa2456a7a28d0c81f759d1d0eaf648285bf7fa4L356' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43012834</div><div id='project'> Project Name: stanford-oval/genienlp</div><div id='commit'> Commit Name: ff10f0db1ebabf529f14812c255f318cd8e774eb</div><div id='time'> Time: 2021-01-12</div><div id='author'> Author: mehrad@stanford.edu</div><div id='file'> File Name: genienlp/paraphrase/transformers_utils.py</div><div id='m_class'> M Class Name: BertEmbeddingsForNER</div><div id='n_method'> N Class Name: BertEmbeddingsForNER</div><div id='m_method'> M Method Name: forward(10)</div><div id='n_method'> N Method Name: forward(9)</div><div id='m_parent_class'> M Parent Class: BertEmbeddings</div><div id='n_parent_class'> N Parent Class: BertEmbeddings</div><div id='m_file'> M File Name: genienlp/paraphrase/transformers_utils.py</div><div id='n_file'> N File Name: genienlp/paraphrase/transformers_utils.py</div><div id='m_start'> M Start Line: 357</div><div id='m_end'> M End Line: 379</div><div id='n_start'> N Start Line: 357</div><div id='n_end'> N End Line: 386</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        d_2 = d // 2

        &#47&#47 Create position indexes `[0, 1, ..., seq_len - 1]`
        seq_idx = <a id="change">torch.arange(</a>seq_len<a id="change">, device=x.device)</a>.type_as(self.theta)

        &#47&#47 Calculate the product of position index and $\theta_i$
        idx_theta = torch.einsum(&quotn,d-&gt;nd&quot, seq_idx, self.theta)

        &#47&#47 Concatenate so that for row $m$ we have
        &#47&#47 $[m \theta_0, m \theta_1, ..., m \theta_{\frac{d}{2}}, m \theta_0, m \theta_1, ..., m \theta_{\frac{d}{2}}]$
        idx_theta2<a id="change"> = </a>torch.cat([idx_theta, idx_theta], dim=1)

        &#47&#47 Calculate $[-x^{(\frac{d}{2} + 1)}, -x^{(\frac{d}{2} + 2)}, ..., -x^{(d)}, x^{(1)}, x^{(2)}, ..., -x^{(\frac{d}{2})}]$
        neg_half_x = torch.cat([-x[:, :, :, d_2:], x[:, :, :, :d_2]], dim=-1)

        &#47&#47 Calculate
        &#47&#47
        &#47&#47 \begin{align}
        &#47&#47 \begin{pmatrix}
        &#47&#47 x^{(i)}_m \cos m \theta_i - x^{(i + \frac{d}{2})}_m \sin m \theta_i \\
        &#47&#47 x^{(i + \frac{d}{2})}_m \cos m\theta_i + x^{(i)}_m \sin m \theta_i \\
        &#47&#47 \end{pmatrix} \\
        &#47&#47 \end{align}
        &#47&#47
        &#47&#47 for $i \in {1, 2, ..., \frac{d}{2}}$
        rx<a id="change"> = </a>(x * idx_theta2.cos()[:, None, None, :]) + (neg_half_x * idx_theta2.sin()[:, None, None, :])

        &#47&#47
        return rx</code></pre><h3>After Change</h3><pre><code class='java'>
        self._build_cache(x)

        &#47&#47 Split the features, we can choose to apply rotary embeddings only to a partial set of features.
        x_rope, x_pass = <a id="change">x[..., :self.d]</a>, x[..., self.d:]

        &#47&#47 Calculate
        &#47&#47 $[-x^{(\frac{d}{2} + 1)}, -x^{(\frac{d}{2} + 2)}, ..., -x^{(d)}, x^{(1)}, x^{(2)}, ..., x^{(\frac{d}{2})}]$</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lab-ml/nn/commit/0ce65adf9e602321109528b05cf99fccb16cd2de#diff-637497eb531dcbf4c4fff534c48c9dc4ec23ef7b14699edc95d8b280b04de206L127' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43012859</div><div id='project'> Project Name: lab-ml/nn</div><div id='commit'> Commit Name: 0ce65adf9e602321109528b05cf99fccb16cd2de</div><div id='time'> Time: 2022-06-03</div><div id='author'> Author: vpjayasiri@gmail.com</div><div id='file'> File Name: labml_nn/transformers/rope/__init__.py</div><div id='m_class'> M Class Name: RotaryPositionalEmbeddings</div><div id='n_method'> N Class Name: RotaryPositionalEmbeddings</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: labml_nn/transformers/rope/__init__.py</div><div id='n_file'> N File Name: labml_nn/transformers/rope/__init__.py</div><div id='m_start'> M Start Line: 132</div><div id='m_end'> M End Line: 163</div><div id='n_start'> N Start Line: 171</div><div id='n_end'> N End Line: 193</div><BR>