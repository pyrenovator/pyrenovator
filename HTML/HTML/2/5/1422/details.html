<html><h3>Pattern ID :1422
</h3><img src='6598501.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>


def reduce_mean(inputs, axis=None, keepdims=False, name=None):
    return <a id="change">Lambda(partial(torch.mean, dim=axis, keepdim=keepdims), name=name)(</a>inputs<a id="change">)</a>


def reduce_sum(inputs, axis=None, keepdims=False, name=None):
    return Lambda(partial(torch.sum, dim=axis, keepdim=keepdims), name=name)(inputs)</code></pre><h3>After Change</h3><pre><code class='java'>


def reduce_mean(inputs, axis=None, keepdims=False, name=None):
    return <a id="change">wrapper(partial(</a>torch.mean<a id="change">, dim=axis, keepdim=keepdims)</a>, inputs<a id="change">, name=name)</a>


def reduce_sum(inputs, axis=None, keepdims=False, name=None):
    if isinstance(inputs, (list, tuple)) and axis == 0:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 15</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/d562ca7add8b907797fd0d69903f7eb0bbf2340c#diff-202c6b6c90bab0fd91b38fd5b79ec62e9cadc7068bf6da9cb932c52d9f506525L136' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6598501</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: d562ca7add8b907797fd0d69903f7eb0bbf2340c</div><div id='time'> Time: 2023-02-09</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: reduce_mean(4)</div><div id='n_method'> N Method Name: reduce_mean(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='n_file'> N File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_start'> M Start Line: 136</div><div id='m_end'> M End Line: 136</div><div id='n_start'> N Start Line: 146</div><div id='n_end'> N End Line: 146</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>


def clip_by_value(inputs, clip_value_min, clip_value_max, name=None):
    return <a id="change">Lambda(partial(torch.clip, min=clip_value_min, max=clip_value_max), name=name)(</a>inputs<a id="change">)</a>


def concat(inputs, axis, name=None):
    return Concatenate(axis=axis, name=name)(inputs)</code></pre><h3>After Change</h3><pre><code class='java'>


def clip_by_value(inputs, clip_value_min, clip_value_max, name=None):
    return <a id="change">wrapper(partial(</a>torch.clip<a id="change">, min=clip_value_min, max=clip_value_max)</a>, inputs<a id="change">, name=name)</a>


def concat(inputs, axis, name=None):
    return Concatenate(axis=axis, name=name)(inputs)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/d562ca7add8b907797fd0d69903f7eb0bbf2340c#diff-202c6b6c90bab0fd91b38fd5b79ec62e9cadc7068bf6da9cb932c52d9f506525L19' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6598529</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: d562ca7add8b907797fd0d69903f7eb0bbf2340c</div><div id='time'> Time: 2023-02-09</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: clip_by_value(4)</div><div id='n_method'> N Method Name: clip_by_value(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='n_file'> N File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_start'> M Start Line: 20</div><div id='m_end'> M End Line: 20</div><div id='n_start'> N Start Line: 23</div><div id='n_end'> N End Line: 23</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>


def transpose(inputs, perm=None, conjugate=False, name=None):
    return <a id="change">Lambda(partial(torch.permute, dims=perm), name=name)(</a>inputs<a id="change">)</a>


def unstack(inputs, axis, name=None):
    axis = len(inputs.shape) + axis if axis &lt; 0 else axis</code></pre><h3>After Change</h3><pre><code class='java'>


def transpose(inputs, perm=None, conjugate=False, name=None):
    return <a id="change">wrapper(partial(</a>torch.permute<a id="change">, dims=perm)</a>, inputs<a id="change">, name=name)</a>


def unstack(inputs, axis, name=None):
    axis = len(inputs.shape) + axis if axis &lt; 0 else axis</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/d562ca7add8b907797fd0d69903f7eb0bbf2340c#diff-202c6b6c90bab0fd91b38fd5b79ec62e9cadc7068bf6da9cb932c52d9f506525L235' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6598510</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: d562ca7add8b907797fd0d69903f7eb0bbf2340c</div><div id='time'> Time: 2023-02-09</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: transpose(4)</div><div id='n_method'> N Method Name: transpose(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='n_file'> N File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_start'> M Start Line: 236</div><div id='m_end'> M End Line: 236</div><div id='n_start'> N Start Line: 249</div><div id='n_end'> N End Line: 249</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>


def gelu(inputs, approximate=False, name=None):
    return <a id="change">Lambda(partial(F.gelu, approximate="tanh" if approximate else "none"), name=name)(</a>inputs<a id="change">)</a>


def l2_normalize(inputs, axis=None, epsilon=1e-12, name=None):
    return Lambda(partial(F.normalize, p=2.0, dim=axis, eps=epsilon), name=name)(inputs)</code></pre><h3>After Change</h3><pre><code class='java'>


def gelu(inputs, approximate=False, name=None):
    return <a id="change">wrapper(partial(</a>F.gelu<a id="change">, approximate="tanh" if approximate else "none")</a>, inputs<a id="change">, name=name)</a>


def l2_normalize(inputs, axis=None, epsilon=1e-12, name=None):
    return wrapper(partial(F.normalize, p=2.0, dim=axis, eps=epsilon), inputs, name=name)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/d562ca7add8b907797fd0d69903f7eb0bbf2340c#diff-202c6b6c90bab0fd91b38fd5b79ec62e9cadc7068bf6da9cb932c52d9f506525L75' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6598511</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: d562ca7add8b907797fd0d69903f7eb0bbf2340c</div><div id='time'> Time: 2023-02-09</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: gelu(3)</div><div id='n_method'> N Method Name: gelu(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='n_file'> N File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_start'> M Start Line: 76</div><div id='m_end'> M End Line: 76</div><div id='n_start'> N Start Line: 79</div><div id='n_end'> N End Line: 79</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>


def log(log, name=None):
    return <a id="change">Lambda(partial(torch.log), name=name)(</a>inputs<a id="change">)</a>


def matmul(xx, yy, name=None):
    return Lambda(lambda inputs: torch.matmul(inputs[0], inputs[1]), name=name)([xx, yy])</code></pre><h3>After Change</h3><pre><code class='java'>


def log(log, name=None):
    return <a id="change">wrapper(partial(</a>torch.log<a id="change">)</a>, inputs<a id="change">, name=name)</a>


def matmul(xx, yy, name=None):
    return wrapper(lambda inputs: torch.matmul(inputs[0], inputs[1]), [xx, yy], name=name)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/d562ca7add8b907797fd0d69903f7eb0bbf2340c#diff-202c6b6c90bab0fd91b38fd5b79ec62e9cadc7068bf6da9cb932c52d9f506525L88' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6598504</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: d562ca7add8b907797fd0d69903f7eb0bbf2340c</div><div id='time'> Time: 2023-02-09</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: log(2)</div><div id='n_method'> N Method Name: log(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='n_file'> N File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_start'> M Start Line: 89</div><div id='m_end'> M End Line: 89</div><div id='n_start'> N Start Line: 92</div><div id='n_end'> N End Line: 92</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>


def moments(inputs, axes, shift=None, keepdims=False, name=None):
    return <a id="change">Lambda(partial(torch.var_mean, dim=axes, keepdim=keepdims), name=name)(</a>inputs<a id="change">)</a>


def maximum(xx, yy, name=None):
    return Lambda(lambda inputs: torch.maximum(inputs[0], inputs[1]), name=name)([xx, yy])</code></pre><h3>After Change</h3><pre><code class='java'>


def moments(inputs, axes, shift=None, keepdims=False, name=None):
    return <a id="change">wrapper(partial(</a>torch.var_mean<a id="change">, dim=axes, keepdim=keepdims)</a>, inputs<a id="change">, name=name)</a>


def maximum(xx, yy, name=None):
    if isinstance(yy, torch.Tensor):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/d562ca7add8b907797fd0d69903f7eb0bbf2340c#diff-202c6b6c90bab0fd91b38fd5b79ec62e9cadc7068bf6da9cb932c52d9f506525L96' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6598505</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: d562ca7add8b907797fd0d69903f7eb0bbf2340c</div><div id='time'> Time: 2023-02-09</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: moments(5)</div><div id='n_method'> N Method Name: moments(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='n_file'> N File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_start'> M Start Line: 97</div><div id='m_end'> M End Line: 97</div><div id='n_start'> N Start Line: 100</div><div id='n_end'> N End Line: 100</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>


def reduce_sum(inputs, axis=None, keepdims=False, name=None):
    return <a id="change">Lambda(partial(torch.sum, dim=axis, keepdim=keepdims), name=name)(</a>inputs<a id="change">)</a>


def relu(inputs, name=None):
    return Lambda(F.relu, name=name)(inputs)</code></pre><h3>After Change</h3><pre><code class='java'>
    if isinstance(inputs, (list, tuple)) and axis == 0:
        return Add(name=name)(inputs)
    else:
        return <a id="change">wrapper(partial(</a>torch.sum<a id="change">, dim=axis, keepdim=keepdims)</a>, inputs<a id="change">, name=name)</a>


def relu(inputs, name=None):
    return wrapper(F.relu, inputs, name=name)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/d562ca7add8b907797fd0d69903f7eb0bbf2340c#diff-202c6b6c90bab0fd91b38fd5b79ec62e9cadc7068bf6da9cb932c52d9f506525L139' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6598507</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: d562ca7add8b907797fd0d69903f7eb0bbf2340c</div><div id='time'> Time: 2023-02-09</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: reduce_sum(4)</div><div id='n_method'> N Method Name: reduce_sum(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='n_file'> N File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_start'> M Start Line: 140</div><div id='m_end'> M End Line: 140</div><div id='n_start'> N Start Line: 150</div><div id='n_end'> N End Line: 155</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>


def squeeze(inputs, axis, name=None):
    return <a id="change">Lambda(partial(torch.squeeze, dim=axis), name=name)(</a>inputs<a id="change">)</a>


def tanh(inputs, name=None):
    return Lambda(F.tanh, name=name)(inputs)</code></pre><h3>After Change</h3><pre><code class='java'>


def squeeze(inputs, axis, name=None):
    return <a id="change">wrapper(partial(</a>torch.squeeze<a id="change">, dim=axis)</a>, inputs<a id="change">, name=name)</a>


def tanh(inputs, name=None):
    return wrapper(F.tanh, inputs, name=name)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/d562ca7add8b907797fd0d69903f7eb0bbf2340c#diff-202c6b6c90bab0fd91b38fd5b79ec62e9cadc7068bf6da9cb932c52d9f506525L227' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6598517</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: d562ca7add8b907797fd0d69903f7eb0bbf2340c</div><div id='time'> Time: 2023-02-09</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: squeeze(3)</div><div id='n_method'> N Method Name: squeeze(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='n_file'> N File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_start'> M Start Line: 228</div><div id='m_end'> M End Line: 228</div><div id='n_start'> N Start Line: 241</div><div id='n_end'> N End Line: 241</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>


def norm(inputs, ord="euclidean", axis=1, keepdims=False, name=None):
    return <a id="change">Lambda(partial(torch.norm, p=2, dim=axis, keepdim=keepdims), name=name)(</a>inputs<a id="change">)</a>


def pad(inputs, paddings, mode="CONSTANT", constant_values=0, name=None):
    </code></pre><h3>After Change</h3><pre><code class='java'>
    return wrapper(func, [xx, yy], name=name)

def norm(inputs, ord="euclidean", axis=1, keepdims=False, name=None):
    return <a id="change">wrapper(partial(</a>torch.norm<a id="change">, p=2, dim=axis, keepdim=keepdims)</a>, inputs<a id="change">, name=name)</a>


def pad(inputs, paddings, mode="CONSTANT", constant_values=0, name=None):
    </code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/d562ca7add8b907797fd0d69903f7eb0bbf2340c#diff-202c6b6c90bab0fd91b38fd5b79ec62e9cadc7068bf6da9cb932c52d9f506525L108' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6598519</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: d562ca7add8b907797fd0d69903f7eb0bbf2340c</div><div id='time'> Time: 2023-02-09</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: norm(5)</div><div id='n_method'> N Method Name: norm(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='n_file'> N File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_start'> M Start Line: 109</div><div id='m_end'> M End Line: 109</div><div id='n_start'> N Start Line: 119</div><div id='n_end'> N End Line: 119</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>


def l2_normalize(inputs, axis=None, epsilon=1e-12, name=None):
    return <a id="change">Lambda(partial(F.normalize, p=2.0, dim=axis, eps=epsilon), name=name)(</a>inputs<a id="change">)</a>


def linspace(start, stop, num, name=None, axis=0):
    &#47&#47 return Lambda(partial(torch.linspace, start=start, end=stop, steps=num), name=name)(inputs)</code></pre><h3>After Change</h3><pre><code class='java'>


def l2_normalize(inputs, axis=None, epsilon=1e-12, name=None):
    return <a id="change">wrapper(partial(</a>F.normalize<a id="change">, p=2.0, dim=axis, eps=epsilon)</a>, inputs<a id="change">, name=name)</a>


def linspace(start, stop, num, name=None, axis=0):
    &#47&#47 return Lambda(partial(torch.linspace, start=start, end=stop, steps=num), name=name)(inputs)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/d562ca7add8b907797fd0d69903f7eb0bbf2340c#diff-202c6b6c90bab0fd91b38fd5b79ec62e9cadc7068bf6da9cb932c52d9f506525L79' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6598513</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: d562ca7add8b907797fd0d69903f7eb0bbf2340c</div><div id='time'> Time: 2023-02-09</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: l2_normalize(4)</div><div id='n_method'> N Method Name: l2_normalize(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='n_file'> N File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_start'> M Start Line: 80</div><div id='m_end'> M End Line: 80</div><div id='n_start'> N Start Line: 83</div><div id='n_end'> N End Line: 83</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>


def softmax(inputs, axis=None, name=None):
    return <a id="change">Lambda(partial(F.softmax, dim=axis), name=name)(</a>inputs<a id="change">)</a>


def softplus(inputs, name=None):
    return Lambda(F.softplus, name=name)(inputs)</code></pre><h3>After Change</h3><pre><code class='java'>


def softmax(inputs, axis=None, name=None):
    return <a id="change">wrapper(partial(</a>torch.softmax<a id="change">, dim=axis)</a>, inputs<a id="change">, name=name)</a>


def softplus(inputs, name=None):
    return wrapper(F.softplus, inputs, name=name)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/d562ca7add8b907797fd0d69903f7eb0bbf2340c#diff-202c6b6c90bab0fd91b38fd5b79ec62e9cadc7068bf6da9cb932c52d9f506525L191' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6598515</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: d562ca7add8b907797fd0d69903f7eb0bbf2340c</div><div id='time'> Time: 2023-02-09</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: softmax(3)</div><div id='n_method'> N Method Name: softmax(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='n_file'> N File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_start'> M Start Line: 192</div><div id='m_end'> M End Line: 192</div><div id='n_start'> N Start Line: 205</div><div id='n_end'> N End Line: 205</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>


def reduce_max(inputs, axis=None, keepdims=False, name=None):
    return <a id="change">Lambda(partial(torch.max, dim=axis, keepdim=keepdims), name=name)(</a>inputs<a id="change">)</a>


def reduce_mean(inputs, axis=None, keepdims=False, name=None):
    return Lambda(partial(torch.mean, dim=axis, keepdim=keepdims), name=name)(inputs)</code></pre><h3>After Change</h3><pre><code class='java'>


def reduce_max(inputs, axis=None, keepdims=False, name=None):
    return <a id="change">wrapper(partial(</a>torch.max<a id="change">, dim=axis, keepdim=keepdims)</a>, inputs<a id="change">, name=name)</a>


def reduce_mean(inputs, axis=None, keepdims=False, name=None):
    return wrapper(partial(torch.mean, dim=axis, keepdim=keepdims), inputs, name=name)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/d562ca7add8b907797fd0d69903f7eb0bbf2340c#diff-202c6b6c90bab0fd91b38fd5b79ec62e9cadc7068bf6da9cb932c52d9f506525L131' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6598525</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: d562ca7add8b907797fd0d69903f7eb0bbf2340c</div><div id='time'> Time: 2023-02-09</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: reduce_max(4)</div><div id='n_method'> N Method Name: reduce_max(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='n_file'> N File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_start'> M Start Line: 132</div><div id='m_end'> M End Line: 132</div><div id='n_start'> N Start Line: 142</div><div id='n_end'> N End Line: 142</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>


def reshape(inputs, shape, name=None):
    return <a id="change">Lambda(partial(torch.reshape, shape=shape), name=name)(</a>inputs<a id="change">)</a>


def resize(inputs, size, method="bilinear", preserve_aspect_ratio=False, antialias=False, name=None):
    if isinstance(inputs, GraphNode):</code></pre><h3>After Change</h3><pre><code class='java'>


def reshape(inputs, shape, name=None):
    return <a id="change">wrapper(partial(</a>torch.reshape<a id="change">, shape=shape)</a>, inputs<a id="change">, name=name)</a>


def resize(inputs, size, method="bilinear", preserve_aspect_ratio=False, antialias=False, name=None):
    if isinstance(inputs, GraphNode):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/d562ca7add8b907797fd0d69903f7eb0bbf2340c#diff-202c6b6c90bab0fd91b38fd5b79ec62e9cadc7068bf6da9cb932c52d9f506525L164' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6598527</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: d562ca7add8b907797fd0d69903f7eb0bbf2340c</div><div id='time'> Time: 2023-02-09</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: reshape(3)</div><div id='n_method'> N Method Name: reshape(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='n_file'> N File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_start'> M Start Line: 165</div><div id='m_end'> M End Line: 165</div><div id='n_start'> N Start Line: 178</div><div id='n_end'> N End Line: 178</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>


def expand_dims(inputs, axis, name=None):
    return <a id="change">Lambda(partial(torch.unsqueeze, dim=axis), name=name)(</a>inputs<a id="change">)</a>


def extract_patches(inputs, sizes, strides=1, rates=1, padding=0, name=None):
    </code></pre><h3>After Change</h3><pre><code class='java'>


def expand_dims(inputs, axis, name=None):
    return <a id="change">wrapper(partial(</a>torch.unsqueeze<a id="change">, dim=axis)</a>, inputs<a id="change">, name=name)</a>


def extract_patches(inputs, sizes, strides=1, rates=1, padding=0, name=None):
    </code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/d562ca7add8b907797fd0d69903f7eb0bbf2340c#diff-202c6b6c90bab0fd91b38fd5b79ec62e9cadc7068bf6da9cb932c52d9f506525L39' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6598521</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: d562ca7add8b907797fd0d69903f7eb0bbf2340c</div><div id='time'> Time: 2023-02-09</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: expand_dims(3)</div><div id='n_method'> N Method Name: expand_dims(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='n_file'> N File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_start'> M Start Line: 40</div><div id='m_end'> M End Line: 40</div><div id='n_start'> N Start Line: 43</div><div id='n_end'> N End Line: 43</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>


def pow(inputs, exponent, name=None):
    return <a id="change">Lambda(partial(torch.pow, exponent=exponent), name=name)(</a>inputs<a id="change">)</a>


def reduce_max(inputs, axis=None, keepdims=False, name=None):
    return Lambda(partial(torch.max, dim=axis, keepdim=keepdims), name=name)(inputs)</code></pre><h3>After Change</h3><pre><code class='java'>


def pow(inputs, exponent, name=None):
    return <a id="change">wrapper(partial(</a>torch.pow<a id="change">, exponent=exponent)</a>, inputs<a id="change">, name=name)</a>


def reduce_max(inputs, axis=None, keepdims=False, name=None):
    return wrapper(partial(torch.max, dim=axis, keepdim=keepdims), inputs, name=name)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/d562ca7add8b907797fd0d69903f7eb0bbf2340c#diff-202c6b6c90bab0fd91b38fd5b79ec62e9cadc7068bf6da9cb932c52d9f506525L127' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6598523</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: d562ca7add8b907797fd0d69903f7eb0bbf2340c</div><div id='time'> Time: 2023-02-09</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: pow(3)</div><div id='n_method'> N Method Name: pow(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='n_file'> N File Name: keras_cv_attention_models/pytorch_backend/functional.py</div><div id='m_start'> M Start Line: 128</div><div id='m_end'> M End Line: 128</div><div id='n_start'> N Start Line: 138</div><div id='n_end'> N End Line: 138</div><BR>