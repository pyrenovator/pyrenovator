<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Construct representation learning dataset of correctly paired (context, target) pairs
        dataset = dataset.pipe(self.target_pair_constructor)
        if self.shuffle_batches:
            dataset<a id="change"> = </a>dataset.shuffle(self.shuffle_buffer_size)
        &#47&#47 Torch chokes when batch_size is a numpy int instead of a Python int,
        &#47&#47 so we need to wrap the batch size in int() in case we&quotre running
        &#47&#47 under skopt (which uses numpy types).
        dataloader = DataLoader(dataset, batch_size=int(self.batch_size))

        loss_record = []

        if self.scheduler_cls is not None:
            if self.scheduler_cls in [CosineAnnealingLR, LinearWarmupCosine]:
                self.scheduler = self.scheduler_cls(self.optimizer, training_epochs, **to_dict(self.scheduler_kwargs))
            else:
                self.scheduler = self.scheduler_cls(self.optimizer, **to_dict(self.scheduler_kwargs))

        self.encoder.train(True)
        self.decoder.train(True)
        batches_trained = 0
        epochs_trained = 0
        training_complete = False
        logging.debug(f"Training with {training_epochs} epochs and {training_batches} batches")
        logging.debug(f"Batch size is {self.batch_size}")

        while not training_complete:
            loss_meter = AverageMeter()
            dataiter<a id="change"> = </a><a id="change">iter(</a>dataloader<a id="change">)</a>
            &#47&#47 Set encoder and decoder to be in training mode

            for step, batch in enumerate(dataiter):
                &#47&#47 Construct batch (currently just using Torch&quots default batch-creator)</code></pre><h3>After Change</h3><pre><code class='java'>
            collation_fn=default_collate))

        &#47&#47 do not start more workers than the number of shards
        wds_workers<a id="change"> = </a>min(<a id="change">len(</a>dataset.urls<a id="change">)</a>, self.dataset_max_workers)
        assert wds_workers &gt; 0
        &#47&#47 TODO(sam): this isn&quott guaranteed to interleave data from different
        &#47&#47 shards randomly. The optimal solution is to write a new</code></pre>