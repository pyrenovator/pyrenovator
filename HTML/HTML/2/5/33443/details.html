<html><h3>Pattern ID :33443
</h3><img src='96256782.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Construct representation learning dataset of correctly paired (context, target) pairs
        dataset = dataset.pipe(self.target_pair_constructor)
        if self.shuffle_batches:
            dataset<a id="change"> = </a>dataset.shuffle(self.shuffle_buffer_size)
        &#47&#47 Torch chokes when batch_size is a numpy int instead of a Python int,
        &#47&#47 so we need to wrap the batch size in int() in case we&quotre running
        &#47&#47 under skopt (which uses numpy types).
        dataloader = DataLoader(dataset, batch_size=int(self.batch_size))

        loss_record = []

        if self.scheduler_cls is not None:
            if self.scheduler_cls in [CosineAnnealingLR, LinearWarmupCosine]:
                self.scheduler = self.scheduler_cls(self.optimizer, training_epochs, **to_dict(self.scheduler_kwargs))
            else:
                self.scheduler = self.scheduler_cls(self.optimizer, **to_dict(self.scheduler_kwargs))

        self.encoder.train(True)
        self.decoder.train(True)
        batches_trained = 0
        epochs_trained = 0
        training_complete = False
        logging.debug(f"Training with {training_epochs} epochs and {training_batches} batches")
        logging.debug(f"Batch size is {self.batch_size}")

        while not training_complete:
            loss_meter = AverageMeter()
            dataiter<a id="change"> = </a><a id="change">iter(</a>dataloader<a id="change">)</a>
            &#47&#47 Set encoder and decoder to be in training mode

            for step, batch in enumerate(dataiter):
                &#47&#47 Construct batch (currently just using Torch&quots default batch-creator)</code></pre><h3>After Change</h3><pre><code class='java'>
            collation_fn=default_collate))

        &#47&#47 do not start more workers than the number of shards
        wds_workers<a id="change"> = </a>min(<a id="change">len(</a>dataset.urls<a id="change">)</a>, self.dataset_max_workers)
        assert wds_workers &gt; 0
        &#47&#47 TODO(sam): this isn&quott guaranteed to interleave data from different
        &#47&#47 shards randomly. The optimal solution is to write a new</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/humancompatibleai/eirli/commit/c52850ae168625bc4797ae36fe5806930beefd27#diff-da75b71a2421881ac87f16bdedbc66a6eee21a60650cb052fb123cfe1436b72fL218' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96256782</div><div id='project'> Project Name: humancompatibleai/eirli</div><div id='commit'> Commit Name: c52850ae168625bc4797ae36fe5806930beefd27</div><div id='time'> Time: 2020-11-18</div><div id='author'> Author: sam@qxcv.net</div><div id='file'> File Name: src/il_representations/algos/representation_learner.py</div><div id='m_class'> M Class Name: RepresentationLearner</div><div id='n_method'> N Class Name: RepresentationLearner</div><div id='m_method'> M Method Name: learn(4)</div><div id='n_method'> N Method Name: learn(4)</div><div id='m_parent_class'> M Parent Class: BaseEnvironmentLearner</div><div id='n_parent_class'> N Parent Class: BaseEnvironmentLearner</div><div id='m_file'> M File Name: src/il_representations/algos/representation_learner.py</div><div id='n_file'> N File Name: src/il_representations/algos/representation_learner.py</div><div id='m_start'> M Start Line: 218</div><div id='m_end'> M End Line: 244</div><div id='n_start'> N Start Line: 226</div><div id='n_end'> N End Line: 242</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Construct representation learning dataset of correctly paired (context, target) pairs
        dataset = dataset.pipe(self.target_pair_constructor)
        if self.shuffle_batches:
            dataset<a id="change"> = </a>dataset.shuffle(self.shuffle_buffer_size)
        &#47&#47 Torch chokes when batch_size is a numpy int instead of a Python int,
        &#47&#47 so we need to wrap the batch size in int() in case we&quotre running
        &#47&#47 under skopt (which uses numpy types).
        dataloader = DataLoader(dataset, batch_size=int(self.batch_size))

        loss_record = []

        if self.scheduler_cls is not None:
            if self.scheduler_cls in [CosineAnnealingLR, LinearWarmupCosine]:
                self.scheduler = self.scheduler_cls(self.optimizer, training_epochs, **to_dict(self.scheduler_kwargs))
            else:
                self.scheduler = self.scheduler_cls(self.optimizer, **to_dict(self.scheduler_kwargs))

        self.encoder.train(True)
        self.decoder.train(True)
        batches_trained = 0
        epochs_trained = 0
        training_complete = False
        logging.debug(f"Training with {training_epochs} epochs and {training_batches} batches")
        logging.debug(f"Batch size is {self.batch_size}")

        while not training_complete:
            loss_meter = AverageMeter()
            dataiter<a id="change"> = </a><a id="change">iter(</a>dataloader<a id="change">)</a>
            &#47&#47 Set encoder and decoder to be in training mode

            for step, batch in enumerate(dataiter):
                &#47&#47 Construct batch (currently just using Torch&quots default batch-creator)</code></pre><h3>After Change</h3><pre><code class='java'>
            collation_fn=default_collate))

        &#47&#47 do not start more workers than the number of shards
        wds_workers<a id="change"> = </a>min(<a id="change">len(</a>dataset.urls<a id="change">)</a>, self.dataset_max_workers)
        assert wds_workers &gt; 0
        &#47&#47 TODO(sam): this isn&quott guaranteed to interleave data from different
        &#47&#47 shards randomly. The optimal solution is to write a new</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/humancompatibleai/eirli/commit/3a20a4834cbb402925efc8e76344dcfdc8e6ce17#diff-da75b71a2421881ac87f16bdedbc66a6eee21a60650cb052fb123cfe1436b72fL208' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96256776</div><div id='project'> Project Name: humancompatibleai/eirli</div><div id='commit'> Commit Name: 3a20a4834cbb402925efc8e76344dcfdc8e6ce17</div><div id='time'> Time: 2020-11-18</div><div id='author'> Author: sam@qxcv.net</div><div id='file'> File Name: src/il_representations/algos/representation_learner.py</div><div id='m_class'> M Class Name: RepresentationLearner</div><div id='n_method'> N Class Name: RepresentationLearner</div><div id='m_method'> M Method Name: learn(4)</div><div id='n_method'> N Method Name: learn(4)</div><div id='m_parent_class'> M Parent Class: BaseEnvironmentLearner</div><div id='n_parent_class'> N Parent Class: BaseEnvironmentLearner</div><div id='m_file'> M File Name: src/il_representations/algos/representation_learner.py</div><div id='n_file'> N File Name: src/il_representations/algos/representation_learner.py</div><div id='m_start'> M Start Line: 218</div><div id='m_end'> M End Line: 244</div><div id='n_start'> N Start Line: 226</div><div id='n_end'> N End Line: 242</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        source.pop(target)
        self.target_loader = self.dataset.get_dataloader(&quottrain&quot, full=True, classes=target,
                                                         batch_size=self.poison_num, shuffle=True, num_workers=0)
        self.source_loader<a id="change"> = </a>self.dataset.get_dataloader(&quottrain&quot, full=True, classes=source,
                                                         batch_size=self.poison_num, shuffle=True, num_workers=0)
        target_imgs, _ = self.model.get_data(next(iter(self.target_loader)))
        source_imgs<a id="change">, _ = </a>self.model.get_data(next(<a id="change">iter(</a>self.source_loader<a id="change">)</a>))
        source_imgs = self.add_mark(source_imgs)
        noise = torch.zeros_like(target_imgs)
        source_feats = self.model.get_layer(source_imgs, layer_output=self.preprocess_layer).detach()</code></pre><h3>After Change</h3><pre><code class='java'>
        source_feats = self.model.get_layer(source_imgs, layer_output=self.preprocess_layer).detach()

        target_imgs, _ = self.model.get_data(next(iter(self.target_loader)))
        target_imgs<a id="change"> = </a>target_imgs[:<a id="change">len(</a>source_imgs<a id="change">)</a>]
        &#47&#47 -----------------------------Poison Frog--------------------------------- &#47&#47

        def loss_func(poison_imgs):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/ain-soph/trojanzoo/commit/ba7a05c0e1d8e0b546a7c0e7c168b1e57ccc0eba#diff-307145e6df05bb45b944d3a184fed6c8f5f4bed53fb2b1a381acee3a66c1bebdL83' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96256785</div><div id='project'> Project Name: ain-soph/trojanzoo</div><div id='commit'> Commit Name: ba7a05c0e1d8e0b546a7c0e7c168b1e57ccc0eba</div><div id='time'> Time: 2020-09-23</div><div id='author'> Author: ain-soph@live.com</div><div id='file'> File Name: trojanzoo/attack/backdoor/hidden_trigger.py</div><div id='m_class'> M Class Name: Hidden_Trigger</div><div id='n_method'> N Class Name: Hidden_Trigger</div><div id='m_method'> M Method Name: generate_poisoned_data(2)</div><div id='n_method'> N Method Name: generate_poisoned_data(1)</div><div id='m_parent_class'> M Parent Class: BadNet</div><div id='n_parent_class'> N Parent Class: BadNet</div><div id='m_file'> M File Name: trojanzoo/attack/backdoor/hidden_trigger.py</div><div id='n_file'> N File Name: trojanzoo/attack/backdoor/hidden_trigger.py</div><div id='m_start'> M Start Line: 106</div><div id='m_end'> M End Line: 136</div><div id='n_start'> N Start Line: 92</div><div id='n_end'> N End Line: 124</div><BR>