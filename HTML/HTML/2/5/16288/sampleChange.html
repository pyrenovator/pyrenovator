<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

            &#47&#47 TODO: make this work on multilpe outputs
            result = op(sample.input, *sample.args, **sample.kwargs)
            <a id="change">if </a>not isinstance(result, torch.Tensor):
                <a id="change">continue</a>

            diff_argnums = tuple(i for i, arg in enumerate(args)
                                 if isinstance(arg, Tensor) and
                                 torch.is_floating_point(arg))</code></pre><h3>After Change</h3><pre><code class='java'>
class TestGradOpInfo(TestCase):
    @ops(op_db, allowed_dtypes=(torch.float,))
    def test_op(self, device, dtype, op):
        op_skip<a id="change"> = </a>{
            &quot__getitem__&quot,
            &quot__rpow__&quot,
            &quotlinalg.cholesky&quot,
            &quotlinalg.inv&quot,
            &quotlinalg.matrix_norm&quot,
            &quotlinalg.matrix_power&quot,
            &quotlinalg.norm&quot,
            &quotnanquantile&quot,
            &quotquantile&quot,
            &quottensor_split&quot,
        }
        if op.name in op_skip:
            self.skipTest("Skipped; Expected failures")
            return

        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return

        samples = op.sample_inputs(device, dtype, requires_grad=True)

        def is_inplace(variant):
            if hasattr(variant, "__wrapped__"):
                return variant.__wrapped__ is op.get_inplace()
            return variant is op.get_inplace()

        for sample in samples:
            &#47&#47 TODO: test in-place
            if is_inplace(op.get_op()):
                self.skipTest("Skipped! NYI: inplace-testing not supported.")
                continue

            args = [sample.input] + list(sample.args)
            kwargs = sample.kwargs

            def diff_arg(arg):
                if is_iterable_of_tensors(arg):
                    if all([a.requires_grad for a in arg]):
                        return True
                    if all([not a.requires_grad for a in arg]):
                        return False
                    raise RuntimeError("NYI: The test runner can&quott handle this")
                return isinstance(arg, Tensor) and arg.requires_grad

            diff_argnums = tuple(i for i, arg in enumerate(args) if diff_arg(arg))
            <a id="change">assert </a>len(diff_argnums) &gt; 0
            diff_args = tuple(args[i] for i in diff_argnums)

            def wrapped_fn(*args, **kwargs):</code></pre>