<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def validation_epoch_end(self, outputs: List[dict]) -&gt; None:
        &#47&#47 to see an average performance over the batches in this specific epoch
        avg_loss, rank_median, rank_var, avg_top1, avg_top10, avg_top100 = <a id="change">self.reduce_outputs(</a>outputs<a id="change">)</a>
        &#47&#47 log the metrics
        self.log("Validation/Average Loss", avg_loss)
        self.log("Validation/Rank Median", rank_median)
        self.log("Validation/Rank Variance", rank_var)</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 to see an average performance over the batches in this specific epoch
        avg_loss = torch.stack([output[&quotloss&quot] for output in outputs]).mean()
        &#47&#47 log the metrics
        rank_mean<a id="change">, rank_median, rank_std, top1, top3, top5</a> = self.rd_metric.compute()
        self.log("Validation/Average Loss", avg_loss)
        self.log("Validation/Rank Mean", rank_mean)
        self.log("Validation/Rank Median", rank_median)
        self.log("Validation/Rank Standard Deviation", rank_std)
        self.log("Validation/Top 1 Accuracy", top1)
        self.log("Validation/Top 3 Accuracy", top3)
        <a id="change">self.log("Validation/Top 5 Accuracy"</a>, <a id="change">top5</a><a id="change">)</a>
        &#47&#47 so that the metrics do not accumulate to the next epoch
        self.rd_metric.reset()

    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int):</code></pre>