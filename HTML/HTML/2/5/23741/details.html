<html><h3>Pattern ID :23741
</h3><img src='73969608.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        return loss

    def allreduce_gradients(self, bucket_size=MEMORY_OPT_ALLREDUCE_SIZE):
        <a id="change">if </a><a id="change">self.is_gradient_accumulation_boundary()</a>:
            self.buffered_allreduce_fallback(elements_per_buffer=bucket_size)

    def backward(self, loss, allreduce_gradients=True):</code></pre><h3>After Change</h3><pre><code class='java'>
        return loss

    def allreduce_gradients(self, bucket_size=MEMORY_OPT_ALLREDUCE_SIZE):
        <a id="change">if </a><a id="change">self.is_gradient_accumulation_boundary()</a>:
            if self.zero_optimization_stage() == ZERO_OPTIMIZATION_OPTIMIZER_STATES:
                assert self.zero_reduce_scatter()
                self.optimizer.reduce_scatter_gradients(
                    postscale_gradients=self.postscale_gradients(),
                    gradient_predivide_factor=self.gradient_predivide_factor,
                    gradient_average=self.gradient_average)
            elif <a id="change"></a>self.zero_optimization_partition_gradients():
                self.optimizer.overlapping_partition_gradients_reduce_epilogue()
            else:
                self.buffered_allreduce_fallback(elements_per_buffer=bucket_size)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/microsoft/deepspeed/commit/f2ac7eafd54c49acb8981650637dedd939e96c14#diff-76270fb395316d963c7efb2779c08d1005bad070da854f1ab101cb13ee18de92L626' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 73969608</div><div id='project'> Project Name: microsoft/deepspeed</div><div id='commit'> Commit Name: f2ac7eafd54c49acb8981650637dedd939e96c14</div><div id='time'> Time: 2020-05-19</div><div id='author'> Author: jerasley@microsoft.com</div><div id='file'> File Name: deepspeed/pt/deepspeed_light.py</div><div id='m_class'> M Class Name: DeepSpeedLight</div><div id='n_method'> N Class Name: DeepSpeedLight</div><div id='m_method'> M Method Name: allreduce_gradients(2)</div><div id='n_method'> N Method Name: allreduce_gradients(2)</div><div id='m_parent_class'> M Parent Class: Module</div><div id='n_parent_class'> N Parent Class: Module</div><div id='m_file'> M File Name: deepspeed/pt/deepspeed_light.py</div><div id='n_file'> N File Name: deepspeed/pt/deepspeed_light.py</div><div id='m_start'> M Start Line: 626</div><div id='m_end'> M End Line: 629</div><div id='n_start'> N Start Line: 689</div><div id='n_end'> N End Line: 701</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        self.tput_timer.stop(report_progress)

        <a id="change">if </a><a id="change">self.is_gradient_accumulation_boundary()</a> and self.tensorboard_enabled(
        ) and torch.distributed.get_rank() == 0:  &#47&#47 deepspeed tensorboard support for lr
            self.summary_events = [(f&quotTrain/Samples/lr&quot,
                                    self.get_lr()[0],</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 Log learning rate
        if self.tensorboard_enabled():
            <a id="change">if </a><a id="change">self.is_gradient_accumulation_boundary()</a>:
                <a id="change">if </a>self.global_rank == 0:
                    self.summary_events = [(f&quotTrain/Samples/lr&quot,
                                            self.get_lr()[0],
                                            self.sample_count)]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/deepspeed/commit/20557f70954f06cbbf8d9fdb1910b63c6af4eb77#diff-76270fb395316d963c7efb2779c08d1005bad070da854f1ab101cb13ee18de92L682' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 73969606</div><div id='project'> Project Name: microsoft/deepspeed</div><div id='commit'> Commit Name: 20557f70954f06cbbf8d9fdb1910b63c6af4eb77</div><div id='time'> Time: 2020-03-26</div><div id='author'> Author: Shaden.Smith@microsoft.com</div><div id='file'> File Name: deepspeed/pt/deepspeed_light.py</div><div id='m_class'> M Class Name: DeepSpeedLight</div><div id='n_method'> N Class Name: DeepSpeedLight</div><div id='m_method'> M Method Name: step(1)</div><div id='n_method'> N Method Name: step(1)</div><div id='m_parent_class'> M Parent Class: Module</div><div id='n_parent_class'> N Parent Class: Module</div><div id='m_file'> M File Name: deepspeed/pt/deepspeed_light.py</div><div id='n_file'> N File Name: deepspeed/pt/deepspeed_light.py</div><div id='m_start'> M Start Line: 715</div><div id='m_end'> M End Line: 754</div><div id='n_start'> N Start Line: 719</div><div id='n_end'> N End Line: 760</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            allreduce_gradients: If this is False, then gradient averaging will be skipped. Default is True.
        

        <a id="change">if </a><a id="change">self.is_gradient_accumulation_boundary()</a> and self.tensorboard_enabled(
        ) and torch.distributed.get_rank(
        ) == 0:  &#47&#47 deepspeed tensorboard support for loss
            self.sample_count += (self.train_micro_batch_size_per_gpu() *</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 Log training Loss
        if self.tensorboard_enabled():
            <a id="change">if </a><a id="change">self.is_gradient_accumulation_boundary()</a>:
                <a id="change">if </a>self.global_rank == 0:
                    self.sample_count += (self.train_micro_batch_size_per_gpu() *
                                          self.dp_world_size *
                                          self.gradient_accumulation_steps())</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/deepspeed/commit/20557f70954f06cbbf8d9fdb1910b63c6af4eb77#diff-76270fb395316d963c7efb2779c08d1005bad070da854f1ab101cb13ee18de92L616' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 73969607</div><div id='project'> Project Name: microsoft/deepspeed</div><div id='commit'> Commit Name: 20557f70954f06cbbf8d9fdb1910b63c6af4eb77</div><div id='time'> Time: 2020-03-26</div><div id='author'> Author: Shaden.Smith@microsoft.com</div><div id='file'> File Name: deepspeed/pt/deepspeed_light.py</div><div id='m_class'> M Class Name: DeepSpeedLight</div><div id='n_method'> N Class Name: DeepSpeedLight</div><div id='m_method'> M Method Name: backward(3)</div><div id='n_method'> N Method Name: backward(3)</div><div id='m_parent_class'> M Parent Class: Module</div><div id='n_parent_class'> N Parent Class: Module</div><div id='m_file'> M File Name: deepspeed/pt/deepspeed_light.py</div><div id='n_file'> N File Name: deepspeed/pt/deepspeed_light.py</div><div id='m_start'> M Start Line: 624</div><div id='m_end'> M End Line: 639</div><div id='n_start'> N Start Line: 627</div><div id='n_end'> N End Line: 642</div><BR>