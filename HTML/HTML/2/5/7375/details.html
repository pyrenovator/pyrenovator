<html><h3>Pattern ID :7375
</h3><img src='24487677.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47         self.L = self.L_history[-6:][np.argmax(np.abs(self.rho_history[-6:]))] * np.exp(np.random.normal(loc = 0, scale = 1))
        h = self.update_h_v3()
        if self.verbose &gt; 2:
            print("h: ", <a id="change">h.detach().cpu().numpy()</a>)
        with torch.no_grad():
            self.current_Y = self.model.full_sample(self.current_state + h, as_representation = True, override_locked = False, flatten = True)
            if self.model.target.has_mask:</code></pre><h3>After Change</h3><pre><code class='java'>
        elif self.iteration &gt; 0:
            rho = self.rho_3(np.nanmin(self.loss_history[:-1]), loss, h)
            if self.verbose &gt; 1:
                <a id="change">AP_config.ap_logger.debug(f"LM loss: {loss.item()}, best loss: {np.nanmin(self.loss_history[:-1])}, loss diff: {np.nanmin(self.loss_history[:-1]) - loss.item()}, L: {self.L}"</a><a id="change">)</a>
            elif self.verbose &gt; 0 and rho &gt; self.epsilon4:
                AP_config.ap_logger.info(f"LM loss: {loss.item()}")
            self.rho_history.append(rho)
            if self.verbose &gt; 1:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/connorstoneastro/autoprof/commit/6a9ca466b2a9f7dd23a7661bddbe588d3676cd17#diff-daabd78fa8f6167a7ea2c6fd6fce88e607b57cd475cf1e759da42fe315dcd1acL161' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 24487677</div><div id='project'> Project Name: connorstoneastro/autoprof</div><div id='commit'> Commit Name: 6a9ca466b2a9f7dd23a7661bddbe588d3676cd17</div><div id='time'> Time: 2023-02-02</div><div id='author'> Author: connorstone628@gmail.com</div><div id='file'> File Name: build/lib/autoprof/fit/lm.py</div><div id='m_class'> M Class Name: LM</div><div id='n_method'> N Class Name: LM</div><div id='m_method'> M Method Name: step_method0(2)</div><div id='n_method'> N Method Name: step_method0(2)</div><div id='m_parent_class'> M Parent Class: BaseOptimizer</div><div id='n_parent_class'> N Parent Class: BaseOptimizer</div><div id='m_file'> M File Name: build/lib/autoprof/fit/lm.py</div><div id='n_file'> N File Name: build/lib/autoprof/fit/lm.py</div><div id='m_start'> M Start Line: 167</div><div id='m_end'> M End Line: 200</div><div id='n_start'> N Start Line: 161</div><div id='n_end'> N End Line: 239</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47         self.L = self.L_history[-6:][np.argmax(np.abs(self.rho_history[-6:]))] * np.exp(np.random.normal(loc = 0, scale = 1))
        h = self.update_h_v2()
        if self.verbose &gt; 2:
            print("h: ", <a id="change">h.detach().cpu().numpy()</a>)
        
        with torch.no_grad():
            self.current_Y = self.model.full_sample(self.current_state + h, as_representation = True, override_locked = False, flatten = True)</code></pre><h3>After Change</h3><pre><code class='java'>
            AP_config.ap_logger.debug("full jac")
            self.update_J_AD()
        else:
            <a id="change">AP_config.ap_logger.debug("Broyden jac"</a><a id="change">)</a>
            self.update_J_Broyden(h, self.prev_Y[0], self.current_Y)

        self.update_hess()
        self.update_grad(self.prev_Y[1])</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/connorstoneastro/autoprof/commit/10b2a7770d988d066380285137108a388ebd1bad#diff-52615ad08cfc1cfad1befd14d3c383d34ae33d73e933e1359265221e8731fb61L245' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 24487676</div><div id='project'> Project Name: connorstoneastro/autoprof</div><div id='commit'> Commit Name: 10b2a7770d988d066380285137108a388ebd1bad</div><div id='time'> Time: 2023-01-14</div><div id='author'> Author: connorstone628@gmail.com</div><div id='file'> File Name: autoprof/fit/lm.py</div><div id='m_class'> M Class Name: LM</div><div id='n_method'> N Class Name: LM</div><div id='m_method'> M Method Name: step_method1(2)</div><div id='n_method'> N Method Name: step_method1(2)</div><div id='m_parent_class'> M Parent Class: BaseOptimizer</div><div id='n_parent_class'> N Parent Class: BaseOptimizer</div><div id='m_file'> M File Name: autoprof/fit/lm.py</div><div id='n_file'> N File Name: autoprof/fit/lm.py</div><div id='m_start'> M Start Line: 250</div><div id='m_end'> M End Line: 328</div><div id='n_start'> N Start Line: 251</div><div id='n_end'> N End Line: 317</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47         self.L = self.L_history[-6:][np.argmax(np.abs(self.rho_history[-6:]))] * np.exp(np.random.normal(loc = 0, scale = 1))
        h = self.update_h_v3()
        if self.verbose &gt; 2:
            print("h: ", <a id="change">h.detach().cpu().numpy()</a>)
        with torch.no_grad():
            self.current_Y = self.model.full_sample(self.current_state + h, as_representation = True, override_locked = False, flatten = True)
            if self.model.target.has_mask:</code></pre><h3>After Change</h3><pre><code class='java'>
        elif self.iteration &gt; 0:
            rho = self.rho_3(np.nanmin(self.loss_history[:-1]), loss, h)
            if self.verbose &gt; 1:
                <a id="change">AP_config.ap_logger.debug(f"LM loss: {loss.item()}, best loss: {np.nanmin(self.loss_history[:-1])}, loss diff: {np.nanmin(self.loss_history[:-1]) - loss.item()}, L: {self.L}"</a><a id="change">)</a>
            elif self.verbose &gt; 0 and rho &gt; self.epsilon4:
                AP_config.ap_logger.info(f"LM loss: {loss.item()}")
            self.rho_history.append(rho)
            if self.verbose &gt; 1:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/connorstoneastro/autoprof/commit/10b2a7770d988d066380285137108a388ebd1bad#diff-52615ad08cfc1cfad1befd14d3c383d34ae33d73e933e1359265221e8731fb61L151' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 24487673</div><div id='project'> Project Name: connorstoneastro/autoprof</div><div id='commit'> Commit Name: 10b2a7770d988d066380285137108a388ebd1bad</div><div id='time'> Time: 2023-01-14</div><div id='author'> Author: connorstone628@gmail.com</div><div id='file'> File Name: autoprof/fit/lm.py</div><div id='m_class'> M Class Name: LM</div><div id='n_method'> N Class Name: LM</div><div id='m_method'> M Method Name: step_method0(2)</div><div id='n_method'> N Method Name: step_method0(2)</div><div id='m_parent_class'> M Parent Class: BaseOptimizer</div><div id='n_parent_class'> N Parent Class: BaseOptimizer</div><div id='m_file'> M File Name: autoprof/fit/lm.py</div><div id='n_file'> N File Name: autoprof/fit/lm.py</div><div id='m_start'> M Start Line: 167</div><div id='m_end'> M End Line: 232</div><div id='n_start'> N Start Line: 161</div><div id='n_end'> N End Line: 239</div><BR>