<html><h3>Pattern ID :4827
</h3><img src='17047468.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.layers = nn.ModuleList([])

        for _ in range(depth):
            layer<a id="change"> = </a>Residual(PreNorm(dim, AdjacentAttention(
                dim = dim,
                dim_head = dim_head,
                heads = heads
            )))
            <a id="change">self.layers.append(</a>layer<a id="change">)</a>

    def forward(self, x, adjacency_mat, mask = None):
        device = x.device
</code></pre><h3>After Change</h3><pre><code class='java'>
        self.layers = nn.ModuleList([])

        for _ in range(depth):
            self.layers.append(nn.ModuleList(<a id="change">[
                </a>Residual(PreNorm(dim, AdjacentAttention(
                    dim = dim,
                    dim_head = dim_head,
                    heads = heads
                ))),
                Residual(<a id="change">PreNorm(</a>dim, <a id="change">FeedForward(
                    dim = dim
                ))</a>)<a id="change"></a>
            ]))

    def forward(self, x, adjacency_mat, mask = None):
        device = x.device</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/adjacent-attention-network/commit/fa7a72d7d257d72c159b59572297133ee91905cb#diff-71f2ddd08dfbd4dae492be6a13060eb54c6e79f8b54246b5f612c2e7c269ab4fL103' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 17047468</div><div id='project'> Project Name: lucidrains/adjacent-attention-network</div><div id='commit'> Commit Name: fa7a72d7d257d72c159b59572297133ee91905cb</div><div id='time'> Time: 2020-12-13</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: adjacent_attention_network/adjacent_attention_network.py</div><div id='m_class'> M Class Name: AdjacentAttentionNetwork</div><div id='n_method'> N Class Name: AdjacentAttentionNetwork</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: adjacent_attention_network/adjacent_attention_network.py</div><div id='n_file'> N File Name: adjacent_attention_network/adjacent_attention_network.py</div><div id='m_start'> M Start Line: 103</div><div id='m_end'> M End Line: 111</div><div id='n_start'> N Start Line: 114</div><div id='n_end'> N End Line: 126</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            if not receives_context:
                continue

            layer<a id="change"> = </a>nn.ModuleList([
                PreNorm(dim, SelfAttention(dim, heads, one_kv_head = one_kv_head, psi_fn = psi_fn, receives_context = True)),
                PreNorm(dim, Chunk(ff_chunks, FeedForward(dim), along_dim = 1))
            ])
            <a id="change">layers.append(</a>layer<a id="change">)</a>

        execute_type = ReversibleSequence if reversible else SequentialSequence

        attn_context_layer = ((True, False),) if receives_context else tuple()</code></pre><h3>After Change</h3><pre><code class='java'>
            ]))

            if attend_axially:
                layers.append(nn.ModuleList(<a id="change">[
                    </a>PreNorm(dim, FoldAxially(local_attn_window_size, SelfAttention(dim, heads, causal, one_kv_head = one_kv_head, psi_fn = psi_fn))),
                    <a id="change">PreNorm(</a>dim, Chunk(ff_chunks, <a id="change">FeedForward(</a>dim<a id="change">)</a>, along_dim = 1)<a id="change">)</a>
                ]))

            if receives_context:
                layers.append(nn.ModuleList([</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/linear-attention-transformer/commit/1e04f31292ccbe233d2b9a8bc0b6f21c0f66f071#diff-eee6cf669463e9683bcf9643c4b19a493056f98ded32854287144d47f02a7032L314' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 17047469</div><div id='project'> Project Name: lucidrains/linear-attention-transformer</div><div id='commit'> Commit Name: 1e04f31292ccbe233d2b9a8bc0b6f21c0f66f071</div><div id='time'> Time: 2020-06-09</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='m_class'> M Class Name: LinearAttentionTransformer</div><div id='n_method'> N Class Name: LinearAttentionTransformer</div><div id='m_method'> M Method Name: __init__(18)</div><div id='n_method'> N Method Name: __init__(17)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='n_file'> N File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='m_start'> M Start Line: 322</div><div id='m_end'> M End Line: 351</div><div id='n_start'> N Start Line: 336</div><div id='n_end'> N End Line: 374</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            should_cache = i &gt; 0 and weight_tie_layers
            cache_args = {&quot_cache&quot: should_cache}

            self_attns<a id="change"> = </a>nn.ModuleList([])

            for _ in range(self_per_cross_attn):
                self_attns.append(nn.ModuleList([
                    get_latent_attn(**cache_args),
                    get_latent_ff(**cache_args)
                ]))

            <a id="change">self.layers.append(</a>nn.ModuleList([
                get_cross_attn(**cache_args),
                get_cross_ff(**cache_args),
                self_attns
            ])<a id="change">)</a>

        self.decoder_cross_attn = PreNorm(queries_dim, Attention(queries_dim, latent_dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = latent_dim)
        self.decoder_ff = PreNorm(queries_dim, FeedForward(queries_dim)) if decoder_ff else None
</code></pre><h3>After Change</h3><pre><code class='java'>
        super().__init__()
        self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))

        self.cross_attend_blocks = nn.ModuleList(<a id="change">[
            </a>PreNorm(latent_dim, Attention(latent_dim, dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = dim),
            <a id="change">PreNorm(</a>latent_dim, <a id="change">FeedForward(</a>latent_dim<a id="change">))</a>
        ])

        get_latent_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, heads = latent_heads, dim_head = latent_dim_head))
        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/perceiver-pytorch/commit/dc530de88e6035a2f08d7e35ce23e57abe8371bd#diff-d109c4942e7a2f6d51cd8e55cbab114efc5eb0b0ce37d8e682449385df84ec13L106' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 17047466</div><div id='project'> Project Name: lucidrains/perceiver-pytorch</div><div id='commit'> Commit Name: dc530de88e6035a2f08d7e35ce23e57abe8371bd</div><div id='time'> Time: 2021-08-30</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: perceiver_pytorch/perceiver_io.py</div><div id='m_class'> M Class Name: PerceiverIO</div><div id='n_method'> N Class Name: PerceiverIO</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: perceiver_pytorch/perceiver_io.py</div><div id='n_file'> N File Name: perceiver_pytorch/perceiver_io.py</div><div id='m_start'> M Start Line: 126</div><div id='m_end'> M End Line: 152</div><div id='n_start'> N Start Line: 125</div><div id='n_end'> N End Line: 143</div><BR>