<html><h3>Pattern ID :32903
</h3><img src='95424176.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            queries_per_block = min(L, 1024//k) 
            threads = k * queries_per_block
            blocks = ((L*k)//threads) + C + 1
            query_map = <a id="change">torch.ones(</a>(N, H, blocks)<a id="change">, dtype=torch.int32)</a>.cuda() * L 
            blocks_map = torch.ones((N, H, blocks), dtype=torch.int32).cuda() * -1 
            _, sorted_group_indices = torch.sort(groups, descending=True, dim=-1)
</code></pre><h3>After Change</h3><pre><code class='java'>
        else:
            &#47&#47 Allocate bookkeeping parameters to facilitate the kernel
            with torch.no_grad():
                Q_pb<a id="change"> = 16</a>
                block_counts = (counts + Q_pb - 1) // Q_pb
                block_counts = block_counts.int()
                block_counts_cumsum = block_counts.view(-1).cumsum(-1).view(N, H, C).int()
                indx_maps = torch.ones(
                    (block_counts.sum(), 4),
                    device=Q.device,
                    dtype=torch.int32
                )
                counts_cumsum = counts.cumsum(-1).int()
                total_blocks<a id="change"> = </a><a id="change">block_counts.sum().item()</a>

            &#47&#47 Actually perform the dot product
            ClusteredSparseDotProduct.dot[device.type](
                Q,</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/idiap/fast-transformers/commit/ac1fd6316f59b56faa3b4e9236810d4e97ed5b15#diff-e91bf79b7e2f1026ed509f7acfa914077ef9851d0eb373421f7aa40468c45da4L164' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95424176</div><div id='project'> Project Name: idiap/fast-transformers</div><div id='commit'> Commit Name: ac1fd6316f59b56faa3b4e9236810d4e97ed5b15</div><div id='time'> Time: 2020-11-25</div><div id='author'> Author: avyas@idiap.ch</div><div id='file'> File Name: fast_transformers/sparse_product/__init__.py</div><div id='m_class'> M Class Name: ClusteredSparseDotProduct</div><div id='n_method'> N Class Name: ClusteredSparseDotProduct</div><div id='m_method'> M Method Name: forward(7)</div><div id='n_method'> N Method Name: forward(7)</div><div id='m_parent_class'> M Parent Class: torch.autograd.Function</div><div id='n_parent_class'> N Parent Class: torch.autograd.Function</div><div id='m_file'> M File Name: fast_transformers/sparse_product/__init__.py</div><div id='n_file'> N File Name: fast_transformers/sparse_product/__init__.py</div><div id='m_start'> M Start Line: 185</div><div id='m_end'> M End Line: 201</div><div id='n_start'> N Start Line: 164</div><div id='n_end'> N End Line: 208</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        queries_per_block = min(L, 1024) 
        threads = queries_per_block
        blocks = (L//threads) + C + 1
        query_map = <a id="change">torch.ones(</a>(N, H, blocks)<a id="change">,
                               dtype=torch.int32,
                               device=Y.device)</a> * L 
        blocks_map = torch.ones_like(query_map,
                                     dtype=torch.int32,
                                     device=Y.device) * -1 </code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 broadcast kernel that takes advantage of clustering
        &#47&#47 More information can be found in the cuda file
        with torch.no_grad():
            threads<a id="change"> = 256</a>
            G = set_group(C, E)
            group_counts = counts.view(N, H, G, -1).sum(-1)
            block_counts = (group_counts + threads - 1) // threads
            total_blocks<a id="change"> = </a><a id="change">block_counts.sum().item()</a>
            indx_maps = torch.ones(
                (total_blocks, 5),
                device=X.device,
                dtype=torch.int32</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/idiap/fast-transformers/commit/8e4d4469091761280523efe62ffda6277d02ce87#diff-1e33ed565253c17c8a36f5b467de6127f401bed60ced4cf14d371ea1201dc2baL58' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95424174</div><div id='project'> Project Name: idiap/fast-transformers</div><div id='commit'> Commit Name: 8e4d4469091761280523efe62ffda6277d02ce87</div><div id='time'> Time: 2020-11-25</div><div id='author'> Author: avyas@idiap.ch</div><div id='file'> File Name: fast_transformers/aggregate/__init__.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: clustered_broadcast(5)</div><div id='n_method'> N Method Name: clustered_broadcast(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: fast_transformers/aggregate/__init__.py</div><div id='n_file'> N File Name: fast_transformers/aggregate/__init__.py</div><div id='m_start'> M Start Line: 68</div><div id='m_end'> M End Line: 94</div><div id='n_start'> N Start Line: 72</div><div id='n_end'> N End Line: 109</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

                    if "whamr" in self.hparams.data_folder:
                        targets = self.hparams.reverb(
                            targets[0].t(), <a id="change">torch.ones(</a>targets.size(-1)<a id="change">)</a>
                        )
                        targets = targets.t().unsqueeze(0)
                        mix = targets.sum(-1)
</code></pre><h3>After Change</h3><pre><code class='java'>

                    if "whamr" in self.hparams.data_folder:

                        mix<a id="change"> = 0</a>
                        for mic in rirs:
                            &#47&#47 rir_cat = torch.flip(torch.stack(mic), [1]).unsqueeze(0)
                            rir_cat = (torch.stack(mic)).unsqueeze(0)

                            rir_cat = rir_cat.to(self.device)

                            mix = mix + F.conv1d(
                                targets.permute(0, 2, 1), rir_cat
                            )
                        mix = mix.squeeze(1)

                        &#47&#47 fix the levels
                        coef = (
                            targets.abs().max().item() / <a id="change">mix.abs().max().item()</a>
                        )
                        mix<a id="change"> = </a>mix * coef

                        &#47&#47 torchaudio.save(&quotreverbtest.wav&quot, mix.cpu(), 8000)
                        &#47&#47 torchaudio.save(&quottarget.wav&quot, targets[:, :, 0].cpu(), 8000)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/speechbrain/speechbrain/commit/59bc3bf412dc5c1c2e9baf687ede623cc1c4c588#diff-eb60f0baa9cae2dfcac548ade392e883ffc7b89290c90c680e1c9d6200f1c1a5L42' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95424175</div><div id='project'> Project Name: speechbrain/speechbrain</div><div id='commit'> Commit Name: 59bc3bf412dc5c1c2e9baf687ede623cc1c4c588</div><div id='time'> Time: 2021-03-19</div><div id='author'> Author: csubakan@gmail.com</div><div id='file'> File Name: recipes/WSJ0Mix/separation/train.py</div><div id='m_class'> M Class Name: Separation</div><div id='n_method'> N Class Name: Separation</div><div id='m_method'> M Method Name: compute_forward(6)</div><div id='n_method'> N Method Name: compute_forward(5)</div><div id='m_parent_class'> M Parent Class: sb.Brain</div><div id='n_parent_class'> N Parent Class: sb.Brain</div><div id='m_file'> M File Name: recipes/WSJ0Mix/separation/train.py</div><div id='n_file'> N File Name: recipes/WSJ0Mix/separation/train.py</div><div id='m_start'> M Start Line: 50</div><div id='m_end'> M End Line: 65</div><div id='n_start'> N Start Line: 42</div><div id='n_end'> N End Line: 79</div><BR>