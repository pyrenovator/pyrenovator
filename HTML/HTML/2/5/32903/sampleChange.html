<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            queries_per_block = min(L, 1024//k) 
            threads = k * queries_per_block
            blocks = ((L*k)//threads) + C + 1
            query_map = <a id="change">torch.ones(</a>(N, H, blocks)<a id="change">, dtype=torch.int32)</a>.cuda() * L 
            blocks_map = torch.ones((N, H, blocks), dtype=torch.int32).cuda() * -1 
            _, sorted_group_indices = torch.sort(groups, descending=True, dim=-1)
</code></pre><h3>After Change</h3><pre><code class='java'>
        else:
            &#47&#47 Allocate bookkeeping parameters to facilitate the kernel
            with torch.no_grad():
                Q_pb<a id="change"> = 16</a>
                block_counts = (counts + Q_pb - 1) // Q_pb
                block_counts = block_counts.int()
                block_counts_cumsum = block_counts.view(-1).cumsum(-1).view(N, H, C).int()
                indx_maps = torch.ones(
                    (block_counts.sum(), 4),
                    device=Q.device,
                    dtype=torch.int32
                )
                counts_cumsum = counts.cumsum(-1).int()
                total_blocks<a id="change"> = </a><a id="change">block_counts.sum().item()</a>

            &#47&#47 Actually perform the dot product
            ClusteredSparseDotProduct.dot[device.type](
                Q,</code></pre>