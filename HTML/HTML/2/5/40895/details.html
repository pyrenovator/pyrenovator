<html><h3>Pattern ID :40895
</h3><img src='115283060.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    

    def __init__(self, learning_rate, momentum=0.0, *args, **kwargs):
        <a id="change">super()</a>.__init__(learning_rate, momentum, *args, **kwargs)


class Lamb(object):</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self, learning_rate=0.01, momentum=0.0, nesterov=False, weight_decay=0.0, grad_clip=None):
        self.learning_rate = learning_rate
        self.momentum = momentum
        <a id="change">if weight_decay &lt; 0.0</a>:
            raise ValueError("weight_decay should not smaller than 0.0, but got {}".format(weight_decay))
        self.weight_decay = float(weight_decay)
        self.grad_clip = grad_clip
        self.nesterov = nesterov
        self.sgd<a id="change"> = </a><a id="change">tf.optimizers.SGD(learning_rate=self.learning_rate, momentum=self.momentum, nesterov=self.nesterov)</a>

    def apply_gradients(self, grads_and_vars):
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL314' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 115283060</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e</div><div id='time'> Time: 2022-03-25</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: Momentum</div><div id='n_method'> N Class Name: Momentum</div><div id='m_method'> M Method Name: __init__(6)</div><div id='n_method'> N Method Name: __init__(3)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: tf.compat.v1.train.MomentumOptimizer</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 314</div><div id='m_end'> M End Line: 314</div><div id='n_start'> N Start Line: 617</div><div id='n_end'> N End Line: 625</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            self.optimizer=torch.optim.SGD(self.classifier.parameters(), self.learning_rate)

    def __new__(cls, **kwargs):
        x = <a id="change">super(</a>Optimizer, cls<a id="change">)</a>.__new__(**kwargs)
        return x.optimizer

</code></pre><h3>After Change</h3><pre><code class='java'>
            self.optimizer=torch.optim.ASGD(self.classifier.parameters(), self.learning_rate)
        if self.type==&quotRMSprop&quot:
            self.optimizer=torch.optim.RMSprop(self.classifier.parameters(), self.learning_rate)
        <a id="change">if self.type==&quotSGD&quot</a>:
            self.optimizer<a id="change">=</a><a id="change">torch.optim.SGD(</a>self.classifier.parameters(), self.learning_rate<a id="change">)</a>
        return self.optimizer


def create_loss_function(type):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/radtorch/radtorch/commit/74eba02269194c439ff9240c292170a18abc83c0#diff-203dbc007b1e7a27456d652e0fd87290400c8cf18342d22638277a19a4266119L80' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 115283059</div><div id='project'> Project Name: radtorch/radtorch</div><div id='commit'> Commit Name: 74eba02269194c439ff9240c292170a18abc83c0</div><div id='time'> Time: 2020-04-06</div><div id='author'> Author: elbanan@users.noreply.github.com</div><div id='file'> File Name: radtorch/test.py</div><div id='m_class'> M Class Name: Optimizer</div><div id='n_method'> N Class Name: Optimizer</div><div id='m_method'> M Method Name: __new__(1)</div><div id='n_method'> N Method Name: __new__(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: radtorch/test.py</div><div id='n_file'> N File Name: radtorch/test.py</div><div id='m_start'> M Start Line: 81</div><div id='m_end'> M End Line: 82</div><div id='n_start'> N Start Line: 64</div><div id='n_end'> N End Line: 74</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    

    def __init__(self, learning_rate=0.01, momentum=0.0, nesterov=False, *args, **kwargs):
        <a id="change">super()</a>.__init__(learning_rate, momentum, nesterov, *args, **kwargs)


class Momentum(tf.compat.v1.train.MomentumOptimizer):</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self, learning_rate=0.01, momentum=0.0, weight_decay=0.0, grad_clip=None):
        self.learning_rate = learning_rate
        self.momentum = momentum
        <a id="change">if weight_decay &lt; 0.0</a>:
            raise ValueError("weight_decay should not smaller than 0.0, but got {}".format(weight_decay))
        self.weight_decay = float(weight_decay)
        self.grad_clip = grad_clip
        self.sgd<a id="change"> = </a><a id="change">tf.optimizers.SGD(learning_rate=self.learning_rate, momentum=self.momentum, nesterov=False)</a>

    def apply_gradients(self, grads_and_vars):
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL276' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 115283058</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e</div><div id='time'> Time: 2022-03-25</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: SGD</div><div id='n_method'> N Class Name: SGD</div><div id='m_method'> M Method Name: __init__(5)</div><div id='n_method'> N Method Name: __init__(4)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: tf.optimizers.SGD</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 277</div><div id='m_end'> M End Line: 277</div><div id='n_start'> N Start Line: 548</div><div id='n_end'> N End Line: 554</div><BR>