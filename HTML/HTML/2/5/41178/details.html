<html><h3>Pattern ID :41178
</h3><img src='116172636.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        for ind, local_heads in zip(range(depth), n_local_attn_heads):
            attn = SelfAttention(dim, depth, max_seq_len, heads, local_heads, window_size, causal = causal, local_attn_window_size = local_attn_window_size, attn_dropout = attn_dropout, dropout = attn_layer_dropout, kmeans_ema_decay = kmeans_ema_decay, commitment_factor = commitment_factor)
            ff = Chunk(ff_chunks, <a id="change">FeedForward(</a>dim<a id="change">, dropout = ff_dropout, glu = ff_glu)</a>, along_dim=1)

            attn, ff = map(fn_wrapper, (attn, ff))
            layers.append(nn.ModuleList([attn, ff]))</code></pre><h3>After Change</h3><pre><code class='java'>
        get_context_attn = lambda: SelfAttention(dim, depth, max_seq_len, heads, 0, window_size, local_attn_window_size = local_attn_window_size, attn_dropout = attn_dropout, dropout = attn_layer_dropout, kmeans_ema_decay = kmeans_ema_decay, commitment_factor = commitment_factor, receives_context = True, context_window_size = context_window_size)
        get_context_ff = lambda: Chunk(ff_chunks, FeedForward(dim, dropout = ff_dropout, glu = ff_glu), along_dim=1)

        <a id="change">if weight_tie</a>:
            assert len(set(n_local_attn_heads)) == 1, &quotyou can only weight tie if number of local attention heads for all layers is the same&quot
            get_attn<a id="change">, get_ff, get_context_attn, get_context_ff = </a>map(cache_fn, (get_attn, get_ff, get_context_attn, get_context_ff))

        for ind, local_heads in zip(range(depth), n_local_attn_heads):
            attn = get_attn(local_heads)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/routing-transformer/commit/899fbf85db9b5f734283552132c7b77265f64d2f#diff-1a96ccaa437a86e24c768ea078dd19db87f8694dae807ecbb231a703aba4702dL588' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116172636</div><div id='project'> Project Name: lucidrains/routing-transformer</div><div id='commit'> Commit Name: 899fbf85db9b5f734283552132c7b77265f64d2f</div><div id='time'> Time: 2020-05-27</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: routing_transformer/routing_transformer.py</div><div id='m_class'> M Class Name: RoutingTransformer</div><div id='n_method'> N Class Name: RoutingTransformer</div><div id='m_method'> M Method Name: __init__(21)</div><div id='n_method'> N Method Name: __init__(20)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: routing_transformer/routing_transformer.py</div><div id='n_file'> N File Name: routing_transformer/routing_transformer.py</div><div id='m_start'> M Start Line: 588</div><div id='m_end'> M End Line: 599</div><div id='n_start'> N Start Line: 591</div><div id='n_end'> N End Line: 623</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                PreNorm(latent_dim, Attention(latent_dim, input_dim, dropout = attn_dropout), context_dim = input_dim),
                PreNorm(latent_dim, FeedForward(latent_dim, dropout = ff_dropout)),
                PreNorm(latent_dim, Attention(latent_dim, dropout = attn_dropout)),
                PreNorm(latent_dim, <a id="change">FeedForward(</a>latent_dim<a id="change">, dropout = ff_dropout)</a>)
            ]))

        self.to_logits = nn.Linear(latent_dim, num_classes)</code></pre><h3>After Change</h3><pre><code class='java'>
        get_latent_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, dropout = attn_dropout))
        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim, dropout = ff_dropout))

        <a id="change">if weight_tie_layers</a>:
            get_cross_attn<a id="change">, get_cross_ff, get_latent_attn, get_latent_ff = </a>map(cache_fn, (get_cross_attn, get_cross_ff, get_latent_attn, get_latent_ff))

        self.layers = nn.ModuleList([])
        for _ in range(depth):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/perceiver-pytorch/commit/f0455b6ff59331de8151bf659b62ddf97ac802bd#diff-3a107568743779cad64d2e3582fceaafb7b32d2fd87d09209dce56be844a44a4L91' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116172638</div><div id='project'> Project Name: lucidrains/perceiver-pytorch</div><div id='commit'> Commit Name: f0455b6ff59331de8151bf659b62ddf97ac802bd</div><div id='time'> Time: 2021-03-04</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: perceiver_pytorch/perceiver_pytorch.py</div><div id='m_class'> M Class Name: Perceiver</div><div id='n_method'> N Class Name: Perceiver</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: perceiver_pytorch/perceiver_pytorch.py</div><div id='n_file'> N File Name: perceiver_pytorch/perceiver_pytorch.py</div><div id='m_start'> M Start Line: 115</div><div id='m_end'> M End Line: 120</div><div id='n_start'> N Start Line: 127</div><div id='n_end'> N End Line: 142</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        blocks = []
        for _ in range(depth):
            attn = LSHSelfAttention(emb, heads, bucket_size, n_hashes, causal = causal)
            ff_net = <a id="change">FeedForward(</a>emb<a id="change">)</a>

            f = WithLayerNorm(emb, attn)
            g = Chunk(ff_chunks, WithLayerNorm(emb, ff_net), along_dim = -2)
            blocks.append(ReversibleBlock(f, g, split_along_dim=-1))</code></pre><h3>After Change</h3><pre><code class='java'>
        get_attn = lambda: LSHSelfAttention(emb, heads, bucket_size, n_hashes, causal = causal)
        get_ff = lambda: FeedForward(emb)

        <a id="change">if weight_tie</a>:
            get_attn<a id="change"> = </a>cache_fn(get_attn)
            get_ff = cache_fn(get_ff)

        blocks = []</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/reformer-pytorch/commit/f83b19cd7b4c7dc0179b5b4fd86103c619420662#diff-777a08056194cc0ee6fd61062317c84e7162c2735a40d98372f35214eec961d4L300' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116172634</div><div id='project'> Project Name: lucidrains/reformer-pytorch</div><div id='commit'> Commit Name: f83b19cd7b4c7dc0179b5b4fd86103c619420662</div><div id='time'> Time: 2020-01-10</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: reformer.py</div><div id='m_class'> M Class Name: Reformer</div><div id='n_method'> N Class Name: Reformer</div><div id='m_method'> M Method Name: __init__(11)</div><div id='n_method'> N Method Name: __init__(10)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: reformer.py</div><div id='n_file'> N File Name: reformer.py</div><div id='m_start'> M Start Line: 309</div><div id='m_end'> M End Line: 310</div><div id='n_start'> N Start Line: 310</div><div id='n_end'> N End Line: 328</div><BR>