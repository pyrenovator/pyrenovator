<html><h3>Pattern ID :31077
</h3><img src='91304241.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        fn_input.detach_()
        args = (fn_input, sequence_input, linear_param0, linear_param1, bn_weight0, bn_bias0, bn_weight1, bn_bias1)
        grad_out = ReversibleRNNFunction._forward_pass(*args)
        <a id="change">return </a><a id="change">torch.autograd.grad(</a>grad_out, args, grad_output<a id="change">)</a> + (None,)


class RevRNN(torch.nn.Module):</code></pre><h3>After Change</h3><pre><code class='java'>
            args = (fn_input, sequence_input, linear_param0, linear_param1, bn_weight0, bn_bias0, bn_weight1, bn_bias1)
            grad_out = ReversibleRNNFunction._forward_pass(*args)
        grad_out.requires_grad_(True)
        grad_out<a id="change"> = </a><a id="change">torch.autograd.grad(</a>grad_out, args, grad_output<a id="change">)</a>
        fn_input.detach_()
        fn_input.requires_grad_(True)
        if not ctx.top:
            ctx.output_list.append(fn_input)
        <a id="change">return </a>grad_out + (None, None)


class RevRNN(torch.nn.Module):</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/homebrewnlp/homebrewnlp/commit/a0f0b4030e607ddb8baa74812668409814dc9a48#diff-c98bba8dbdd18eb4e9fa8ae11c28e350c08ee816fce50f9fb86d61cbcd345276L50' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 91304241</div><div id='project'> Project Name: homebrewnlp/homebrewnlp</div><div id='commit'> Commit Name: a0f0b4030e607ddb8baa74812668409814dc9a48</div><div id='time'> Time: 2020-07-13</div><div id='author'> Author: 39779310+ClashLuke@users.noreply.github.com</div><div id='file'> File Name: module.py</div><div id='m_class'> M Class Name: ReversibleRNNFunction</div><div id='n_method'> N Class Name: ReversibleRNNFunction</div><div id='m_method'> M Method Name: backward(2)</div><div id='n_method'> N Method Name: backward(2)</div><div id='m_parent_class'> M Parent Class: torch.autograd.Function</div><div id='n_parent_class'> N Parent Class: torch.autograd.Function</div><div id='m_file'> M File Name: module.py</div><div id='n_file'> N File Name: module.py</div><div id='m_start'> M Start Line: 50</div><div id='m_end'> M End Line: 58</div><div id='n_start'> N Start Line: 51</div><div id='n_end'> N End Line: 67</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def gradlogP(self, z):
        z_ = z.detach().requires_grad_()
        logp = self.target.log_prob(z_)
        <a id="change">return </a><a id="change">torch.autograd.grad(</a>logp, z_<a id="change">, grad_outputs=torch.ones_like(logp))</a>[0]
</code></pre><h3>After Change</h3><pre><code class='java'>
    def gradlogP(self, z):
        z_ = z.detach().requires_grad_()
        logp = self.target.log_prob(z_)
        grad<a id="change"> = </a><a id="change">torch.autograd.grad(</a>logp, z_<a id="change">, grad_outputs=torch.ones_like(logp))</a>[0]
        if self.max_abs_grad:
            grad = torch.clamp(grad, max=self.max_abs_grad, min=-self.max_abs_grad)
        <a id="change">return </a>grad
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/vincentstimper/normalizing-flows/commit/972d22d8e60c8f2a7644d92471443437015752c3#diff-bb34da99d62a6b8f1bc91987d0cb22f2d97897eb9c720c15b496b85979eb18ccL100' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 91304240</div><div id='project'> Project Name: vincentstimper/normalizing-flows</div><div id='commit'> Commit Name: 972d22d8e60c8f2a7644d92471443437015752c3</div><div id='time'> Time: 2022-09-07</div><div id='author'> Author: l.midgley@instadeep.com</div><div id='file'> File Name: normflows/flows/stochastic.py</div><div id='m_class'> M Class Name: HamiltonianMonteCarlo</div><div id='n_method'> N Class Name: HamiltonianMonteCarlo</div><div id='m_method'> M Method Name: gradlogP(2)</div><div id='n_method'> N Method Name: gradlogP(2)</div><div id='m_parent_class'> M Parent Class: Flow</div><div id='n_parent_class'> N Parent Class: Flow</div><div id='m_file'> M File Name: normflows/flows/stochastic.py</div><div id='n_file'> N File Name: normflows/flows/stochastic.py</div><div id='m_start'> M Start Line: 103</div><div id='m_end'> M End Line: 103</div><div id='n_start'> N Start Line: 107</div><div id='n_end'> N End Line: 110</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    dummy = [torch.zeros_like(o, requires_grad=True) for o in outputs]
    vjp = torch.autograd.grad(outputs, inputs, grad_outputs=dummy, **kwargs)
    <a id="change">return </a><a id="change">torch.autograd.grad(</a>vjp, dummy<a id="change">, grad_outputs=grad_inputs, **kwargs)</a>
</code></pre><h3>After Change</h3><pre><code class='java'>

    dummy_outputs = [torch.zeros_like(o, requires_grad=True) for o in outputs]
    vjp = torch.autograd.grad(outputs, inputs, grad_outputs=dummy_outputs, **kwargs)
    _jvp<a id="change"> = </a><a id="change">torch.autograd.grad(</a>vjp, dummy_outputs<a id="change">, grad_outputs=grad_inputs, **kwargs)</a>
    <a id="change">return </a>convert_none_to_zeros(_jvp, dummy_outputs)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/google-research/torchsde/commit/572dc57172ead17b376e19972a80661c6fd5eac8#diff-781848bd668a0b9312cf47a18dac26f2a66562b68b82760bf9ea09309b23ba2dL164' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 91304243</div><div id='project'> Project Name: google-research/torchsde</div><div id='commit'> Commit Name: 572dc57172ead17b376e19972a80661c6fd5eac8</div><div id='time'> Time: 2020-08-20</div><div id='author'> Author: 12689993+lxuechen@users.noreply.github.com</div><div id='file'> File Name: torchsde/_core/misc.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: jvp(3)</div><div id='n_method'> N Method Name: jvp(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: torchsde/_core/misc.py</div><div id='n_file'> N File Name: torchsde/_core/misc.py</div><div id='m_start'> M Start Line: 172</div><div id='m_end'> M End Line: 174</div><div id='n_start'> N Start Line: 117</div><div id='n_end'> N End Line: 120</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    if torch.is_tensor(inputs):
        inputs = (inputs,)
    _inputs = [torch.as_strided(input_, (), ()) for input_ in inputs]
    <a id="change">return </a><a id="change">torch.autograd.grad(</a>outputs, inputs<a id="change">, **kwargs)</a>


def jvp(outputs, inputs, grad_inputs=None, **kwargs):
    &#47&#47 `torch.autograd.functional.jvp` takes in `func` and requires re-evaluation.</code></pre><h3>After Change</h3><pre><code class='java'>
        inputs = (inputs,)
    _dummy_inputs = [torch.as_strided(i, (), ()) for i in inputs]

    _grad<a id="change"> = </a><a id="change">torch.autograd.grad(</a>outputs, inputs<a id="change">, **kwargs)</a>
    <a id="change">return </a>convert_none_to_zeros(_grad, inputs)


def jvp(outputs, inputs, grad_inputs=None, **kwargs):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/google-research/torchsde/commit/572dc57172ead17b376e19972a80661c6fd5eac8#diff-781848bd668a0b9312cf47a18dac26f2a66562b68b82760bf9ea09309b23ba2dL155' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 91304244</div><div id='project'> Project Name: google-research/torchsde</div><div id='commit'> Commit Name: 572dc57172ead17b376e19972a80661c6fd5eac8</div><div id='time'> Time: 2020-08-20</div><div id='author'> Author: 12689993+lxuechen@users.noreply.github.com</div><div id='file'> File Name: torchsde/_core/misc.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: grad(2)</div><div id='n_method'> N Method Name: grad(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: torchsde/_core/misc.py</div><div id='n_file'> N File Name: torchsde/_core/misc.py</div><div id='m_start'> M Start Line: 157</div><div id='m_end'> M End Line: 161</div><div id='n_start'> N Start Line: 100</div><div id='n_end'> N End Line: 106</div><BR>