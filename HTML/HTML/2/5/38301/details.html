<html><h3>Pattern ID :38301
</h3><img src='109557378.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        data = []
        labels = []
        pointer = self.data_idx
        <a id="change">for data_point</a> in range(self.num_data_points)<a id="change">:
            </a>datum, label = self.dataloader.dataset[pointer]
            data += [datum]
            labels += [torch.as_tensor(label)]
            pointer<a id="change"> += </a>server_payload[&quotdata&quot].classes
            pointer = pointer % len(self.dataloader.dataset)
        data = torch.stack(data).to(**self.setup)
        labels = torch.stack(labels).to(device=self.setup[&quotdevice&quot])</code></pre><h3>After Change</h3><pre><code class='java'>
            for step in range(self.num_local_updates):

                data = user_data[seen_data_idx: seen_data_idx + self.num_data_per_local_update_step]
                labels = user_labels[seen_data_idx: seen_data_idx<a id="change"> + </a>self.num_data_per_local_update_step]
                seen_data_idx += self.num_data_per_local_update_step
                seen_data_idx = seen_data_idx % self.num_data_points

                optimizer.zero_grad()
                &#47&#47 Compute the forward pass
                outputs = self.model(data)
                loss = self.loss(outputs, labels)
                <a id="change">loss.backward()</a>
                optimizer.step()

            &#47&#47 Share differential to server version:
            &#47&#47 This is equivalent to sending the new stuff and letting the server do it, but in line</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/jonasgeiping/breaching/commit/1ab2867fea20551797c9aea8ae67099093ec7180#diff-222118a39af19077a8b428b60923841d31a00f445f3963533670a0eaa87a9924L151' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 109557378</div><div id='project'> Project Name: jonasgeiping/breaching</div><div id='commit'> Commit Name: 1ab2867fea20551797c9aea8ae67099093ec7180</div><div id='time'> Time: 2021-10-01</div><div id='author'> Author: jonas.geiping@googlemail.com</div><div id='file'> File Name: breaching/cases/users.py</div><div id='m_class'> M Class Name: UserMultiStep</div><div id='n_method'> N Class Name: UserMultiStep</div><div id='m_method'> M Method Name: compute_local_updates(2)</div><div id='n_method'> N Method Name: compute_local_updates(2)</div><div id='m_parent_class'> M Parent Class: UserSingleStep</div><div id='n_parent_class'> N Parent Class: UserSingleStep</div><div id='m_file'> M File Name: breaching/cases/users.py</div><div id='n_file'> N File Name: breaching/cases/users.py</div><div id='m_start'> M Start Line: 151</div><div id='m_end'> M End Line: 187</div><div id='n_start'> N Start Line: 158</div><div id='n_end'> N End Line: 200</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 the non-checkpointed run.
        output_not_checkpointed = out.data.clone()
        grad_not_checkpointed = {}
        <a id="change">for </a>name, <a id="change">param</a> in model.named_parameters()<a id="change">:
            </a>grad_not_checkpointed[name]<a id="change"> = </a>param.grad.data.clone()

        model.enable_gradient_checkpointing()
        out = model(**inputs_dict).sample</code></pre><h3>After Change</h3><pre><code class='java'>
        model.zero_grad()

        labels = torch.randn_like(out)
        loss = (out<a id="change"> - </a>labels).mean()
        <a id="change">loss.backward()</a>

        &#47&#47 re-instantiate the model now enabling gradient checkpointing
        model_2 = self.model_class(**init_dict)
        &#47&#47 clone model</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/huggingface/diffusers/commit/22963ed82682465b5fdfd1bd474e1b0f2579b4db#diff-e65cd8679b47855997f7a1e7b0bc1db8231acd2168d2cb6616d03d1fc201c1d7L270' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 109557376</div><div id='project'> Project Name: huggingface/diffusers</div><div id='commit'> Commit Name: 22963ed82682465b5fdfd1bd474e1b0f2579b4db</div><div id='time'> Time: 2022-10-10</div><div id='author'> Author: patrick.v.platen@gmail.com</div><div id='file'> File Name: tests/test_models_unet.py</div><div id='m_class'> M Class Name: UNet2DConditionModelTests</div><div id='n_method'> N Class Name: UNet2DConditionModelTests</div><div id='m_method'> M Method Name: test_gradient_checkpointing(1)</div><div id='n_method'> N Method Name: test_gradient_checkpointing(1)</div><div id='m_parent_class'> M Parent Class: unittest.TestCase,ModelTesterMixin</div><div id='n_parent_class'> N Parent Class: unittest.TestCase,ModelTesterMixin</div><div id='m_file'> M File Name: tests/test_models_unet.py</div><div id='n_file'> N File Name: tests/test_models_unet.py</div><div id='m_start'> M Start Line: 273</div><div id='m_end'> M End Line: 333</div><div id='n_start'> N Start Line: 273</div><div id='n_end'> N End Line: 308</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            train_losses_this_epoch.append(float(grad_accum[-1]))
            if len(grad_accum) % gradient_accumulation == 0:
                train_loss = 0.0
                <a id="change">for accum_loss</a> in grad_accum<a id="change">:
                    </a>train_loss<a id="change"> += </a>accum_loss
                train_loss /= len(grad_accum)
                grad_accum = list()
                optimizer.zero_grad()</code></pre><h3>After Change</h3><pre><code class='java'>
                             train_datapoint[2].to(device),
                             train_datapoint[3].to(device))
            train_losses_this_epoch.append(float(train_loss))
            <a id="change">(train_loss / gradient_accumulation).backward()</a>
            del train_loss
            grad_accum += 1
            if grad_accum % gradient_accumulation == 0:
                grad_accum = 0</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/b3a6d9d487ef8387f96cc83cf6d4a4e51077d88c#diff-3733df8b039701cd2c523886f2a148aaa4acda8b5a48effada89068247bad70eL54' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 109557374</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: b3a6d9d487ef8387f96cc83cf6d4a4e51077d88c</div><div id='time'> Time: 2021-02-24</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: PipelineTransformerTTS_CSS10.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train_loop(9)</div><div id='n_method'> N Method Name: train_loop(9)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: PipelineTransformerTTS_CSS10.py</div><div id='n_file'> N File Name: PipelineTransformerTTS_CSS10.py</div><div id='m_start'> M Start Line: 93</div><div id='m_end'> M End Line: 108</div><div id='n_start'> N Start Line: 93</div><div id='n_end'> N End Line: 106</div><BR>