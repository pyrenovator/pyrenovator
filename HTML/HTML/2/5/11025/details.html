<html><h3>Pattern ID :11025
</h3><img src='37938434.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    :param use_unsigned_symmetric: Whether to use signed/unsigned in symmetric case
    :return: Tuple of delta and offset and num_steps
    
    num_steps = torch.pow(<a id="change">torch.tensor(</a>[2]<a id="change">, device=encoding_min.device)</a>, bitwidth) - 1
    if use_symmetric_encodings and use_strict_symmetric:
        num_steps -= 1

    &#47&#47 NOTE: This assumes that the use_* flags reflect true condition (regardless of the encoding_* values)
    if use_symmetric_encodings and (not use_unsigned_symmetric):
        &#47&#47 signed symmetric
        absmax = torch.max(torch.abs(encoding_min), torch.abs(encoding_max))
        half_num_steps = torch.div(num_steps, 2)

        delta = absmax / torch.floor(half_num_steps)
        offset = -torch.ceil(half_num_steps)
    else:
        delta = (encoding_max - encoding_min) / num_steps
        if use_symmetric_encodings:
            &#47&#47 unsigned symmetric
            offset = encoding_min / delta
        else:
            &#47&#47 asymmetric
            b_zero = torch.round(-encoding_min / delta)
            b_zero = torch.min(num_steps, torch.max(<a id="change">torch.tensor(</a>[0]<a id="change">, device=encoding_min.device)</a>, b_zero))
            offset = torch.tensor(-b_zero, device=encoding_min.device)

    return delta, offset, num_steps</code></pre><h3>After Change</h3><pre><code class='java'>
            zero_tensor = constant_tensor_factory(0., device)
            b_zero = torch.round(-encoding_min / delta)
            b_zero = torch.min(num_steps_tensor, torch.max(zero_tensor, b_zero))
            offset = -<a id="change">b_zero.clone().detach()</a>

    return delta, offset, num_steps_tensor

</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/quic/aimet/commit/d26dbba1690dee6af39dcf88fbea5308ec1b65ce#diff-b2f96b89478a93e69873eb98010efb6e08d1224ae44c6034c14355f04497a669L315' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 37938434</div><div id='project'> Project Name: quic/aimet</div><div id='commit'> Commit Name: d26dbba1690dee6af39dcf88fbea5308ec1b65ce</div><div id='time'> Time: 2022-11-23</div><div id='author'> Author: quic_geunlee@quicinc.com</div><div id='file'> File Name: TrainingExtensions/torch/src/python/aimet_torch/quantsim_straight_through_grad.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: get_computed_encodings(6)</div><div id='n_method'> N Method Name: get_computed_encodings(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: TrainingExtensions/torch/src/python/aimet_torch/quantsim_straight_through_grad.py</div><div id='n_file'> N File Name: TrainingExtensions/torch/src/python/aimet_torch/quantsim_straight_through_grad.py</div><div id='m_start'> M Start Line: 336</div><div id='m_end'> M End Line: 359</div><div id='n_start'> N Start Line: 315</div><div id='n_end'> N End Line: 340</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        obj_energy = x[:, :, 4:5, :, :]
        class_energy = x[:, :, 5:, :, :]

        bbox_xywh = <a id="change">torch.tensor(</a>xywh_energy<a id="change">, device=self.device)</a>

        &#47&#47 Cell offsets C_x and C_y.
        cx = torch.linspace(0, w - 1, w, device=self.device).repeat(h, 1)
        cy = torch.linspace(
            0, h - 1, h, device=self.device
        ).repeat(w, 1).t().contiguous()

        &#47&#47 Get bbox center x and y coordinates.
        bbox_xywh[:, :, 0, :, :].sigmoid_().add_(cx).div_(w)
        bbox_xywh[:, :, 1, :, :].sigmoid_().add_(cy).div_(h)

        &#47&#47 Anchor priors P_w and P_h.
        anchors = self.anchors
        anchor_w = torch.tensor(
            anchors, device=self.device
        )[:, 0].reshape(1, num_anchors, 1, 1)
        anchor_h = torch.tensor(
            anchors, device=self.device
        )[:, 1].reshape(1, num_anchors, 1, 1)

        &#47&#47 Get bbox width and height.
        bbox_xywh[:, :, 2, :, :].exp_().mul_(anchor_w)
        bbox_xywh[:, :, 3, :, :].exp_().mul_(anchor_h)

        &#47&#47 Get objectness and class scores.
        obj_score = torch.tensor(obj_energy, device=self.device).sigmoid()

        class_score = F.softmax(
            <a id="change">torch.tensor(</a>class_energy<a id="change">, device=self.device)</a>, dim=2
        )

        max_class_score, max_class_idx = torch.max(class_score, 2, keepdim=True)</code></pre><h3>After Change</h3><pre><code class='java'>
        bbox_xywh[:, :, 3, :, :].exp_().mul_(anchor_h)

        &#47&#47 Get objectness and class scores.
        obj_score = <a id="change">obj_energy.clone().detach()</a>.sigmoid()
        class_score = F.softmax(class_energy.clone().detach(), dim=2)

        max_class_score, max_class_idx = torch.max(class_score, 2, keepdim=True)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/nrsyed/pytorch-yolov3/commit/e8e0039e215b0e6e77cccf37604ff0004a011793#diff-31cc18a53d5b8fedb96834a67a29230fa9b0941beceb2bf9ce43eb893a3080b9L37' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 37938432</div><div id='project'> Project Name: nrsyed/pytorch-yolov3</div><div id='commit'> Commit Name: e8e0039e215b0e6e77cccf37604ff0004a011793</div><div id='time'> Time: 2020-04-01</div><div id='author'> Author: najam.r.syed@gmail.com</div><div id='file'> File Name: darknet.py</div><div id='m_class'> M Class Name: YOLOLayer</div><div id='n_method'> N Class Name: YOLOLayer</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: darknet.py</div><div id='n_file'> N File Name: darknet.py</div><div id='m_start'> M Start Line: 46</div><div id='m_end'> M End Line: 79</div><div id='n_start'> N Start Line: 46</div><div id='n_end'> N End Line: 77</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Update the values
        bn.eps = 0
        bn.track_running_stats = False
        bn.weight.copy_(<a id="change">torch.tensor(</a>weight<a id="change">, device=bn.weight.device, dtype=bn.weight.dtype)</a>.reshape_as(bn.weight))
        bn.bias.copy_(<a id="change">torch.tensor(</a>bias<a id="change">, device=bn.bias.device, dtype=bn.bias.dtype)</a>.reshape_as(bn.bias))
        bn.running_mean.copy_(torch.zeros(bn.running_mean.shape, device=bn.running_mean.device, dtype=bn.running_mean.dtype).reshape_as(bn.running_mean))
        bn.running_var.copy_(torch.ones(bn.running_var.shape, device=bn.running_var.device, dtype=bn.running_var.dtype).reshape_as(bn.running_var))
</code></pre><h3>After Change</h3><pre><code class='java'>
        bn.eps = 0
        bn.track_running_stats = False
        bn.weight.copy_(weight.clone().detach())
        bn.bias.copy_(<a id="change">bias.clone().detach()</a>)
        bn.running_mean = torch.zeros(bn.running_mean.shape, device=bn.running_mean.device, dtype=bn.running_mean.dtype)
        bn.running_var = torch.ones(bn.running_var.shape, device=bn.running_var.device, dtype=bn.running_var.dtype)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/quic/aimet/commit/c1bbcbc5d3e0bc1d1f4daec202f4713c36a91f50#diff-481ed223eef209da6647ee5a59b88cc044f808238c4215c64ad369756c172e68L527' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 37938406</div><div id='project'> Project Name: quic/aimet</div><div id='commit'> Commit Name: c1bbcbc5d3e0bc1d1f4daec202f4713c36a91f50</div><div id='time'> Time: 2023-04-18</div><div id='author'> Author: quic_ristha@quicinc.com</div><div id='file'> File Name: TrainingExtensions/torch/src/python/aimet_torch/batch_norm_fold.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: convert_batchnorm_parameters(2)</div><div id='n_method'> N Method Name: convert_batchnorm_parameters(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: TrainingExtensions/torch/src/python/aimet_torch/batch_norm_fold.py</div><div id='n_file'> N File Name: TrainingExtensions/torch/src/python/aimet_torch/batch_norm_fold.py</div><div id='m_start'> M Start Line: 539</div><div id='m_end'> M End Line: 548</div><div id='n_start'> N Start Line: 539</div><div id='n_end'> N End Line: 548</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    :param use_unsigned_symmetric: Whether to use signed/unsigned in symmetric case
    :return: Tuple of delta and offset and num_steps
    
    num_steps = torch.pow(<a id="change">torch.tensor(</a>[2]<a id="change">, device=encoding_min.device)</a>, bitwidth) - 1
    if use_symmetric_encodings and use_strict_symmetric:
        num_steps -= 1

    &#47&#47 NOTE: This assumes that the use_* flags reflect true condition (regardless of the encoding_* values)
    if use_symmetric_encodings and (not use_unsigned_symmetric):
        &#47&#47 signed symmetric
        absmax = torch.max(torch.abs(encoding_min), torch.abs(encoding_max))
        half_num_steps = torch.div(num_steps, 2)

        delta = absmax / torch.floor(half_num_steps)
        offset = -torch.ceil(half_num_steps)
    else:
        delta = (encoding_max - encoding_min) / num_steps
        if use_symmetric_encodings:
            &#47&#47 unsigned symmetric
            offset = encoding_min / delta
        else:
            &#47&#47 asymmetric
            b_zero = torch.round(-encoding_min / delta)
            b_zero = torch.min(num_steps, torch.max(<a id="change">torch.tensor(</a>[0]<a id="change">, device=encoding_min.device)</a>, b_zero))
            offset = torch.tensor(-b_zero, device=encoding_min.device)

    return delta, offset, num_steps</code></pre><h3>After Change</h3><pre><code class='java'>
            zero_tensor = constant_tensor_factory(0., device)
            b_zero = torch.round(-encoding_min / delta)
            b_zero = torch.min(num_steps_tensor, torch.max(zero_tensor, b_zero))
            offset = -<a id="change">b_zero.clone().detach()</a>

    return delta, offset, num_steps_tensor

</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/quic/aimet/commit/d26dbba1690dee6af39dcf88fbea5308ec1b65ce#diff-b2f96b89478a93e69873eb98010efb6e08d1224ae44c6034c14355f04497a669L319' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 37938421</div><div id='project'> Project Name: quic/aimet</div><div id='commit'> Commit Name: d26dbba1690dee6af39dcf88fbea5308ec1b65ce</div><div id='time'> Time: 2022-11-23</div><div id='author'> Author: quic_geunlee@quicinc.com</div><div id='file'> File Name: TrainingExtensions/torch/src/python/aimet_torch/quantsim_straight_through_grad.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: get_computed_encodings(6)</div><div id='n_method'> N Method Name: get_computed_encodings(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: TrainingExtensions/torch/src/python/aimet_torch/quantsim_straight_through_grad.py</div><div id='n_file'> N File Name: TrainingExtensions/torch/src/python/aimet_torch/quantsim_straight_through_grad.py</div><div id='m_start'> M Start Line: 336</div><div id='m_end'> M End Line: 359</div><div id='n_start'> N Start Line: 315</div><div id='n_end'> N End Line: 340</div><BR>