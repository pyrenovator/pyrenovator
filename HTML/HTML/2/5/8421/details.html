<html><h3>Pattern ID :8421
</h3><img src='29413538.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    
    shape = distogram.shape
    n_bins = torch.ones(shape[-1] + 1) * min_t
    n_bins[1:]<a id="change"> = </a>torch.tensor(bins)
    &#47&#47 center - median
    cum_dist = torch.cumsum(distogram, dim=-1)
    central  = torch.searchsorted(cum_dist, 0.5)
    for i in range(shape[-1]):
        central[central==i] = (n_bins[i]+n_bins[i+1])/2
    &#47&#47 mask diagonal to 0 dist
    central[np.arange(shape[-2]), np.arange(shape[-3])] = 0.
    &#47&#47 provide weights
    weights<a id="change"> = </a><a id="change">torch.ones_like(</a>central<a id="change">)</a>
    return central, weights

def mds_torch(dist_mat, weights=None, iters=10, tol=1e-5, verbose=2):
     Gets distance matrix. Outputs 3d. See below for wrapper. </code></pre><h3>After Change</h3><pre><code class='java'>
        TODO: return confidence/weights
    
    shape = distogram.shape
    bins  = bins.to(distogram.device) - (<a id="change">bins[1]</a>-bins[2])/2
    &#47&#47 calculate measures of centrality and dispersion
    if center == "median":
        cum_dist = torch.cumsum(distogram, dim=-1)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/alphafold2/commit/5013886fc413e143b7f3341db644d320a11c3804#diff-09d0cbaf891d623ac245ccf84485a687b554217349ca55557c60b33234b969aeL110' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 29413538</div><div id='project'> Project Name: lucidrains/alphafold2</div><div id='commit'> Commit Name: 5013886fc413e143b7f3341db644d320a11c3804</div><div id='time'> Time: 2021-01-14</div><div id='author'> Author: ericacaide1@gmail.com</div><div id='file'> File Name: utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: center_distogram_torch(5)</div><div id='n_method'> N Method Name: center_distogram_torch(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: utils.py</div><div id='n_file'> N File Name: utils.py</div><div id='m_start'> M Start Line: 119</div><div id='m_end'> M End Line: 130</div><div id='n_start'> N Start Line: 110</div><div id='n_end'> N End Line: 139</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 Construct the target coordinates
    &#47&#47 The target coordinates do not require gradients
    pos = torch.arange(h * w, **dkwargs)
    pos_i<a id="change"> = </a>(pos // w).float()
    pos_j = (pos % w).float()
    &#47&#47 Map the target coordinates to the source coordinates
    &#47&#47 This implements the backward warping
    pos_tar = torch.stack([pos_j, pos_i, torch.ones_like(pos_i)], dim=0)
    pos_src = torch.matmul(m.inverse(), pos_tar)
    pos_src = pos_src[:2] / pos_src[-1, :]
    &#47&#47 Out of the image
    pos_bound = pos_src.new_tensor([x.size(-1), x.size(-2)]) - 0.5
    pos_bound.unsqueeze_(-1)
    pos_in = torch.logical_and(pos_src.ge(-0.5), pos_src.lt(pos_bound))
    pos_in = pos_in.all(0)
    &#47&#47 Remove the outside region
    pos_src = pos_src[..., pos_in]

    kernels = {&quotnearest&quot: 1, &quotbilinear&quot: 2, &quotbicubic&quot: 4}
    if isinstance(kernel, str):
        if kernel in kernels:
            kernel_size = kernels[kernel]
        else:
            raise ValueError(&quotkernel: {} is not supported!&quot.format(kernel))

        if kernel_size % 2 == 0:
            pos_discrete = pos_src.ceil().long()
            pos_frac = pos_src - pos_src.floor()
        else:
            pos_discrete = pos_src.round().long()
            pos_frac<a id="change"> = </a><a id="change">torch.ones_like(</a>pos_src<a id="change">)</a>

        pad = kernel_size // 2
        &#47&#47 (2, 1, HW)
        pos_frac.unsqueeze_(1)</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 Construct the target coordinates
    &#47&#47 The target coordinates do not require gradients
    pos = torch.arange(sizes[0] * sizes[1], **dkwargs)
    pos_i = (pos // <a id="change">sizes[1]</a>).float()
    pos_j = (pos % sizes[1]).float()
    &#47&#47 Map the target coordinates to the source coordinates
    &#47&#47 This implements the backward warping</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thstkdgus35/bicubic_pytorch/commit/eb261bd72c7d717dab243fa09c6be01c8cdce6cb#diff-b450ef659f3c8d1a1deba7ce15262780c34c49f50a06e98c32aaafb14564c273L34' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 29413545</div><div id='project'> Project Name: thstkdgus35/bicubic_pytorch</div><div id='commit'> Commit Name: eb261bd72c7d717dab243fa09c6be01c8cdce6cb</div><div id='time'> Time: 2020-07-26</div><div id='author'> Author: sonsang35@gmail.com</div><div id='file'> File Name: core_warp.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: warp_by_size(6)</div><div id='n_method'> N Method Name: warp_by_size(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: core_warp.py</div><div id='n_file'> N File Name: core_warp.py</div><div id='m_start'> M Start Line: 38</div><div id='m_end'> M End Line: 104</div><div id='n_start'> N Start Line: 38</div><div id='n_end'> N End Line: 92</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        conved = conved.transpose(0, 1)
        states = (conved * mask).transpose(0, 1)
        states += torch.ones_like(states)
        pooled_states<a id="change"> = </a>F.max_pool1d(input=states, kernel_size=seq_length).contiguous().view(batch_size, hidden_size)
        pooled_states<a id="change"> -= </a><a id="change">torch.ones_like(</a>pooled_states<a id="change">)</a>
        return pooled_states

    def forward(self, 
                hidden_states: torch.Tensor, </code></pre><h3>After Change</h3><pre><code class='java'>
        batch_size, seq_length, hidden_size = hidden_states.size()
        mask = mask.unsqueeze(2)
        states = hidden_states * mask + mask * 100
        pooled_states = <a id="change">torch.max(states, dim=1)[0]</a>
        pooled_states -= 100
        return pooled_states

    def forward(self, </code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thu-keg/omnievent/commit/deca3b2f494a538ebd7d8ad7692a16a5c709cc57#diff-2fb3950489de5eb42fafc7c3fc906a7160035b429c5bbcb1034a4107a98c3090L181' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 29413548</div><div id='project'> Project Name: thu-keg/omnievent</div><div id='commit'> Commit Name: deca3b2f494a538ebd7d8ad7692a16a5c709cc57</div><div id='time'> Time: 2022-12-20</div><div id='author'> Author: penghao20170136@163.com</div><div id='file'> File Name: OmniEvent/aggregation/aggregation.py</div><div id='m_class'> M Class Name: DynamicPooling</div><div id='n_method'> N Class Name: DynamicPooling</div><div id='m_method'> M Method Name: max_pooling(3)</div><div id='n_method'> N Method Name: max_pooling(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: OmniEvent/aggregation/aggregation.py</div><div id='n_file'> N File Name: OmniEvent/aggregation/aggregation.py</div><div id='m_start'> M Start Line: 186</div><div id='m_end'> M End Line: 191</div><div id='n_start'> N Start Line: 196</div><div id='n_end'> N End Line: 199</div><BR>