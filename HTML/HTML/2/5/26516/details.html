<html><h3>Pattern ID :26516
</h3><img src='79575742.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        device = x.get_device()
        dic_z = self.dic.to(device)
        prior = self.prior.to(device)
        if <a id="change">len(</a>proposals<a id="change">)</a> == 2:
            proposals1 = proposals[0].bbox.size(0)
            proposals2<a id="change"> = </a>proposals[1].bbox.size(0)

            x1 = x[:proposals1]
            x2 = x[proposals1:]</code></pre><h3>After Change</h3><pre><code class='java'>
        dic_z = self.dic.to(device)
        prior = self.prior.to(device)

        box_size_list<a id="change"> = </a>[<a id="change">proposal.bbox.size(0</a><a id="change">)</a> for proposal in proposals]
        feature_split = x.split(box_size_list)
        xzs = [self.z_dic(feature_pre_obj, dic_z, prior) for feature_pre_obj in feature_split]
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/wangt-cn/vc-r-cnn/commit/51a882a36e8777715669773236cd8285b0d38986#diff-be62d5533104d4c97b3017be21afc11ab8aaaf90848ecea92447a04acbbb4e06L91' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 79575742</div><div id='project'> Project Name: wangt-cn/vc-r-cnn</div><div id='commit'> Commit Name: 51a882a36e8777715669773236cd8285b0d38986</div><div id='time'> Time: 2020-02-21</div><div id='author'> Author: wangt97@hotmail.com</div><div id='file'> File Name: vc_rcnn/vc_rcnn/modeling/roi_heads/box_head/roi_box_predictors.py</div><div id='m_class'> M Class Name: CausalPredictor</div><div id='n_method'> N Class Name: CausalPredictor</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: vc_rcnn/vc_rcnn/modeling/roi_heads/box_head/roi_box_predictors.py</div><div id='n_file'> N File Name: vc_rcnn/vc_rcnn/modeling/roi_heads/box_head/roi_box_predictors.py</div><div id='m_start'> M Start Line: 96</div><div id='m_end'> M End Line: 123</div><div id='n_start'> N Start Line: 91</div><div id='n_end'> N End Line: 101</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    &#47&#47last_temporal_transform = TemporalSpecificCrop(0, size=1)

    with torch.no_grad():
        for i in range (<a id="change">len(</a>data<a id="change">)</a>):
            &#47&#47 Get center frame
            &#47&#47input, target, vid_path = data._get_video_custom_temporal(i, center_temporal_transform)
            &#47&#47rgb_center_img_tensor = input[0:3]
            &#47&#47mask = input[3]

            &#47&#47 Get full video
            &#47&#47input, target, vid_path = data._get_video_custom_temporal(i)

            &#47&#47 Get beginning, middle, and end
            &#47&#47input_center, target, vid_path = data._get_video_custom_temporal(i, center_temporal_transform)
            &#47&#47input_first, target, vid_path = data._get_video_custom_temporal(i, first_temporal_transform)
            &#47&#47input_last, target, vid_path = data._get_video_custom_temporal(i, last_temporal_transform)
            &#47&#47rgb_center_img_tensor = input_center[0:3]
            &#47&#47mask = (input_center[3] + input_first[3] + input_last[3]) / 2
            
            &#47&#47 Get cropped video
            input, target, vid_path = data.__getitem__(i)   &#47&#47 4 channels x num_frames x H x W
            num_frames = input.shape[1]
            rgb_center_img_tensor = input[:3, num_frames // 2, :, :].unsqueeze(1)  &#47&#47 3 x 1 x H x W
            masks = input[3]  
            mask = torch.mean(masks, dim=0)  &#47&#47 H x W
            &#47&#47mask = torch.ceil(masks)
            &#47&#47mask = torch.round(masks)

            center_img_salient = rgb_center_img_tensor*mask     &#47&#47 3 channels x 1 frame x H x W
            center_img_salient<a id="change"> = </a>center_img_salient.squeeze(1)

            &#47&#47 Put image values into range [0, 1] and then normalize using 
            &#47&#47 mean and std for ImageNet</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47mask[i] = torch.ceil(mask[i])
            &#47&#47mask[i] = torch.round(mask[i])

            batch_size<a id="change"> = </a><a id="change">inputs.size(0</a><a id="change">)</a>

            center_img_salient_batch = []
            for i in range(batch_size):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/rvl-lab-utoronto/video_similarity_search/commit/18ec38547447c76ceed935f971282fce78095ca3#diff-98f1ab4b8aa34886d0f0a9d861b630e102aafdb9b6fed0569e138b1facf9d4efL49' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 79575728</div><div id='project'> Project Name: rvl-lab-utoronto/video_similarity_search</div><div id='commit'> Commit Name: 18ec38547447c76ceed935f971282fce78095ca3</div><div id='time'> Time: 2020-08-28</div><div id='author'> Author: salar77h@gmail.com</div><div id='file'> File Name: clustering/cluster_masks.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: get_embeddings_mask_regions(4)</div><div id='n_method'> N Method Name: get_embeddings_mask_regions(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: clustering/cluster_masks.py</div><div id='n_file'> N File Name: clustering/cluster_masks.py</div><div id='m_start'> M Start Line: 60</div><div id='m_end'> M End Line: 125</div><div id='n_start'> N Start Line: 49</div><div id='n_end'> N End Line: 111</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

            predicted_probs, predicted = self.get_predicted(outputs)
            &#47&#47 if segmentation reshape the predictions and labels
            if <a id="change">len(</a>predicted.shape<a id="change">)</a> &gt; 2:
                predicted<a id="change"> = </a>predicted.T.reshape(
                    predicted.shape[0] * predicted.shape[2] * predicted.shape[3],
                    predicted.shape[1],
                )</code></pre><h3>After Change</h3><pre><code class='java'>
            ):  &#47&#47 if it is multiclass, then we need one hot encoding for the predictions
                &#47&#47print(labels.size(0), self.num_classes)
                one_hot = torch.zeros(labels.size(0), self.num_classes)
                predicted<a id="change"> = </a>predicted.reshape(<a id="change">predicted.size(0</a><a id="change">)</a>)
                one_hot[torch.arange(labels.size(0)), predicted.type(torch.long)] = 1
                predicted = one_hot
                predicted = predicted.to(self.device)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/biasvariancelabs/aitlas/commit/91b0085c154780bb7c833f1ecca31419c87ea510#diff-8c90d9e0305520faf5687b35e4d44d83709cb7e5160a7973610665e9a7133487L217' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 79575760</div><div id='project'> Project Name: biasvariancelabs/aitlas</div><div id='commit'> Commit Name: 91b0085c154780bb7c833f1ecca31419c87ea510</div><div id='time'> Time: 2021-04-26</div><div id='author'> Author: ivica.dimitrovski@yahoo.com</div><div id='file'> File Name: aitlas/base/models.py</div><div id='m_class'> M Class Name: BaseModel</div><div id='n_method'> N Class Name: BaseModel</div><div id='m_method'> M Method Name: evaluate_model(4)</div><div id='n_method'> N Method Name: evaluate_model(4)</div><div id='m_parent_class'> M Parent Class: nn.Module,Configurable</div><div id='n_parent_class'> N Parent Class: nn.Module,Configurable</div><div id='m_file'> M File Name: aitlas/base/models.py</div><div id='n_file'> N File Name: aitlas/base/models.py</div><div id='m_start'> M Start Line: 233</div><div id='m_end'> M End Line: 251</div><div id='n_start'> N Start Line: 221</div><div id='n_end'> N End Line: 280</div><BR>