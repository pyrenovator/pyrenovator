<html><h3>Pattern ID :37595
</h3><img src='108137954.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            check.false(self._use_apex, "Must call wrap_model() before configure_apex_amp.")

            model = model.to(self.device)
            if not self.distributed.size &gt; 1 and <a id="change">self.n_gpus &gt; 1</a>:
                check.eq(
                    self._aggregation_frequency,
                    1,
                    "Please enable `optimized_parallel` to use aggregation "
                    "frequency greater than 1 for single machine multi-GPU "
                    "training.",
                )
                model = nn.DataParallel(model)
                <a id="change">logging.debug("Initialized model for native parallel training."</a><a id="change">)</a>

        model_id = len(self.models)
        self._main_model.__setattr__(f"model_{model_id}", model)
</code></pre><h3>After Change</h3><pre><code class='java'>
            else:
                wrapped_model = model

            <a id="change">self._wrapped_models[wrapped_model]</a> = model
        else:
            wrapped_model = model
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/determined-ai/determined/commit/a35d49a77469711a9b3b7fa2207219d0eb7d10b0#diff-d0efbe04f5f1ad69350b2a3257125b2bec5dfc90a822e95089384fdf0fbd25d8L170' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 108137954</div><div id='project'> Project Name: determined-ai/determined</div><div id='commit'> Commit Name: a35d49a77469711a9b3b7fa2207219d0eb7d10b0</div><div id='time'> Time: 2022-05-03</div><div id='author'> Author: 83614683+azhou-determined@users.noreply.github.com</div><div id='file'> File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='m_class'> M Class Name: PyTorchTrialContext</div><div id='n_method'> N Class Name: PyTorchTrialContext</div><div id='m_method'> M Method Name: wrap_model(2)</div><div id='n_method'> N Method Name: wrap_model(2)</div><div id='m_parent_class'> M Parent Class: det.TrialContext,pytorch._PyTorchReducerContext</div><div id='n_parent_class'> N Parent Class: det.TrialContext,pytorch._PyTorchReducerContext</div><div id='m_file'> M File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='n_file'> N File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='m_start'> M Start Line: 170</div><div id='m_end'> M End Line: 189</div><div id='n_start'> N Start Line: 176</div><div id='n_end'> N End Line: 195</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        raise ValueError(f"Character span %r outside the range of the given tokens.")
    &#47&#47 start_index should now be pointing at the span start index.
    if token_offsets[start_index][0] &gt; character_span[0]:
        if <a id="change">start_index &lt;= 0</a>:
            raise ValueError(f"Character span %r outside the range of the given tokens.")
        &#47&#47 In this case, a tokenization or labeling issue made us go too far - the character span we&quotre looking for
        &#47&#47 actually starts in the previous token. We&quotll back up one. Note that this might have us starting at a None
        &#47&#47 token.
        <a id="change">logger.debug("Bad labelling or tokenization - start offset doesn&quott match"</a><a id="change">)</a>
        start_index -= 1
    if token_offsets[start_index] is None or token_offsets[start_index][0] != character_span[0]:
        error = True
</code></pre><h3>After Change</h3><pre><code class='java'>
            and token_offsets[start_index - 1][1] &gt; character_span[0]
        )
        or (
            <a id="change">token_offsets[start_index]</a> is not None
            and token_offsets[start_index][0] &gt; character_span[0]
        )
    ):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/allenai/allennlp-models/commit/92e6a85fbf9001024c362dbd12c6233aff944266#diff-f3a968f541985c4e5ce4df107823255983c54dd854662b2210bb104ef03f08eeL47' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 108137955</div><div id='project'> Project Name: allenai/allennlp-models</div><div id='commit'> Commit Name: 92e6a85fbf9001024c362dbd12c6233aff944266</div><div id='time'> Time: 2020-04-24</div><div id='author'> Author: dirkg@allenai.org</div><div id='file'> File Name: allennlp_models/rc/common/reader_utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: char_span_to_token_span(2)</div><div id='n_method'> N Method Name: char_span_to_token_span(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: allennlp_models/rc/common/reader_utils.py</div><div id='n_file'> N File Name: allennlp_models/rc/common/reader_utils.py</div><div id='m_start'> M Start Line: 79</div><div id='m_end'> M End Line: 95</div><div id='n_start'> N Start Line: 78</div><div id='n_end'> N End Line: 96</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                                                                                     inp_data, orig_out_data,
                                                                                     opt_params.reg_param, warm_start,
                                                                                     beta, channels_index)
            if cur_iteration == 0 or <a id="change">cur_iteration % 100 == 0</a>:
                <a id="change">logger.debug("After iterations=%d, Total loss=%5f, Recons. loss=%5f, Rounding loss=%5f"</a>,
                             cur_iteration, float(total_loss), float(recon_loss), float(round_loss)<a id="change">)</a>

        wrapper.use_soft_rounding.assign(False)
        hard_rounded_weight = wrapper.adaround_weights()
</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 across the channels index
        channels_index = len(orig_out_data.shape) - 1

        inp_shape = <a id="change">inp_data.shape[1:]</a>
        inp_layer = tf.keras.Input(shape=inp_shape)
        adaround_out_tensor = wrapper(inp_layer)

        &#47&#47 If followed by an activation function</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/quic/aimet/commit/ad43d566c521ad2f61cfbd2c50ae8105b5a574a4#diff-a80969f5ffd7a06a1bfc06d11ee00cce33cb71d4819172fb96e4557dcf0f834fL124' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 108137957</div><div id='project'> Project Name: quic/aimet</div><div id='commit'> Commit Name: ad43d566c521ad2f61cfbd2c50ae8105b5a574a4</div><div id='time'> Time: 2022-12-14</div><div id='author'> Author: quic_cgulecha@quicinc.com</div><div id='file'> File Name: TrainingExtensions/tensorflow/src/python/aimet_tensorflow/keras/adaround/adaround_optimizer.py</div><div id='m_class'> M Class Name: AdaroundOptimizer</div><div id='n_method'> N Class Name: AdaroundOptimizer</div><div id='m_method'> M Method Name: optimize_rounding(5)</div><div id='n_method'> N Method Name: optimize_rounding(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: TrainingExtensions/tensorflow/src/python/aimet_tensorflow/keras/adaround/adaround_optimizer.py</div><div id='n_file'> N File Name: TrainingExtensions/tensorflow/src/python/aimet_tensorflow/keras/adaround/adaround_optimizer.py</div><div id='m_start'> M Start Line: 136</div><div id='m_end'> M End Line: 165</div><div id='n_start'> N Start Line: 203</div><div id='n_end'> N End Line: 225</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>


def train(args):
    is_distributed = <a id="change">len(args.hosts) &gt; 1</a> and args.backend is not None
    logger.debug("Distributed training - {}".format(is_distributed))
    use_cuda = (args.processor == &quotgpu&quot) or (args.num_gpus &gt; 0)
    logger.debug("Number of gpus available - {}".format(args.num_gpus))
    kwargs = {&quotnum_workers&quot: 1, &quotpin_memory&quot: True} if use_cuda else {}
    device = torch.device("cuda" if use_cuda else "cpu")

    if is_distributed:
        &#47&#47 Initialize the distributed environment.
        world_size = len(args.hosts)
        os.environ[&quotWORLD_SIZE&quot] = str(world_size)
        host_rank = args.hosts.index(args.current_host)
        os.environ[&quotRANK&quot] = str(host_rank)
        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)
        logger.info(&quotInitialized the distributed environment: \&quot{}\&quot backend on {} nodes. &quot.format(
            args.backend, dist.get_world_size()) + &quotCurrent host rank is {}. Number of gpus: {}&quot.format(
            dist.get_rank(), args.num_gpus))

    &#47&#47 set the seed for generating random numbers
    torch.manual_seed(args.seed)
    if use_cuda:
        torch.cuda.manual_seed(args.seed)

    train_loader = _get_train_data_loader(args.batch_size, args.data_dir, is_distributed, **kwargs)
    test_loader = _get_test_data_loader(args.test_batch_size, args.data_dir, **kwargs)

    &#47&#47 TODO: assert the logs when we move to the SDK local mode
    logger.debug("Processes {}/{} ({:.0f}%) of train data".format(
        len(train_loader.sampler), len(train_loader.dataset),
        100. * len(train_loader.sampler) / len(train_loader.dataset)
    ))

    logger.debug("Processes {}/{} ({:.0f}%) of test data".format(
        len(test_loader.sampler), len(test_loader.dataset),
        100. * len(test_loader.sampler) / len(test_loader.dataset)
    ))

    model = Net()
    if is_distributed and use_cuda:
        &#47&#47 multi-machine multi-gpu case
        <a id="change">logger.debug("Multi-machine multi-gpu: using DistributedDataParallel."</a><a id="change">)</a>
        &#47&#47 establish host rank and set device on this node
        torch.cuda.set_device(host_rank)
        model.cuda(host_rank)
        &#47&#47 for multiprocessing distributed, the DDP constructor should always set</code></pre><h3>After Change</h3><pre><code class='java'>
        test(model, test_loader, device)
    save_model(model, args.model_dir, args)

    if len(args.hosts) == 1 or <a id="change">os.environ[&quotRANK&quot]</a> == 0:
        assert_can_track_sagemaker_experiments()

</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/aws/deep-learning-containers/commit/53f99fafb1c247634df1eb80ca25587bf6b3d100#diff-7d121effda9eaa4782a76f0efe76bd90c71226eb18fcc5eb04a281e4d9ac5da2L111' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 108137964</div><div id='project'> Project Name: aws/deep-learning-containers</div><div id='commit'> Commit Name: 53f99fafb1c247634df1eb80ca25587bf6b3d100</div><div id='time'> Time: 2023-04-14</div><div id='author'> Author: shx26@pitt.edu</div><div id='file'> File Name: test/sagemaker_tests/pytorch/training/resources/mnist/mnist.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train(1)</div><div id='n_method'> N Method Name: train(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: test/sagemaker_tests/pytorch/training/resources/mnist/mnist.py</div><div id='n_file'> N File Name: test/sagemaker_tests/pytorch/training/resources/mnist/mnist.py</div><div id='m_start'> M Start Line: 112</div><div id='m_end'> M End Line: 195</div><div id='n_start'> N Start Line: 112</div><div id='n_end'> N End Line: 193</div><BR>