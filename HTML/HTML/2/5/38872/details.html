<html><h3>Pattern ID :38872
</h3><img src='110941325.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 setting the device

        <a id="change">if </a>not exists(accelerator) and not exists(device):
            diffusion_prior_device<a id="change"> = </a>next(<a id="change">diffusion_prior.parameters()</a>).device
            self.print(f&quotaccelerator not given, and device not specified: defaulting to device of diffusion prior parameters - {diffusion_prior_device}&quot)
            self.device = diffusion_prior_device
        else:</code></pre><h3>After Change</h3><pre><code class='java'>
            and self.diffusion_prior.clip is not None
            ):
            &#47&#47 Then we need to make sure clip is using the correct precision or else deepspeed will error
            cast_type_map<a id="change"> = </a><a id="change">{
                </a>"fp16": torch.half,
                "bf16": torch.bfloat16,
                "no": torch.float<a id="change">
            }</a>
            precision_type = cast_type_map[accelerator.mixed_precision]
            assert precision_type == torch.float, "DeepSpeed currently only supports float32 precision when using on the fly embedding generation from clip"
            self.diffusion_prior.clip.to(precision_type)
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/dalle2-pytorch/commit/f9423d308b6f36e51152c2c45045ff4ebb308287#diff-617450527162fa367141dbf45e8b201673573479820af0ffc56ba93b7f70947fL177' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 110941325</div><div id='project'> Project Name: lucidrains/dalle2-pytorch</div><div id='commit'> Commit Name: f9423d308b6f36e51152c2c45045ff4ebb308287</div><div id='time'> Time: 2022-07-20</div><div id='author'> Author: 51308183+nousr@users.noreply.github.com</div><div id='file'> File Name: dalle2_pytorch/trainer.py</div><div id='m_class'> M Class Name: DiffusionPriorTrainer</div><div id='n_method'> N Class Name: DiffusionPriorTrainer</div><div id='m_method'> M Method Name: __init__(10)</div><div id='n_method'> N Method Name: __init__(12)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dalle2_pytorch/trainer.py</div><div id='n_file'> N File Name: dalle2_pytorch/trainer.py</div><div id='m_start'> M Start Line: 182</div><div id='m_end'> M End Line: 240</div><div id='n_start'> N Start Line: 177</div><div id='n_end'> N End Line: 246</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            f"Model type {model_type} not implemented yet. Available model types [lstm, transformer]"
        )

    <a id="change">if </a>optimizer_name == "SGD":
        optimizer<a id="change"> = </a>optim.SGD(
            <a id="change">model.parameters()</a>, lr=learning_rate, momentum=0.9, nesterov=True
        )
    elif optimizer_name == "Adam":
        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-04)</code></pre><h3>After Change</h3><pre><code class='java'>
    

    if dropout_images_prob is None:
        dropout_images_prob<a id="change"> = </a><a id="change">[</a>0.0, 0.0, 0.0, 0.0, 0.0<a id="change"></a>]

    model: Tedd1104ModelPL = Tedd1104ModelPL(
        resnet=resnet,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/ikergarcia1996/self-driving-car-in-video-games/commit/a1ab4ad40dbe62c5f2084ead328936bab9bdde02#diff-ed183d67207df065a11e1289f19d34cc2abbc5448dea952683cfe9728c342b95L371' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 110941334</div><div id='project'> Project Name: ikergarcia1996/self-driving-car-in-video-games</div><div id='commit'> Commit Name: a1ab4ad40dbe62c5f2084ead328936bab9bdde02</div><div id='time'> Time: 2021-09-10</div><div id='author'> Author: igarciaf896@gmail.com</div><div id='file'> File Name: train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train_new_model(30)</div><div id='n_method'> N Method Name: train_new_model(34)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: train.py</div><div id='n_file'> N File Name: train.py</div><div id='m_start'> M Start Line: 372</div><div id='m_end'> M End Line: 531</div><div id='n_start'> N Start Line: 96</div><div id='n_end'> N End Line: 203</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                    )

    def _validate_min_wrap_condition(self, should_wrap: Dict, mod_paths: List) -&gt; Dict:
        <a id="change">if </a>any([w for w in should_wrap.values()]):
            return should_wrap
        init_layer_sizes = {}
        for modp in mod_paths:
            init_layer_sizes[modp] = sum(p.numel() for p in <a id="change">self.fts_handle.pl_module.get_submodule(modp).parameters()</a>)
        force_wrap_m<a id="change"> = </a>max(init_layer_sizes, key=lambda k: init_layer_sizes[k])
        &#47&#47 TODO: update this warning message with override options
        auto_wrap_ref = "The auto_wrap_policy you have specified would not"
        rank_zero_warn(</code></pre><h3>After Change</h3><pre><code class='java'>
            self._gen_ft_sched_module_map()

    def _validate_min_wrap_condition(self) -&gt; Optional[str]:
        wrapped_statuses<a id="change"> = </a><a id="change">[]</a>
        for m in self._ft_schedule_module_map[0]:
            is_wrapped = isinstance(self.fts_handle.pl_module.get_submodule(m), FullyShardedDataParallel)
            wrapped_statuses.append(is_wrapped)
        if not any(wrapped_statuses):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/speediedan/finetuning-scheduler/commit/8644475a355e890b7c656a60df6cf545b3a81d45#diff-e6c1e09a6939512d68c7696b32693d3813f5329e168a46b42ba377c9375a046dL263' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 110941330</div><div id='project'> Project Name: speediedan/finetuning-scheduler</div><div id='commit'> Commit Name: 8644475a355e890b7c656a60df6cf545b3a81d45</div><div id='time'> Time: 2022-12-16</div><div id='author'> Author: danny.dale@gmail.com</div><div id='file'> File Name: src/finetuning_scheduler/strategy_adapters/fsdp.py</div><div id='m_class'> M Class Name: FSDPStrategyAdapter</div><div id='n_method'> N Class Name: FSDPStrategyAdapter</div><div id='m_method'> M Method Name: _validate_min_wrap_condition(1)</div><div id='n_method'> N Method Name: _validate_min_wrap_condition(3)</div><div id='m_parent_class'> M Parent Class: StrategyAdapter</div><div id='n_parent_class'> N Parent Class: StrategyAdapter</div><div id='m_file'> M File Name: src/finetuning_scheduler/strategy_adapters/fsdp.py</div><div id='n_file'> N File Name: src/finetuning_scheduler/strategy_adapters/fsdp.py</div><div id='m_start'> M Start Line: 263</div><div id='m_end'> M End Line: 278</div><div id='n_start'> N Start Line: 261</div><div id='n_end'> N End Line: 276</div><BR>