<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    gen_loss = gen_loss.cuda()
    optimizer_g.zero_grad()
    gen_loss.backward()
    <a id="change">optimizer_g.step()</a>

    &#47&#47 Computing discriminator loss
    if (GAN_FLAG):
        t_discrim_fake_loss = torch.log(1 - tdiscrim_fake_output + args.EPS)</code></pre><h3>After Change</h3><pre><code class='java'>
    Frame_t_pre = r_inputs[:, 0:-1, :, :, :]
    Frame_t = r_inputs[:, 1:, :, :, :]
    &#47&#47 Reshaping the fnet input and passing it to the model
    <a id="change">with autocast</a><a id="change">():

        </a>fnet_input = torch.reshape(Frame_t_pre, (
            args.batch_size * (inputimages - 1), output_channel, args.crop_size, args.crop_size))
        &#47&#47 Preparing generator input
        gen_flow = upscale_four(fnet_input * 4.)

        gen_flow = torch.reshape(gen_flow[:, 0:2],
                                 (args.batch_size, (inputimages - 1), 2, args.crop_size * 4, args.crop_size * 4))
        input_frames = torch.reshape(Frame_t,
                                     (args.batch_size * (inputimages - 1), output_channel, args.crop_size,
                                      args.crop_size))
        s_input_warp = F.grid_sample(torch.reshape(Frame_t_pre, (
            args.batch_size * (inputimages - 1), output_channel, args.crop_size, args.crop_size)),
                                     torch.reshape(Frame_t[:, :, 0:2], (args.batch_size * (inputimages - 1), 32, 32, 2)))

        input0 = torch.cat(
            (r_inputs[:, 0, :, :, :], torch.zeros(size=(args.batch_size, 3 * 4 * 4, args.crop_size, args.crop_size),
                                                  dtype=torch.float32).cuda()), dim=1)
        &#47&#47 Passing inputs into model and reshaping output
        gen_pre_output = generator_F(input0.detach())
        gen_pre_output = gen_pre_output.view(args.batch_size, 3, args.crop_size * 4, args.crop_size * 4)
        gen_outputs.append(gen_pre_output)
        &#47&#47 Getting outputs of generator for each frame
        for frame_i in range(inputimages - 1):
            cur_flow = gen_flow[:, frame_i, :, :, :]
            cur_flow = cur_flow.view(args.batch_size, args.crop_size * 4, args.crop_size * 4, 2)

            gen_pre_output_warp = F.grid_sample(gen_pre_output, cur_flow.half())
            gen_warppre.append(gen_pre_output_warp)

            gen_pre_output_warp = preprocessLr(deprocess(gen_pre_output_warp))
            gen_pre_output_reshape = gen_pre_output_warp.view(args.batch_size, 3, args.crop_size, 4, args.crop_size, 4)
            gen_pre_output_reshape = gen_pre_output_reshape.permute(0, 1, 3, 5, 2, 4)

            gen_pre_output_reshape = torch.reshape(gen_pre_output_reshape,
                                                   (args.batch_size, 3 * 4 * 4, args.crop_size, args.crop_size))
            inputs = torch.cat((r_inputs[:, frame_i + 1, :, :, :], gen_pre_output_reshape), dim=1)
            gen_output = generator_F(inputs.detach())
            gen_outputs.append(gen_output)
            gen_pre_output = gen_output
            gen_pre_output = gen_pre_output.view(args.batch_size, 3, args.crop_size * 4, args.crop_size * 4)
        &#47&#47 Converting list of gen outputs and reshaping
        gen_outputs = torch.stack(gen_outputs, dim=1)
        gen_outputs = gen_outputs.view(args.batch_size, inputimages, 3, args.crop_size * 4, args.crop_size * 4)

        gen_warppre = torch.stack(gen_warppre, dim=1)

        gen_warppre = gen_warppre.view(args.batch_size, inputimages - 1, 3, args.crop_size * 4, args.crop_size * 4)

        s_gen_output = torch.reshape(gen_outputs,
                                     (args.batch_size * inputimages, 3, args.crop_size * 4, args.crop_size * 4))
        s_targets = torch.reshape(r_targets, (args.batch_size * inputimages, 3, args.crop_size * 4, args.crop_size * 4))

        update_list = []
        update_list_name = []

        &#47&#47 Preparing vgg layers
        if args.vgg_scaling &gt; 0.0:
            vgg_layer_labels = [&quotvgg_19/conv2_2&quot, &quotvgg_19/conv3_4&quot, &quotvgg_19/conv4_4&quot]
            gen_vgg = VGG19_slim(s_gen_output, deep_list=vgg_layer_labels)
            target_vgg = VGG19_slim(s_targets, deep_list=vgg_layer_labels)

        if (GAN_FLAG):
            t_size = int(3 * (inputimages // 3))
            t_gen_output = torch.reshape(gen_outputs[:, :t_size, :, :, :],
                                         (args.batch_size * t_size, 3, args.crop_size * 4, args.crop_size * 4))
            t_targets = torch.reshape(r_targets[:, :t_size, :, :, :],
                                      (args.batch_size * t_size, 3, args.crop_size * 4, args.crop_size * 4))
            t_batch = args.batch_size * t_size // 3

            &#47&#47 Preparing inputs for discriminator
            if not args.pingpang:
                fnet_input_back = torch.cat((r_inputs[:, 2:t_size:3, :, :, :], r_inputs[:, 1:t_size:3, :, :, :]), dim=1)
                fnet_input_back = torch.reshape(fnet_input_back,
                                                (t_batch, 2 * output_channel, args.crop_size, args.crop_size))



                gen_flow_back = upscale_four(fnet_input_back[:, 0:2] * 4.0)

                gen_flow_back = torch.reshape(gen_flow_back,
                                              (args.batch_size, t_size // 3, 2, args.crop_size * 4, args.crop_size * 4))

                T_inputs_VPre_batch = identity(gen_flow[:, 0:t_size:3, :, :, :])
                T_inputs_V_batch = torch.zeros_like(T_inputs_VPre_batch)
                T_inputs_VNxt_batch = preprocess(gen_flow_back)

            else:
                T_inputs_VPre_batch = identity(gen_flow[:, 0:t_size:3, :, :, :])
                T_inputs_V_batch = torch.zeros_like(T_inputs_VPre_batch)
                T_inputs_VNxt_batch = torch.flip(gen_flow, dims=[1])[:, 1: t_size:3, :, :, :]

            T_vel = torch.stack([T_inputs_VPre_batch, T_inputs_V_batch, T_inputs_VNxt_batch], axis=2)
            T_vel = torch.reshape(T_vel, (args.batch_size * t_size, args.crop_size * 4, args.crop_size * 4, 2))
            T_vel = T_vel.detach()

            if args.crop_dt &lt; 1.0:
                crop_size_dt = int(args.crop_size * 4 * args.crop_dt)
                offset_dt = (args.crop_size * 4 - crop_size_dt) // 2
                crop_size_dt = args.crop_size * 4 - offset_dt * 2
                paddings = (offset_dt, offset_dt, offset_dt, offset_dt)
            real_warp0 = F.grid_sample(t_targets, T_vel)

            real_warp = torch.reshape(real_warp0, (t_batch, 9, args.crop_size * 4, args.crop_size * 4))
            if (args.crop_dt &lt; 1.0):
                real_warp = functional.resized_crop(real_warp, offset_dt, offset_dt, crop_size_dt, crop_size_dt,
                                                    [crop_size_dt, crop_size_dt])
            &#47&#47 Passing real inputs to discriminator
            if (args.Dt_mergeDs):
                if (args.crop_dt &lt; 1.0):
                    real_warp = F.pad(real_warp, paddings, "constant")
                before_warp = torch.reshape(t_targets, (t_batch, 9, args.crop_size * 4, args.crop_size * 4))
                t_input = torch.reshape(r_inputs[:, :t_size, :, :, :],
                                        (t_batch, 9, args.crop_size, args.crop_size))
                input_hi = functional.resize(t_input, [args.crop_size * 4, args.crop_size * 4])
                real_warp = torch.cat((before_warp, real_warp, input_hi), dim=1)

                tdiscrim_real_output, real_layers = discriminator_F(real_warp)

            else:
                tdiscrim_real_output = discriminator_F(real_warp.cuda())

            &#47&#47 Reshaping Generator output to pass to Discriminator
            fake_warp0 = F.grid_sample(t_gen_output, T_vel.half())

            fake_warp = torch.reshape(fake_warp0, (t_batch, 9, args.crop_size * 4, args.crop_size * 4))
            if (args.crop_dt &lt; 1.0):
                fake_warp = functional.resized_crop(fake_warp, offset_dt, offset_dt, crop_size_dt, crop_size_dt,
                                                    size=[crop_size_dt, crop_size_dt])
            &#47&#47 Passing generated images to discriminator
            if (args.Dt_mergeDs):
                if (args.crop_dt &lt; 1.0):
                    fake_warp = F.pad(fake_warp, paddings, "constant")
                before_warp = torch.reshape(before_warp, (t_batch, 9, args.crop_size * 4, args.crop_size * 4))
                fake_warp = torch.cat((before_warp, fake_warp, input_hi), dim=1)
                tdiscrim_fake_output, fake_layers = discriminator_F(fake_warp.cuda().detach())

            else:
                tdiscrim_fake_output = discriminator_F(fake_warp.cuda().detach())

            &#47&#47 Computing the layer loss using the VGG network and discriminator outputs
            if (args.D_LAYERLOSS):
                Fix_Range = 0.02
                Fix_margin = 0.0

                sum_layer_loss = 0
                d_layer_loss = 0

                layer_loss_list = []
                layer_n = len(real_layers)
                layer_norm = [12.0, 14.0, 24.0, 48.0, 100.0]
                for layer_i in range(layer_n):
                    real_layer = real_layers[layer_i]
                    false_layer = fake_layers[layer_i]

                    layer_diff = real_layer.detach() - false_layer.detach()
                    layer_loss = torch.mean(torch.sum(torch.abs(layer_diff), dim=[3]))

                    layer_loss_list += [layer_loss]

                    scaled_layer_loss = Fix_Range * layer_loss / layer_norm[layer_i]

                    sum_layer_loss += scaled_layer_loss
                    if Fix_margin &gt; 0.0:
                        d_layer_loss += torch.max(0.0, torch.tensor(Fix_margin - scaled_layer_loss))

                update_list += layer_loss_list
                update_list_name += [("D_layer_%d_loss" % _) for _ in range(layer_n)]
                update_list += [sum_layer_loss]
                update_list_name += ["D_layer_loss_sum"]

                if Fix_margin &gt; 0.0:
                    update_list += [d_layer_loss]
                    update_list_name += ["D_layer_loss_for_D_sum"]
        &#47&#47 Computing the generator and fnet losses
        diff1_mse = s_gen_output - s_targets

        content_loss = torch.mean(torch.sum(torch.square(diff1_mse), dim=[3]))
        update_list += [content_loss]
        update_list_name += ["l2_content_loss"]
        gen_loss = content_loss
        fnet_loss = content_loss

        diff2_mse = input_frames - s_input_warp

        warp_loss = torch.mean(torch.sum(torch.square(diff2_mse), dim=[3]))
        update_list += [warp_loss]
        update_list_name += ["l2_warp_loss"]

        vgg_loss = None
        vgg_loss_list = []
        if args.vgg_scaling &gt; 0.0:
            vgg_wei_list = [1.0, 1.0, 1.0, 1.0]
            vgg_loss = 0
            vgg_layer_n = len(vgg_layer_labels)

            for layer_i in range(vgg_layer_n):
                curvgg_diff = torch.sum(gen_vgg[vgg_layer_labels[layer_i]] * target_vgg[vgg_layer_labels[layer_i]], dim=[3])
                scaled_layer_loss = vgg_wei_list[layer_i] * curvgg_diff
                vgg_loss_list += [curvgg_diff]
                vgg_loss += scaled_layer_loss

            gen_loss += args.vgg_scaling * vgg_loss
            fnet_loss += args.vgg_scaling * vgg_loss.detach()
            vgg_loss_list += [vgg_loss]

            update_list += vgg_loss_list
            update_list_name += ["vgg_loss_%d" % (_ + 2) for _ in range(len(vgg_loss_list) - 1)]
            update_list_name += ["vgg_all"]

        if args.pingpang:
            gen_out_first = gen_outputs[:, 0:args.RNN_N - 1, :, :, :]
            gen_out_last_rev = torch.flip(gen_outputs, dims=[1])[:, :args.RNN_N - 1:1, :, :, :]

            pploss = torch.mean(torch.abs(gen_out_first - gen_out_last_rev))

            if args.pp_scaling &gt; 0:
                gen_loss += pploss.cpu() * args.pp_scaling
                fnet_loss += pploss.cpu() * args.pp_scaling
            update_list += [pploss]
            update_list_name += ["PingPang"]

        if (GAN_FLAG):
            t_adversarial_loss = torch.mean(-torch.log(tdiscrim_fake_output.detach() + args.EPS))
            d_adversarial_loss = torch.mean(-torch.log(tdiscrim_fake_output + args.EPS))
            dt_ratio = torch.min(torch.tensor(args.Dt_ratio_max),
                                 args.Dt_ratio_0 + args.Dt_ratio_add * torch.tensor(Global_step, dtype=torch.float32))

        gen_loss += args.ratio * t_adversarial_loss.cpu()
        fnet_loss += args.ratio * t_adversarial_loss.cpu()
        update_list += [t_adversarial_loss]
        update_list_name += ["t_adversarial_loss"]
        &#47&#47 Computing gradients for Generator and updating weights
        if (args.D_LAYERLOSS):
            gen_loss += sum_layer_loss.cpu() * dt_ratio
        gen_loss = gen_loss.cuda()


        &#47&#47 Computing discriminator loss
        if (GAN_FLAG):
            t_discrim_fake_loss = torch.log(1 - tdiscrim_fake_output + args.EPS)
            t_discrim_real_loss = torch.log(tdiscrim_real_output + args.EPS)

            t_discrim_loss = torch.mean(-(t_discrim_fake_loss + t_discrim_real_loss))
            t_balance = torch.mean(t_discrim_real_loss) + d_adversarial_loss

            update_list += [t_discrim_loss]
            update_list_name += ["t_discrim_loss"]

            update_list += [torch.mean(tdiscrim_real_output), torch.mean(tdiscrim_fake_output)]
            update_list_name += ["t_discrim_real_output", "t_discrim_fake_output"]

            if (args.D_LAYERLOSS and Fix_margin &gt; 0.0):
                discrim_loss = t_discrim_loss + d_layer_loss * dt_ratio

            &#47&#47 Computing gradients for Discriminator and updating weights
            else:
                discrim_loss = t_discrim_loss
            discrim_loss = discrim_loss.cuda()

            tb_exp_averager = EMA(0.99)
            init_average = torch.zeros_like(t_balance)
            tb_exp_averager.register("TB_average", init_average)
            tb = tb_exp_averager.forward("TB_average", t_balance)

            update_list += [gen_loss]
            update_list_name += ["All_loss_Gen"]

            tb_exp_averager.register("Loss_average", init_average)
            update_list_avg = [tb_exp_averager.forward("Loss_average", _) for _ in update_list]
        &#47&#47 Computing gradients for fnet and updating weights
        optimizer_g.zero_grad()
        scaler.scale(gen_loss).backward()
        scaler.step(optimizer_g)
        scaler.update()
        optimizer_d.zero_grad()
        <a id="change">scaler.scale(discrim_loss).backward()</a>
        scaler.step(optimizer_d)
        scaler.update()
        &#47&#47fnet_loss = fnet_loss.cuda()
        &#47&#47fnet_optimizer.zero_grad()</code></pre>