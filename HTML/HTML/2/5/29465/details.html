<html><h3>Pattern ID :29465
</h3><img src='87455575.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            source_bb = source_layer["positional_embedding:0"]  &#47&#47 weights
        else:
            source_bb = source_layer.bb  &#47&#47 layer
        hh = ww = int(<a id="change">tf.math.sqrt(</a>float(source_bb.shape[0])<a id="change">)</a>)
        ss = tf.reshape(source_bb, (hh, ww, source_bb.shape[-1]))  &#47&#47 [hh, ww, num_heads]
        &#47&#47 target_hh = target_ww = int(tf.math.sqrt(float(self.bb.shape[0])))
        tt = tf.image.resize(ss, [self.k_blocks_h, self.k_blocks_w], method=method)  &#47&#47 [target_hh, target_ww, num_heads]</code></pre><h3>After Change</h3><pre><code class='java'>
            source_bb = list(source_layer.values())[0]  &#47&#47 weights
        else:
            source_bb = source_layer.bb  &#47&#47 layer
        source_tt<a id="change"> = </a><a id="change">np.array(</a><a id="change">source_tt.detach()</a><a id="change"> if </a>hasattr(source_tt, "detach")<a id="change"> else </a>source_tt<a id="change">)</a>
        hh = ww = int(float(source_bb.shape[0]) ** 0.5)
        ss = source_bb.reshape((hh, ww, source_bb.shape[-1]))  &#47&#47 [hh, ww, num_heads]
        &#47&#47 target_hh = target_ww = int(float(self.bb.shape[0]) ** 0.5)
        tt = backend.numpy_image_resize(ss, target_shape=[self.k_blocks_h, self.k_blocks_w], method=method)  &#47&#47 [target_hh, target_ww, num_heads]</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/c870bf2e8d3e6b8b0e969d5468d550085414c0cd#diff-419ea771811e38fb2e106cf19724ad992d4d6f2fbae3c511c734548a78b6d63bL66' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 87455575</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: c870bf2e8d3e6b8b0e969d5468d550085414c0cd</div><div id='time'> Time: 2023-02-05</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/levit/levit.py</div><div id='m_class'> M Class Name: MultiHeadPositionalEmbedding</div><div id='n_method'> N Class Name: MultiHeadPositionalEmbedding</div><div id='m_method'> M Method Name: load_resized_weights(3)</div><div id='n_method'> N Method Name: load_resized_weights(3)</div><div id='m_parent_class'> M Parent Class: layers.Layer</div><div id='n_parent_class'> N Parent Class: keras.layers.Layer</div><div id='m_file'> M File Name: keras_cv_attention_models/levit/levit.py</div><div id='n_file'> N File Name: keras_cv_attention_models/levit/levit.py</div><div id='m_start'> M Start Line: 66</div><div id='m_end'> M End Line: 74</div><div id='n_start'> N Start Line: 73</div><div id='n_end'> N End Line: 82</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        else:
            source_tt = source_layer.bb  &#47&#47 layer
        num_heads = source_tt.shape[0]
        source_query_hh = source_query_ww = int(<a id="change">tf.math.sqrt(</a>float(source_tt.shape[1])<a id="change">)</a>)  &#47&#47 assume source weights are all square shape
        source_kv_hh = source_kv_ww = int(tf.math.sqrt(float(source_tt.shape[2])))  &#47&#47 assume source weights are all square shape

        tt = tf.reshape(source_tt, [num_heads, source_query_hh, source_query_ww, source_kv_hh * source_kv_ww])  &#47&#47 resize on query dimension first</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 source_tt = source_layer["pos_emb:0"]  &#47&#47 weights
        else:
            source_tt = source_layer.bb  &#47&#47 layer
        source_tt<a id="change"> = </a><a id="change">np.array(</a><a id="change">source_tt.detach()</a><a id="change"> if </a>hasattr(source_tt, "detach")<a id="change"> else </a>source_tt<a id="change">)</a>

        num_heads = source_tt.shape[0]
        source_query_hh = source_query_ww = int(float(source_tt.shape[1]) ** 0.5)  &#47&#47 assume source weights are all square shape
        source_kv_hh = source_kv_ww = int(float(source_tt.shape[2]) ** 0.5)  &#47&#47 assume source weights are all square shape</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/2ba27b0132168f3590dd4b3bead9edc15a70ba7d#diff-f52477ea03b2a8f18695211ce6217a2ea1e5dd5afaa7adfd75a231c5a04692bcL57' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 87455578</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 2ba27b0132168f3590dd4b3bead9edc15a70ba7d</div><div id='time'> Time: 2023-02-11</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/cmt/cmt.py</div><div id='m_class'> M Class Name: BiasPositionalEmbedding</div><div id='n_method'> N Class Name: BiasPositionalEmbedding</div><div id='m_method'> M Method Name: load_resized_weights(3)</div><div id='n_method'> N Method Name: load_resized_weights(3)</div><div id='m_parent_class'> M Parent Class: layers.Layer</div><div id='n_parent_class'> N Parent Class: keras.layers.Layer</div><div id='m_file'> M File Name: keras_cv_attention_models/cmt/cmt.py</div><div id='n_file'> N File Name: keras_cv_attention_models/cmt/cmt.py</div><div id='m_start'> M Start Line: 59</div><div id='m_end'> M End Line: 74</div><div id='n_start'> N Start Line: 61</div><div id='n_end'> N End Line: 79</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def show_pos_emb(self, rows=1, base_size=2):
        import matplotlib.pyplot as plt

        hh = ww = int(<a id="change">tf.math.sqrt(</a>float(self.bb.shape[0])<a id="change">)</a>)
        ss = tf.reshape(self.bb, (hh, ww, -1)).numpy()
        cols = int(tf.math.ceil(ss.shape[-1] / rows))
        fig, axes = plt.subplots(rows, cols, figsize=(base_size * cols, base_size * rows))</code></pre><h3>After Change</h3><pre><code class='java'>
        import matplotlib.pyplot as plt

        hh = ww = int(float(self.bb.shape[0]) ** 0.5)
        ss = <a id="change">np.array(</a><a id="change">self.bb.detach()</a><a id="change"> if </a>hasattr(self.bb, "detach")<a id="change"> else </a>self.bb<a id="change">)</a>
        ss<a id="change"> = </a>ss.reshape((hh, ww, -1))
        cols = int(math.ceil(ss.shape[-1] / rows))
        fig, axes = plt.subplots(rows, cols, figsize=(base_size * cols, base_size * rows))
        for id, ax in enumerate(axes.flatten()):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/c870bf2e8d3e6b8b0e969d5468d550085414c0cd#diff-419ea771811e38fb2e106cf19724ad992d4d6f2fbae3c511c734548a78b6d63bL76' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 87455576</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: c870bf2e8d3e6b8b0e969d5468d550085414c0cd</div><div id='time'> Time: 2023-02-05</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/levit/levit.py</div><div id='m_class'> M Class Name: MultiHeadPositionalEmbedding</div><div id='n_method'> N Class Name: MultiHeadPositionalEmbedding</div><div id='m_method'> M Method Name: show_pos_emb(3)</div><div id='n_method'> N Method Name: show_pos_emb(3)</div><div id='m_parent_class'> M Parent Class: layers.Layer</div><div id='n_parent_class'> N Parent Class: keras.layers.Layer</div><div id='m_file'> M File Name: keras_cv_attention_models/levit/levit.py</div><div id='n_file'> N File Name: keras_cv_attention_models/levit/levit.py</div><div id='m_start'> M Start Line: 79</div><div id='m_end'> M End Line: 81</div><div id='n_start'> N Start Line: 87</div><div id='n_end'> N End Line: 90</div><BR>