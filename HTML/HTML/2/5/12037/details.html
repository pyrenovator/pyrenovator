<html><h3>Pattern ID :12037
</h3><img src='40705773.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        KW = self.w_k(K)
        KW = torch.einsum("nk,bnc-&gt;bkc", E, KW)
        QW = self.w_q(Q)
        QW<a id="change"> = </a><a id="change">torch.einsum("bnc,bkc-&gt;bnk"</a>, QW, KW<a id="change">)</a>
        P_bar = QW/torch.sqrt(torch.tensor(self.dim, dtype=torch.float32))
        P_bar = P_bar.softmax(dim=-1)
        P_bar = self.dropout(P_bar)
</code></pre><h3>After Change</h3><pre><code class='java'>
        KW = torch.matmul(E, KW)
        KW = torch.transpose(KW, 1, 2)
        QW = self.w_q(Q)
        QW<a id="change"> = </a><a id="change">torch.matmul(</a>QW, KW<a id="change">)</a>

        &#47&#47 TODO: Possibly change the dtype?
        P_bar = QW/torch.sqrt(torch.tensor(self.dim, dtype=torch.float32))
        P_bar = P_bar.softmax(dim=-1)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 5</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tatp22/linformer-pytorch/commit/29cf2aba0b8c0d5509e11f3cf4de621630803857#diff-1ce692e4c944ce1a9eafe989300fabd5878d3e81dff51127062721b098ce8962L43' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 40705773</div><div id='project'> Project Name: tatp22/linformer-pytorch</div><div id='commit'> Commit Name: 29cf2aba0b8c0d5509e11f3cf4de621630803857</div><div id='time'> Time: 2020-06-12</div><div id='author'> Author: peterta@ethz.ch</div><div id='file'> File Name: linformer_pytorch/linformer_pytorch.py</div><div id='m_class'> M Class Name: LinearAttentionHead</div><div id='n_method'> N Class Name: LinearAttentionHead</div><div id='m_method'> M Method Name: forward(6)</div><div id='n_method'> N Method Name: forward(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: linformer_pytorch/linformer_pytorch.py</div><div id='n_file'> N File Name: linformer_pytorch/linformer_pytorch.py</div><div id='m_start'> M Start Line: 43</div><div id='m_end'> M End Line: 52</div><div id='n_start'> N Start Line: 43</div><div id='n_end'> N End Line: 55</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    The details of relative position is explained in: https://arxiv.org/pdf/1803.02155.pdf
    
    B, Nh, H, W, _ = q.shape
    rel_logits<a id="change"> = </a><a id="change">paddlenlp.ops.einsum("b n h w d, m d -&gt; b n h w m"</a>, q, rel_k<a id="change">)</a>
    &#47&#47 Collapse height and heads
    rel_logits = paddle.reshape(rel_logits, [-1, Nh * H, W, 2 * W - 1])
    rel_logits = rel_to_abs(rel_logits)
    rel_logits = paddle.reshape(rel_logits, [-1, Nh, H, W, W])</code></pre><h3>After Change</h3><pre><code class='java'>

def relative_logits_1d(q, rel_k):
    B, Nh, H, W, _ = q.shape
    rel_logits<a id="change"> = </a><a id="change">paddle.matmul(</a>q, rel_k.T<a id="change">)</a>
    &#47&#47 Collapse height and heads
    rel_logits = paddle.reshape(rel_logits, [-1, Nh * H, W, 2 * W - 1])
    rel_logits = rel_to_abs(rel_logits)
    rel_logits = paddle.reshape(rel_logits, [-1, Nh, H, W, W])</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/br-idl/paddlevit/commit/8d48d071238f6c8f7a8d6832d9b3f8f901311d5f#diff-c089174a4db639b1dbef7b2dbd67010117c0b6fc3e97f29e6575e7de310dde5dL49' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 40705756</div><div id='project'> Project Name: br-idl/paddlevit</div><div id='commit'> Commit Name: 8d48d071238f6c8f7a8d6832d9b3f8f901311d5f</div><div id='time'> Time: 2021-12-14</div><div id='author'> Author: dhwlh123@163.com</div><div id='file'> File Name: image_classification/BoTNet/botnet.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: relative_logits_1d(2)</div><div id='n_method'> N Method Name: relative_logits_1d(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: image_classification/BoTNet/botnet.py</div><div id='n_file'> N File Name: image_classification/BoTNet/botnet.py</div><div id='m_start'> M Start Line: 57</div><div id='m_end'> M End Line: 57</div><div id='n_start'> N Start Line: 27</div><div id='n_end'> N End Line: 27</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 Last iteration with original u_hat to pass gradient
        c = b.softmax(dim=1)
        s<a id="change"> = </a><a id="change">torch.einsum(&quotijk, ijkl -&gt; ijl&quot</a>, c, u_hat_temp<a id="change">)</a>
        v = squash(s)

        return v
</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 W @ x =
        &#47&#47 (1, num_caps, in_caps, dim_caps, in_dim) @ (batch_size, 1, in_caps, in_dim, 1) =
        &#47&#47 (batch_size, num_caps, in_caps, dim_caps, 1)
        u_hat = <a id="change">torch.matmul(</a>self.W, x<a id="change">)</a>
        &#47&#47 (batch_size, num_caps, in_caps, dim_caps)
        u_hat<a id="change"> = </a>u_hat.squeeze(-1)
        &#47&#47 detach u_hat during routing iterations to prevent gradients from flowing
        temp_u_hat = u_hat.detach()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/riroaki/capsnet/commit/e62f83faad1731befd8a1e434aaf902e2140aecb#diff-903964c7bdce50a4b995b4520374f243c677f45b2f8c7ed7b15abeeb6f26af12L49' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 40705769</div><div id='project'> Project Name: riroaki/capsnet</div><div id='commit'> Commit Name: e62f83faad1731befd8a1e434aaf902e2140aecb</div><div id='time'> Time: 2020-03-08</div><div id='author'> Author: aki@akideMacBook-Pro.local</div><div id='file'> File Name: capsnet.py</div><div id='m_class'> M Class Name: DigitCaps</div><div id='n_method'> N Class Name: DigitCaps</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: capsnet.py</div><div id='n_file'> N File Name: capsnet.py</div><div id='m_start'> M Start Line: 50</div><div id='m_end'> M End Line: 79</div><div id='n_start'> N Start Line: 59</div><div id='n_end'> N End Line: 93</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        xy, z = torch.split(points, [2, 1], dim=-1)
        c, s = torch.cos(theta), torch.sin(theta)
        R = torch.stack((c, -s, s, c), dim=-1).view(1, -1, 2, 2)
        xy<a id="change"> = </a><a id="change">torch.einsum(&quotijkl,imjl-&gt;imjk&quot</a>, R, xy<a id="change">)</a>
        xyz = torch.cat((xy, z), dim=-1)
        return xyz

    def sample_gridpoints(self, proposals):</code></pre><h3>After Change</h3><pre><code class='java'>
        xy, z = torch.split(points, [2, 1], dim=-1)
        c, s = torch.cos(theta), torch.sin(theta)
        R = torch.stack((c, -s, s, c), dim=-1).view(b, n, m, 2, 2)
        xy<a id="change"> = </a><a id="change">torch.matmul(</a>R, xy.unsqueeze(-1)<a id="change">)</a>
        xyz = torch.cat((xy.squeeze(-1), z), dim=-1)
        return xyz

    def sample_gridpoints(self, proposals):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/jhultman/vision3d/commit/1a652d6cae90ba6dca963f3725b0cb1b9049e39c#diff-596adef6d1bbcd96ed6ba8c423875fd8a1aefeed5888e630ab0b4ac1f57a0197L32' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 40705798</div><div id='project'> Project Name: jhultman/vision3d</div><div id='commit'> Commit Name: 1a652d6cae90ba6dca963f3725b0cb1b9049e39c</div><div id='time'> Time: 2020-02-12</div><div id='author'> Author: 27909223+jhultman@users.noreply.github.com</div><div id='file'> File Name: pvrcnn/roi_grid_pool.py</div><div id='m_class'> M Class Name: RoiGridPool</div><div id='n_method'> N Class Name: RoiGridPool</div><div id='m_method'> M Method Name: rotate_z(3)</div><div id='n_method'> N Method Name: rotate_z(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: pvrcnn/roi_grid_pool.py</div><div id='n_file'> N File Name: pvrcnn/roi_grid_pool.py</div><div id='m_start'> M Start Line: 41</div><div id='m_end'> M End Line: 43</div><div id='n_start'> N Start Line: 39</div><div id='n_end'> N End Line: 45</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def absolute_logits(self, inputs):
        pos_emb = tf.expand_dims(self.pos_emb_h, 2) + tf.expand_dims(self.pos_emb_w, 1)
        abs_logits<a id="change"> = </a><a id="change">tf.einsum("bxyhd,dpq-&gt;bhxypq"</a>, inputs, pos_emb<a id="change">)</a>
        return abs_logits

    def call(self, inputs):
        pos_emb = self.absolute_logits(inputs) if self.use_absolute_pos else self.relative_logits(inputs)</code></pre><h3>After Change</h3><pre><code class='java'>
    def absolute_logits(self, inputs):
        &#47&#47 pos_emb = tf.expand_dims(self.pos_emb_w, -2) + tf.expand_dims(self.pos_emb_h, -1)
        &#47&#47 return tf.einsum("bxyhd,dpq-&gt;bhxypq", inputs, pos_emb)
        rel_logits_w<a id="change"> = </a><a id="change">tf.matmul(</a>inputs, self.pos_emb_w<a id="change">)</a>
        rel_logits_h = tf.matmul(inputs, self.pos_emb_h)
        return tf.expand_dims(rel_logits_w, axis=-2) + tf.expand_dims(rel_logits_h, axis=-1)

    def call(self, inputs):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/65211d256d2398b82f3327cfea4778a23ac8cf1b#diff-4163380308eb60d4185b09d8359be777af3cee4f5ffc9acbf77fc54a76f12e5bL94' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 40705750</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 65211d256d2398b82f3327cfea4778a23ac8cf1b</div><div id='time'> Time: 2021-12-22</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/botnet/botnet.py</div><div id='m_class'> M Class Name: RelativePositionalEmbedding</div><div id='n_method'> N Class Name: RelativePositionalEmbedding</div><div id='m_method'> M Method Name: absolute_logits(2)</div><div id='n_method'> N Method Name: absolute_logits(2)</div><div id='m_parent_class'> M Parent Class: keras.layers.Layer</div><div id='n_parent_class'> N Parent Class: keras.layers.Layer</div><div id='m_file'> M File Name: keras_cv_attention_models/botnet/botnet.py</div><div id='n_file'> N File Name: keras_cv_attention_models/botnet/botnet.py</div><div id='m_start'> M Start Line: 95</div><div id='m_end'> M End Line: 97</div><div id='n_start'> N Start Line: 99</div><div id='n_end'> N End Line: 101</div><BR>