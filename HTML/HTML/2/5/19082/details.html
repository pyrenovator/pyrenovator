<html><h3>Pattern ID :19082
</h3><img src='62197014.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def __getitem__(self, index):
        file = self.file_list[index]
        <a id="change">if </a><a id="change">isinstance(</a>file, dict<a id="change">)</a>:
            return {instrument: self.read_file_and_crop(file[instrument]) for instrument in file.keys()}
        else:
            return self.read_file_and_crop(file)</code></pre><h3>After Change</h3><pre><code class='java'>
                raise IOError(&quotfile too short for requested training sequence length; pre-filter file list&quot)
            elif self.on_too_short == &quotpad&quot:
                &#47&#47 XXX: maybe in different cases padding should be left/right/centered
                <a id="change">raise </a><a id="change">NotImplementedError(&quotfile too short for requested training sequence length; implement padding&quot</a><a id="change">)</a>
            else:
                raise ValueError(&quotinvalid on_too_short&quot)
        else:
            b = np.random.randint(0, n - self.seq_len + 1)  &#47&#47 [lo, hi[</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/wuaalb/pytorch_template_audio/commit/c4139268ca0f749f4ec4c0a299d889225b89b003#diff-11bb3b632c84e01e0bf1b576e72c513fd062811e900ebcb5f22df0eac7d3b0d9L50' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 62197014</div><div id='project'> Project Name: wuaalb/pytorch_template_audio</div><div id='commit'> Commit Name: c4139268ca0f749f4ec4c0a299d889225b89b003</div><div id='time'> Time: 2020-11-23</div><div id='author'> Author: mblaauw@gmail.com</div><div id='file'> File Name: dataset.py</div><div id='m_class'> M Class Name: WavDataset</div><div id='n_method'> N Class Name: WavDataset</div><div id='m_method'> M Method Name: __getitem__(2)</div><div id='n_method'> N Method Name: __getitem__(2)</div><div id='m_parent_class'> M Parent Class: torch.utils.data.Dataset</div><div id='n_parent_class'> N Parent Class: torch.utils.data.Dataset</div><div id='m_file'> M File Name: dataset.py</div><div id='n_file'> N File Name: dataset.py</div><div id='m_start'> M Start Line: 72</div><div id='m_end'> M End Line: 78</div><div id='n_start'> N Start Line: 50</div><div id='n_end'> N End Line: 85</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def _wrap_model(self, model, training=True):
        if is_sagemaker_mp_enabled():
            &#47&#47 Wrapping the base model twice in a DistributedModel will raise an error.
            <a id="change">if </a><a id="change">isinstance(</a>self.model_wrapped, smp.model.DistributedModel<a id="change">)</a>:
                return self.model_wrapped
            return smp.DistributedModel(model, backward_passes_per_step=self.args.gradient_accumulation_steps)
</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 Distributed training (should be after apex fp16 initialization)
        if self.sharded_ddp is not None:
            <a id="change">raise </a><a id="change">NotImplementedError(
                "Fairscale&quots distributed data parallel features are not supported by `ORTTrainer` yet. Stay tuned!"</a><a id="change">
            )</a>
        elif is_sagemaker_dp_enabled():
            model = nn.parallel.DistributedDataParallel(
                model, device_ids=[int(os.getenv("SMDATAPARALLEL_LOCAL_RANK"))]
            )</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/huggingface/optimum/commit/1c4b5b1caf1f1a58c32898ffb208538d7364b73a#diff-2fce6a27b91f51e97c2c1e0b4c9b86dae5896189543dac9e0a8e4f6cec6b62f9L1265' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 62197009</div><div id='project'> Project Name: huggingface/optimum</div><div id='commit'> Commit Name: 1c4b5b1caf1f1a58c32898ffb208538d7364b73a</div><div id='time'> Time: 2022-04-29</div><div id='author'> Author: 44135271+JingyaHuang@users.noreply.github.com</div><div id='file'> File Name: optimum/onnxruntime/trainer.py</div><div id='m_class'> M Class Name: ORTTrainer</div><div id='n_method'> N Class Name: ORTTrainer</div><div id='m_method'> M Method Name: _wrap_model(3)</div><div id='n_method'> N Method Name: _wrap_model(3)</div><div id='m_parent_class'> M Parent Class: Trainer</div><div id='n_parent_class'> N Parent Class: Trainer</div><div id='m_file'> M File Name: optimum/onnxruntime/trainer.py</div><div id='n_file'> N File Name: optimum/onnxruntime/trainer.py</div><div id='m_start'> M Start Line: 1268</div><div id='m_end'> M End Line: 1315</div><div id='n_start'> N Start Line: 1233</div><div id='n_end'> N End Line: 1267</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                    logits = smp_nested_concat(logits_mb)
                else:
                    loss = None
                    <a id="change">if </a><a id="change">isinstance(</a>raw_outputs, dict<a id="change">)</a>:
                        logits_mb = tuple(v for k, v in raw_outputs.items() if k not in ignore_keys)
                    else:
                        logits_mb = raw_outputs</code></pre><h3>After Change</h3><pre><code class='java'>

        with torch.no_grad():
            if is_sagemaker_mp_enabled():
                <a id="change">raise </a><a id="change">NotImplementedError(
                    "Sagemaker&quots distributed data parallel features are not supported by `ORTTrainer` yet. Stay tuned!"</a><a id="change">
                )</a>
            else:
                if has_labels:
                    with self.autocast_smart_context_manager():
                        loss, outputs = self.compute_loss_ort(</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/huggingface/optimum/commit/1c4b5b1caf1f1a58c32898ffb208538d7364b73a#diff-2fce6a27b91f51e97c2c1e0b4c9b86dae5896189543dac9e0a8e4f6cec6b62f9L1135' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 62197008</div><div id='project'> Project Name: huggingface/optimum</div><div id='commit'> Commit Name: 1c4b5b1caf1f1a58c32898ffb208538d7364b73a</div><div id='time'> Time: 2022-04-29</div><div id='author'> Author: 44135271+JingyaHuang@users.noreply.github.com</div><div id='file'> File Name: optimum/onnxruntime/trainer.py</div><div id='m_class'> M Class Name: ORTTrainer</div><div id='n_method'> N Class Name: ORTTrainer</div><div id='m_method'> M Method Name: prediction_step_ort(6)</div><div id='n_method'> N Method Name: prediction_step_ort(6)</div><div id='m_parent_class'> M Parent Class: Trainer</div><div id='n_parent_class'> N Parent Class: Trainer</div><div id='m_file'> M File Name: optimum/onnxruntime/trainer.py</div><div id='n_file'> N File Name: optimum/onnxruntime/trainer.py</div><div id='m_start'> M Start Line: 1144</div><div id='m_end'> M End Line: 1193</div><div id='n_start'> N Start Line: 1126</div><div id='n_end'> N End Line: 1140</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.embedding_size = raw_embedding.weight.shape[-1]
        self.num_token = num_token

        <a id="change">if </a><a id="change">isinstance(</a>self.config, T5Config<a id="change">)</a>:
            self.n_layer = self.config.num_layers
            self.n_embd = self.config.d_model
            self.n_head = self.config.num_heads</code></pre><h3>After Change</h3><pre><code class='java'>
            self.n_embd = self.config.n_embd
            self.n_head = self.config.n_head
        else:
            <a id="change">raise </a><a id="change">NotImplementedError("This template currently only support GPT2, please consider using PrefixTuningTemplatePro."</a><a id="change">)</a>
        self.mid_dim = mid_dim

        self.match_n_layer = self.n_layer
        self.match_n_head = self.n_head</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thunlp/openprompt/commit/e13980ab1a1b37f25c2ebbc58a3a5fee76e87ed9#diff-b0dfb87a14118405c6443fe8a97150575594ca7549aef2e49aeedcbba35bf18eL41' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 62197005</div><div id='project'> Project Name: thunlp/openprompt</div><div id='commit'> Commit Name: e13980ab1a1b37f25c2ebbc58a3a5fee76e87ed9</div><div id='time'> Time: 2021-11-05</div><div id='author'> Author: shengdinghu@gmail.com</div><div id='file'> File Name: openprompt/prompts/prefix_tuning_template.py</div><div id='m_class'> M Class Name: PrefixTuningTemplate</div><div id='n_method'> N Class Name: PrefixTuningTemplate</div><div id='m_method'> M Method Name: __init__(10)</div><div id='n_method'> N Method Name: __init__(10)</div><div id='m_parent_class'> M Parent Class: Template</div><div id='n_parent_class'> N Parent Class: Template</div><div id='m_file'> M File Name: openprompt/prompts/prefix_tuning_template.py</div><div id='n_file'> N File Name: openprompt/prompts/prefix_tuning_template.py</div><div id='m_start'> M Start Line: 56</div><div id='m_end'> M End Line: 71</div><div id='n_start'> N Start Line: 61</div><div id='n_end'> N End Line: 67</div><BR>