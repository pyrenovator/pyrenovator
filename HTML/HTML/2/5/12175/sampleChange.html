<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            alpha2Hx = (alpha * 2.0) * H * ctx.x  &#47&#47 componentwise product
            A_grad -= ctx.A_val * alpha2Hx[:, ctx.A_col_ind.type(torch.long)]

        <a id="change">return </a>A_grad, AH, None, None, None, None, None, None
</code></pre><h3>After Change</h3><pre><code class='java'>
        batch_size = grad_output.shape[0]

        H = grad_output.clone()
        H_double = <a id="change">H.double()</a>
        ctx.numeric_decomposition.solve(H_double)  &#47&#47 solve in place

        A_args = (
            ctx.sparse_structure.num_cols,
            ctx.A_row_ptr,
            ctx.A_col_ind,
            ctx.A_val_double,
        )
        AH = mat_vec(batch_size, *A_args, H_double)
        b_Ax = ctx.b - mat_vec(batch_size, *A_args, ctx.x)

        &#47&#47 now we fill values of a matrix with structure identical to A with
        &#47&#47 selected entries from the difference of tensor products:
        &#47&#47   b_Ax (X) H - AH (X) x
        &#47&#47 NOTE: this row-wise manipulation can be much faster in C++ or Cython
        A_col_ind = ctx.sparse_structure.col_ind
        A_row_ptr = ctx.sparse_structure.row_ptr
        batch_size = grad_output.shape[0]
        A_grad = torch.empty(
            size=(batch_size, len(A_col_ind)),
            dtype=torch.double,
            device=grad_output.device,
        )  &#47&#47 return value, A&quots grad
        for r in range(len(A_row_ptr) - 1):
            start, end = A_row_ptr[r], A_row_ptr[r + 1]
            columns = A_col_ind[start:end]  &#47&#47 col indices, for this row
            A_grad[:, start:end] = (
                b_Ax[:, r].unsqueeze(1) * H_double[:, columns]
                - AH[:, r].unsqueeze(1) * ctx.x[:, columns]
            )

        &#47&#47 apply correction if there is a multiplicative damping
        if (
            ctx.damping_alpha_beta is not None
            and (ctx.damping_alpha_beta[0] &gt; 0.0).any()
        ):
            alpha = ctx.damping_alpha_beta[0].view(-1, 1)
            alpha2Hx = (alpha * 2.0) * H_double * ctx.x  &#47&#47 componentwise product
            A_grad -= ctx.A_val_double * alpha2Hx[:, ctx.A_col_ind.type(torch.long)]

        <a id="change">return </a>(
            A_grad.to(dtype=grad_output.dtype)<a id="change">,
            AH.to(dtype=grad_output.dtype),
            None,
            None,
            None,
            None,
            None,
            None</a>,
        )
</code></pre>