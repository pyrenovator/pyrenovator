<html><h3>Pattern ID :33470
</h3><img src='96283837.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def compare(self, quantized_model, float_model, input_x=None, quantization_info=None):
        &#47&#47 verify chosen activation bitwidth config
        &#47&#47 kpi is infinity -&gt; should give best model - 8bits
        activation_bits = [<a id="change">quantized_model.layers[i].inbound_nodes[0].call_kwargs.get(&quotnum_bits&quot</a><a id="change">)</a> for i in self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [8, 8, 8]))

        self.verify_quantization(quantized_model, input_x,</code></pre><h3>After Change</h3><pre><code class='java'>
    def compare(self, quantized_model, float_model, input_x=None, quantization_info=None):
        &#47&#47 verify chosen activation bitwidth config
        &#47&#47 kpi is infinity -&gt; should give best model - 8bits
        activation_bits = [<a id="change">quantized_model.layers[i].activation_quantizers[0].get_config()[&quotnum_bits&quot]</a> for i in self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [8, 8, 8]))

        self.verify_quantization(quantized_model, input_x,</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 13</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/sony/model_optimization/commit/0242581af77f282d8ef063beb9f6a2f952b8f9d3#diff-3398a8d561f1763282df28883ea2701b4d4eee52fede99d66866ea668437b4faL318' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96283837</div><div id='project'> Project Name: sony/model_optimization</div><div id='commit'> Commit Name: 0242581af77f282d8ef063beb9f6a2f952b8f9d3</div><div id='time'> Time: 2023-03-12</div><div id='author'> Author: 44209964+reuvenperetz@users.noreply.github.com</div><div id='file'> File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_class'> M Class Name: MixedPrecisionActivationOnlyTest</div><div id='n_method'> N Class Name: MixedPrecisionActivationOnlyTest</div><div id='m_method'> M Method Name: compare(5)</div><div id='n_method'> N Method Name: compare(5)</div><div id='m_parent_class'> M Parent Class: MixedPrecisionActivationBaseTest</div><div id='n_parent_class'> N Parent Class: MixedPrecisionActivationBaseTest</div><div id='m_file'> M File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='n_file'> N File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_start'> M Start Line: 318</div><div id='m_end'> M End Line: 318</div><div id='n_start'> N Start Line: 320</div><div id='n_end'> N End Line: 320</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def compare(self, quantized_model, float_model, input_x=None, quantization_info: UserInformation = None):
        &#47&#47 verify chosen activation bitwidth config
        activation_bits = [<a id="change">quantized_model.layers[i].inbound_nodes[0].call_kwargs.get(&quotnum_bits&quot</a><a id="change">)</a> for i in
                           self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [2, 2]))
</code></pre><h3>After Change</h3><pre><code class='java'>

    def compare(self, quantized_model, float_model, input_x=None, quantization_info: UserInformation = None):
        &#47&#47 verify chosen activation bitwidth config
        activation_bits = [<a id="change">quantized_model.layers[i].activation_quantizers[0].get_config()[&quotnum_bits&quot]</a> for i in
                           self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [2, 2]))
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sony/model_optimization/commit/0242581af77f282d8ef063beb9f6a2f952b8f9d3#diff-3398a8d561f1763282df28883ea2701b4d4eee52fede99d66866ea668437b4faL516' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96283836</div><div id='project'> Project Name: sony/model_optimization</div><div id='commit'> Commit Name: 0242581af77f282d8ef063beb9f6a2f952b8f9d3</div><div id='time'> Time: 2023-03-12</div><div id='author'> Author: 44209964+reuvenperetz@users.noreply.github.com</div><div id='file'> File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_class'> M Class Name: MixedPrecisionReducedTotalKPISearchTest</div><div id='n_method'> N Class Name: MixedPrecisionReducedTotalKPISearchTest</div><div id='m_method'> M Method Name: compare(5)</div><div id='n_method'> N Method Name: compare(5)</div><div id='m_parent_class'> M Parent Class: MixedPrecisionActivationBaseTest</div><div id='n_parent_class'> N Parent Class: MixedPrecisionActivationBaseTest</div><div id='m_file'> M File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='n_file'> N File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_start'> M Start Line: 518</div><div id='m_end'> M End Line: 518</div><div id='n_start'> N Start Line: 520</div><div id='n_end'> N End Line: 520</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Note that since we&quotre using default max aggregation for activation KPI, then there is no guarantee that the
        &#47&#47 activation bitwidth for each layer would be 4-bit, this assertion tests the expected result for this specific
        &#47&#47 test with its current setup (therefore, we don&quott check the relu layer&quots bitwidth)
        activation_bits = [<a id="change">quantized_model.layers[i].inbound_nodes[0].call_kwargs.get(&quotnum_bits&quot</a><a id="change">)</a> for i in self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [4]))

</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 Note that since we&quotre using default max aggregation for activation KPI, then there is no guarantee that the
        &#47&#47 activation bitwidth for each layer would be 4-bit, this assertion tests the expected result for this specific
        &#47&#47 test with its current setup (therefore, we don&quott check the relu layer&quots bitwidth)
        activation_bits = [<a id="change">quantized_model.layers[i].activation_quantizers[0].get_config()[&quotnum_bits&quot]</a> for i in self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [4]))

</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sony/model_optimization/commit/0242581af77f282d8ef063beb9f6a2f952b8f9d3#diff-3398a8d561f1763282df28883ea2701b4d4eee52fede99d66866ea668437b4faL251' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96283839</div><div id='project'> Project Name: sony/model_optimization</div><div id='commit'> Commit Name: 0242581af77f282d8ef063beb9f6a2f952b8f9d3</div><div id='time'> Time: 2023-03-12</div><div id='author'> Author: 44209964+reuvenperetz@users.noreply.github.com</div><div id='file'> File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_class'> M Class Name: MixedPrecisionActivationDepthwise4BitTest</div><div id='n_method'> N Class Name: MixedPrecisionActivationDepthwise4BitTest</div><div id='m_method'> M Method Name: compare(5)</div><div id='n_method'> N Method Name: compare(5)</div><div id='m_parent_class'> M Parent Class: MixedPrecisionActivationBaseTest</div><div id='n_parent_class'> N Parent Class: MixedPrecisionActivationBaseTest</div><div id='m_file'> M File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='n_file'> N File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_start'> M Start Line: 257</div><div id='m_end'> M End Line: 257</div><div id='n_start'> N Start Line: 259</div><div id='n_end'> N End Line: 259</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def compare(self, quantized_model, float_model, input_x=None, quantization_info=None):
        &#47&#47 verify chosen activation bitwidth config
        &#47&#47 kpi is infinity -&gt; should give best model - 8bits
        activation_bits = [<a id="change">quantized_model.layers[i].inbound_nodes[0].call_kwargs.get(&quotnum_bits&quot</a><a id="change">)</a> for i in self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [8, 8, 8]))

        self.verify_quantization(quantized_model, input_x,</code></pre><h3>After Change</h3><pre><code class='java'>
    def compare(self, quantized_model, float_model, input_x=None, quantization_info=None):
        &#47&#47 verify chosen activation bitwidth config
        &#47&#47 kpi is infinity -&gt; should give best model - 8bits
        activation_bits = [<a id="change">quantized_model.layers[i].activation_quantizers[0].get_config()[&quotnum_bits&quot]</a> for i in self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [8, 8, 8]))

        self.verify_quantization(quantized_model, input_x,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sony/model_optimization/commit/0242581af77f282d8ef063beb9f6a2f952b8f9d3#diff-3398a8d561f1763282df28883ea2701b4d4eee52fede99d66866ea668437b4faL126' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96283838</div><div id='project'> Project Name: sony/model_optimization</div><div id='commit'> Commit Name: 0242581af77f282d8ef063beb9f6a2f952b8f9d3</div><div id='time'> Time: 2023-03-12</div><div id='author'> Author: 44209964+reuvenperetz@users.noreply.github.com</div><div id='file'> File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_class'> M Class Name: MixedPrecisionActivationSearchTest</div><div id='n_method'> N Class Name: MixedPrecisionActivationSearchTest</div><div id='m_method'> M Method Name: compare(5)</div><div id='n_method'> N Method Name: compare(5)</div><div id='m_parent_class'> M Parent Class: MixedPrecisionActivationBaseTest</div><div id='n_parent_class'> N Parent Class: MixedPrecisionActivationBaseTest</div><div id='m_file'> M File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='n_file'> N File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_start'> M Start Line: 129</div><div id='m_end'> M End Line: 129</div><div id='n_start'> N Start Line: 131</div><div id='n_end'> N End Line: 131</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def compare(self, quantized_model, float_model, input_x=None, quantization_info=None):
        &#47&#47 verify chosen activation bitwidth config
        &#47&#47 kpi is infinity -&gt; should give best model - 8bits
        activation_bits = [<a id="change">quantized_model.layers[i].inbound_nodes[0].call_kwargs.get(&quotnum_bits&quot</a><a id="change">)</a> for i in self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [8, 8]))

</code></pre><h3>After Change</h3><pre><code class='java'>
    def compare(self, quantized_model, float_model, input_x=None, quantization_info=None):
        &#47&#47 verify chosen activation bitwidth config
        &#47&#47 kpi is infinity -&gt; should give best model - 8bits
        activation_bits = [<a id="change">quantized_model.layers[i].activation_quantizers[0].get_config()[&quotnum_bits&quot]</a> for i in self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [8, 8]))

</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sony/model_optimization/commit/0242581af77f282d8ef063beb9f6a2f952b8f9d3#diff-3398a8d561f1763282df28883ea2701b4d4eee52fede99d66866ea668437b4faL219' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96283848</div><div id='project'> Project Name: sony/model_optimization</div><div id='commit'> Commit Name: 0242581af77f282d8ef063beb9f6a2f952b8f9d3</div><div id='time'> Time: 2023-03-12</div><div id='author'> Author: 44209964+reuvenperetz@users.noreply.github.com</div><div id='file'> File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_class'> M Class Name: MixedPrecisionActivationDepthwiseTest</div><div id='n_method'> N Class Name: MixedPrecisionActivationDepthwiseTest</div><div id='m_method'> M Method Name: compare(5)</div><div id='n_method'> N Method Name: compare(5)</div><div id='m_parent_class'> M Parent Class: MixedPrecisionActivationBaseTest</div><div id='n_parent_class'> N Parent Class: MixedPrecisionActivationBaseTest</div><div id='m_file'> M File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='n_file'> N File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_start'> M Start Line: 222</div><div id='m_end'> M End Line: 222</div><div id='n_start'> N Start Line: 224</div><div id='n_end'> N End Line: 224</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def compare(self, quantized_model, float_model, input_x=None, quantization_info=None):
        &#47&#47 verify chosen activation bitwidth config
        &#47&#47 kpi is infinity -&gt; should give best model - 8bits
        activation_bits = [<a id="change">quantized_model.layers[i].inbound_nodes[0].call_kwargs.get(&quotnum_bits&quot</a><a id="change">)</a> for i in self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [8, 8, 8, 8, 8, 8, 8, 8]))

        self.verify_quantization(quantized_model, input_x,</code></pre><h3>After Change</h3><pre><code class='java'>
    def compare(self, quantized_model, float_model, input_x=None, quantization_info=None):
        &#47&#47 verify chosen activation bitwidth config
        &#47&#47 kpi is infinity -&gt; should give best model - 8bits
        activation_bits = [<a id="change">quantized_model.layers[i].activation_quantizers[0].get_config()[&quotnum_bits&quot]</a> for i in self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [8, 8, 8, 8, 8, 8, 8, 8, 8]))

        self.verify_quantization(quantized_model, input_x,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sony/model_optimization/commit/0242581af77f282d8ef063beb9f6a2f952b8f9d3#diff-3398a8d561f1763282df28883ea2701b4d4eee52fede99d66866ea668437b4faL438' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96283845</div><div id='project'> Project Name: sony/model_optimization</div><div id='commit'> Commit Name: 0242581af77f282d8ef063beb9f6a2f952b8f9d3</div><div id='time'> Time: 2023-03-12</div><div id='author'> Author: 44209964+reuvenperetz@users.noreply.github.com</div><div id='file'> File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_class'> M Class Name: MixedPrecisionActivationMultipleInputsTest</div><div id='n_method'> N Class Name: MixedPrecisionActivationMultipleInputsTest</div><div id='m_method'> M Method Name: compare(5)</div><div id='n_method'> N Method Name: compare(5)</div><div id='m_parent_class'> M Parent Class: MixedPrecisionActivationBaseTest</div><div id='n_parent_class'> N Parent Class: MixedPrecisionActivationBaseTest</div><div id='m_file'> M File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='n_file'> N File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_start'> M Start Line: 441</div><div id='m_end'> M End Line: 441</div><div id='n_start'> N Start Line: 443</div><div id='n_end'> N End Line: 444</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def compare(self, quantized_model, float_model, input_x=None, quantization_info: UserInformation = None):
        &#47&#47 verify chosen activation bitwidth config
        activation_bits = [<a id="change">quantized_model.layers[i].inbound_nodes[0].call_kwargs.get(&quotnum_bits&quot</a><a id="change">)</a> for i in
                           self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [4, 4]))
</code></pre><h3>After Change</h3><pre><code class='java'>

    def compare(self, quantized_model, float_model, input_x=None, quantization_info: UserInformation = None):
        &#47&#47 verify chosen activation bitwidth config
        activation_bits = [<a id="change">quantized_model.layers[i].activation_quantizers[0].get_config()[&quotnum_bits&quot]</a> for i in
                           self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [4, 4]))
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sony/model_optimization/commit/0242581af77f282d8ef063beb9f6a2f952b8f9d3#diff-3398a8d561f1763282df28883ea2701b4d4eee52fede99d66866ea668437b4faL458' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96283844</div><div id='project'> Project Name: sony/model_optimization</div><div id='commit'> Commit Name: 0242581af77f282d8ef063beb9f6a2f952b8f9d3</div><div id='time'> Time: 2023-03-12</div><div id='author'> Author: 44209964+reuvenperetz@users.noreply.github.com</div><div id='file'> File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_class'> M Class Name: MixedPrecisionTotalKPISearchTest</div><div id='n_method'> N Class Name: MixedPrecisionTotalKPISearchTest</div><div id='m_method'> M Method Name: compare(5)</div><div id='n_method'> N Method Name: compare(5)</div><div id='m_parent_class'> M Parent Class: MixedPrecisionActivationBaseTest</div><div id='n_parent_class'> N Parent Class: MixedPrecisionActivationBaseTest</div><div id='m_file'> M File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='n_file'> N File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_start'> M Start Line: 460</div><div id='m_end'> M End Line: 460</div><div id='n_start'> N Start Line: 462</div><div id='n_end'> N End Line: 462</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def compare(self, quantized_model, float_model, input_x=None, quantization_info=None):
        &#47&#47 verify chosen activation bitwidth config
        &#47&#47 kpi is infinity -&gt; should give best model - 8bits
        activation_bits = [<a id="change">quantized_model.layers[i].inbound_nodes[0].call_kwargs.get(&quotnum_bits&quot</a><a id="change">)</a> for i in self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [8, 8, 8]))

        self.verify_quantization(quantized_model, input_x,</code></pre><h3>After Change</h3><pre><code class='java'>
    def compare(self, quantized_model, float_model, input_x=None, quantization_info=None):
        &#47&#47 verify chosen activation bitwidth config
        &#47&#47 kpi is infinity -&gt; should give best model - 8bits
        activation_bits = [<a id="change">quantized_model.layers[i].activation_quantizers[0].get_config()[&quotnum_bits&quot]</a> for i in self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [8, 8, 8]))

        self.verify_quantization(quantized_model, input_x,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sony/model_optimization/commit/0242581af77f282d8ef063beb9f6a2f952b8f9d3#diff-3398a8d561f1763282df28883ea2701b4d4eee52fede99d66866ea668437b4faL277' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96283847</div><div id='project'> Project Name: sony/model_optimization</div><div id='commit'> Commit Name: 0242581af77f282d8ef063beb9f6a2f952b8f9d3</div><div id='time'> Time: 2023-03-12</div><div id='author'> Author: 44209964+reuvenperetz@users.noreply.github.com</div><div id='file'> File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_class'> M Class Name: MixedPrecisionActivationSplitLayerTest</div><div id='n_method'> N Class Name: MixedPrecisionActivationSplitLayerTest</div><div id='m_method'> M Method Name: compare(5)</div><div id='n_method'> N Method Name: compare(5)</div><div id='m_parent_class'> M Parent Class: MixedPrecisionActivationBaseTest</div><div id='n_parent_class'> N Parent Class: MixedPrecisionActivationBaseTest</div><div id='m_file'> M File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='n_file'> N File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_start'> M Start Line: 280</div><div id='m_end'> M End Line: 280</div><div id='n_start'> N Start Line: 282</div><div id='n_end'> N End Line: 282</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def compare(self, quantized_model, float_model, input_x=None, quantization_info=None):
        &#47&#47 verify chosen activation bitwidth config
        &#47&#47 kpi is 4 bit average
        activation_bits = [<a id="change">quantized_model.layers[i].inbound_nodes[0].call_kwargs.get(&quotnum_bits&quot</a><a id="change">)</a> for i in self.activation_layers_idx]
        &#47&#47 Note that since we&quotre using default max aggregation for activation KPI, then there is no guarantee that the
        &#47&#47 activation bitwidth for each layer would be 4-bit, this assertion tests the expected result for this specific
        &#47&#47 test with its current setup (therefore, we don&quott check the input layer&quots bitwidth)</code></pre><h3>After Change</h3><pre><code class='java'>
    def compare(self, quantized_model, float_model, input_x=None, quantization_info=None):
        &#47&#47 verify chosen activation bitwidth config
        &#47&#47 kpi is 4 bit average
        activation_bits = [<a id="change">quantized_model.layers[i].activation_quantizers[0].get_config()[&quotnum_bits&quot]</a> for i in self.activation_layers_idx]
        &#47&#47 Note that since we&quotre using default max aggregation for activation KPI, then there is no guarantee that the
        &#47&#47 activation bitwidth for each layer would be 4-bit, this assertion tests the expected result for this specific
        &#47&#47 test with its current setup (therefore, we don&quott check the input layer&quots bitwidth)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sony/model_optimization/commit/0242581af77f282d8ef063beb9f6a2f952b8f9d3#diff-3398a8d561f1763282df28883ea2701b4d4eee52fede99d66866ea668437b4faL156' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96283846</div><div id='project'> Project Name: sony/model_optimization</div><div id='commit'> Commit Name: 0242581af77f282d8ef063beb9f6a2f952b8f9d3</div><div id='time'> Time: 2023-03-12</div><div id='author'> Author: 44209964+reuvenperetz@users.noreply.github.com</div><div id='file'> File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_class'> M Class Name: MixedPrecisionActivationSearchKPI4BitsAvgTest</div><div id='n_method'> N Class Name: MixedPrecisionActivationSearchKPI4BitsAvgTest</div><div id='m_method'> M Method Name: compare(5)</div><div id='n_method'> N Method Name: compare(5)</div><div id='m_parent_class'> M Parent Class: MixedPrecisionActivationBaseTest</div><div id='n_parent_class'> N Parent Class: MixedPrecisionActivationBaseTest</div><div id='m_file'> M File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='n_file'> N File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_start'> M Start Line: 159</div><div id='m_end'> M End Line: 159</div><div id='n_start'> N Start Line: 161</div><div id='n_end'> N End Line: 161</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Note that since we&quotre using default max aggregation for activation KPI, then there is no guarantee that the
        &#47&#47 activation bitwidth for each layer would be 2-bit, this assertion tests the expected result for this specific
        &#47&#47 test with its current setup (therefore, we don&quott check the input layer&quots bitwidth)
        activation_bits = [<a id="change">quantized_model.layers[i].inbound_nodes[0].call_kwargs.get(&quotnum_bits&quot</a><a id="change">)</a> for i in self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [2, 2]))

        self.verify_quantization(quantized_model, input_x,</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 Note that since we&quotre using default max aggregation for activation KPI, then there is no guarantee that the
        &#47&#47 activation bitwidth for each layer would be 2-bit, this assertion tests the expected result for this specific
        &#47&#47 test with its current setup (therefore, we don&quott check the input layer&quots bitwidth)
        activation_bits = [<a id="change">quantized_model.layers[i].activation_quantizers[0].get_config()[&quotnum_bits&quot]</a> for i in self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [2, 2]))

        self.verify_quantization(quantized_model, input_x,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sony/model_optimization/commit/0242581af77f282d8ef063beb9f6a2f952b8f9d3#diff-3398a8d561f1763282df28883ea2701b4d4eee52fede99d66866ea668437b4faL181' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96283841</div><div id='project'> Project Name: sony/model_optimization</div><div id='commit'> Commit Name: 0242581af77f282d8ef063beb9f6a2f952b8f9d3</div><div id='time'> Time: 2023-03-12</div><div id='author'> Author: 44209964+reuvenperetz@users.noreply.github.com</div><div id='file'> File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_class'> M Class Name: MixedPrecisionActivationSearchKPI2BitsAvgTest</div><div id='n_method'> N Class Name: MixedPrecisionActivationSearchKPI2BitsAvgTest</div><div id='m_method'> M Method Name: compare(5)</div><div id='n_method'> N Method Name: compare(5)</div><div id='m_parent_class'> M Parent Class: MixedPrecisionActivationBaseTest</div><div id='n_parent_class'> N Parent Class: MixedPrecisionActivationBaseTest</div><div id='m_file'> M File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='n_file'> N File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_start'> M Start Line: 187</div><div id='m_end'> M End Line: 187</div><div id='n_start'> N Start Line: 189</div><div id='n_end'> N End Line: 189</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def compare(self, quantized_model, float_model, input_x=None, quantization_info=None):
        &#47&#47 verify chosen activation bitwidth config
        &#47&#47 kpi is infinity -&gt; should give best model - 8bits
        activation_bits = [<a id="change">quantized_model.layers[i].inbound_nodes[0].call_kwargs.get(&quotnum_bits&quot</a><a id="change">)</a> for i in self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [8, 8, 8]))

        self.verify_quantization(quantized_model, input_x,</code></pre><h3>After Change</h3><pre><code class='java'>
    def compare(self, quantized_model, float_model, input_x=None, quantization_info=None):
        &#47&#47 verify chosen activation bitwidth config
        &#47&#47 kpi is infinity -&gt; should give best model - 8bits
        activation_bits = [<a id="change">quantized_model.layers[i].activation_quantizers[0].get_config()[&quotnum_bits&quot]</a> for i in self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [8, 8, 8]))

        self.verify_quantization(quantized_model, input_x,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sony/model_optimization/commit/0242581af77f282d8ef063beb9f6a2f952b8f9d3#diff-3398a8d561f1763282df28883ea2701b4d4eee52fede99d66866ea668437b4faL361' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96283840</div><div id='project'> Project Name: sony/model_optimization</div><div id='commit'> Commit Name: 0242581af77f282d8ef063beb9f6a2f952b8f9d3</div><div id='time'> Time: 2023-03-12</div><div id='author'> Author: 44209964+reuvenperetz@users.noreply.github.com</div><div id='file'> File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_class'> M Class Name: MixedPrecisionActivationOnlyWeightsDisabledTest</div><div id='n_method'> N Class Name: MixedPrecisionActivationOnlyWeightsDisabledTest</div><div id='m_method'> M Method Name: compare(5)</div><div id='n_method'> N Method Name: compare(5)</div><div id='m_parent_class'> M Parent Class: MixedPrecisionActivationBaseTest</div><div id='n_parent_class'> N Parent Class: MixedPrecisionActivationBaseTest</div><div id='m_file'> M File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='n_file'> N File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_start'> M Start Line: 364</div><div id='m_end'> M End Line: 364</div><div id='n_start'> N Start Line: 366</div><div id='n_end'> N End Line: 366</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def compare(self, quantized_model, float_model, input_x=None, quantization_info=None):
        &#47&#47 verify chosen activation bitwidth config
        &#47&#47 kpi is infinity -&gt; should give best model - 8bits
        activation_bits = [<a id="change">quantized_model.layers[i].inbound_nodes[0].call_kwargs.get(&quotnum_bits&quot</a><a id="change">)</a> for i in self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [8, 8, 8]))

        self.verify_quantization(quantized_model, input_x,</code></pre><h3>After Change</h3><pre><code class='java'>
    def compare(self, quantized_model, float_model, input_x=None, quantization_info=None):
        &#47&#47 verify chosen activation bitwidth config
        &#47&#47 kpi is infinity -&gt; should give best model - 8bits
        activation_bits = [<a id="change">quantized_model.layers[i].activation_quantizers[0].get_config()[&quotnum_bits&quot]</a> for i in self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [8, 8, 8]))

        self.verify_quantization(quantized_model, input_x,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sony/model_optimization/commit/0242581af77f282d8ef063beb9f6a2f952b8f9d3#diff-3398a8d561f1763282df28883ea2701b4d4eee52fede99d66866ea668437b4faL388' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96283843</div><div id='project'> Project Name: sony/model_optimization</div><div id='commit'> Commit Name: 0242581af77f282d8ef063beb9f6a2f952b8f9d3</div><div id='time'> Time: 2023-03-12</div><div id='author'> Author: 44209964+reuvenperetz@users.noreply.github.com</div><div id='file'> File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_class'> M Class Name: MixedPrecisionActivationAddLayerTest</div><div id='n_method'> N Class Name: MixedPrecisionActivationAddLayerTest</div><div id='m_method'> M Method Name: compare(5)</div><div id='n_method'> N Method Name: compare(5)</div><div id='m_parent_class'> M Parent Class: MixedPrecisionActivationBaseTest</div><div id='n_parent_class'> N Parent Class: MixedPrecisionActivationBaseTest</div><div id='m_file'> M File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='n_file'> N File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_start'> M Start Line: 391</div><div id='m_end'> M End Line: 391</div><div id='n_start'> N Start Line: 393</div><div id='n_end'> N End Line: 393</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def compare(self, quantized_model, float_model, input_x=None, quantization_info: UserInformation = None):
        &#47&#47 verify chosen activation bitwidth config
        activation_bits = [<a id="change">quantized_model.layers[i].inbound_nodes[0].call_kwargs.get(&quotnum_bits&quot</a><a id="change">)</a> for i in
                           self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [4, 4]))
</code></pre><h3>After Change</h3><pre><code class='java'>

    def compare(self, quantized_model, float_model, input_x=None, quantization_info: UserInformation = None):
        &#47&#47 verify chosen activation bitwidth config
        activation_bits = [<a id="change">quantized_model.layers[i].activation_quantizers[0].get_config()[&quotnum_bits&quot]</a> for i in
                           self.activation_layers_idx]
        self.unit_test.assertTrue((activation_bits == [4, 4]))
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sony/model_optimization/commit/0242581af77f282d8ef063beb9f6a2f952b8f9d3#diff-3398a8d561f1763282df28883ea2701b4d4eee52fede99d66866ea668437b4faL487' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96283842</div><div id='project'> Project Name: sony/model_optimization</div><div id='commit'> Commit Name: 0242581af77f282d8ef063beb9f6a2f952b8f9d3</div><div id='time'> Time: 2023-03-12</div><div id='author'> Author: 44209964+reuvenperetz@users.noreply.github.com</div><div id='file'> File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_class'> M Class Name: MixedPrecisionMultipleKPIsTightSearchTest</div><div id='n_method'> N Class Name: MixedPrecisionMultipleKPIsTightSearchTest</div><div id='m_method'> M Method Name: compare(5)</div><div id='n_method'> N Method Name: compare(5)</div><div id='m_parent_class'> M Parent Class: MixedPrecisionActivationBaseTest</div><div id='n_parent_class'> N Parent Class: MixedPrecisionActivationBaseTest</div><div id='m_file'> M File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='n_file'> N File Name: tests/keras_tests/feature_networks_tests/feature_networks/mixed_precision_tests.py</div><div id='m_start'> M Start Line: 489</div><div id='m_end'> M End Line: 489</div><div id='n_start'> N Start Line: 491</div><div id='n_end'> N End Line: 491</div><BR>