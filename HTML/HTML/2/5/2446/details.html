<html><h3>Pattern ID :2446
</h3><img src='10290193.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            shared_kv_proj = default(shared_kv_proj, attn.to_kv)
            attn.to_kv = shared_kv_proj

            self.layers.append(nn.ModuleList(<a id="change">[
                </a>Residual(<a id="change">PreNorm(dim</a>, attn<a id="change">)</a>),
                Residual(<a id="change">PreNorm(dim</a>, ff<a id="change">)</a>)<a id="change"></a>
            ]))

        &#47&#47 memory parameters
</code></pre><h3>After Change</h3><pre><code class='java'>
                memory_is_empty = lambda *args, **kwargs: not exists(kwargs[&quotmemory&quot])
                attn = SkipIf(memory_is_empty, attn)

            self.layers.append(nn.ModuleList(<a id="change">[
                </a>attn,
                ff<a id="change"></a>
            ]))

        &#47&#47 memory parameters
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/feedback-transformer-pytorch/commit/60f98187905b58c55f0a3061201d10a8f7d49122#diff-6b348c286a3e8b8e064ec9c64a1f61ec6d6b66e7f14bae4c1b91af070d30c72eL204' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 10290193</div><div id='project'> Project Name: lucidrains/feedback-transformer-pytorch</div><div id='commit'> Commit Name: 60f98187905b58c55f0a3061201d10a8f7d49122</div><div id='time'> Time: 2021-02-03</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: feedback_transformer_pytorch/feedback_transformer_pytorch.py</div><div id='m_class'> M Class Name: FeedbackTransformer</div><div id='n_method'> N Class Name: FeedbackTransformer</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: feedback_transformer_pytorch/feedback_transformer_pytorch.py</div><div id='n_file'> N File Name: feedback_transformer_pytorch/feedback_transformer_pytorch.py</div><div id='m_start'> M Start Line: 204</div><div id='m_end'> M End Line: 213</div><div id='n_start'> N Start Line: 217</div><div id='n_end'> N End Line: 232</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        super().__init__()
        self.layers = nn.LayerList([])
        for _ in range(depth):
            self.layers.append(nn.LayerList(<a id="change">[
                </a><a id="change">PreNorm(</a>dim, Attention(dim, heads, dim_head, dropout)<a id="change">)</a>,
                <a id="change">PreNorm(</a>dim, FeedForward(dim, mlp_dim, dropout)<a id="change">)</a>
            ]))
    
    def forward(self, x):
        for attn, ff in self.layers:</code></pre><h3>After Change</h3><pre><code class='java'>
        super().__init__()
        depth_decay = [x.item() for x in paddle.linspace(0, droppath, depth)]

        layer_list = <a id="change">[]</a>
        for i in range(depth):
            layer_list.append(EncoderLayer(embed_dim, 
                                           num_heads,
                                           qkv_bias,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/br-idl/paddlevit/commit/8830bbf1fbf940d9ae0cfd9625201c78addcf9f5#diff-d311fd2dc051ce5f1c6d31872d3962ebb51b529b0bbcf6e02c1202591c0f6b0fL86' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 10290180</div><div id='project'> Project Name: br-idl/paddlevit</div><div id='commit'> Commit Name: 8830bbf1fbf940d9ae0cfd9625201c78addcf9f5</div><div id='time'> Time: 2021-10-20</div><div id='author'> Author: xperzy@gmail.com</div><div id='file'> File Name: image_classification/MobileViT/mobile_vit.py</div><div id='m_class'> M Class Name: Transformer</div><div id='n_method'> N Class Name: Transformer</div><div id='m_method'> M Method Name: __init__(9)</div><div id='n_method'> N Method Name: __init__(7)</div><div id='m_parent_class'> M Parent Class: nn.Layer</div><div id='n_parent_class'> N Parent Class: nn.Layer</div><div id='m_file'> M File Name: image_classification/MobileViT/mobile_vit.py</div><div id='n_file'> N File Name: image_classification/MobileViT/mobile_vit.py</div><div id='m_start'> M Start Line: 86</div><div id='m_end'> M End Line: 95</div><div id='n_start'> N Start Line: 190</div><div id='n_end'> N End Line: 215</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            shared_kv_proj = default(shared_kv_proj, attn.to_kv)
            attn.to_kv = shared_kv_proj

            self.layers.append(nn.ModuleList(<a id="change">[
                </a>Residual(<a id="change">PreNorm(</a>dim, attn<a id="change">)</a>),
                Residual(<a id="change">PreNorm(</a>dim, ff<a id="change">)</a>)<a id="change"></a>
            ]))

        &#47&#47 memory parameters
</code></pre><h3>After Change</h3><pre><code class='java'>
                memory_is_empty = lambda *args, **kwargs: not exists(kwargs[&quotmemory&quot])
                attn = SkipIf(memory_is_empty, attn)

            self.layers.append(nn.ModuleList(<a id="change">[
                </a>attn,
                ff<a id="change"></a>
            ]))

        &#47&#47 memory parameters
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/feedback-transformer-pytorch/commit/60f98187905b58c55f0a3061201d10a8f7d49122#diff-6b348c286a3e8b8e064ec9c64a1f61ec6d6b66e7f14bae4c1b91af070d30c72eL177' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 10290183</div><div id='project'> Project Name: lucidrains/feedback-transformer-pytorch</div><div id='commit'> Commit Name: 60f98187905b58c55f0a3061201d10a8f7d49122</div><div id='time'> Time: 2021-02-03</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: feedback_transformer_pytorch/feedback_transformer_pytorch.py</div><div id='m_class'> M Class Name: FeedbackTransformer</div><div id='n_method'> N Class Name: FeedbackTransformer</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: feedback_transformer_pytorch/feedback_transformer_pytorch.py</div><div id='n_file'> N File Name: feedback_transformer_pytorch/feedback_transformer_pytorch.py</div><div id='m_start'> M Start Line: 204</div><div id='m_end'> M End Line: 213</div><div id='n_start'> N Start Line: 217</div><div id='n_end'> N End Line: 232</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList(<a id="change">[
                </a>PreNorm(latent_dim, Attention(latent_dim, input_dim, dropout = attn_dropout), context_dim = input_dim),
                <a id="change">PreNorm(</a>latent_dim, FeedForward(latent_dim, dropout = ff_dropout)<a id="change">)</a>,
                <a id="change">PreNorm(</a>latent_dim, Attention(latent_dim, dropout = attn_dropout)<a id="change">)</a>,
                PreNorm(latent_dim, FeedForward(latent_dim, dropout = ff_dropout))<a id="change"></a>
            ]))

        self.to_logits = nn.Linear(latent_dim, num_classes)
</code></pre><h3>After Change</h3><pre><code class='java'>

        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList(<a id="change">[
                </a>get_cross_attn(),
                get_cross_ff(),
                get_latent_attn(),
                get_latent_ff()<a id="change"></a>
            ]))

        self.to_logits = nn.Linear(latent_dim, num_classes)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/perceiver-pytorch/commit/f0455b6ff59331de8151bf659b62ddf97ac802bd#diff-3a107568743779cad64d2e3582fceaafb7b32d2fd87d09209dce56be844a44a4L91' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 10290192</div><div id='project'> Project Name: lucidrains/perceiver-pytorch</div><div id='commit'> Commit Name: f0455b6ff59331de8151bf659b62ddf97ac802bd</div><div id='time'> Time: 2021-03-04</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: perceiver_pytorch/perceiver_pytorch.py</div><div id='m_class'> M Class Name: Perceiver</div><div id='n_method'> N Class Name: Perceiver</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: perceiver_pytorch/perceiver_pytorch.py</div><div id='n_file'> N File Name: perceiver_pytorch/perceiver_pytorch.py</div><div id='m_start'> M Start Line: 115</div><div id='m_end'> M End Line: 120</div><div id='n_start'> N Start Line: 127</div><div id='n_end'> N End Line: 142</div><BR>