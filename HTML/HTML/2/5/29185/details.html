<html><h3>Pattern ID :29185
</h3><img src='85898429.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47compute v loss
        target_v_value = (new_min_curr_state_q_value - new_curr_state_log_pi).detach()
        v_loss<a id="change"> = </a><a id="change">F.mse_loss(</a>curr_state_v_value, target_v_value<a id="change">)</a>
        v_loss_value = v_loss.detach().cpu().numpy()
        self.v_optimizer.zero_grad()
        v_loss.backward()
        self.v_optimizer.step()</code></pre><h3>After Change</h3><pre><code class='java'>
        new_curr_state_q1_value = self.q1_network(state_batch, new_curr_state_action)
        new_curr_state_q2_value = self.q2_network(state_batch, new_curr_state_action)

        next_state_q1_value<a id="change"> = </a>self.target_q1_network(next_state_batch, next_state_action)
        next_state_q2_value<a id="change"> = </a>self.target_q2_network(next_state_batch, next_state_action)
        next_state_min_q = torch.min(next_state_q1_value, next_state_q2_value)
        target_q = (next_state_min_q - self.alpha * next_state_log_pi)
        target_q = reward_batch + self.gamma * (1. - done_batch) * target_q

        new_min_curr_state_q_value = torch.min(new_curr_state_q1_value, new_curr_state_q2_value)

        &#47&#47compute q loss
        q1_loss = F.mse_loss(curr_state_q1_value, target_q.detach())
        q2_loss = F.mse_loss(curr_state_q2_value, target_q.detach())
        q1_loss_value = q1_loss.detach().cpu().numpy()
        q2_loss_value = q2_loss.detach().cpu().numpy()
        self.q1_optimizer.zero_grad()
        q1_loss.backward()
        self.q1_optimizer.step()
        self.q2_optimizer.zero_grad()
        q2_loss.backward()
        self.q2_optimizer.step()

        &#47&#47compute policy loss
        policy_loss = ((self.alpha * new_curr_state_log_pi) - new_min_curr_state_q_value).mean()
        policy_loss_value = policy_loss.detach().cpu().numpy()
        self.policy_optimizer.zero_grad()
        policy_loss.backward()
        self.policy_optimizer.step()

        &#47&#47compute entropy loss
        if self.automatic_entropy_tuning:
            alpha_loss = -(self.log_alpha * (new_curr_state_log_pi + self.target_entropy).detach()).mean()
            alpha_loss_value = alpha_loss.detach().cpu().numpy()
            self.alpha_optim.zero_grad()
            alpha_loss.backward()
            self.alpha_optim.step()

            self.alpha = self.log_alpha.exp()
            alpha_value = self.alpha.detach().cpu().numpy()
        else:
            alpha_loss = torch.tensor(0.).to(util.device)
            alpha_value = self.alpha.detach().cpu().numpy()
        self.tot_update_count += 1
        return q1_loss_value<a id="change">, q2_loss_value, policy_loss_value, alpha_loss_value, alpha_value</a>

    def try_update_target_network(self):
        if self.tot_update_count % self.update_target_network_interval == 0:
            util.soft_update_network(self.q1_network, self.target_q1_network, self.target_smoothing_tau)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/x35f/unstable_baselines/commit/0fc82ae6328814fe2dad0c8e0ae1b172d3e5f981#diff-5db644cd5bd54e3d73a4e86c3b2e245fd8f43751389307d15efbb99043bc1b0fL75' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 85898429</div><div id='project'> Project Name: x35f/unstable_baselines</div><div id='commit'> Commit Name: 0fc82ae6328814fe2dad0c8e0ae1b172d3e5f981</div><div id='time'> Time: 2021-03-12</div><div id='author'> Author: ym8411012@126.com</div><div id='file'> File Name: sac/models.py</div><div id='m_class'> M Class Name: SACAgent</div><div id='n_method'> N Class Name: SACAgent</div><div id='m_method'> M Method Name: update(2)</div><div id='n_method'> N Method Name: update(2)</div><div id='m_parent_class'> M Parent Class: BaseAgent,torch.nn.Module</div><div id='n_parent_class'> N Parent Class: BaseAgent,torch.nn.Module</div><div id='m_file'> M File Name: sac/models.py</div><div id='n_file'> N File Name: sac/models.py</div><div id='m_start'> M Start Line: 75</div><div id='m_end'> M End Line: 129</div><div id='n_start'> N Start Line: 83</div><div id='n_end'> N End Line: 129</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            target_q = self.target_model(next_state).max(1)[0] * (1-done) + reward
        q = self.model(state)
        q = q.gather(1, action.unsqueeze(-1)).squeeze(-1)
        q_loss<a id="change"> = </a><a id="change">F.mse_loss(</a>q, target_q<a id="change">)</a>

        self.q_optimizer.zero_grad()
        q_loss.backward()
        if self.use_grad_clip:</code></pre><h3>After Change</h3><pre><code class='java'>
            target_q = self.target_model(next_state).max(1)[0] * (1-done) + reward
        q = self.model(state)
        q = q.gather(1, action.unsqueeze(-1)).squeeze(-1)
        q_loss<a id="change"> = </a>((q - target_q).pow(2)*weights).mean()
        td_error<a id="change"> = </a>torch.abs(q.detach() - target_q.detach()) &#47&#47 used in prioritized experience replay

        self.q_optimizer.zero_grad()
        q_loss.backward()
        if self.use_grad_clip:
            nn.utils.clip_grad_norm_(self.model.q_params, self.max_grad_norm)
        self.q_optimizer.step()

        return [q_loss.item()]<a id="change">, td_error</a>

    def update_target(self):
        for target_param, param in zip(self.target_model.parameters(), self.model.parameters()):
            target_param.detach_()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/karlxing/rlcodebase/commit/3dff497e3c5cff01ef5340a2240d90eae0c5b8d5#diff-d1da58865e49185accc152438bed1c813d905ab4bca674ea6ee79db151ae9f08L30' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 85898442</div><div id='project'> Project Name: karlxing/rlcodebase</div><div id='commit'> Commit Name: 3dff497e3c5cff01ef5340a2240d90eae0c5b8d5</div><div id='time'> Time: 2020-10-11</div><div id='author'> Author: jinweixing1006@gmail.com</div><div id='file'> File Name: rlcodebase/policy/dqn_policy.py</div><div id='m_class'> M Class Name: DQNPolicy</div><div id='n_method'> N Class Name: DQNPolicy</div><div id='m_method'> M Method Name: learn_on_batch(2)</div><div id='n_method'> N Method Name: learn_on_batch(2)</div><div id='m_parent_class'> M Parent Class: BasePolicy</div><div id='n_parent_class'> N Parent Class: BasePolicy</div><div id='m_file'> M File Name: rlcodebase/policy/dqn_policy.py</div><div id='n_file'> N File Name: rlcodebase/policy/dqn_policy.py</div><div id='m_start'> M Start Line: 38</div><div id='m_end'> M End Line: 47</div><div id='n_start'> N Start Line: 32</div><div id='n_end'> N End Line: 49</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

def StegoLoss(secret, cover, container, container_2x, revealed, beta):

	loss_cover<a id="change"> = </a><a id="change">F.mse_loss(</a>cover, container<a id="change">)</a>
	loss_secret = nn.L1Loss()
	loss_spectrum = F.mse_loss(container, container_2x)
	loss = (1 - beta) * (loss_cover) + beta * loss_secret(secret, revealed)
	return loss, loss_cover, loss_secret(secret, revealed), loss_spectrum</code></pre><h3>After Change</h3><pre><code class='java'>

def StegoLoss(secret, cover, container, container_2x, revealed, beta):
	loss_L1 = nn.L1Loss()
	loss_cover<a id="change"> = </a>loss_L1(cover, container)
	loss_secret = loss_L1(secret, revealed)
	loss_spectrum<a id="change"> = </a>loss_L1(container, container_2x)
	loss = (1 - beta) * loss_cover + beta * loss_secret
	return loss<a id="change">, loss_cover, loss_secret, loss_spectrum</a>
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/margaritageleta/pixinwav/commit/c85f63097aebb653073b3bf5ed5026c6382deccb#diff-d1e2ecb67d8cfabe062fa8cabae3017222f9e88ccfacf88223e5857e9a28dfb4L86' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 85898427</div><div id='project'> Project Name: margaritageleta/pixinwav</div><div id='commit'> Commit Name: c85f63097aebb653073b3bf5ed5026c6382deccb</div><div id='time'> Time: 2021-03-08</div><div id='author'> Author: noticiasmundiales99@gmail.com</div><div id='file'> File Name: src/losses.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: StegoLoss(6)</div><div id='n_method'> N Method Name: StegoLoss(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/losses.py</div><div id='n_file'> N File Name: src/losses.py</div><div id='m_start'> M Start Line: 88</div><div id='m_end'> M End Line: 92</div><div id='n_start'> N Start Line: 87</div><div id='n_end'> N End Line: 92</div><BR>