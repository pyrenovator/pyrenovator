<html><h3>Pattern ID :41382
</h3><img src='116445282.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                2) + torch.finfo(torch.float32).eps)
            seq_len_ = seq_len.expand(-1, mask.size(1), -1)
            mask = mask &gt;= seq_len_
            <a id="change">if </a>self.pooling_type == &quotmax&quot:
                batch_seq_embeddings<a id="change"> = </a><a id="change">batch_seq_embeddings.masked_fill(
                    </a>mask, <a id="change">0.0</a><a id="change">)</a>
                if not self.keepdim:
                    result = batch_seq_embeddings.max(dim=1)
                else:
                    result = batch_seq_embeddings.max(dim=1).unsqueeze(1)</code></pre><h3>After Change</h3><pre><code class='java'>
        if weight is not None:
            batch_seq_embeddings = weight.unsqueeze(-1) * batch_seq_embeddings

        <a id="change">if </a>self.pooling_type == &quotmask&quot:
            &#47&#47 Data type of mask_token should be bool and 
            &#47&#47 the shape of mask_token should be [B, L]
            assert mask_token != None, "mask_token can be None when pooling_type is &quotmask&quot."</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/ustcml/recstudio/commit/fdd37471c7a73d0c5c0efb207c5ec53e563e6fa5#diff-679f2f47157831f14d073b36c7d0056116ae30fcb4236442f93be583e7cbe5d4L234' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116445282</div><div id='project'> Project Name: ustcml/recstudio</div><div id='commit'> Commit Name: fdd37471c7a73d0c5c0efb207c5ec53e563e6fa5</div><div id='time'> Time: 2022-09-08</div><div id='author'> Author: chenxiaolong0502@163.com</div><div id='file'> File Name: recstudio/model/module/layers.py</div><div id='m_class'> M Class Name: SeqPoolingLayer</div><div id='n_method'> N Class Name: SeqPoolingLayer</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: recstudio/model/module/layers.py</div><div id='n_file'> N File Name: recstudio/model/module/layers.py</div><div id='m_start'> M Start Line: 241</div><div id='m_end'> M End Line: 283</div><div id='n_start'> N Start Line: 234</div><div id='n_end'> N End Line: 291</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        ) // self.conv.stride[0] + 1

    def forward(self, x, lens):
        <a id="change">if </a>self.use_mask:
            lens = lens.to(dtype=torch.long)
            max_len = x.size(2)
            mask = torch.arange(max_len).to(lens.device).expand(
                len(lens), max_len
            ) &gt;= lens.unsqueeze(1)
            x<a id="change"> = </a><a id="change">x.masked_fill(</a>mask.unsqueeze(1).to(device=x.device), <a id="change">0</a><a id="change">)</a>
            &#47&#47 del mask
            lens = self.get_seq_len(lens)

        sh = x.shape</code></pre><h3>After Change</h3><pre><code class='java'>
        Returns:
            Both the signal processed by the convolution and the resulting lengths
        
        <a id="change">if </a>self.use_mask:
            x = self.mask_fill(x, lens)
        out = self.conv(x)
        return out, self.get_seq_len(lens)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/scart97/thunder-speech/commit/1eb7dfb6a8a7b1e3d30aaebb26a0f0ca0390e542#diff-466f09dc94a8d0261c5affeb35283ea929f8f99c0560176aff78070c963baf18L165' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116445273</div><div id='project'> Project Name: scart97/thunder-speech</div><div id='commit'> Commit Name: 1eb7dfb6a8a7b1e3d30aaebb26a0f0ca0390e542</div><div id='time'> Time: 2021-01-30</div><div id='author'> Author: scart.lucas@gmail.com</div><div id='file'> File Name: src/thunder/jasper/blocks.py</div><div id='m_class'> M Class Name: MaskedConv1d</div><div id='n_method'> N Class Name: MaskedConv1d</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/thunder/jasper/blocks.py</div><div id='n_file'> N File Name: src/thunder/jasper/blocks.py</div><div id='m_start'> M Start Line: 165</div><div id='m_end'> M End Line: 185</div><div id='n_start'> N Start Line: 225</div><div id='n_end'> N End Line: 239</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                weighted by the attention score (&#47&#47batch, time1, time2).
        
        n_batch = value.size(0)
        <a id="change">if </a>mask is not None:
            mask = mask.unsqueeze(1).eq(0)  &#47&#47 (batch, 1, *, time2)
            min_value = float(
                numpy.finfo(torch.tensor(0, dtype=scores.dtype).numpy().dtype).min
            )
            scores = scores.masked_fill(mask, min_value)
            self.attn<a id="change"> = </a><a id="change">torch.softmax(scores, dim=-1).masked_fill(
                </a>mask, <a id="change">0.0</a><a id="change">
            )</a>  &#47&#47 (batch, head, time1, time2)
        else:
            self.attn = torch.softmax(scores, dim=-1)  &#47&#47 (batch, head, time1, time2)
</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward_attention(self, value, scores, mask):
        n_batch = value.size(0)
        
        <a id="change">if </a>mask is not None:
            mask = mask.unsqueeze(1).eq(0)
            scores = scores.masked_fill(mask, -1e-9)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/qute012/kosr/commit/f3453031ba358fa025b347c39388192b8afe2b47#diff-7f36dd0730f0eb0464c1216d16594610a364ce19e2fc429c6737625c44fe0c38L101' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116445263</div><div id='project'> Project Name: qute012/kosr</div><div id='commit'> Commit Name: f3453031ba358fa025b347c39388192b8afe2b47</div><div id='time'> Time: 2021-01-25</div><div id='author'> Author: ejrwls012@gmail.com</div><div id='file'> File Name: kosr/model/attention.py</div><div id='m_class'> M Class Name: MultiHeadAttention</div><div id='n_method'> N Class Name: MultiHeadAttention</div><div id='m_method'> M Method Name: forward_attention(4)</div><div id='n_method'> N Method Name: forward_attention(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: kosr/model/attention.py</div><div id='n_file'> N File Name: kosr/model/attention.py</div><div id='m_start'> M Start Line: 112</div><div id='m_end'> M End Line: 124</div><div id='n_start'> N Start Line: 34</div><div id='n_end'> N End Line: 38</div><BR>