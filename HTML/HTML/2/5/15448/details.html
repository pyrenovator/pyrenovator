<html><h3>Pattern ID :15448
</h3><img src='52532481.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        assert len(signals) == params.shape[2]
        signals = torch.stack(signals, dim=1)

        <a id="change">return </a><a id="change">torch.matmul(</a>params, signals<a id="change">)</a>.as_subclass(Signal)


class CrossfadeKnob(SynthModule):</code></pre><h3>After Change</h3><pre><code class='java'>
        assert len(signals) == params.shape[2]
        signals = torch.stack(signals, dim=1)

        modulation<a id="change"> = </a>torch.chunk(<a id="change">torch.matmul(</a>params, signals<a id="change">)</a>, self.n_output, dim=1)
        <a id="change">return </a>tuple(m.squeeze(1).as_subclass(Signal) for m in modulation)


class CrossfadeKnob(SynthModule):</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 5</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/turian/torchsynth/commit/68db954a82944813d015fdb6ffa13e2bc1500382#diff-b8f8a320dfa5b5be5e5fa643ac1e25c4bcb6a59494686bb4e1cc35b4b28dcb4aL628' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 52532481</div><div id='project'> Project Name: turian/torchsynth</div><div id='commit'> Commit Name: 68db954a82944813d015fdb6ffa13e2bc1500382</div><div id='time'> Time: 2021-03-20</div><div id='author'> Author: jordieshier@gmail.com</div><div id='file'> File Name: torchsynth/module.py</div><div id='m_class'> M Class Name: ModulationMixer</div><div id='n_method'> N Class Name: ModulationMixer</div><div id='m_method'> M Method Name: forward(1)</div><div id='n_method'> N Method Name: forward(1)</div><div id='m_parent_class'> M Parent Class: SynthModule</div><div id='n_parent_class'> N Parent Class: SynthModule</div><div id='m_file'> M File Name: torchsynth/module.py</div><div id='n_file'> N File Name: torchsynth/module.py</div><div id='m_start'> M Start Line: 646</div><div id='m_end'> M End Line: 652</div><div id='n_start'> N Start Line: 628</div><div id='n_end'> N End Line: 635</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        q, k, v = (paddle.to_tensor(i,stop_gradient=False) 
                        for i in map(lambda t: rearrange(t, &quotb p n (h d) -&gt; b p h n d&quot, h = 1), 
                        (i.numpy() for i in qkv)))
        dots = <a id="change">paddle.matmul(</a>q, k.transpose((0,1,2,4,3))<a id="change">)</a> * self.scale
        attn = self.attend(dots)
        out = paddle.matmul(attn, v)
        out = paddle.to_tensor(rearrange(out.numpy(), &quotb p h n d -&gt; b p n (h d)&quot),stop_gradient=False)
        <a id="change">return </a>self.to_out(out)

class Transformer(nn.Layer):
    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):</code></pre><h3>After Change</h3><pre><code class='java'>
        qkv = self.qkv(x).chunk(3, axis=-1)
        q, k, v = map(self.transpose_multihead, qkv)

        attn = <a id="change">paddle.matmul(</a>q, k<a id="change">, transpose_y=True)</a>
        attn = attn * self.scales
        attn<a id="change"> = </a>self.softmax(attn)
        attn = self.attn_dropout(attn)
        &#47&#47 [batch_size, P, num_heads, N, N]

        z = paddle.matmul(attn, v)
        &#47&#47 [batch_size, P, num_heads, N, d]
        z = z.transpose([0, 1, 3, 2, 4])
        B, P, N, H, D = z.shape
        z = z.reshape([B, P, N, H * D])
        z = self.proj(z)
        z = self.proj_dropout(z)
        <a id="change">return </a>z


&#47&#47 DONE</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/br-idl/paddlevit/commit/8830bbf1fbf940d9ae0cfd9625201c78addcf9f5#diff-d311fd2dc051ce5f1c6d31872d3962ebb51b529b0bbcf6e02c1202591c0f6b0fL73' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 52532480</div><div id='project'> Project Name: br-idl/paddlevit</div><div id='commit'> Commit Name: 8830bbf1fbf940d9ae0cfd9625201c78addcf9f5</div><div id='time'> Time: 2021-10-20</div><div id='author'> Author: xperzy@gmail.com</div><div id='file'> File Name: image_classification/MobileViT/mobile_vit.py</div><div id='m_class'> M Class Name: Attention</div><div id='n_method'> N Class Name: Attention</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Layer</div><div id='n_parent_class'> N Parent Class: nn.Layer</div><div id='m_file'> M File Name: image_classification/MobileViT/mobile_vit.py</div><div id='n_file'> N File Name: image_classification/MobileViT/mobile_vit.py</div><div id='m_start'> M Start Line: 74</div><div id='m_end'> M End Line: 83</div><div id='n_start'> N Start Line: 132</div><div id='n_end'> N End Line: 148</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            Mel-cepstral coefficients.

        
        c = <a id="change">torch.matmul(</a>b, self.A if b.dtype == self.A.dtype else self.A.double()<a id="change">)</a>
        <a id="change">return </a>c
</code></pre><h3>After Change</h3><pre><code class='java'>

        
        A = self.A if b.dtype == self.A.dtype else self.A.double()
        mc<a id="change"> = </a><a id="change">torch.matmul(</a>b, A<a id="change">)</a>
        <a id="change">return </a>mc
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sp-nitech/diffsptk/commit/1a00aca4573f667ebc80cc24dbf4c978fc99f1fb#diff-73e7de3bd4455b45100f576df90cacc53c928759a4b10fe7c4764a2ab762f6a3L49' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 52532483</div><div id='project'> Project Name: sp-nitech/diffsptk</div><div id='commit'> Commit Name: 1a00aca4573f667ebc80cc24dbf4c978fc99f1fb</div><div id='time'> Time: 2022-03-12</div><div id='author'> Author: takenori.yoshimura24@gmail.com</div><div id='file'> File Name: diffsptk/b2mc.py</div><div id='m_class'> M Class Name: MLSADigitalFilterCoefficientsToMelCepstrum</div><div id='n_method'> N Class Name: MLSADigitalFilterCoefficientsToMelCepstrum</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: diffsptk/b2mc.py</div><div id='n_file'> N File Name: diffsptk/b2mc.py</div><div id='m_start'> M Start Line: 63</div><div id='m_end'> M End Line: 64</div><div id='n_start'> N Start Line: 65</div><div id='n_end'> N End Line: 67</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        garbage_component = self.pca_param_dict[
            "bot_1_eigenvector"
        ]  &#47&#47 TODO: generalize to more evecs
        garbage_variance = <a id="change">torch.matmul(</a>data_arr.T, garbage_component.T<a id="change">)</a>
        <a id="change">return </a>torch.linalg.norm(garbage_variance)

    def training_step(self, data, batch_idx):
        &#47&#47 x, y_heatmap, y_keypoints = data</code></pre><h3>After Change</h3><pre><code class='java'>
        keypoints = keypoints[:, :, :2]
        data_arr = format_mouse_data(keypoints)
        abs_proj_discarded = torch.abs(
            <a id="change">torch.matmul(</a>data_arr.T, self.pca_param_dict["discarded_eigenvectors"].T<a id="change">)</a>
        )
        epsilon_masked_proj<a id="change"> = </a>abs_proj_discarded.masked_fill(
            mask=abs_proj_discarded &gt; self.pca_param_dict["epsilon"], value=0.0
        )
        assert (epsilon_masked_proj &gt;= 0.0).all()  &#47&#47 every element should be positive
        assert torch.mean(epsilon_masked_proj) &lt;= torch.mean(
            abs_proj_discarded
        )  &#47&#47 the scalar loss should be smaller after zeroing out elements.
        <a id="change">return </a>torch.mean(epsilon_masked_proj)

    def training_step(self, data, batch_idx):
        &#47&#47 x, y_heatmap, y_keypoints = data</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/danbider/lightning-pose/commit/55d46e2d5e4099b9bed0754557f7336b2ab52cd6#diff-890353b4f8b4f5145f0dc065ee06322208b29f3527b00fabb372f519c2541645L160' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 52532474</div><div id='project'> Project Name: danbider/lightning-pose</div><div id='commit'> Commit Name: 55d46e2d5e4099b9bed0754557f7336b2ab52cd6</div><div id='time'> Time: 2021-09-06</div><div id='author'> Author: </div><div id='file'> File Name: pose_est_nets/models/heatmap_tracker.py</div><div id='m_class'> M Class Name: DLC</div><div id='n_method'> N Class Name: DLC</div><div id='m_method'> M Method Name: pca_2view_loss(2)</div><div id='n_method'> N Method Name: pca_2view_loss(2)</div><div id='m_parent_class'> M Parent Class: LightningModule</div><div id='n_parent_class'> N Parent Class: LightningModule</div><div id='m_file'> M File Name: pose_est_nets/models/heatmap_tracker.py</div><div id='n_file'> N File Name: pose_est_nets/models/heatmap_tracker.py</div><div id='m_start'> M Start Line: 176</div><div id='m_end'> M End Line: 180</div><div id='n_start'> N Start Line: 177</div><div id='n_end'> N End Line: 187</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        output, attention_weights
    

    matmul_qk = <a id="change">tf.matmul(</a>q, k<a id="change">, transpose_b=True)</a>  &#47&#47 (..., seq_len_q, seq_len_k)
    &#47&#47 scale matmul_qk
    dk = tf.cast(tf.shape(k)[-1], q.dtype)
    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)
    &#47&#47 add the mask to the scaled tensor.
    if mask is not None:
        scaled_attention_logits += (tf.cast(mask, dtype=q.dtype) * -1e9)
    &#47&#47 softmax is normalized on the last axis (seq_len_k) so that the scores
    &#47&#47 add up to 1.
    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  &#47&#47 (..., seq_len_q, seq_len_k)
    output = tf.matmul(attention_weights, v)  &#47&#47 (..., seq_len_q, depth_v)
    <a id="change">return </a>output


class MultiHeadAttention(tf.keras.layers.Layer):</code></pre><h3>After Change</h3><pre><code class='java'>
) -&gt; Tuple[tf.Tensor, tf.Tensor]:
     Scaled Dot-Product Attention 

    scores = <a id="change">tf.matmul(</a>query, tf.transpose(key, perm=[0, 1, 3, 2])<a id="change">)</a> / math.sqrt(query.shape[-1])
    if mask is not None:
        scores<a id="change"> = </a>tf.where(mask == 0, -1e9, scores)
    p_attn = tf.nn.softmax(scores, axis=-1)
    <a id="change">return </a>tf.matmul(p_attn, value), p_attn


class PositionwiseFeedForward(layers.Layer, NestedObject):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/mindee/doctr/commit/9530f81d15395006b4844299236bdadba11c1dde#diff-e1106a3628d5365c5fe2e4fec7304ea8e40c58a5439f23d8f20513e722955178L70' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 52532477</div><div id='project'> Project Name: mindee/doctr</div><div id='commit'> Commit Name: 9530f81d15395006b4844299236bdadba11c1dde</div><div id='time'> Time: 2022-07-01</div><div id='author'> Author: felixdittrich92@gmail.com</div><div id='file'> File Name: doctr/models/recognition/transformer/tensorflow.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: scaled_dot_product_attention(4)</div><div id='n_method'> N Method Name: scaled_dot_product_attention(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: doctr/models/recognition/transformer/tensorflow.py</div><div id='n_file'> N File Name: doctr/models/recognition/transformer/tensorflow.py</div><div id='m_start'> M Start Line: 88</div><div id='m_end'> M End Line: 99</div><div id='n_start'> N Start Line: 63</div><div id='n_end'> N End Line: 67</div><BR>