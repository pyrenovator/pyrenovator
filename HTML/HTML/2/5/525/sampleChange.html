<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                pl_penalty = (pl_lengths - pl_mean).square()
                loss_Gpl = pl_penalty * self.pl_weight

                loss_Gpl = (gen_img[:, 0, 0, 0] * 0 + loss_Gpl).mean()<a id="change"> * </a>float(gain)
                loss_numpy[&quotloss_Gpl&quot]<a id="change"> = </a><a id="change">loss_Gpl.cpu().detach().numpy()</a>
            with torch.autograd.profiler.record_function(&quotGpl_backward&quot):
                loss_Gpl.backward()  &#47&#47 咩酱：gain即上文提到的这个阶段的训练间隔。
                if self.align_grad:
                    mapping = self.mapping.module if self.is_distributed else self.mapping</code></pre><h3>After Change</h3><pre><code class='java'>
                &#47&#47 loss3 = loss_Dgen * float(gain)
            with torch.autograd.profiler.record_function(&quotDgen_backward&quot):
                &#47&#47 loss3.backward()  &#47&#47 咩酱：gain即上文提到的这个阶段的训练间隔。
                <a id="change">loss_Dgen.mean().mul(gain).backward()</a>
                if self.align_grad:
                    mapping = self.mapping.module if self.is_distributed else self.mapping
                    synthesis = self.synthesis.module if self.is_distributed else self.synthesis
                    discriminator = self.discriminator.module if self.is_distributed else self.discriminator</code></pre>