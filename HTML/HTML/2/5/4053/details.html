<html><h3>Pattern ID :4053
</h3><img src='15087334.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        sim = einsum(&quotb i d, b j d -&gt; b i j&quot, q, k)

        if self.causal:
            sim = sim / rearrange(<a id="change">torch.arange(</a>seq_len<a id="change">, device = device) + </a>1, &quot... -&gt; ... 1&quot)
        else:
            sim = sim / seq_len
</code></pre><h3>After Change</h3><pre><code class='java'>
        attn = self.dropout(attn)

        if exists(mask):
            mask = <a id="change">rearrange(</a>mask, <a id="change">&quotb j -&gt; b 1 j&quot</a><a id="change">)</a>
            attn<a id="change"> = </a>attn.masked_fill(~mask, 0.)

        if self.causal:
            causal_mask = torch.ones((seq_len, seq_len), dtype = torch.bool, device = device).triu(1)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/flash-pytorch/commit/6b0cc2e2316bf9c93b8b48916a11f774209d7bf1#diff-6408ce30d3c8730249ca81344fdd04a1dd01ed7b072e84e9d752276e93661682L162' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15087334</div><div id='project'> Project Name: lucidrains/flash-pytorch</div><div id='commit'> Commit Name: 6b0cc2e2316bf9c93b8b48916a11f774209d7bf1</div><div id='time'> Time: 2022-03-28</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: flash_pytorch/flash_pytorch.py</div><div id='m_class'> M Class Name: GAU</div><div id='n_method'> N Class Name: GAU</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: flash_pytorch/flash_pytorch.py</div><div id='n_file'> N File Name: flash_pytorch/flash_pytorch.py</div><div id='m_start'> M Start Line: 171</div><div id='m_end'> M End Line: 178</div><div id='n_start'> N Start Line: 162</div><div id='n_end'> N End Line: 184</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        * bool: whether to return as array of idxs or boolean values
        Outputs: (N_mask, CA_mask)
    
    lengths = <a id="change">torch.arange(</a>scn_seq.shape[-1]*l_aa<a id="change">)</a>
    &#47&#47 repeat if needed:
    if len(lengths.shape) == 2:
        lengths = repeat(lengths, &quotl -&gt; b l&quot, b=scn_seq.shape[0])
    &#47&#47 N is the first atom in every AA. CA is the 2nd.
    N_mask  = lengths%l_aa == 0
    CA_mask = lengths<a id="change">%</a>l_aa == 1
    if boolean:
        return N_mask, CA_mask
    return N_mask.nonzero(), CA_mask.nonzero()</code></pre><h3>After Change</h3><pre><code class='java'>
    wrapper[:, 0] = 1
    wrapper[:, 1] = 2
    wrapper[:, 2] = 3
    wrapper = <a id="change">rearrange(</a>wrapper, <a id="change">&quot... l c -&gt; ... (l c)&quot</a><a id="change">)</a>
    &#47&#47 find idxs
    N_mask<a id="change">  = </a>wrapper == 1
    CA_mask = wrapper == 2
    C_mask  = wrapper == 3 
    if boolean:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/alphafold2/commit/1e5f99521575995589d1fbe94d2bd693f7e274c9#diff-f9b2e05be1f80257f9a80ec7cc1290e3337a54fcc62aeee31b23a4cb827f12efL182' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15087335</div><div id='project'> Project Name: lucidrains/alphafold2</div><div id='commit'> Commit Name: 1e5f99521575995589d1fbe94d2bd693f7e274c9</div><div id='time'> Time: 2021-03-03</div><div id='author'> Author: ericalcaide1@gmail.com</div><div id='file'> File Name: alphafold2_pytorch/utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: scn_backbone_mask(3)</div><div id='n_method'> N Method Name: scn_backbone_mask(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: alphafold2_pytorch/utils.py</div><div id='n_file'> N File Name: alphafold2_pytorch/utils.py</div><div id='m_start'> M Start Line: 189</div><div id='m_end'> M End Line: 198</div><div id='n_start'> N Start Line: 189</div><div id='n_end'> N End Line: 201</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 add dynamic positional bias

        i_pos = torch.arange(wsz, device = device)
        j_pos = <a id="change">torch.arange(</a>wsz<a id="change">, device = device)</a>
        grid = torch.stack(torch.meshgrid(i_pos, j_pos))
        grid = rearrange(grid, &quotc i j -&gt; (i j) c&quot)
        rel_ij = grid[:, None]<a id="change"> - </a>grid[None, :]
        rel_pos_bias = self.dpb(rel_ij.float())

        sim = sim + rel_pos_bias</code></pre><h3>After Change</h3><pre><code class='java'>

        pos = torch.arange(-wsz, wsz + 1, device = device)
        rel_pos = torch.stack(torch.meshgrid(pos, pos))
        rel_pos<a id="change"> = </a><a id="change">rearrange(</a>rel_pos, <a id="change">&quotc i j -&gt; (i j) c&quot</a><a id="change">)</a>
        biases = self.dpb(rel_pos.float())
        rel_pos_bias = biases[self.rel_pos_indices]

        sim = sim + rel_pos_bias</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/vit-pytorch/commit/b69b5af34f7759948425113f6dc3b30dfb91d4d1#diff-65c2de0175e6c0cd43148fc01483ce4dcd04a52ba15130b05fdc9f6521caf494L112' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15087368</div><div id='project'> Project Name: lucidrains/vit-pytorch</div><div id='commit'> Commit Name: b69b5af34f7759948425113f6dc3b30dfb91d4d1</div><div id='time'> Time: 2021-11-22</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: vit_pytorch/crossformer.py</div><div id='m_class'> M Class Name: Attention</div><div id='n_method'> N Class Name: Attention</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: vit_pytorch/crossformer.py</div><div id='n_file'> N File Name: vit_pytorch/crossformer.py</div><div id='m_start'> M Start Line: 139</div><div id='m_end'> M End Line: 144</div><div id='n_start'> N Start Line: 152</div><div id='n_end'> N End Line: 156</div><BR>