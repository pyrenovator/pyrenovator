<html><h3>Pattern ID :35600
</h3><img src='101500788.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                nn.Linear(
                    in_features=context_time_features, out_features=out_channels * 2
                ),
            )<a id="change">
            if </a>exists(context_time_features)<a id="change">
            else </a>None
        )

        self.block1 = ConvBlock1d(</code></pre><h3>After Change</h3><pre><code class='java'>
    ) -&gt; None:
        super().__init__()

        self.use_mapping = <a id="change">exists(</a>context_mapping_features<a id="change">)</a>
        self.use_embedding = exists(context_embedding_features)

        self.block1 = ConvBlock1d(
            in_channels=in_channels,
            out_channels=out_channels,
            num_groups=num_groups,
            dilation=dilation,
        )

        <a id="change">if </a>self.use_mapping:
            <a id="change">assert </a>exists(context_mapping_features)
            self.to_scale_shift<a id="change"> = </a>MappingToScaleShift(
                features=context_mapping_features, channels=out_channels
            )
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/archinetai/audio-diffusion-pytorch/commit/3c710edf168da06dcc89c23ef12023d0f8b63043#diff-daa41e4baa3d2f79ac9df5a1fdce9cb4f87f4b10c9335e3653c38e64ecae3966L147' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 101500788</div><div id='project'> Project Name: archinetai/audio-diffusion-pytorch</div><div id='commit'> Commit Name: 3c710edf168da06dcc89c23ef12023d0f8b63043</div><div id='time'> Time: 2022-09-21</div><div id='author'> Author: flavio.schneider.97@gmail.com</div><div id='file'> File Name: audio_diffusion_pytorch/modules.py</div><div id='m_class'> M Class Name: ResnetBlock1d</div><div id='n_method'> N Class Name: ResnetBlock1d</div><div id='m_method'> M Method Name: __init__(0)</div><div id='n_method'> N Method Name: __init__(0)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: audio_diffusion_pytorch/modules.py</div><div id='n_file'> N File Name: audio_diffusion_pytorch/modules.py</div><div id='m_start'> M Start Line: 167</div><div id='m_end'> M End Line: 176</div><div id='n_start'> N Start Line: 147</div><div id='n_end'> N End Line: 163</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        self.channels = channels
        self.self_condition = self_condition
        input_channels = channels * (2<a id="change"> if </a>self_condition<a id="change"> else </a>1)

        init_dim = default(init_dim, dim)
        self.init_conv = nn.Conv2d(input_channels, init_dim, 7, padding = 3)</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 for initial dwt transform (or whatever transform researcher wants to try here)

        <a id="change">if </a>exists(init_img_transform) and <a id="change">exists(</a>final_img_itransform<a id="change">)</a>:
            init_shape<a id="change"> = </a>torch.Size(1, 1, 32, 32)
            mock_tensor = torch.randn(init_shape)
            <a id="change">assert </a>final_img_itransform(init_img_transform(mock_tensor)).shape == init_shape

        self.init_img_transform = default(init_img_transform, identity)
        self.final_img_itransform = default(final_img_itransform, identity)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/denoising-diffusion-pytorch/commit/1ca516ad6487b8356a6fffbe5b307f74df76d1d1#diff-932ae956742e2e7e4ca0d46053f9844b2504bc2cf2dca187505413a2f363e4a3L281' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 101500789</div><div id='project'> Project Name: lucidrains/denoising-diffusion-pytorch</div><div id='commit'> Commit Name: 1ca516ad6487b8356a6fffbe5b307f74df76d1d1</div><div id='time'> Time: 2023-01-31</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: denoising_diffusion_pytorch/simple_diffusion.py</div><div id='m_class'> M Class Name: UViT</div><div id='n_method'> N Class Name: UViT</div><div id='m_method'> M Method Name: __init__(17)</div><div id='n_method'> N Method Name: __init__(15)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: denoising_diffusion_pytorch/simple_diffusion.py</div><div id='n_file'> N File Name: denoising_diffusion_pytorch/simple_diffusion.py</div><div id='m_start'> M Start Line: 293</div><div id='m_end'> M End Line: 366</div><div id='n_start'> N Start Line: 302</div><div id='n_end'> N End Line: 391</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            ConvOut1d(
                channels=out_channels,
                kernel_sizes=kernel_sizes_out,
            )<a id="change">
            if </a>exists(kernel_sizes_out)<a id="change">
            else </a>nn.Identity(),
        )

    def get_channels(</code></pre><h3>After Change</h3><pre><code class='java'>
        self.use_context_time = use_context_time
        self.use_context_features = use_context_features
        self.use_context_channels = use_context_channels
        self.use_post_out_block = <a id="change">exists(</a>kernel_sizes_out<a id="change">)</a>

        context_channels_pad_length = num_layers + 1 - len(context_channels)
        context_channels = context_channels + [0] * context_channels_pad_length
        self.context_channels = context_channels

        if use_context_channels:
            has_context = [c &gt; 0 for c in context_channels]
            self.has_context = has_context
            self.channels_ids = [sum(has_context[:i]) for i in range(len(has_context))]

        assert (
            len(factors) == num_layers
            and len(attentions) == num_layers
            and len(num_blocks) == num_layers
        )

        self.to_in = nn.Sequential(
            Rearrange("b c (l p) -&gt; b (c p) l", p=patch_size),
            CrossEmbed1d(
                in_channels=(in_channels + context_channels[0]) * patch_size,
                out_channels=channels,
                kernel_sizes=kernel_sizes_init,
                stride=1,
            ),
        )

        if use_context_time or use_context_features:
            context_mapping_features = channels * 4

            self.to_mapping = nn.Sequential(
                nn.Linear(context_mapping_features, context_mapping_features),
                nn.GELU(),
                nn.Linear(context_mapping_features, context_mapping_features),
                nn.GELU(),
            )

        if use_context_time:
            assert exists(context_mapping_features)
            self.to_time = nn.Sequential(
                TimePositionalEmbedding(
                    dim=channels, out_features=context_mapping_features
                ),
                nn.GELU(),
            )

        if use_context_features:
            assert exists(context_features) and exists(context_mapping_features)
            self.to_features = nn.Sequential(
                nn.Linear(
                    in_features=context_features, out_features=context_mapping_features
                ),
                nn.GELU(),
            )

        self.downsamples = nn.ModuleList(
            [
                DownsampleBlock1d(
                    in_channels=channels * multipliers[i],
                    out_channels=channels * multipliers[i + 1],
                    context_mapping_features=context_mapping_features,
                    context_channels=context_channels[i + 1],
                    context_embedding_features=context_embedding_features,
                    num_layers=num_blocks[i],
                    factor=factors[i],
                    kernel_multiplier=kernel_multiplier_downsample,
                    num_groups=resnet_groups,
                    use_pre_downsample=True,
                    use_skip=True,
                    use_attention=attentions[i],
                    attention_heads=attention_heads,
                    attention_features=attention_features,
                    attention_multiplier=attention_multiplier,
                )
                for i in range(num_layers)
            ]
        )

        self.bottleneck = BottleneckBlock1d(
            channels=channels * multipliers[-1],
            context_mapping_features=context_mapping_features,
            context_embedding_features=context_embedding_features,
            num_groups=resnet_groups,
            use_attention=use_attention_bottleneck,
            attention_heads=attention_heads,
            attention_features=attention_features,
        )

        self.upsamples = nn.ModuleList(
            [
                UpsampleBlock1d(
                    in_channels=channels * multipliers[i + 1],
                    out_channels=channels * multipliers[i],
                    context_mapping_features=context_mapping_features,
                    context_embedding_features=context_embedding_features,
                    num_layers=num_blocks[i] + (1 if attentions[i] else 0),
                    factor=factors[i],
                    use_nearest=use_nearest_upsample,
                    num_groups=resnet_groups,
                    use_skip_scale=use_skip_scale,
                    use_pre_upsample=False,
                    use_skip=True,
                    skip_channels=channels * multipliers[i + 1],
                    use_attention=attentions[i],
                    attention_heads=attention_heads,
                    attention_features=attention_features,
                    attention_multiplier=attention_multiplier,
                )
                for i in reversed(range(num_layers))
            ]
        )

        self.to_pre_out = ResnetBlock1d(
            in_channels=channels,
            out_channels=channels,
            num_groups=resnet_groups,
            context_mapping_features=context_mapping_features,
        )

        self.to_out = nn.Sequential(
            Conv1d(
                in_channels=channels,
                out_channels=out_channels * patch_size,
                kernel_size=1,
            ),
            Rearrange("b (c p) l -&gt; b c (l p)", p=patch_size),
        )

        <a id="change">if </a>self.use_post_out_block:
            <a id="change">assert </a>exists(kernel_sizes_out)
            self.to_post_out<a id="change"> = </a>ConvOut1d(
                channels=out_channels,
                kernel_sizes=kernel_sizes_out,
            )</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/archinetai/audio-diffusion-pytorch/commit/3c710edf168da06dcc89c23ef12023d0f8b63043#diff-daa41e4baa3d2f79ac9df5a1fdce9cb4f87f4b10c9335e3653c38e64ecae3966L796' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 101500790</div><div id='project'> Project Name: archinetai/audio-diffusion-pytorch</div><div id='commit'> Commit Name: 3c710edf168da06dcc89c23ef12023d0f8b63043</div><div id='time'> Time: 2022-09-21</div><div id='author'> Author: flavio.schneider.97@gmail.com</div><div id='file'> File Name: audio_diffusion_pytorch/modules.py</div><div id='m_class'> M Class Name: UNet1d</div><div id='n_method'> N Class Name: UNet1d</div><div id='m_method'> M Method Name: __init__(23)</div><div id='n_method'> N Method Name: __init__(21)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: audio_diffusion_pytorch/modules.py</div><div id='n_file'> N File Name: audio_diffusion_pytorch/modules.py</div><div id='m_start'> M Start Line: 823</div><div id='m_end'> M End Line: 938</div><div id='n_start'> N Start Line: 842</div><div id='n_end'> N End Line: 997</div><BR>