<html><h3>Pattern ID :32954
</h3><img src='95437612.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 TODO: implement

        if not self.bwd_rcev_buffers:
            self.bwd_rcev_buffers<a id="change"> = </a>self.comm_handler.create_gradients_rcv_buffers(
                self.device)
        g = self.bwd_rcev_buffers

        &#47&#47 Solution to the DAMN bug with 4 partitions.
        &#47&#47 TODO: understnad why zero_() is the solution
        &#47&#47 I added detach just in case.
        for b in g:
            &#47&#47 b.detach_()
            b.detach_().zero_()
            &#47&#47 b.zero_()
            &#47&#47 if not (b.grad is None):
            &#47&#47     b.grad._zero()

        request_objects = self.comm_handler.recv_gradients(g, batch_idx)

        &#47&#47 recv for bwd
        <a id="change">for obj</a> in request_objects<a id="change">:
            </a>while <a id="change">not obj.is_completed()</a>:
                pass
                &#47&#47 obj.wait()
</code></pre><h3>After Change</h3><pre><code class='java'>
    def run_batch_backward(self, batch_idx, num_batches):
        &#47&#47 TODO: implement

        if not <a id="change">self.bwd_rcev_buffers.is_initialized()</a>:
            self.bwd_rcev_buffers.create()

        recved_all = False</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/c5d99022b6a12748aff63f8f1a3931050b7a1d1a#diff-5e076251e3138c824f1e98c9223b2f4507d35059d327da1d43d1ec4e31c68bd3L247' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95437612</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: c5d99022b6a12748aff63f8f1a3931050b7a1d1a</div><div id='time'> Time: 2020-01-09</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: pipeline/partition_manager.py</div><div id='m_class'> M Class Name: SinglePartitionManager</div><div id='n_method'> N Class Name: SinglePartitionManager</div><div id='m_method'> M Method Name: run_batch_backward(3)</div><div id='n_method'> N Method Name: run_batch_backward(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: pipeline/partition_manager.py</div><div id='n_file'> N File Name: pipeline/partition_manager.py</div><div id='m_start'> M Start Line: 247</div><div id='m_end'> M End Line: 310</div><div id='n_start'> N Start Line: 247</div><div id='n_end'> N End Line: 315</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        else:
            if not self.fwd_rcev_buffers:
                self.fwd_rcev_buffers<a id="change"> = </a>self.comm_handler.create_activations_recv_buffers(
                    self.device)
            x = self.fwd_rcev_buffers

            request_objects = self.comm_handler.recv_activations(x, batch_idx)

            &#47&#47 recv for fwd
            <a id="change">for obj</a> in request_objects<a id="change">:
                &#47&#47 print(f"-I- {self.stage} waiting on rcv")
                </a>while<a id="change">(not obj.is_completed())</a>:
                    pass
                    &#47&#47 obj.wait()
                &#47&#47 print(f"-I- {self.stage} DONE waiting on rcv")</code></pre><h3>After Change</h3><pre><code class='java'>
                send_ctx, batch_idx)

        else:
            if not <a id="change">self.fwd_rcev_buffers.is_initialized()</a>:
                self.fwd_rcev_buffers.create()

            recved_all = False</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/c5d99022b6a12748aff63f8f1a3931050b7a1d1a#diff-5e076251e3138c824f1e98c9223b2f4507d35059d327da1d43d1ec4e31c68bd3L118' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95437613</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: c5d99022b6a12748aff63f8f1a3931050b7a1d1a</div><div id='time'> Time: 2020-01-09</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: pipeline/partition_manager.py</div><div id='m_class'> M Class Name: SinglePartitionManager</div><div id='n_method'> N Class Name: SinglePartitionManager</div><div id='m_method'> M Method Name: run_batch_forward(4)</div><div id='n_method'> N Method Name: run_batch_forward(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: pipeline/partition_manager.py</div><div id='n_file'> N File Name: pipeline/partition_manager.py</div><div id='m_start'> M Start Line: 156</div><div id='m_end'> M End Line: 172</div><div id='n_start'> N Start Line: 157</div><div id='n_end'> N End Line: 177</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 check dist config
    ensure_divisibility(world_size, tensor_model_parallel_size_)
    data_parallel_size_<a id="change"> = </a>world_size // tensor_model_parallel_size_

    &#47&#47 Build the data-parallel groups.
    global _DATA_PARALLEL_GROUP
    assert _DATA_PARALLEL_GROUP is None, \
        &quotdata parallel group is already initialized&quot
    for i in range(tensor_model_parallel_size_):
        ranks = range(i, world_size, tensor_model_parallel_size_)
        group = dist.new_group(ranks)
        if rank in ranks:
            _DATA_PARALLEL_GROUP = group

    global _TENSOR_MODEL_PARALLEL_GROUP
    assert _TENSOR_MODEL_PARALLEL_GROUP is None, \
        &quottensor model parallel group is already initialized&quot
    &#47&#47 Build the model-parallel groups.
    <a id="change">for i</a> in range(data_parallel_size_)<a id="change">:
        </a>ranks = range(i * tensor_model_parallel_size_, (i + 1)<a id="change"> * </a>tensor_model_parallel_size_)
        group = dist.new_group(ranks)
        if rank in ranks:
            _TENSOR_MODEL_PARALLEL_GROUP = group</code></pre><h3>After Change</h3><pre><code class='java'>
        else:
            tensor_model_parallel_size_ = 1

    if <a id="change">torch.torch.distributed.is_initialized()</a>:
        _logger = colossalai.logging.get_dist_logger()
        _logger.error(
            "use fastfold.distributed.init_dap instead of torch.distributed.init_process_group!")</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/hpcaitech/fastfold/commit/aa140059ee9898496b7a110d7eb2523cf6938836#diff-dc07565d3128889e694315e7f3dc7ce3d4b4ad8491fa9ce445caeb05a6f683b3L18' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95437614</div><div id='project'> Project Name: hpcaitech/fastfold</div><div id='commit'> Commit Name: aa140059ee9898496b7a110d7eb2523cf6938836</div><div id='time'> Time: 2022-04-11</div><div id='author'> Author: csg19971016@gmail.com</div><div id='file'> File Name: fastfold/distributed/core.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: init_dap(1)</div><div id='n_method'> N Method Name: init_dap(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: fastfold/distributed/core.py</div><div id='n_file'> N File Name: fastfold/distributed/core.py</div><div id='m_start'> M Start Line: 20</div><div id='m_end'> M End Line: 53</div><div id='n_start'> N Start Line: 16</div><div id='n_end'> N End Line: 39</div><BR>