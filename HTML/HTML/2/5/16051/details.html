<html><h3>Pattern ID :16051
</h3><img src='53844853.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if opt.aug_type is not None:
            logit_aug_t = model_t(input_aug)
            logit_aug_s = model_s(input_aug)
            pred_lbl_t<a id="change"> = </a><a id="change">logit_aug_t.argmax(</a>1<a id="change">)</a>

        &#47&#47 cls + kl div
        loss_cls_nat = criterion_cls(logit_s, target)
</code></pre><h3>After Change</h3><pre><code class='java'>
                    input_aug = opt.aug_lambda * input_aug + (1 - opt.aug_lambda) * input_aug_b
                elif opt.aug_lambda == -1:
                    &#47&#47 compute mixup samples using the beta distribution
                    lambda_aug<a id="change"> = </a>np.random.beta(opt.aug_alpha, opt.aug_alpha, size=[bs, 1, 1, 1])
                    lambda_aug<a id="change"> = </a><a id="change">torch.from_numpy(lambda_aug).type(torch.FloatTensor).to(</a>opt.device<a id="change">)</a>
                    input_aug = lambda_aug * input_aug + (1 - lambda_aug) * input_aug_b


</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/alldbi/supermix/commit/fddc8b4e6fb166c7f5e5bcaf15fdab12549e363f#diff-aa1243455f09375196b22da370a4fe6d4b340171aa7a89a01c368967f8a3eb4dL141' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 53844853</div><div id='project'> Project Name: alldbi/supermix</div><div id='commit'> Commit Name: fddc8b4e6fb166c7f5e5bcaf15fdab12549e363f</div><div id='time'> Time: 2020-01-21</div><div id='author'> Author: ali.dabouei@gmail.com</div><div id='file'> File Name: helper/loops.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train_distill(11)</div><div id='n_method'> N Method Name: train_distill(11)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: helper/loops.py</div><div id='n_file'> N File Name: helper/loops.py</div><div id='m_start'> M Start Line: 144</div><div id='m_end'> M End Line: 188</div><div id='n_start'> N Start Line: 141</div><div id='n_end'> N End Line: 196</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        pred_rgb = pred_rgb.argmax(dim=2).squeeze()  &#47&#47 [T, h, w]
        pred_then_colorized_vis = colorize_semseg(postprocess_mask(pred_rgb), num_classes=SYNPICK_CLASSES) &#47&#47 [T, 3, h, w]

        frames_seg = [<a id="change">seg_model(frames[:, i]).argmax(dim=1)</a> for i in range(frames.shape[1])]
        frames_seg = torch.stack(frames_seg, dim=1)  &#47&#47 [1, 1, h, w]
        input_seg = frames_seg[:, :VIDEO_IN_LENGTH]  &#47&#47 [1, t, 1, h, w]

        pred_mask = pred_mask_model.pred_n(input_seg, pred_length=VIDEO_PRED_LENGTH)
        pred_mask = pred_mask.argmax(dim=2)  &#47&#47 [1, T, 1, h, w]
        pred_mask = postprocess_mask(torch.cat([input_seg, pred_mask], dim=1).squeeze())  &#47&#47 [T, h, w]
        pred_mask_vis = colorize_semseg(pred_mask, num_classes=SYNPICK_CLASSES)  &#47&#47 [T, 3, h, w]

        frames_colorized = colorize_semseg(postprocess_mask(frames_seg.squeeze()), num_classes=SYNPICK_CLASSES).unsqueeze(dim=0) &#47&#47 [1, T, 3, h, w]
        frames_colorized_vis<a id="change"> = </a>postprocess_img(frames_colorized.squeeze(dim=0))  &#47&#47 [T, 3, h, w]
        input_colorized = frames_colorized[:VIDEO_IN_LENGTH]

        colorized_then_pred = pred_colorized_mask_model.pred_n(input_colorized, pred_length=VIDEO_PRED_LENGTH)</code></pre><h3>After Change</h3><pre><code class='java'>
    test_data = SynpickVideoDataset(data_dir=data_dir, vid_type=("rgb", 3), num_frames=VIDEO_TOT_LENGTH,
                                    step=4, allow_overlap=VID_DATA_ALLOW_OVERLAP)
    test_loader = DataLoader(test_data, batch_size=1, shuffle=True, num_workers=4)
    iter_loader<a id="change"> = </a>iter(test_loader)

    with torch.no_grad():
        for i in tqdm(range(10)):

            frames = <a id="change">next(iter_loader).to(</a>DEVICE<a id="change">)</a>  &#47&#47 [1, T, 3, h, w]
            frames_vis = postprocess_img(frames.squeeze(dim=0))  &#47&#47 [T, 3, h, w]
            input = frames[:, :VIDEO_IN_LENGTH]  &#47&#47 [1, t, 3, h, w]

            pred_rgb = pred_rgb_model.pred_n(input, pred_length=VIDEO_PRED_LENGTH)
            pred_rgb = torch.cat([input, pred_rgb], dim=1)  &#47&#47 [1, T, 3, h, w]
            pred_rgb_vis = postprocess_img(pred_rgb.squeeze(dim=0))  &#47&#47 [T, 3, h, w]

            pred_rgb = torch.stack([seg_model(pred_rgb[:, i]) for i in range(pred_rgb.shape[1])], dim=1)
            pred_rgb = pred_rgb.argmax(dim=2).squeeze()  &#47&#47 [T, h, w]
            pred_then_colorized_vis = colorize_semseg(postprocess_mask(pred_rgb), num_classes=SYNPICK_CLASSES).transpose(0, 3, 1, 2) &#47&#47 [T, 3, h, w]

            frames_seg = torch.stack([seg_model(frames[:, i]) for i in range(frames.shape[1])], dim=1).argmax(dim=2)  &#47&#47 [1, T, 1, h, w]
            frames_seg_in = torch.stack([(frames_seg == i) for i in range(SYNPICK_CLASSES)], dim=2).float()  &#47&#47 [1, T, c, h, w] one-hot float
            input_seg = frames_seg_in[:, :VIDEO_IN_LENGTH]  &#47&#47 [1, t, c, h, w]
            pred_mask = pred_mask_model.pred_n(input_seg, pred_length=VIDEO_PRED_LENGTH).argmax(dim=2)  &#47&#47 [1, n, 1, h, w]
            pred_mask = torch.cat([input_seg.argmax(dim=2), pred_mask], dim=1).squeeze()  &#47&#47 [T, h, w]
            pred_mask_vis = colorize_semseg(postprocess_mask(pred_mask), num_classes=SYNPICK_CLASSES).transpose(0, 3, 1, 2)  &#47&#47 [T, 3, h, w]

            frames_colorized = colorize_semseg(postprocess_mask(frames_seg.squeeze()), num_classes=SYNPICK_CLASSES)
            frames_colorized_vis<a id="change"> = </a>frames_colorized.transpose(0, 3, 1, 2)  &#47&#47 [T, 3, h, w]

            input_colorized = preprocess_img(frames_colorized[:VIDEO_IN_LENGTH]).to(DEVICE).unsqueeze(dim=0)  &#47&#47 [b, t, 3, h, w]
            colorized_then_pred = pred_colorized_mask_model.pred_n(input_colorized, pred_length=VIDEO_PRED_LENGTH)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/ais-bonn/vp-suite/commit/13016d4ab8ba4f8e7ee087155a6c5171f4d00ba3#diff-e07d70acfca9139cbf2c37b7a0074bdfbb966d41dc6c7edcc2cef05e78c00af0L13' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 53844820</div><div id='project'> Project Name: ais-bonn/vp-suite</div><div id='commit'> Commit Name: 13016d4ab8ba4f8e7ee087155a6c5171f4d00ba3</div><div id='time'> Time: 2021-08-02</div><div id='author'> Author: boltres@ais.uni-bonn.de</div><div id='file'> File Name: scripts/visualize_4_way.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: visualize_4_way(1)</div><div id='n_method'> N Method Name: visualize_4_way(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: scripts/visualize_4_way.py</div><div id='n_file'> N File Name: scripts/visualize_4_way.py</div><div id='m_start'> M Start Line: 16</div><div id='m_end'> M End Line: 73</div><div id='n_start'> N Start Line: 17</div><div id='n_end'> N End Line: 74</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        token_tensor = token_tensor.unsqueeze(-1)
        predictions = self.model(token_tensor)
        &#47&#47 convert results to tags
        top_predictions<a id="change"> = </a><a id="change">predictions.argmax(</a>-1<a id="change">)</a>
        predicted_tags = [self.data.tag_field.vocab.itos[t.item()] for t in top_predictions]
        &#47&#47 print inferred tags
        max_len_token = max([len(token) for token in tokens] + [len(&quotword&quot)])
        max_len_tag = max([len(tag) for tag in predicted_tags] + [len(&quotpred&quot)])</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 begin prediction
        token_tensor = torch.as_tensor(numericalized_tokens)
        token_tensor = token_tensor.unsqueeze(-1).to(self.device)
        char_tensor<a id="change"> = </a>torch.as_tensor(numericalized_chars)
        char_tensor<a id="change"> = </a><a id="change">char_tensor.unsqueeze(0).to(</a>self.device<a id="change">)</a>
        predictions, _ = self.model(token_tensor, char_tensor)
        &#47&#47 convert results to tags
        predicted_tags = [self.data.tag_field.vocab.itos[t] for t in predictions[0]]
        &#47&#47 print inferred tags</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/yoseflaw/nerindo/commit/a70e55e7c0489cba1290ebd51512a9e878c6e0ed#diff-8cab4f44e154e1111c4ee01fed014cbc93efbaa697c98a0f44a624cd8e8f013fL84' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 53844881</div><div id='project'> Project Name: yoseflaw/nerindo</div><div id='commit'> Commit Name: a70e55e7c0489cba1290ebd51512a9e878c6e0ed</div><div id='time'> Time: 2020-08-09</div><div id='author'> Author: yosefardhitowin@gmail.com</div><div id='file'> File Name: nerindo/trainer.py</div><div id='m_class'> M Class Name: Trainer</div><div id='n_method'> N Class Name: Trainer</div><div id='m_method'> M Method Name: infer(3)</div><div id='n_method'> N Method Name: infer(3)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: nerindo/trainer.py</div><div id='n_file'> N File Name: nerindo/trainer.py</div><div id='m_start'> M Start Line: 88</div><div id='m_end'> M End Line: 100</div><div id='n_start'> N Start Line: 184</div><div id='n_end'> N End Line: 205</div><BR>