<html><h3>Pattern ID :6432
</h3><img src='22337100.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        self.to_embed = nn.Embedding(num_tokens, dim) if exists(num_tokens) else nn.Identity()

        self.layers = nn.ModuleList(<a id="change">[Residual(PreNorm(dim, gMLPBlock(dim = dim, dim_ff = dim_ff, seq_len = seq_len, heads = heads, window = window))) for i in range(depth)]</a>)

        self.to_logits = nn.Sequential(
            nn.LayerNorm(dim),</code></pre><h3>After Change</h3><pre><code class='java'>
        self.to_embed = nn.Embedding(num_tokens, dim)

        window = cast_tuple(window, depth)
        layers = <a id="change">nn.ModuleList(</a>[]<a id="change">)</a>

        for ind, w in zip(range(depth), window):
            layer_blocks = nn.ModuleList([
                PreNorm(dim, gMLPBlock(dim = dim, dim_ff = dim_ff, seq_len = seq_len, heads = heads, window = w))
            ])

            if reversible:
                layer_blocks.append(PreNorm(dim, gMLPBlock(dim = dim, dim_ff = dim_ff, seq_len = seq_len, heads = heads, window = w)))

            layers.append(layer_blocks)

        execute_klass = SequentialSequence if not reversible else ReversibleSequence
        self.net<a id="change"> = </a>execute_klass(layers)

        self.to_logits = nn.Sequential(
            nn.LayerNorm(dim),</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/g-mlp-gpt/commit/7642e36ff19c6b299a77e5c1ace038e9e6726202#diff-9e1c762f97dc7e52cf231d6bec8dbb8412bc9383faaf89eadd9b1c02e46fea80L188' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 22337100</div><div id='project'> Project Name: lucidrains/g-mlp-gpt</div><div id='commit'> Commit Name: 7642e36ff19c6b299a77e5c1ace038e9e6726202</div><div id='time'> Time: 2021-05-20</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: g_mlp_gpt/g_mlp_gpt.py</div><div id='m_class'> M Class Name: gMLPGPT</div><div id='n_method'> N Class Name: gMLPGPT</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: g_mlp_gpt/g_mlp_gpt.py</div><div id='n_file'> N File Name: g_mlp_gpt/g_mlp_gpt.py</div><div id='m_start'> M Start Line: 188</div><div id='m_end'> M End Line: 195</div><div id='n_start'> N Start Line: 194</div><div id='n_end'> N End Line: 215</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
class LinearAttentionTransformer(nn.Module):
    def __init__(self, dim, depth, max_seq_len, heads = 8, bucket_size = 64, causal = False, one_kv_head = False):
        super().__init__()
        self.attn_layers = nn.ModuleList(<a id="change">[PreNorm(dim, SelfAttention(dim, heads, causal, one_kv_head = one_kv_head)) for _ in range(depth)]</a>)
        self.ff_layers = nn.ModuleList([PreNorm(dim, FeedForward(dim)) for _ in range(depth)])

    def forward(self, x, **kwargs):</code></pre><h3>After Change</h3><pre><code class='java'>
class LinearAttentionTransformer(nn.Module):
    def __init__(self, dim, depth, max_seq_len, heads = 8, bucket_size = 64, causal = False, one_kv_head = False):
        super().__init__()
        layers = <a id="change">nn.ModuleList(</a>[]<a id="change">)</a>

        for _ in range(depth):
            layer = nn.ModuleList([
                PreNorm(dim, SelfAttention(dim, heads, causal, one_kv_head = one_kv_head)),
                PreNorm(dim, FeedForward(dim))
            ])
            layers.append(layer)

        self.layers<a id="change"> = </a>ReversibleSequence(layers)

    def forward(self, x, **kwargs):
        x = torch.cat([x, x], dim = -1)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/linear-attention-transformer/commit/a0a35e7e9727b6428c7527ec34f5192529ab5e82#diff-eee6cf669463e9683bcf9643c4b19a493056f98ded32854287144d47f02a7032L189' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 22337102</div><div id='project'> Project Name: lucidrains/linear-attention-transformer</div><div id='commit'> Commit Name: a0a35e7e9727b6428c7527ec34f5192529ab5e82</div><div id='time'> Time: 2020-06-04</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='m_class'> M Class Name: LinearAttentionTransformer</div><div id='n_method'> N Class Name: LinearAttentionTransformer</div><div id='m_method'> M Method Name: __init__(8)</div><div id='n_method'> N Method Name: __init__(8)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='n_file'> N File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='m_start'> M Start Line: 191</div><div id='m_end'> M End Line: 192</div><div id='n_start'> N Start Line: 193</div><div id='n_end'> N End Line: 202</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                    hidden_units[i + 1]) for i in range(len(hidden_units) - 1)
                ])
        self.activation_layers = nn.ModuleList(
            <a id="change">[activation_gen(
                activation,
                hidden_units[i + 1],
                dice_dim) for i in range(len(hidden_units) - 1)
            ]</a>)

        for name, tensor in self.linear_layers.named_parameters():
            if &quotweight&quot in name:</code></pre><h3>After Change</h3><pre><code class='java'>
        self.linear_layers = nn.ModuleList()
        self.activation_layers = nn.ModuleList()
        if self.use_bn:
            self.bn_layers<a id="change"> = </a><a id="change">nn.ModuleList()</a>
        
        for i in range(len(hidden_units) - 1):
            self.linear_layers.append(
                nn.Linear(hidden_units[i], hidden_units[i + 1]))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/ryantd/veloce/commit/74675f3569dcd22861151eae9245540bd891c826#diff-1fae5f29f685e022e7bee571c33f34a6d639ca10ff29fed7d4d103e75251f34cL8' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 22337098</div><div id='project'> Project Name: ryantd/veloce</div><div id='commit'> Commit Name: 74675f3569dcd22861151eae9245540bd891c826</div><div id='time'> Time: 2022-01-04</div><div id='author'> Author: xiaoyu.zhai@hotmail.com</div><div id='file'> File Name: phetware/layer/core.py</div><div id='m_class'> M Class Name: DNN</div><div id='n_method'> N Class Name: DNN</div><div id='m_method'> M Method Name: __init__(11)</div><div id='n_method'> N Method Name: __init__(11)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: phetware/layer/core.py</div><div id='n_file'> N File Name: phetware/layer/core.py</div><div id='m_start'> M Start Line: 21</div><div id='m_end'> M End Line: 38</div><div id='n_start'> N Start Line: 20</div><div id='n_end'> N End Line: 37</div><BR>