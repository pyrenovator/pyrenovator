<html><h3>Pattern ID :11423
</h3><img src='38991698.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.feat_norm = feat_norm
        self.adj_norm_func = adj_norm_func
        if type(hidden_features) is int:
            <a id="change">hidden_features</a> = <a id="change">[</a>hidden_features<a id="change"></a>]

        self.layers = nn.ModuleList()
        <a id="change">if layer_norm</a>:
            <a id="change">self.layers.append(nn</a><a id="change">.LayerNorm(</a>in_features<a id="change">))</a>

        <a id="change">self.layers.append(</a>TAGConv(in_features=in_features,
                                   out_features=hidden_features[0],
                                   k=k,
                                   batch_norm=batch_norm,
                                   activation=activation,
                                   dropout=dropout)<a id="change">)</a>
        for i in range(<a id="change">len(hidden_features</a><a id="change">)</a><a id="change"> - 1</a>):
            if layer_norm:
                <a id="change">self.layers.append(</a><a id="change">nn.LayerNorm(hidden_features[i]</a><a id="change">))</a>
            self.layers.append(TAGConv(in_features=hidden_features[i],
                                       out_features=hidden_features[i + 1],
                                       k=k,
                                       batch_norm=batch_norm,
                                       activation=activation,
                                       dropout=dropout))
        <a id="change">self.layers.append(</a>TAGConv(in_features=hidden_features[-1],
                                   out_features=out_features,
                                   k=k,
                                   batch_norm=None,
                                   activation=None,
                                   dropout=0.0)<a id="change">)</a>
        self.reset_parameters()

    @property
    def model_type(self):</code></pre><h3>After Change</h3><pre><code class='java'>
        self.feat_norm = feat_norm
        self.adj_norm_func = adj_norm_func
        if type(hidden_features) is int:
            <a id="change">hidden_features</a> = <a id="change">[hidden_features</a>]<a id="change"> * </a>(n_layers<a id="change"> - 1</a>)
        elif <a id="change">type(hidden_features) is list or type(hidden_features) is tuple</a>:
            <a id="change">assert </a>len(hidden_features) == (n_layers - 1), "Incompatible sizes between hidden_features and n_layers."
        <a id="change">n_features</a><a id="change"> = </a><a id="change">[</a><a id="change">in_features] + hidden_features + [</a>out_features<a id="change"></a>]

        self.layers = nn.ModuleList()
        for i in range(n_layers):
            if layer_norm:
                <a id="change">self.layers.append(</a><a id="change">nn.LayerNorm(n_features[i]</a><a id="change">))</a>
            self.layers.append(TAGConv(in_features=n_features[i],
                                       out_features=n_features[i + 1],
                                       k=k,
                                       batch_norm=batch_norm if i != n_layers - 1 else False,</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 31</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/thudm/grb/commit/033f2d036673be7a58f544095c76780c09618477#diff-0541ff7793729626c14259dc6e374a5f00c7ad04914d0cca7ec47e90ae8d7b74L47' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 38991698</div><div id='project'> Project Name: thudm/grb</div><div id='commit'> Commit Name: 033f2d036673be7a58f544095c76780c09618477</div><div id='time'> Time: 2021-08-17</div><div id='author'> Author: 2363471925@qq.com</div><div id='file'> File Name: grb/model/torch/tagcn.py</div><div id='m_class'> M Class Name: TAGCN</div><div id='n_method'> N Class Name: TAGCN</div><div id='m_method'> M Method Name: __init__(12)</div><div id='n_method'> N Method Name: __init__(11)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: grb/model/torch/tagcn.py</div><div id='n_file'> N File Name: grb/model/torch/tagcn.py</div><div id='m_start'> M Start Line: 58</div><div id='m_end'> M End Line: 84</div><div id='n_start'> N Start Line: 47</div><div id='n_end'> N End Line: 69</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.feat_norm = feat_norm
        self.adj_norm_func = adj_norm_func
        if type(hidden_features) is int:
            <a id="change">hidden_features</a> = <a id="change">[</a>hidden_features<a id="change"></a>]

        <a id="change">self.layers</a> = nn.ModuleList()
        <a id="change">if layer_norm</a>:
            <a id="change">self.layers.append(</a><a id="change">nn.LayerNorm(</a>in_features<a id="change">))</a>
        <a id="change">self.layers.append(</a>GCNConv(in_features=in_features,
                                   out_features=hidden_features[0],
                                   activation=activation,
                                   residual=residual,
                                   dropout=dropout)<a id="change">)</a>
        for i in range(<a id="change">len(</a>hidden_features<a id="change">)</a><a id="change"> - 1</a>):
            if layer_norm:
                <a id="change">self.layers.append(</a><a id="change">nn.LayerNorm(hidden_features[i]</a><a id="change">))</a>
            self.layers.append(GCNConv(in_features=hidden_features[i],
                                       out_features=hidden_features[i + 1],
                                       activation=activation,
                                       residual=residual,
                                       dropout=dropout))
        <a id="change">self.layers.append(</a>GCNConv(in_features=hidden_features[-1],
                                   out_features=out_features,
                                   activation=None,
                                   residual=False,
                                   dropout=0.0)<a id="change">)</a>
        self.reset_parameters()

    @property
    def model_type(self):</code></pre><h3>After Change</h3><pre><code class='java'>
        self.feat_norm = feat_norm
        self.adj_norm_func = adj_norm_func
        if type(hidden_features) is int:
            <a id="change">hidden_features</a> = <a id="change">[</a>hidden_features<a id="change"></a>]<a id="change"> * </a>(n_layers<a id="change"> - 1</a>)
        elif <a id="change">type(hidden_features) is list or type(hidden_features) is tuple</a>:
            <a id="change">assert </a>len(hidden_features) == (n_layers - 1), "Incompatible sizes between hidden_features and n_layers."
        <a id="change">n_features</a><a id="change"> = </a><a id="change">[</a><a id="change">in_features] + hidden_features + [</a>out_features<a id="change"></a>]

        self.layers = nn.ModuleList()
        for i in range(n_layers):
            if layer_norm:
                <a id="change">self.layers.append(</a><a id="change">nn.LayerNorm(n_features[i]</a><a id="change">))</a>
            self.layers.append(GCNConv(in_features=n_features[i],
                                       out_features=n_features[i + 1],
                                       activation=activation if i != n_layers - 1 else None,
                                       residual=residual if i != n_layers - 1 else False,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thudm/grb/commit/033f2d036673be7a58f544095c76780c09618477#diff-b9babed1ba33df105da1f9f5b009a4d372977748df1641b317306bd00f7679fcL39' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 38991699</div><div id='project'> Project Name: thudm/grb</div><div id='commit'> Commit Name: 033f2d036673be7a58f544095c76780c09618477</div><div id='time'> Time: 2021-08-17</div><div id='author'> Author: 2363471925@qq.com</div><div id='file'> File Name: grb/model/torch/gcn.py</div><div id='m_class'> M Class Name: GCN</div><div id='n_method'> N Class Name: GCN</div><div id='m_method'> M Method Name: __init__(11)</div><div id='n_method'> N Method Name: __init__(10)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: grb/model/torch/gcn.py</div><div id='n_file'> N File Name: grb/model/torch/gcn.py</div><div id='m_start'> M Start Line: 55</div><div id='m_end'> M End Line: 77</div><div id='n_start'> N Start Line: 45</div><div id='n_end'> N End Line: 66</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.feat_norm = feat_norm
        self.adj_norm_func = adj_norm_func
        if type(hidden_features) is int:
            <a id="change">hidden_features</a> = <a id="change">[</a>hidden_features<a id="change"></a>]

        <a id="change">self.layers</a> = nn.ModuleList()
        <a id="change">if layer_norm</a>:
            <a id="change">self.layers.append(</a><a id="change">nn.LayerNorm(</a>in_features<a id="change">))</a>

        <a id="change">self.layers.append(</a>TAGConv(in_features=in_features,
                                   out_features=hidden_features[0],
                                   k=k,
                                   batch_norm=batch_norm,
                                   activation=activation,
                                   dropout=dropout)<a id="change">)</a>
        for i in range(<a id="change">len(</a>hidden_features<a id="change">)</a><a id="change"> - 1</a>):
            if layer_norm:
                <a id="change">self.layers.append(</a><a id="change">nn.LayerNorm(hidden_features[i]</a><a id="change">))</a>
            self.layers.append(TAGConv(in_features=hidden_features[i],
                                       out_features=hidden_features[i + 1],
                                       k=k,
                                       batch_norm=batch_norm,
                                       activation=activation,
                                       dropout=dropout))
        <a id="change">self.layers.append(</a>TAGConv(in_features=hidden_features[-1],
                                   out_features=out_features,
                                   k=k,
                                   batch_norm=None,
                                   activation=None,
                                   dropout=0.0)<a id="change">)</a>
        self.reset_parameters()

    @property
    def model_type(self):</code></pre><h3>After Change</h3><pre><code class='java'>
        self.feat_norm = feat_norm
        self.adj_norm_func = adj_norm_func
        if type(hidden_features) is int:
            <a id="change">hidden_features</a> = <a id="change">[</a>hidden_features<a id="change"></a>]<a id="change"> * </a>(n_layers<a id="change"> - 1</a>)
        elif <a id="change">type(hidden_features) is list or type(hidden_features) is tuple</a>:
            <a id="change">assert </a>len(hidden_features) == (n_layers - 1), "Incompatible sizes between hidden_features and n_layers."
        <a id="change">n_features</a><a id="change"> = </a><a id="change">[</a><a id="change">in_features] + hidden_features + [</a>out_features<a id="change"></a>]

        self.layers = nn.ModuleList()
        for i in range(n_layers):
            if layer_norm:
                <a id="change">self.layers.append(</a><a id="change">nn.LayerNorm(n_features[i]</a><a id="change">))</a>
            self.layers.append(TAGConv(in_features=n_features[i],
                                       out_features=n_features[i + 1],
                                       k=k,
                                       batch_norm=batch_norm if i != n_layers - 1 else False,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thudm/grb/commit/033f2d036673be7a58f544095c76780c09618477#diff-0541ff7793729626c14259dc6e374a5f00c7ad04914d0cca7ec47e90ae8d7b74L41' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 38991696</div><div id='project'> Project Name: thudm/grb</div><div id='commit'> Commit Name: 033f2d036673be7a58f544095c76780c09618477</div><div id='time'> Time: 2021-08-17</div><div id='author'> Author: 2363471925@qq.com</div><div id='file'> File Name: grb/model/torch/tagcn.py</div><div id='m_class'> M Class Name: TAGCN</div><div id='n_method'> N Class Name: TAGCN</div><div id='m_method'> M Method Name: __init__(12)</div><div id='n_method'> N Method Name: __init__(11)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: grb/model/torch/tagcn.py</div><div id='n_file'> N File Name: grb/model/torch/tagcn.py</div><div id='m_start'> M Start Line: 58</div><div id='m_end'> M End Line: 84</div><div id='n_start'> N Start Line: 47</div><div id='n_end'> N End Line: 69</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.adj_norm_func = adj_norm_func

        if type(hidden_features) is int:
            <a id="change">hidden_features</a> = <a id="change">[</a>hidden_features<a id="change"></a>]

        <a id="change">self.layers</a> = nn.ModuleList()
        <a id="change">if layer_norm</a>:
            <a id="change">self.layers.append(</a><a id="change">nn.LayerNorm(</a>in_features<a id="change">))</a>
        <a id="change">self.layers.append(</a>SAGEConv(in_features=in_features,
                                    pool_features=in_features,
                                    out_features=hidden_features[0],
                                    activation=activation,
                                    mu=mu,
                                    dropout=dropout)<a id="change">)</a>
        for i in range(<a id="change">len(</a>hidden_features<a id="change">)</a><a id="change"> - 1</a>):
            if layer_norm:
                <a id="change">self.layers.append(</a><a id="change">nn.LayerNorm(hidden_features[i]</a><a id="change">))</a>
            self.layers.append(SAGEConv(in_features=hidden_features[i],
                                        pool_features=hidden_features[i],
                                        out_features=hidden_features[i + 1],
                                        activation=activation,
                                        mu=mu,
                                        dropout=dropout))
        <a id="change">self.layers.append(</a>SAGEConv(in_features=hidden_features[-1],
                                    pool_features=hidden_features[-1],
                                    out_features=out_features,
                                    activation=None,
                                    mu=mu,
                                    dropout=0.0)<a id="change">)</a>

    @property
    def model_type(self):
        Indicate type of implementation.</code></pre><h3>After Change</h3><pre><code class='java'>
        self.feat_norm = feat_norm
        self.adj_norm_func = adj_norm_func
        if type(hidden_features) is int:
            <a id="change">hidden_features</a> = <a id="change">[</a>hidden_features<a id="change"></a>]<a id="change"> * </a>(n_layers<a id="change"> - 1</a>)
        elif <a id="change">type(hidden_features) is list or type(hidden_features) is tuple</a>:
            <a id="change">assert </a>len(hidden_features) == (n_layers - 1), "Incompatible sizes between hidden_features and n_layers."
        <a id="change">n_features</a><a id="change"> = </a><a id="change">[</a><a id="change">in_features] + hidden_features + [</a>out_features<a id="change"></a>]

        self.layers = nn.ModuleList()
        for i in range(n_layers):
            if layer_norm:
                <a id="change">self.layers.append(</a><a id="change">nn.LayerNorm(n_features[i]</a><a id="change">))</a>
            self.layers.append(SAGEConv(in_features=n_features[i],
                                        pool_features=n_features[i],
                                        out_features=n_features[i + 1],
                                        activation=activation if i != n_layers - 1 else None,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thudm/grb/commit/033f2d036673be7a58f544095c76780c09618477#diff-7830a336eb76eaf542894115a54298afda5528f3a59371477700cb9c291b6325L39' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 38991702</div><div id='project'> Project Name: thudm/grb</div><div id='commit'> Commit Name: 033f2d036673be7a58f544095c76780c09618477</div><div id='time'> Time: 2021-08-17</div><div id='author'> Author: 2363471925@qq.com</div><div id='file'> File Name: grb/model/torch/graphsage.py</div><div id='m_class'> M Class Name: GraphSAGE</div><div id='n_method'> N Class Name: GraphSAGE</div><div id='m_method'> M Method Name: __init__(11)</div><div id='n_method'> N Method Name: __init__(10)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: grb/model/torch/graphsage.py</div><div id='n_file'> N File Name: grb/model/torch/graphsage.py</div><div id='m_start'> M Start Line: 56</div><div id='m_end'> M End Line: 81</div><div id='n_start'> N Start Line: 45</div><div id='n_end'> N End Line: 73</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.feat_norm = None
        self.adj_norm_func = None
        if type(hidden_features) is int:
            <a id="change">hidden_features</a> = <a id="change">[</a>hidden_features<a id="change"></a>]

        <a id="change">self.layers</a> = nn.ModuleList()
        <a id="change">if layer_norm</a>:
            <a id="change">self.layers.append(</a><a id="change">nn.LayerNorm(</a>in_features<a id="change">))</a>
        <a id="change">self.layers.append(</a>MLPLayer(in_features=in_features,
                                    out_features=hidden_features[0],
                                    activation=activation,
                                    dropout=dropout)<a id="change">)</a>
        for i in range(<a id="change">len(</a>hidden_features<a id="change">)</a><a id="change"> - 1</a>):
            if layer_norm:
                <a id="change">self.layers.append(</a><a id="change">nn.LayerNorm(hidden_features[i]</a><a id="change">))</a>
            self.layers.append(MLPLayer(in_features=hidden_features[i],
                                        out_features=hidden_features[i + 1],
                                        activation=activation,
                                        dropout=dropout))
        <a id="change">self.layers.append(</a>MLPLayer(in_features=hidden_features[-1],
                                    out_features=out_features,
                                    activation=None,
                                    dropout=0.0)<a id="change">)</a>
        self.reset_parameters()

    @property
    def model_type(self):</code></pre><h3>After Change</h3><pre><code class='java'>
        self.feat_norm = feat_norm
        self.adj_norm_func = adj_norm_func
        if type(hidden_features) is int:
            <a id="change">hidden_features</a> = <a id="change">[</a>hidden_features<a id="change"></a>]<a id="change"> * </a>(n_layers<a id="change"> - 1</a>)
        elif <a id="change">type(hidden_features) is list or type(hidden_features) is tuple</a>:
            <a id="change">assert </a>len(hidden_features) == (n_layers - 1), "Incompatible sizes between hidden_features and n_layers."
        <a id="change">n_features</a><a id="change"> = </a><a id="change">[</a><a id="change">in_features] + hidden_features + [</a>out_features<a id="change"></a>]

        self.layers = nn.ModuleList()
        for i in range(n_layers):
            if layer_norm:
                <a id="change">self.layers.append(</a><a id="change">nn.LayerNorm(n_features[i]</a><a id="change">))</a>
            self.layers.append(MLPLayer(in_features=n_features[i],
                                        out_features=n_features[i + 1],
                                        activation=activation if i != n_layers - 1 else None,
                                        dropout=dropout if i != n_layers - 1 else 0.0))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thudm/grb/commit/033f2d036673be7a58f544095c76780c09618477#diff-03ceab261dd3b593e9f11edd22eb4d3f789b1b0907da0714ca3ecefa22885e70L31' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 38991703</div><div id='project'> Project Name: thudm/grb</div><div id='commit'> Commit Name: 033f2d036673be7a58f544095c76780c09618477</div><div id='time'> Time: 2021-08-17</div><div id='author'> Author: 2363471925@qq.com</div><div id='file'> File Name: grb/model/torch/mlp.py</div><div id='m_class'> M Class Name: MLP</div><div id='n_method'> N Class Name: MLP</div><div id='m_method'> M Method Name: __init__(10)</div><div id='n_method'> N Method Name: __init__(7)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: grb/model/torch/mlp.py</div><div id='n_file'> N File Name: grb/model/torch/mlp.py</div><div id='m_start'> M Start Line: 41</div><div id='m_end'> M End Line: 63</div><div id='n_start'> N Start Line: 37</div><div id='n_end'> N End Line: 57</div><BR>