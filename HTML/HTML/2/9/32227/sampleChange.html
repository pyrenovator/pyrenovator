<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    ed = st + span_info.dim
                    data_t.append(torch.sigmoid(data[:, st:ed]))
                    st = ed
                elif <a id="change">span_info.activation_fn == ActivationFn.SOFTMAX</a>:
                    ed = st + span_info.dim
                    transformed = self._gumbel_softmax(data[:, st:ed], tau=0.2)
                    <a id="change">data_t.append(</a>transformed<a id="change">)</a>
                    st<a id="change"> = </a>ed
                else:
                    <a id="change">raise </a><a id="change">ValueError(
                        f"Unexpected activation function {span_info.activation_fn}."</a><a id="change">
                    )</a>

        return torch.cat(data_t, dim=1)

    def _cond_loss(self, data, c, m):</code></pre><h3>After Change</h3><pre><code class='java'>

    def _apply_activate(self, data):
        Apply proper activation function to the output of the generator.
        data_t = <a id="change">[
            activation_fn(data[:, st:ed])
            for st, ed, activation_fn in self._activation_fns
        ]</a>

        return torch.cat(data_t, dim=1)

    def _cond_loss(self, data, c, m):</code></pre>