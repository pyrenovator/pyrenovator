<html><h3>Pattern ID :30002
</h3><img src='88970956.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                state[&quotstep&quot] += 1

                if group[&quotweight_decay&quot] != 0:
                    grad<a id="change"> = </a><a id="change">grad.add(</a>p<a id="change">, alpha=group[&quotweight_decay&quot])</a>

                &#47&#47 decay term
                p.mul_(1 - group[&quotlambd&quot] * state[&quoteta&quot])
</code></pre><h3>After Change</h3><pre><code class='java'>
            with torch.enable_grad():
                loss = closure()

        <a id="change">for </a>group in self.param_groups<a id="change">:
            </a>params_with_grad = []
            grads = []
            mus = []
            axs = []
            etas = []
            state_steps<a id="change"> = </a>[]

            for p in group[&quotparams&quot]:
                <a id="change">if </a>p.grad is not None:
                    <a id="change">params_with_grad.append(</a>p<a id="change">)</a>
                    if p.grad.is_sparse:
                        raise RuntimeError(&quotASGD does not support sparse gradients&quot)
                    grads.append(p.grad)

                    state<a id="change"> = </a>self.state[p]
                    &#47&#47 State initialization
                    if len(state) == 0:
                        state[&quotstep&quot] = 0
                        state[&quoteta&quot] = group[&quotlr&quot]
                        state[&quotmu&quot] = 1
                        state[&quotax&quot] = torch.zeros_like(p, memory_format=torch.preserve_format)

                    mus.append(state[&quotmu&quot])
                    axs.append(state[&quotax&quot])
                    etas.append(state[&quoteta&quot])

                    state[&quotstep&quot] += 1
                    state_steps.append(state[&quotstep&quot])

            F.asgd(params_with_grad,
                   grads,
                   axs,
                   mus,
                   etas,
                   weight_decay=group[&quotweight_decay&quot],
                   lambd=group[&quotlambd&quot])

            &#47&#47 update eta and mu
            <a id="change">for </a>p, mu, eta in zip(params_with_grad, mus, etas)<a id="change">:
                </a>state = self.state[p]
                state[&quoteta&quot] = (group[&quotlr&quot] /
                                math.pow((1 + group[&quotlambd&quot] * group[&quotlr&quot] * state[&quotstep&quot]), group[&quotalpha&quot]))
                state[&quotmu&quot] = 1 / max(1, state[&quotstep&quot] - group[&quott0&quot])</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/9a622f4cd9318d69896462823891588ef0bf719c#diff-b5441178f7d0764941fd9b5dc89b596d57da114d4665b7b31138beac59e93c7dL48' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 88970956</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 9a622f4cd9318d69896462823891588ef0bf719c</div><div id='time'> Time: 2021-05-19</div><div id='author'> Author: iramazanli@fb.com</div><div id='file'> File Name: torch/optim/asgd.py</div><div id='m_class'> M Class Name: ASGD</div><div id='n_method'> N Class Name: ASGD</div><div id='m_method'> M Method Name: step(2)</div><div id='n_method'> N Method Name: step(2)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: torch/optim/asgd.py</div><div id='n_file'> N File Name: torch/optim/asgd.py</div><div id='m_start'> M Start Line: 48</div><div id='m_end'> M End Line: 86</div><div id='n_start'> N Start Line: 49</div><div id='n_end'> N End Line: 94</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                      b=ivy.array([2, 3, 4]))
    y = ivy.Container(a=ivy.array([4, 5, 6]), \
                           b=ivy.array([5, 6, 7]))
    z<a id="change"> = </a><a id="change">ivy.add(</a>x, y<a id="change">)</a>
    print(type(z))


    ivy.unset_backend()</code></pre><h3>After Change</h3><pre><code class='java'>
    to_skip+=skip_list_temp


    <a id="change">for </a>k, <a id="change">v</a> in ivy.__dict__.copy().items()<a id="change">:
        </a>if k == "Array":
            for method_name in dir(v):
                method = getattr(ivy.Array, method_name)
                if helpers.docstring_examples_run(method, from_array=True):
                    continue
                success<a id="change"> = </a>False
                failures.append("Array." + method_name)

        elif k == "Container":
            <a id="change">for </a>method_name in dir(v)<a id="change">:
                </a>method = getattr(ivy.Container, method_name)
                if helpers.docstring_examples_run(method, from_container=True):
                    continue
                success = False
                failures.append("Container." + method_name)

        else:
            <a id="change">if </a>k in to_skip or helpers.docstring_examples_run(v):
                continue
            success<a id="change"> = </a>False
            <a id="change">failures.append(</a>k<a id="change">)</a>

    if not success:
        ivy.warn(
            "the following methods had failing docstrings:\n\n{}".format(</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/unifyai/ivy/commit/61d7d0f6bee98e7f97d635143cef92f6b7eeece0#diff-48ffb3a283811c82d46de1000e0dbf654f102f8fbb23f5dcf9c974e21964bc90L14' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 88970991</div><div id='project'> Project Name: unifyai/ivy</div><div id='commit'> Commit Name: 61d7d0f6bee98e7f97d635143cef92f6b7eeece0</div><div id='time'> Time: 2022-06-30</div><div id='author'> Author: rishabhkkumarsc1@gmail.com</div><div id='file'> File Name: ivy_tests/test_docstrings.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: test_docstrings(1)</div><div id='n_method'> N Method Name: test_docstrings(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: ivy_tests/test_docstrings.py</div><div id='n_file'> N File Name: ivy_tests/test_docstrings.py</div><div id='m_start'> M Start Line: 81</div><div id='m_end'> M End Line: 86</div><div id='n_start'> N Start Line: 17</div><div id='n_end'> N End Line: 85</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                state[&quotstep&quot] += 1

                if group[&quotweight_decay&quot] != 0:
                    grad<a id="change"> = </a><a id="change">grad.add(</a>p<a id="change">, alpha=group[&quotweight_decay&quot])</a>

                &#47&#47 decay term
                p.mul_(1 - group[&quotlambd&quot] * state[&quoteta&quot])
</code></pre><h3>After Change</h3><pre><code class='java'>
            with torch.enable_grad():
                loss = closure()

        <a id="change">for group</a> in self.param_groups<a id="change">:
            </a>params_with_grad = []
            grads = []
            mus = []
            axs = []
            etas = []
            state_steps<a id="change"> = </a>[]

            for p in group[&quotparams&quot]:
                <a id="change">if </a>p.grad is not None:
                    params_with_grad.append(p)
                    if p.grad.is_sparse:
                        raise RuntimeError(&quotASGD does not support sparse gradients&quot)
                    <a id="change">grads.append(</a>p.grad<a id="change">)</a>

                    state<a id="change"> = </a>self.state[p]
                    &#47&#47 State initialization
                    if len(state) == 0:
                        state[&quotstep&quot] = 0
                        state[&quoteta&quot] = group[&quotlr&quot]
                        state[&quotmu&quot] = 1
                        state[&quotax&quot] = torch.zeros_like(p, memory_format=torch.preserve_format)

                    mus.append(state[&quotmu&quot])
                    axs.append(state[&quotax&quot])
                    etas.append(state[&quoteta&quot])

                    state[&quotstep&quot] += 1
                    state_steps.append(state[&quotstep&quot])

            F.asgd(params_with_grad,
                   grads,
                   axs,
                   mus,
                   etas,
                   weight_decay=group[&quotweight_decay&quot],
                   lambd=group[&quotlambd&quot])

            &#47&#47 update eta and mu
            <a id="change">for </a>p, mu, eta in zip(params_with_grad, mus, etas)<a id="change">:
                </a>state = self.state[p]
                state[&quoteta&quot] = (group[&quotlr&quot] /
                                math.pow((1 + group[&quotlambd&quot] * group[&quotlr&quot] * state[&quotstep&quot]), group[&quotalpha&quot]))
                state[&quotmu&quot] = 1 / max(1, state[&quotstep&quot] - group[&quott0&quot])</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/9a622f4cd9318d69896462823891588ef0bf719c#diff-b5441178f7d0764941fd9b5dc89b596d57da114d4665b7b31138beac59e93c7dL36' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 88970958</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 9a622f4cd9318d69896462823891588ef0bf719c</div><div id='time'> Time: 2021-05-19</div><div id='author'> Author: iramazanli@fb.com</div><div id='file'> File Name: torch/optim/asgd.py</div><div id='m_class'> M Class Name: ASGD</div><div id='n_method'> N Class Name: ASGD</div><div id='m_method'> M Method Name: step(2)</div><div id='n_method'> N Method Name: step(2)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: torch/optim/asgd.py</div><div id='n_file'> N File Name: torch/optim/asgd.py</div><div id='m_start'> M Start Line: 48</div><div id='m_end'> M End Line: 86</div><div id='n_start'> N Start Line: 49</div><div id='n_end'> N End Line: 94</div><BR>