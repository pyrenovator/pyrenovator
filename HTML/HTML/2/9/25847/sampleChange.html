<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        checkpoint = torch.load(logger.get_checkpoint_path(&quotbest&quot), map_location=&quotcpu&quot)
        classifier.load_state_dict(checkpoint)

    <a id="change">if </a>args.phase == &quottest&quot:
        acc1 = validate(test_loader, classifier, args)
        print(acc1)
        return


    &#47&#47 compute relationship

    relationship_path = args.relationship
    if not os.path.exists(relationship_path):
        r = Relationship(determin_train_loader, val_loader, classifier)
        relationship = r.get_relationship(direct=args.direct)
        np.save(relationship_path, relationship)
    else:
        relationship<a id="change"> = np.load(</a>relationship_path<a id="change">)</a>

    &#47&#47 start training
    best_acc1 = 0.0

    for epoch in range(args.epochs):
        &#47&#47 train for one epoch
        train(train_iter, classifier, optimizer,
              epoch, relationship, args)
        lr_scheduler.step()
        &#47&#47 evaluate on validation set
        acc1 = validate(val_loader, classifier, args)

        &#47&#47 remember best acc@1 and save checkpoint
        torch.save(classifier.state_dict(), logger.get_checkpoint_path(&quotlatest&quot))
        if acc1 &gt; best_acc1:
            shutil.copy(logger.get_checkpoint_path(&quotlatest&quot), logger.get_checkpoint_path(&quotbest&quot))
        best_acc1 = max(acc1, best_acc1)

    print("best_acc1 = {:3.1f}".format(best_acc1))

    &#47&#47 evaluate on test set
    classifier.load_state_dict(torch.load(logger.get_checkpoint_path(&quotbest&quot)))
    acc1 = validate(test_loader, classifier, args)
    <a id="change">print(</a><a id="change">"test_acc1 = {:3.1f}".format(</a>acc1<a id="change">))</a>

    logger.close()
</code></pre><h3>After Change</h3><pre><code class='java'>
    print("=&gt; using pre-trained model &quot{}&quot".format(args.arch))
    backbone = models.__dict__[args.arch](pretrained=True)
    num_classes = train_dataset.num_classes
    classifier = <a id="change">Classifier(backbone, num_classes, head_source=backbone.copy_head()).to(</a>device<a id="change">)</a>

    &#47&#47 define optimizer and lr scheduler
    optimizer = SGD(classifier.get_parameters(args.lr), momentum=args.momentum, weight_decay=args.wd, nesterov=True)
    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, args.lr_decay_epochs, gamma=args.lr_gamma)

    &#47&#47 resume from the best checkpoint
    if args.phase == &quottest&quot:
        checkpoint = torch.load(logger.get_checkpoint_path(&quotbest&quot), map_location=&quotcpu&quot)
        classifier.load_state_dict(checkpoint)
        acc1<a id="change"> = </a>validate(val_loader, classifier, args)
        print(acc1)
        return
</code></pre>