<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    tensor_id += 1

    &#47&#47 卷积层写入权重。参考了onnx2ncnn，开头写个0
    s = <a id="change">struct.pack(&quoti&quot</a>, <a id="change">0</a><a id="change">)</a>
    <a id="change">bp.write(</a>s<a id="change">)</a>

    conv_w = conv.weight.cpu().detach().numpy()
    for i1 in range(out_C):
        for i2 in range(in_C):
            for i3 in range(kH):
                for i4 in range(kW):
                    s = struct.pack(&quotf&quot, conv_w[i1][i2][i3][i4])
                    <a id="change">bp.write(</a>s<a id="change">)</a>
    if conv.bias is not None:
        conv_b = conv.bias.cpu().detach().numpy()
        for i1 in range(out_C):
            s = struct.pack(&quotf&quot, conv_b[i1])
            <a id="change">bp.write(</a>s<a id="change">)</a>
    ncnn_data[&quotbp&quot] = bp
    ncnn_data[&quotpp&quot] = pp
    ncnn_data[&quotlayer_id&quot] = layer_id
    ncnn_data[&quottensor_id&quot] = tensor_id</code></pre><h3>After Change</h3><pre><code class='java'>

def conv2d(ncnn_data, bottom_names, conv, act_name=None, act_param_dict=None):
    bottom_names = check_bottom_names(bottom_names)
    <a id="change">bp</a> = ncnn_data[&quotbp&quot]
    pp = ncnn_data[&quotpp&quot]
    layer_id = ncnn_data[&quotlayer_id&quot]
    tensor_id = ncnn_data[&quottensor_id&quot]

    top_names = create_top_names(ncnn_data, num=1)
    pp += &quotConvolution\tlayer_%.8d\t1 1 %s %s&quot % (layer_id, bottom_names[0], top_names[0])
    pp += &quot 0=%d&quot % conv.out_channels
    if len(conv.kernel_size) == 2:
        pp += &quot 1=%d&quot % conv.kernel_size[1]
        pp += &quot 11=%d&quot % conv.kernel_size[0]
    else:
        raise NotImplementedError("not implemented.")
    if len(conv.stride) == 2:
        pp += &quot 3=%d&quot % conv.stride[1]
        pp += &quot 13=%d&quot % conv.stride[0]
    else:
        raise NotImplementedError("not implemented.")
    if len(conv.padding) == 2:
        pp += &quot 4=%d&quot % conv.padding[1]
        pp += &quot 14=%d&quot % conv.padding[0]
    else:
        raise NotImplementedError("not implemented.")
    if conv.bias is not None:
        pp += &quot 5=1&quot
    else:
        pp += &quot 5=0&quot
    out_C, in_C, kH, kW = conv.weight.shape
    w_ele_num = out_C * in_C * kH * kW
    pp += &quot 6=%d&quot % w_ele_num
    assert conv.groups == 1
    &#47&#47 合并激活
    pp = fused_activation(pp, act_name, act_param_dict)
    pp += &quot\n&quot
    layer_id += 1
    tensor_id += 1

    &#47&#47 卷积层写入权重。
    bp = <a id="change">bp_write_tag(bp</a><a id="change">)</a>

    conv_w = conv.weight.cpu().detach().numpy()
    for i1 in range(out_C):
        for i2 in range(in_C):</code></pre>