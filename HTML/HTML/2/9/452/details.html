<html><h3>Pattern ID :452
</h3><img src='2477589.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        myreq = ctx.myreq
        rsi = myreq.rsi

        <a id="change">if rsi.codecs is not None</a>:
            grad_output<a id="change"> = </a><a id="change">rsi.codecs.backward.encode(</a>grad_output<a id="change">)</a>
        grad_inputs = grad_output.new_empty(rsi.input_sizes)

        with record_function("&#47&#47&#47&#47 reduce_scatter_base_bw (all_gather) &#47&#47&#47&#47"):
            req = dist._all_gather_base(
                grad_inputs,
                grad_output.contiguous(),
                group=ctx.pg,
                async_op=True,
            )
        myreq.req = req
        myreq.tensor = grad_inputs
        <a id="change">return </a>(<a id="change">None</a><a id="change">, None, myreq.dummy_tensor</a>)


class AllGatherBase_Req(Function):</code></pre><h3>After Change</h3><pre><code class='java'>
            )
        myreq.req = req
        myreq.tensor = grad_inputs
        <a id="change">return </a>(None<a id="change">, None, grad_output</a>)


class AllGatherBase_Req(Function):</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 8</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/6fc5e96d292bf4f3d85645a7f5f36ed219975d9b#diff-233b62fdfbb8e887bafc580edd784c08250a8cc36a3b206dfb03524e49e098dbL1110' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2477589</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: 6fc5e96d292bf4f3d85645a7f5f36ed219975d9b</div><div id='time'> Time: 2022-08-03</div><div id='author'> Author: xingl@fb.com</div><div id='file'> File Name: torchrec/distributed/comm_ops.py</div><div id='m_class'> M Class Name: ReduceScatterBase_Wait</div><div id='n_method'> N Class Name: ReduceScatterBase_Wait</div><div id='m_method'> M Method Name: backward(2)</div><div id='n_method'> N Method Name: backward(2)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: torchrec/distributed/comm_ops.py</div><div id='n_file'> N File Name: torchrec/distributed/comm_ops.py</div><div id='m_start'> M Start Line: 1220</div><div id='m_end'> M End Line: 1236</div><div id='n_start'> N Start Line: 1110</div><div id='n_end'> N End Line: 1110</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        input_splits = a2ai.output_splits
        output_splits = a2ai.input_splits

        <a id="change">if a2ai.codecs is not None</a>:
            sharded_grad_output<a id="change"> = </a><a id="change">a2ai.codecs.backward.encode(</a>sharded_grad_output<a id="change">)</a>

        sharded_grad_input = torch.empty(
            sum(output_splits),
            device=sharded_grad_output.device,
            dtype=sharded_grad_output.dtype,
        )

        with record_function("&#47&#47&#47&#47 alltoall_seq_embedding_bwd_single &#47&#47&#47&#47"):
            req = dist.all_to_all_single(
                output=sharded_grad_input,
                input=sharded_grad_output.view(-1),
                output_split_sizes=output_splits,
                input_split_sizes=input_splits,
                group=pg,
                async_op=True,
            )
        myreq.req = req
        myreq.tensor = sharded_grad_input

        <a id="change">return </a>(None<a id="change">, None, myreq.dummy_tensor</a>)


class All2Allv_Req(Function):</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 Note - this mismatch is by design! We return sharded_grad_output
        &#47&#47 to allow PyTorch shape matching to proceed correctly.
        <a id="change">return </a>(None<a id="change">, None, sharded_grad_output.view(-1)</a>)


class All2Allv_Req(Function):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/6fc5e96d292bf4f3d85645a7f5f36ed219975d9b#diff-233b62fdfbb8e887bafc580edd784c08250a8cc36a3b206dfb03524e49e098dbL884' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2477588</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: 6fc5e96d292bf4f3d85645a7f5f36ed219975d9b</div><div id='time'> Time: 2022-08-03</div><div id='author'> Author: xingl@fb.com</div><div id='file'> File Name: torchrec/distributed/comm_ops.py</div><div id='m_class'> M Class Name: All2All_Seq_Req_Wait</div><div id='n_method'> N Class Name: All2All_Seq_Req_Wait</div><div id='m_method'> M Method Name: backward(2)</div><div id='n_method'> N Method Name: backward(2)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: torchrec/distributed/comm_ops.py</div><div id='n_file'> N File Name: torchrec/distributed/comm_ops.py</div><div id='m_start'> M Start Line: 885</div><div id='m_end'> M End Line: 912</div><div id='n_start'> N Start Line: 855</div><div id='n_end'> N End Line: 855</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def backward(ctx, grad_output: Tensor) -&gt; Tuple[None, None, Tensor]:
        myreq = ctx.myreq
        rsi = myreq.rsi
        <a id="change">if rsi.codecs is not None</a>:
            grad_output<a id="change"> = </a><a id="change">rsi.codecs.backward.encode(</a>grad_output<a id="change">)</a>
        grad_inputs = [
            grad_output.new_empty(
                in_size,
                dtype=grad_output.dtype,
                device=grad_output.device,
            )
            for in_size in rsi.input_sizes
        ]
        with record_function("&#47&#47&#47&#47 reduce_scatter_bw (all_gather) &#47&#47&#47&#47"):
            req = dist.all_gather(
                grad_inputs,
                grad_output.contiguous(),
                group=ctx.pg,
                async_op=True,
            )

        myreq.req = req
        myreq.tensor = grad_inputs
        <a id="change">return </a>(None<a id="change">, None, myreq.dummy_tensor</a>)


class ReduceScatterBase_Req(Function):</code></pre><h3>After Change</h3><pre><code class='java'>
            )
        myreq.req = req
        myreq.tensor = grad_inputs
        <a id="change">return </a>(None<a id="change">, None, grad_output</a>)


class ReduceScatterBase_Req(Function):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/6fc5e96d292bf4f3d85645a7f5f36ed219975d9b#diff-233b62fdfbb8e887bafc580edd784c08250a8cc36a3b206dfb03524e49e098dbL1112' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2477591</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: 6fc5e96d292bf4f3d85645a7f5f36ed219975d9b</div><div id='time'> Time: 2022-08-03</div><div id='author'> Author: xingl@fb.com</div><div id='file'> File Name: torchrec/distributed/comm_ops.py</div><div id='m_class'> M Class Name: ReduceScatter_Wait</div><div id='n_method'> N Class Name: ReduceScatter_Wait</div><div id='m_method'> M Method Name: backward(2)</div><div id='n_method'> N Method Name: backward(2)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: torchrec/distributed/comm_ops.py</div><div id='n_file'> N File Name: torchrec/distributed/comm_ops.py</div><div id='m_start'> M Start Line: 1113</div><div id='m_end'> M End Line: 1135</div><div id='n_start'> N Start Line: 1036</div><div id='n_end'> N End Line: 1036</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        myreq = ctx.myreq
        agi = myreq.agi

        <a id="change">if agi.codecs is not None</a>:
            grad_outputs<a id="change"> = </a><a id="change">agi.codecs.backward.encode(</a>grad_outputs<a id="change">)</a>
        grad_input = grad_outputs.new_empty(agi.input_size)

        with record_function("&#47&#47&#47&#47 all_gather_base_bw (reduce_scatter) &#47&#47&#47&#47"):
            req = dist._reduce_scatter_base(
                grad_input,
                grad_outputs.contiguous(),
                group=ctx.pg,
                async_op=True,
            )
        myreq.req = req
        myreq.tensor = grad_input
        <a id="change">return </a>(None<a id="change">, None, myreq.dummy_tensor</a>)
</code></pre><h3>After Change</h3><pre><code class='java'>
            )
        myreq.req = req
        myreq.tensor = grad_input
        <a id="change">return </a>(None<a id="change">, None, grad_outputs</a>)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/6fc5e96d292bf4f3d85645a7f5f36ed219975d9b#diff-233b62fdfbb8e887bafc580edd784c08250a8cc36a3b206dfb03524e49e098dbL1317' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2477590</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: 6fc5e96d292bf4f3d85645a7f5f36ed219975d9b</div><div id='time'> Time: 2022-08-03</div><div id='author'> Author: xingl@fb.com</div><div id='file'> File Name: torchrec/distributed/comm_ops.py</div><div id='m_class'> M Class Name: AllGatherBase_Wait</div><div id='n_method'> N Class Name: AllGatherBase_Wait</div><div id='m_method'> M Method Name: backward(2)</div><div id='n_method'> N Method Name: backward(2)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: torchrec/distributed/comm_ops.py</div><div id='n_file'> N File Name: torchrec/distributed/comm_ops.py</div><div id='m_start'> M Start Line: 1318</div><div id='m_end'> M End Line: 1334</div><div id='n_start'> N Start Line: 1183</div><div id='n_end'> N End Line: 1183</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        myreq = ctx.myreq
        rsi = myreq.rsi

        <a id="change">if rsi.codecs is not None</a>:
            grad_output<a id="change"> = </a><a id="change">rsi.codecs.backward.encode(</a>grad_output<a id="change">)</a>
        grad_inputs = grad_output.new_empty(rsi.input_sizes)

        with record_function("&#47&#47&#47&#47 reduce_scatter_base_bw (all_gather) &#47&#47&#47&#47"):
            req = dist._all_gather_base(
                grad_inputs,
                grad_output.contiguous(),
                group=ctx.pg,
                async_op=True,
            )
        myreq.req = req
        myreq.tensor = grad_inputs
        <a id="change">return </a>(None<a id="change">, None, myreq.dummy_tensor</a>)


class AllGatherBase_Req(Function):</code></pre><h3>After Change</h3><pre><code class='java'>
            )
        myreq.req = req
        myreq.tensor = grad_inputs
        <a id="change">return </a>(None<a id="change">, None, grad_output</a>)


class AllGatherBase_Req(Function):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/6fc5e96d292bf4f3d85645a7f5f36ed219975d9b#diff-233b62fdfbb8e887bafc580edd784c08250a8cc36a3b206dfb03524e49e098dbL1219' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2477593</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: 6fc5e96d292bf4f3d85645a7f5f36ed219975d9b</div><div id='time'> Time: 2022-08-03</div><div id='author'> Author: xingl@fb.com</div><div id='file'> File Name: torchrec/distributed/comm_ops.py</div><div id='m_class'> M Class Name: ReduceScatterBase_Wait</div><div id='n_method'> N Class Name: ReduceScatterBase_Wait</div><div id='m_method'> M Method Name: backward(2)</div><div id='n_method'> N Method Name: backward(2)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: torchrec/distributed/comm_ops.py</div><div id='n_file'> N File Name: torchrec/distributed/comm_ops.py</div><div id='m_start'> M Start Line: 1220</div><div id='m_end'> M End Line: 1236</div><div id='n_start'> N Start Line: 1110</div><div id='n_end'> N End Line: 1110</div><BR>