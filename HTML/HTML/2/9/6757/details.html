<html><h3>Pattern ID :6757
</h3><img src='23119328.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    elif opt_lower == &quotnvnovograd&quot:
        optimizer = NvNovoGrad(parameters, **opt_args)
    elif opt_lower == &quotfusedsgd&quot:
        <a id="change">del opt_args[&quoteps&quot]</a>
        optimizer = FusedSGD(parameters, momentum=args.momentum, nesterov=True, **opt_args)
    elif opt_lower == &quotfusedmomentum&quot:
        del <a id="change">opt_args[&quoteps&quot]</a>
        optimizer = FusedSGD(parameters, momentum=args.momentum, nesterov=False, **opt_args)
    elif opt_lower == &quotfusedadam&quot:
        optimizer = FusedAdam(parameters, adam_w_mode=False, **opt_args)
    elif opt_lower == &quotfusedadamw&quot:</code></pre><h3>After Change</h3><pre><code class='java'>
    elif opt_lower == &quotnvnovograd&quot:
        optimizer = NvNovoGrad(parameters, **opt_args)
    elif opt_lower == &quotfusedsgd&quot:
        <a id="change">opt_args.pop(&quoteps&quot</a>, None<a id="change">)</a>
        optimizer = FusedSGD(parameters, momentum=args.momentum, nesterov=True, **opt_args)
    elif opt_lower == &quotfusedmomentum&quot:
        opt_args.pop(&quoteps&quot, None)
        optimizer = FusedSGD(parameters, momentum=args.momentum, nesterov=False, **opt_args)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/feng-lab/pytorch-image-models/commit/30ab4a1494a8c9f440be940297b31497e0cbf411#diff-27961ee816d5f8e3ea9840ed8bfbf5e9836163a0368d14fb8c1ca2f58369a05fL55' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 23119328</div><div id='project'> Project Name: feng-lab/pytorch-image-models</div><div id='commit'> Commit Name: 30ab4a1494a8c9f440be940297b31497e0cbf411</div><div id='time'> Time: 2020-10-31</div><div id='author'> Author: rwightman@gmail.com</div><div id='file'> File Name: timm/optim/optim_factory.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: create_optimizer(3)</div><div id='n_method'> N Method Name: create_optimizer(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: timm/optim/optim_factory.py</div><div id='n_file'> N File Name: timm/optim/optim_factory.py</div><div id='m_start'> M Start Line: 55</div><div id='m_end'> M End Line: 101</div><div id='n_start'> N Start Line: 55</div><div id='n_end'> N End Line: 101</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    if &quotfused&quot in opt_lower:
        assert has_apex and torch.cuda.is_available(), &quotAPEX and CUDA required for fused optimizers&quot

    <a id="change">opt_args</a> = dict(lr=args.lr, weight_decay=weight_decay)
    if hasattr(args, &quotopt_eps&quot) and args.opt_eps is not None:
        opt_args[&quoteps&quot] = args.opt_eps
    if hasattr(args, &quotopt_betas&quot) and args.opt_betas is not None:
        opt_args[&quotbetas&quot] = args.opt_betas

    opt_split = opt_lower.split(&quot_&quot)
    opt_lower = opt_split[-1]
    if opt_lower == &quotsgd&quot or opt_lower == &quotnesterov&quot:
        del <a id="change">opt_args[&quoteps&quot]</a>
        optimizer = optim.SGD(parameters, momentum=args.momentum, nesterov=True, **opt_args)
    elif opt_lower == &quotmomentum&quot:
        del opt_args[&quoteps&quot]
        optimizer = optim.SGD(parameters, momentum=args.momentum, nesterov=False, **opt_args)
    elif opt_lower == &quotadam&quot:
        optimizer = optim.Adam(parameters, **opt_args)
    elif opt_lower == &quotadamw&quot:
        optimizer = optim.AdamW(parameters, **opt_args)
    elif opt_lower == &quotnadam&quot:
        optimizer = Nadam(parameters, **opt_args)
    elif opt_lower == &quotradam&quot:
        optimizer = RAdam(parameters, **opt_args)
    elif opt_lower == &quotadamp&quot:        
        optimizer = AdamP(parameters, wd_ratio=0.01, nesterov=True, **opt_args)
    elif opt_lower == &quotsgdp&quot:        
        optimizer = SGDP(parameters, momentum=args.momentum, nesterov=True, **opt_args)
    elif opt_lower == &quotadadelta&quot:
        optimizer = optim.Adadelta(parameters, **opt_args)
    elif opt_lower == &quotadafactor&quot:
        if not args.lr:
            opt_args[&quotlr&quot] = None
        optimizer = Adafactor(parameters, **opt_args)
    elif opt_lower == &quotadahessian&quot:
        optimizer = Adahessian(parameters, **opt_args)
    elif opt_lower == &quotrmsprop&quot:
        optimizer = optim.RMSprop(parameters, alpha=0.9, momentum=args.momentum, **opt_args)
    elif opt_lower == &quotrmsproptf&quot:
        optimizer = RMSpropTF(parameters, alpha=0.9, momentum=args.momentum, **opt_args)
    elif opt_lower == &quotnovograd&quot:
        optimizer = NovoGrad(parameters, **opt_args)
    elif opt_lower == &quotnvnovograd&quot:
        optimizer = NvNovoGrad(parameters, **opt_args)
    elif opt_lower == &quotfusedsgd&quot:
        <a id="change">del opt_args[&quoteps&quot]</a>
        optimizer = FusedSGD(parameters, momentum=args.momentum, nesterov=True, **opt_args)
    elif opt_lower == &quotfusedmomentum&quot:
        del opt_args[&quoteps&quot]
        optimizer = FusedSGD(parameters, momentum=args.momentum, nesterov=False, **opt_args)</code></pre><h3>After Change</h3><pre><code class='java'>
    if &quotfused&quot in opt_lower:
        assert has_apex and torch.cuda.is_available(), &quotAPEX and CUDA required for fused optimizers&quot

    <a id="change">opt_args</a> = dict(lr=args.lr, weight_decay=weight_decay)
    if hasattr(args, &quotopt_eps&quot) and args.opt_eps is not None:
        opt_args[&quoteps&quot] = args.opt_eps
    if hasattr(args, &quotopt_betas&quot) and args.opt_betas is not None:
        opt_args[&quotbetas&quot] = args.opt_betas

    opt_split = opt_lower.split(&quot_&quot)
    opt_lower = opt_split[-1]
    if opt_lower == &quotsgd&quot or opt_lower == &quotnesterov&quot:
        opt_args.pop(&quoteps&quot, None)
        optimizer = optim.SGD(parameters, momentum=args.momentum, nesterov=True, **opt_args)
    elif opt_lower == &quotmomentum&quot:
        opt_args.pop(&quoteps&quot, None)
        optimizer = optim.SGD(parameters, momentum=args.momentum, nesterov=False, **opt_args)
    elif opt_lower == &quotadam&quot:
        optimizer = optim.Adam(parameters, **opt_args)
    elif opt_lower == &quotadamw&quot:
        optimizer = optim.AdamW(parameters, **opt_args)
    elif opt_lower == &quotnadam&quot:
        optimizer = Nadam(parameters, **opt_args)
    elif opt_lower == &quotradam&quot:
        optimizer = RAdam(parameters, **opt_args)
    elif opt_lower == &quotadamp&quot:        
        optimizer = AdamP(parameters, wd_ratio=0.01, nesterov=True, **opt_args)
    elif opt_lower == &quotsgdp&quot:        
        optimizer = SGDP(parameters, momentum=args.momentum, nesterov=True, **opt_args)
    elif opt_lower == &quotadadelta&quot:
        optimizer = optim.Adadelta(parameters, **opt_args)
    elif opt_lower == &quotadafactor&quot:
        if not args.lr:
            opt_args[&quotlr&quot] = None
        optimizer = Adafactor(parameters, **opt_args)
    elif opt_lower == &quotadahessian&quot:
        optimizer = Adahessian(parameters, **opt_args)
    elif opt_lower == &quotrmsprop&quot:
        optimizer = optim.RMSprop(parameters, alpha=0.9, momentum=args.momentum, **opt_args)
    elif opt_lower == &quotrmsproptf&quot:
        optimizer = RMSpropTF(parameters, alpha=0.9, momentum=args.momentum, **opt_args)
    elif opt_lower == &quotnovograd&quot:
        optimizer = NovoGrad(parameters, **opt_args)
    elif opt_lower == &quotnvnovograd&quot:
        optimizer = NvNovoGrad(parameters, **opt_args)
    elif opt_lower == &quotfusedsgd&quot:
        <a id="change">opt_args.pop(&quoteps&quot</a>, None<a id="change">)</a>
        optimizer = FusedSGD(parameters, momentum=args.momentum, nesterov=True, **opt_args)
    elif opt_lower == &quotfusedmomentum&quot:
        opt_args.pop(&quoteps&quot, None)
        optimizer = FusedSGD(parameters, momentum=args.momentum, nesterov=False, **opt_args)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/feng-lab/pytorch-image-models/commit/30ab4a1494a8c9f440be940297b31497e0cbf411#diff-27961ee816d5f8e3ea9840ed8bfbf5e9836163a0368d14fb8c1ca2f58369a05fL40' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 23119329</div><div id='project'> Project Name: feng-lab/pytorch-image-models</div><div id='commit'> Commit Name: 30ab4a1494a8c9f440be940297b31497e0cbf411</div><div id='time'> Time: 2020-10-31</div><div id='author'> Author: rwightman@gmail.com</div><div id='file'> File Name: timm/optim/optim_factory.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: create_optimizer(3)</div><div id='n_method'> N Method Name: create_optimizer(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: timm/optim/optim_factory.py</div><div id='n_file'> N File Name: timm/optim/optim_factory.py</div><div id='m_start'> M Start Line: 55</div><div id='m_end'> M End Line: 101</div><div id='n_start'> N Start Line: 55</div><div id='n_end'> N End Line: 101</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        current_time = time.time()
        if "groups" in kwargs:
            <a id="change">kwargs</a> = kwargs.copy()
            if self._task == "rank":
                kwargs["group"] = group_counts(<a id="change">kwargs["groups"]</a>)
                &#47&#47 groups_val = kwargs.get(&quotgroups_val&quot)
                &#47&#47 if groups_val is not None:
                &#47&#47     kwargs[&quoteval_group&quot] = [group_counts(groups_val)]
                &#47&#47     kwargs[&quoteval_set&quot] = [
                &#47&#47         (kwargs[&quotX_val&quot], kwargs[&quoty_val&quot])]
                &#47&#47     kwargs[&quotverbose&quot] = False
                &#47&#47     del kwargs[&quotgroups_val&quot], kwargs[&quotX_val&quot], kwargs[&quoty_val&quot]
            <a id="change">del kwargs["groups"]</a>
        X_train = self._preprocess(X_train)
        model = self.estimator_class(**self.params)
        model.fit(X_train, y_train, **kwargs)</code></pre><h3>After Change</h3><pre><code class='java'>

        current_time = time.time()
        if "groups" in kwargs:
            <a id="change">kwargs</a> = kwargs.copy()
            groups = <a id="change">kwargs.pop("groups"</a><a id="change">)</a>
            if self._task == "rank":
                kwargs["group"] = group_counts(groups)
                &#47&#47 groups_val = kwargs.get(&quotgroups_val&quot)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/flaml/commit/f48ca2618fed2193ffa929af7a44d3031dfb16dd#diff-bcb081d24bf68e74b77ef08dd168d58ea7b1dc90855e686f58586ee83a1f4cadL81' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 23119348</div><div id='project'> Project Name: microsoft/flaml</div><div id='commit'> Commit Name: f48ca2618fed2193ffa929af7a44d3031dfb16dd</div><div id='time'> Time: 2021-10-08</div><div id='author'> Author: wang.chi@microsoft.com</div><div id='file'> File Name: flaml/model.py</div><div id='m_class'> M Class Name: BaseEstimator</div><div id='n_method'> N Class Name: BaseEstimator</div><div id='m_method'> M Method Name: _fit(3)</div><div id='n_method'> N Method Name: _fit(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: flaml/model.py</div><div id='n_file'> N File Name: flaml/model.py</div><div id='m_start'> M Start Line: 85</div><div id='m_end'> M End Line: 95</div><div id='n_start'> N Start Line: 84</div><div id='n_end'> N End Line: 102</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        super().__init__()
        &#47&#47 save locals to take care of some hyperparameters for cascading DDPM

        <a id="change">self._locals</a> = locals()
        <a id="change">del self._locals[&quotself&quot]</a>
        del <a id="change">self._locals[&quot__class__&quot]</a>

        &#47&#47 for eventual cascading diffusion

        self.lowres_cond = lowres_cond</code></pre><h3>After Change</h3><pre><code class='java'>
        super().__init__()
        &#47&#47 save locals to take care of some hyperparameters for cascading DDPM

        <a id="change">self._locals</a> = locals()
        <a id="change">self._locals.pop(&quotself&quot</a>, None<a id="change">)</a>
        self._locals.pop(&quot__class__&quot, None)

        &#47&#47 for eventual cascading diffusion
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/imagen-pytorch/commit/ff0d39fa9764f1aff88feb5a1624ef19e50bcd43#diff-edef3c5fe92797a22c0b8fc6cca1d57b4b84ef03dfdfe802ed9147e21fc88109L784' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 23119305</div><div id='project'> Project Name: lucidrains/imagen-pytorch</div><div id='commit'> Commit Name: ff0d39fa9764f1aff88feb5a1624ef19e50bcd43</div><div id='time'> Time: 2022-05-28</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: imagen_pytorch/imagen_pytorch.py</div><div id='m_class'> M Class Name: Unet</div><div id='n_method'> N Class Name: Unet</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: imagen_pytorch/imagen_pytorch.py</div><div id='n_file'> N File Name: imagen_pytorch/imagen_pytorch.py</div><div id='m_start'> M Start Line: 820</div><div id='m_end'> M End Line: 822</div><div id='n_start'> N Start Line: 819</div><div id='n_end'> N End Line: 821</div><BR>