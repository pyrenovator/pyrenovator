<html><h3>Pattern ID :30575
</h3><img src='90375650.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                logger.record(&quotbatches_trained&quot, batches_trained)
                logger.dump(step=batches_trained)
                batches_trained += 1
                <a id="change">if (training_batches is not None and batches_trained &gt;= training_batches)</a>:
                    <a id="change">logging.info(f"Breaking out of training in epoch {epochs_trained} because max batches "
                                 f"value of {training_batches} has been reached"</a><a id="change">)</a>
                    training_complete = True
                    break

            assert batches_trained &gt; 0, \
                "went through training loop with no batches---empty dataset?"
            if epochs_trained == 0:
                &#47&#47 we infer the size of the dataset from the number of
                &#47&#47 iterations through the loop the first time
                logging.debug(f"Dataset size is {batches_trained}")

            if self.scheduler is not None:
                self.scheduler.step()
            loss_record.append(loss_meter.avg)

            epochs_trained<a id="change"> += </a>1
            <a id="change">if </a>(<a id="change">training_epochs is not None</a> and epochs_trained &gt;= training_epochs):
                training_complete = True

            should_save_checkpoint = (training_complete or</code></pre><h3>After Change</h3><pre><code class='java'>
                "InterleavedDataset will intrinsically shuffle batches; do " \
                "not use multi-task training if shuffle_batches=False is " \
                f"required (got {len(datasets)} datasets)"
            <a id="change">assert </a>self.dataset_max_workers &lt;= 1, \
                "Using more than one dataset worker may shuffle the " \
                "dataset; got dataset_max_workers={dataset_max_workers}"
        interleaved_dataset = InterleavedDataset(</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/humancompatibleai/eirli/commit/579e0a8ca8ba0c924b8c8f393d9445915448617d#diff-da75b71a2421881ac87f16bdedbc66a6eee21a60650cb052fb123cfe1436b72fL220' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 90375650</div><div id='project'> Project Name: humancompatibleai/eirli</div><div id='commit'> Commit Name: 579e0a8ca8ba0c924b8c8f393d9445915448617d</div><div id='time'> Time: 2020-11-19</div><div id='author'> Author: sam@qxcv.net</div><div id='file'> File Name: src/il_representations/algos/representation_learner.py</div><div id='m_class'> M Class Name: RepresentationLearner</div><div id='n_method'> N Class Name: RepresentationLearner</div><div id='m_method'> M Method Name: learn(4)</div><div id='n_method'> N Method Name: learn(4)</div><div id='m_parent_class'> M Parent Class: BaseEnvironmentLearner</div><div id='n_parent_class'> N Parent Class: BaseEnvironmentLearner</div><div id='m_file'> M File Name: src/il_representations/algos/representation_learner.py</div><div id='n_file'> N File Name: src/il_representations/algos/representation_learner.py</div><div id='m_start'> M Start Line: 220</div><div id='m_end'> M End Line: 354</div><div id='n_start'> N Start Line: 220</div><div id='n_end'> N End Line: 343</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                   only_rank0: bool = True,
                   tokenizer: Optional[PreTrainedTokenizerBase] = None) -&gt; None:

        <a id="change">if only_rank0 and dist.get_rank() != 0</a>:
            return None
        unwrapped_model = self._unwrap_model(model)
        &#47&#47 TODO : better way to get torch model from gemini model
        &#47&#47 to get torch model from gemini model

        if isinstance(unwrapped_model, RewardModel):
            state_dict = unwrapped_model.state_dict()
            if only_rank0 and dist.get_rank() != 0:
                return
            torch.save(state_dict, path)
        else:
            try:
                <a id="change">logger.info(f&quotSaving model to {path}&quot</a><a id="change">, ranks=[0])</a>
                unwrapped_model.save_pretrained(path)
                logger.info(f&quotModel saved to {path} Successfully&quot, ranks=[0])
                <a id="change">if tokenizer is not None</a>:
                    logger.info(f&quotSaving tokenizer to {path}&quot, ranks=[0])
                    tokenizer.save_pretrained(path)
                    logger.info(f&quotTokenizer saved to {path} Successfully&quot, ranks=[0])
            except AttributeError:
                state_dict<a id="change"> = </a>unwrapped_model.state_dict()
                if only_rank0 and dist.get_rank() != 0:
                    return
                torch.save(state_dict, path)</code></pre><h3>After Change</h3><pre><code class='java'>
            return
        base_model = get_base_model(model)
        if self.stage == 3:
            <a id="change">assert </a>isinstance(base_model, ZeroDDP)
            &#47&#47 for stage 3, state_dict() method should be called on every rank
            state_dict = base_model.state_dict(only_rank_0=only_rank0)
        else:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/hpcaitech/colossalai/commit/842768a1749bf3d9961a48d2bf96ca5abef7d2da#diff-76f776f2f7f4b67961887595407523859c80d806997106778b4aba8059f66bf4L164' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 90375745</div><div id='project'> Project Name: hpcaitech/colossalai</div><div id='commit'> Commit Name: 842768a1749bf3d9961a48d2bf96ca5abef7d2da</div><div id='time'> Time: 2023-04-27</div><div id='author'> Author: lhx0217@gmail.com</div><div id='file'> File Name: applications/Chat/coati/trainer/strategies/colossalai.py</div><div id='m_class'> M Class Name: ColossalAIStrategy</div><div id='n_method'> N Class Name: ColossalAIStrategy</div><div id='m_method'> M Method Name: save_model(4)</div><div id='n_method'> N Method Name: save_model(5)</div><div id='m_parent_class'> M Parent Class: DDPStrategy</div><div id='n_parent_class'> N Parent Class: DDPStrategy</div><div id='m_file'> M File Name: applications/Chat/coati/trainer/strategies/colossalai.py</div><div id='n_file'> N File Name: applications/Chat/coati/trainer/strategies/colossalai.py</div><div id='m_start'> M Start Line: 168</div><div id='m_end'> M End Line: 196</div><div id='n_start'> N Start Line: 154</div><div id='n_end'> N End Line: 166</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                logger.record(&quotbatches_trained&quot, batches_trained)
                logger.dump(step=batches_trained)
                batches_trained += 1
                <a id="change">if (training_batches is not None and batches_trained &gt;= training_batches)</a>:
                    <a id="change">logging.info(f"Breaking out of training in epoch {epochs_trained} because max batches "
                                 f"value of {training_batches} has been reached"</a><a id="change">)</a>
                    training_complete<a id="change"> = </a>True
                    break

            assert batches_trained &gt; 0, \
                "went through training loop with no batches---empty dataset?"
            if epochs_trained == 0:
                &#47&#47 we infer the size of the dataset from the number of
                &#47&#47 iterations through the loop the first time
                logging.debug(f"Dataset size is {batches_trained}")

            if self.scheduler is not None:
                self.scheduler.step()
            loss_record.append(loss_meter.avg)

            epochs_trained += 1
            <a id="change">if </a>(<a id="change">training_epochs is not None</a> and epochs_trained &gt;= training_epochs):
                training_complete = True

            should_save_checkpoint = (training_complete or</code></pre><h3>After Change</h3><pre><code class='java'>
                torch.save(self.decoder, os.path.join(
                    self.decoder_checkpoints_path, f&quot{epoch_num}_epochs.ckpt&quot))

        <a id="change">assert </a>should_save_checkpoint, "did not save checkpoint on last epoch"

        return loss_record, most_recent_encoder_checkpoint_path
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/humancompatibleai/eirli/commit/ccdc28141c799043c5385b20f1201bc6971e462b#diff-da75b71a2421881ac87f16bdedbc66a6eee21a60650cb052fb123cfe1436b72fL215' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 90375648</div><div id='project'> Project Name: humancompatibleai/eirli</div><div id='commit'> Commit Name: ccdc28141c799043c5385b20f1201bc6971e462b</div><div id='time'> Time: 2020-11-19</div><div id='author'> Author: sam@qxcv.net</div><div id='file'> File Name: src/il_representations/algos/representation_learner.py</div><div id='m_class'> M Class Name: RepresentationLearner</div><div id='n_method'> N Class Name: RepresentationLearner</div><div id='m_method'> M Method Name: learn(4)</div><div id='n_method'> N Method Name: learn(4)</div><div id='m_parent_class'> M Parent Class: BaseEnvironmentLearner</div><div id='n_parent_class'> N Parent Class: BaseEnvironmentLearner</div><div id='m_file'> M File Name: src/il_representations/algos/representation_learner.py</div><div id='n_file'> N File Name: src/il_representations/algos/representation_learner.py</div><div id='m_start'> M Start Line: 220</div><div id='m_end'> M End Line: 354</div><div id='n_start'> N Start Line: 220</div><div id='n_end'> N End Line: 343</div><BR>