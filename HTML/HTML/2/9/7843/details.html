<html><h3>Pattern ID :7843
</h3><img src='28041515.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    j = torch.arange(heatmaps.shape[1]).reshape(1, -1, 1, 1)
    k = locs[:, :, None, 1, None].type(torch.int64)  &#47&#47 y first
    l = locs[:, :, 0, None, None].type(torch.int64)  &#47&#47 x second
    vals = <a id="change">heatmaps[i, j, k, l].squeeze(-1</a><a id="change">)</a>.squeeze(-1)  &#47&#47 get rid of singleton dims
    return vals
</code></pre><h3>After Change</h3><pre><code class='java'>
    l = locs[:, :, 0, None, None].type(torch.int64)
    pix_to_consider = int(np.ceil(sigma * 2.0)) &#47&#47get all pixels within two stds.
    offsets = list(np.arange(-pix_to_consider, pix_to_consider+1))
    vals_all<a id="change"> = </a><a id="change">[]</a>
    for offset in offsets:
        k_offset = k + offset
        k_offset = torch.clamp(k_offset, min=0, max=heatmaps.shape[2]-1)
        <a id="change">for </a>offset_2 in offsets<a id="change">:
            </a>l_offset = l + offset_2
            l_offset<a id="change"> = </a>torch.clamp(l_offset, min=0, max=heatmaps.shape[2]-1)
            vals = heatmaps[i, j, k_offset, l_offset].squeeze(-1).squeeze(-1)  &#47&#47 get rid of singleton dims
            <a id="change">vals_all.append(</a>vals<a id="change">)</a>
    vals = torch.stack(vals_all, 0).sum(0)
    return vals</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/danbider/lightning-pose/commit/c2ea4f5fc3841def2213fb073698b9178edea896#diff-9dd67cdb80bc17d35d73536f0d0097629a0d6213a72e69af6280e984b273233fL293' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 28041515</div><div id='project'> Project Name: danbider/lightning-pose</div><div id='commit'> Commit Name: c2ea4f5fc3841def2213fb073698b9178edea896</div><div id='time'> Time: 2022-07-22</div><div id='author'> Author: colehurwitz@gmail.com</div><div id='file'> File Name: lightning_pose/data/utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: evaluate_heatmaps_at_location(3)</div><div id='n_method'> N Method Name: evaluate_heatmaps_at_location(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: lightning_pose/data/utils.py</div><div id='n_file'> N File Name: lightning_pose/data/utils.py</div><div id='m_start'> M Start Line: 293</div><div id='m_end'> M End Line: 297</div><div id='n_start'> N Start Line: 304</div><div id='n_end'> N End Line: 322</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    j = torch.arange(heatmaps.shape[1]).reshape(1, -1, 1, 1)
    k = locs[:, :, None, 1, None].type(torch.int64)  &#47&#47 y first
    l = locs[:, :, 0, None, None].type(torch.int64)  &#47&#47 x second
    vals = <a id="change">heatmaps[i, j, k, l].squeeze(-1).squeeze(-1</a><a id="change">)</a>  &#47&#47 get rid of singleton dims
    return vals
</code></pre><h3>After Change</h3><pre><code class='java'>
    l = locs[:, :, 0, None, None].type(torch.int64)
    pix_to_consider = int(np.ceil(sigma * 2.0)) &#47&#47get all pixels within two stds.
    offsets = list(np.arange(-pix_to_consider, pix_to_consider+1))
    vals_all<a id="change"> = </a><a id="change">[]</a>
    for <a id="change">offset</a> in offsets:
        k_offset = k + offset
        k_offset = torch.clamp(k_offset, min=0, max=heatmaps.shape[2]-1)
        <a id="change">for offset_2</a> in offsets<a id="change">:
            </a>l_offset = l + offset_2
            l_offset<a id="change"> = </a>torch.clamp(l_offset, min=0, max=heatmaps.shape[2]-1)
            vals = heatmaps[i, j, k_offset, l_offset].squeeze(-1).squeeze(-1)  &#47&#47 get rid of singleton dims
            <a id="change">vals_all.append(</a>vals<a id="change">)</a>
    vals = torch.stack(vals_all, 0).sum(0)
    return vals</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/danbider/lightning-pose/commit/c2ea4f5fc3841def2213fb073698b9178edea896#diff-9dd67cdb80bc17d35d73536f0d0097629a0d6213a72e69af6280e984b273233fL288' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 28041512</div><div id='project'> Project Name: danbider/lightning-pose</div><div id='commit'> Commit Name: c2ea4f5fc3841def2213fb073698b9178edea896</div><div id='time'> Time: 2022-07-22</div><div id='author'> Author: colehurwitz@gmail.com</div><div id='file'> File Name: lightning_pose/data/utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: evaluate_heatmaps_at_location(3)</div><div id='n_method'> N Method Name: evaluate_heatmaps_at_location(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: lightning_pose/data/utils.py</div><div id='n_file'> N File Name: lightning_pose/data/utils.py</div><div id='m_start'> M Start Line: 293</div><div id='m_end'> M End Line: 297</div><div id='n_start'> N Start Line: 304</div><div id='n_end'> N End Line: 322</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    j = torch.arange(heatmaps.shape[1]).reshape(1, -1, 1, 1)
    k = locs[:, :, None, 1, None].type(torch.int64)  &#47&#47 y first
    l = locs[:, :, 0, None, None].type(torch.int64)  &#47&#47 x second
    vals = <a id="change">heatmaps[i, j, k, l].squeeze(-1</a><a id="change">)</a>.squeeze(-1)  &#47&#47 get rid of singleton dims
    return vals
</code></pre><h3>After Change</h3><pre><code class='java'>
    l = locs[:, :, 0, None, None].type(torch.int64)
    pix_to_consider = int(np.ceil(sigma * 2.0)) &#47&#47get all pixels within two stds.
    offsets = list(np.arange(-pix_to_consider, pix_to_consider+1))
    vals_all<a id="change"> = </a><a id="change">[]</a>
    for <a id="change">offset</a> in offsets:
        k_offset = k + offset
        k_offset = torch.clamp(k_offset, min=0, max=heatmaps.shape[2]-1)
        <a id="change">for offset_2</a> in offsets<a id="change">:
            </a>l_offset<a id="change"> = </a>l + offset_2
            l_offset = torch.clamp(l_offset, min=0, max=heatmaps.shape[2]-1)
            vals = heatmaps[i, j, k_offset, l_offset].squeeze(-1).squeeze(-1)  &#47&#47 get rid of singleton dims
            <a id="change">vals_all.append(</a>vals<a id="change">)</a>
    vals = torch.stack(vals_all, 0).sum(0)
    return vals</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/danbider/lightning-pose/commit/09f58d3175233d5d57b8369d64c9a58e5704bcec#diff-9dd67cdb80bc17d35d73536f0d0097629a0d6213a72e69af6280e984b273233fL288' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 28041519</div><div id='project'> Project Name: danbider/lightning-pose</div><div id='commit'> Commit Name: 09f58d3175233d5d57b8369d64c9a58e5704bcec</div><div id='time'> Time: 2022-07-22</div><div id='author'> Author: colehurwitz@gmail.com</div><div id='file'> File Name: lightning_pose/data/utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: evaluate_heatmaps_at_location(3)</div><div id='n_method'> N Method Name: evaluate_heatmaps_at_location(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: lightning_pose/data/utils.py</div><div id='n_file'> N File Name: lightning_pose/data/utils.py</div><div id='m_start'> M Start Line: 293</div><div id='m_end'> M End Line: 297</div><div id='n_start'> N Start Line: 304</div><div id='n_end'> N End Line: 322</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                out = _stateless.functional_call(self._model, {n: p for n, p in zip(keys, params)}, x)
                return out
            self._j_list: tuple[torch.Tensor] = torch.autograd.functional.jacobian(func, values, create_graph=True)
            self._j_list = [<a id="change">j.squeeze(1</a><a id="change">)</a>.flatten(start_dim=1) for j in self._j_list] &#47&#47 remove hidden and predict bias dimensions

            &#47&#47 vectorized hessian (https://github.com/pytorch/pytorch/issues/49171)
            def loss(*params):</code></pre><h3>After Change</h3><pre><code class='java'>
            parameters = dict(self._model.named_parameters())
            keys, values = zip(*parameters.items())

            self._h_list<a id="change"> = </a><a id="change">[]</a>
            if <a id="change">self.hessian_approx</a>:
                &#47&#47 vectorized jacobian (https://github.com/pytorch/pytorch/issues/49171)
                def func(*params: torch.Tensor):
                    out = _stateless.functional_call(self._model, {n: p for n, p in zip(keys, params)}, x)
                    return out
                self._j_list: tuple[torch.Tensor] = torch.autograd.functional.jacobian(func, values, create_graph=False)    &#47&#47 NxCxBxCxHxW
                &#47&#47 create hessian approximation
                <a id="change">for </a>i, <a id="change">j</a> in enumerate(self._j_list)<a id="change">:
                    </a>j = j.flatten(end_dim=len(self._j_list[i].shape)-len(d_p_list[i].shape)-1).flatten(start_dim=1)  &#47&#47 (NC)x(BCHW)
                    h<a id="change"> = </a>j.T.matmul(j)
                    <a id="change">self._h_list.append(</a>h<a id="change">)</a>
                
            else:
                &#47&#47 vectorized hessian (https://github.com/pytorch/pytorch/issues/49171)
                def func(*params: torch.Tensor):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/hahnec/torchimize/commit/d65b33c9e39df589093da6cf91b8998beb6aead2#diff-cf9a1e9bfb69bdf33a7cd99ce087c7d7f64730b5db5485b7d221927020235390L51' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 28041507</div><div id='project'> Project Name: hahnec/torchimize</div><div id='commit'> Commit Name: d65b33c9e39df589093da6cf91b8998beb6aead2</div><div id='time'> Time: 2022-07-23</div><div id='author'> Author: christopher.hahne@unibe.ch</div><div id='file'> File Name: torchimize/optimizer/gna_opt.py</div><div id='m_class'> M Class Name: GNA</div><div id='n_method'> N Class Name: GNA</div><div id='m_method'> M Method Name: step(3)</div><div id='n_method'> N Method Name: step(3)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: torchimize/optimizer/gna_opt.py</div><div id='n_file'> N File Name: torchimize/optimizer/gna_opt.py</div><div id='m_start'> M Start Line: 58</div><div id='m_end'> M End Line: 87</div><div id='n_start'> N Start Line: 62</div><div id='n_end'> N End Line: 84</div><BR>