<html><h3>Pattern ID :12891
</h3><img src='43569081.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        active_clients_first_round = msg_params.get(
            MyMessage.MSG_ARG_KEY_ACTIVE_CLIENTS
        )
        <a id="change">logging.info(
            </a>"Client %d receive active_clients in the first round = %s"
            % (self.get_sender_id(), active_clients_first_round)<a id="change">
        )</a>

        &#47&#47 Compute the aggregate of encoded masks for the active clients
        p = self.prime_number
        aggregate_encoded_mask = compute_aggregate_encoded_mask(</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 Receive the set of active client id in first round
        active_clients = msg_params.get(MyMessage.MSG_ARG_KEY_ACTIVE_CLIENTS)
        &#47&#47 3.1. Send SS
        <a id="change">active_clients_dict</a><a id="change"> = </a><a id="change">dict()</a>
        <a id="change">for </a>client in active_clients<a id="change">:
            active_clients_dict</a><a id="change">[client] = </a>True
        SS_info = np.empty(self.worker_num, dtype="int64")
        for i in range(self.worker_num):
            <a id="change">if </a>i in active_clients_dict:
                SS_info[i] = self.b_u_SS_others[i]
            else:
                SS_info[i] = self.s_sk_SS_others[i]</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/fedml-ai/fedml/commit/6531871041f16c6b08a3d6a728f934b2fcdc864a#diff-c88844018ef8f9a5fb0a2150aa0b3dbe14052feb1620bb1bbaab731c860876deL144' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43569081</div><div id='project'> Project Name: fedml-ai/fedml</div><div id='commit'> Commit Name: 6531871041f16c6b08a3d6a728f934b2fcdc864a</div><div id='time'> Time: 2022-08-26</div><div id='author'> Author: tuozhang@usc.edu</div><div id='file'> File Name: python/fedml/cross_silo/secagg/sa_fedml_client_manager.py</div><div id='m_class'> M Class Name: FedMLClientManager</div><div id='n_method'> N Class Name: FedMLClientManager</div><div id='m_method'> M Method Name: handle_message_receive_active_from_server(2)</div><div id='n_method'> N Method Name: handle_message_receive_active_from_server(2)</div><div id='m_parent_class'> M Parent Class: FedMLCommManager</div><div id='n_parent_class'> N Parent Class: FedMLCommManager</div><div id='m_file'> M File Name: python/fedml/cross_silo/secagg/sa_fedml_client_manager.py</div><div id='n_file'> N File Name: python/fedml/cross_silo/secagg/sa_fedml_client_manager.py</div><div id='m_start'> M Start Line: 168</div><div id='m_end'> M End Line: 183</div><div id='n_start'> N Start Line: 144</div><div id='n_end'> N End Line: 155</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        active_clients_first_round = msg_params.get(
            MyMessage.MSG_ARG_KEY_ACTIVE_CLIENTS
        )
        <a id="change">logging.info(
            </a>"Client %d receive active_clients in the first round = %s"
            % (self.get_sender_id(), active_clients_first_round)<a id="change">
        )</a>

        &#47&#47 Compute the aggregate of encoded masks for the active clients
        p = self.prime_number
        aggregate_encoded_mask = compute_aggregate_encoded_mask(</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 Receive the set of active client id in first round
        active_clients = msg_params.get(MyMessage.MSG_ARG_KEY_ACTIVE_CLIENTS)
        &#47&#47 3.1. Send SS
        <a id="change">active_clients_dict</a><a id="change"> = </a><a id="change">dict()</a>
        <a id="change">for </a><a id="change">client</a> in active_clients<a id="change">:
            </a><a id="change">active_clients_dict[client] = </a>True
        SS_info = np.empty(self.worker_num, dtype="int64")
        for i in range(self.worker_num):
            <a id="change">if </a>i in active_clients_dict:
                SS_info[i] = self.b_u_SS_others[i]
            else:
                SS_info[i] = self.s_sk_SS_others[i]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/fedml-ai/fedml/commit/aaf7b7bb94d228a9bcda3dd0c76b4ead7fef042a#diff-c88844018ef8f9a5fb0a2150aa0b3dbe14052feb1620bb1bbaab731c860876deL165' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43569077</div><div id='project'> Project Name: fedml-ai/fedml</div><div id='commit'> Commit Name: aaf7b7bb94d228a9bcda3dd0c76b4ead7fef042a</div><div id='time'> Time: 2022-08-26</div><div id='author'> Author: tuozhang@usc.edu</div><div id='file'> File Name: python/fedml/cross_silo/secagg/sa_fedml_client_manager.py</div><div id='m_class'> M Class Name: FedMLClientManager</div><div id='n_method'> N Class Name: FedMLClientManager</div><div id='m_method'> M Method Name: handle_message_receive_active_from_server(2)</div><div id='n_method'> N Method Name: handle_message_receive_active_from_server(2)</div><div id='m_parent_class'> M Parent Class: FedMLCommManager</div><div id='n_parent_class'> N Parent Class: FedMLCommManager</div><div id='m_file'> M File Name: python/fedml/cross_silo/secagg/sa_fedml_client_manager.py</div><div id='n_file'> N File Name: python/fedml/cross_silo/secagg/sa_fedml_client_manager.py</div><div id='m_start'> M Start Line: 168</div><div id='m_end'> M End Line: 183</div><div id='n_start'> N Start Line: 144</div><div id='n_end'> N End Line: 155</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    epoch = 0

    while True:
        <a id="change">logging.info(</a>f&quotstarting epoch {epoch}&quot<a id="change">)</a>

        if epoch == 0:
            logger.info(f&quotPreparing iterators&quot)
            train_iters = [(name, to_iter(args, world_size, tok, x, device, token_testing=args.token_testing))</code></pre><h3>After Change</h3><pre><code class='java'>
    local_train_metric_dict = dict()

    task_iteration = dict()
    <a id="change">task_done</a><a id="change"> = </a><a id="change">dict()</a>

    saver = Saver(args.log_dir, world_size, args.max_to_keep)
    epoch = 0

    <a id="change">for </a><a id="change">task</a> in args.train_tasks<a id="change">:
        </a>task_iteration[task] = 1
        <a id="change">task_done[task] = </a>False

    while True:

        logger.info(f&quotstarting epoch {epoch}&quot)
        logger.info(f&quotPreparing iterators&quot)
        train_iters = [(name, to_iter(args, world_size, tok, x, device, token_testing=args.token_testing))
                          for name, x, tok in zip(args.train_tasks, train_sets, args.train_batch_tokens)]
        val_iters = [(name, to_iter(args, world_size, tok, x, device, train=False, token_testing=args.token_testing, sort=False if &quotsql&quot in name else None))
                        for name, x, tok in zip(args.val_tasks, val_sets, args.val_batch_size)]

        if args.use_curriculum:
            &#47&#47 logger.info(f&quotUpdating iterators for curriculum&quot)
            mixed_sets = create_mixed_set(args, train_sets, aux_sets, epoch)
            train_iters = [(name, to_iter(args, world_size, tok, x, device, token_testing=args.token_testing))
                  for name, x, tok in zip(args.train_tasks, mixed_sets, args.train_batch_tokens)]
            val_iters = [(name, to_iter(args, world_size, tok, x, device, train=False, token_testing=args.token_testing, sort=False if &quotsql&quot in name else None))
                  for name, x, tok in zip(args.val_tasks, val_sets, args.val_batch_size)]


        train_iters = [(task, iter(train_iter)) for task, train_iter in train_iters]
        &#47&#47 For some number of rounds, we &quotjump start&quot some subset of the tasks
        &#47&#47 by training them and not others
        &#47&#47 once the specified number of rounds is completed, 
        &#47&#47 switch to normal round robin training
        if rnd &lt; args.jump_start:
            train_iterations = [0]*len(train_iterations)
            for j in range(args.n_jump_start): train_iterations[j] = 1
        else:
            train_iterations = train_iter_deep

        for task_idx, (task, train_iter) in enumerate(train_iters):

            task_iterations = train_iterations[task_idx] if train_iterations is not None else None
            if task_iterations == 0:
                continue
            if task_iterations is not None and task_iteration[task] &gt; task_iterations:
                task_done[task] = True
                continue


            for batch in train_iter:
                if not args.resume or iteration &gt; start_iteration:
                    task_progress = f&quot{task_iteration[task]}/{task_iterations}:&quot if task_iterations is not None else &quot&quot
                    round_progress = f&quotround_{rnd}:&quot if rounds else &quot&quot
    
                    &#47&#47 validate
                    deca_score = None
                    if (val_every is not None and 
                        ((iteration % args.val_every == 0 % args.val_every) or 
                            (args.load and iteration == start_iteration + 1))):
                        
                        deca_score = 0
                        for val_task_idx, (val_task, val_iter) in enumerate(val_iters):
                            val_loss, metric_dict = validate(val_task, val_iter, model, logger, field, world_size, rank, iteration, num_print=args.num_print, args=args)
                            if val_loss is not None:
                                log_entry = f&quot{args.timestamp}:{elapsed_time(logger)}:iteration_{iteration}:{round_progress}train_{task}:{task_progress}val_{val_task}:val_loss{val_loss.item():.4f}:&quot
                                writer.add_scalar(f&quotloss/{val_task}/val&quot, val_loss.item(), iteration)
                            else:
                                log_entry = f&quot{args.timestamp}:{elapsed_time(logger)}:iteration_{iteration}:{round_progress}train_{task}:{task_progress}val_{val_task}:&quot
                               
                            metric_entry = &quot&quot
                            for metric_key, metric_value in metric_dict.items():
                                metric_entry += f&quot{metric_key}_{metric_value:.2f}:&quot
                            metric_entry = metric_entry[:-1]
                           
                            deca_score += metric_dict[args.task_to_metric[val_task]]
                           
                            &#47&#47 val log
                            logger.info(log_entry + metric_entry)
                            if writer is not None:
                                for metric_key, metric_value in metric_dict.items():
                                    writer.add_scalar(f&quot{metric_key}/{val_task}/val&quot, metric_value, iteration)
                                    writer.add_scalar(f&quot{val_task}/{metric_key}/val&quot, metric_value, iteration)
                        writer.add_scalar(&quotdeca/val&quot, deca_score, iteration)
                        logger.info(f&quot{args.timestamp}:{elapsed_time(logger)}:iteration_{iteration}:{round_progress}train_{task}:{task_progress}val_deca:deca_{deca_score:.2f}&quot)

                    &#47&#47 saving
                    if save_every is not None and (iteration % args.save_every == 0):
                        if rank is not None and rank == 0:
                            should_save_best = False
                            if deca_score is not None and (best_decascore is None or best_decascore &lt; deca_score):
                                best_decascore = deca_score
                                should_save_best = True
                                
                            save_model_state_dict = {&quotmodel_state_dict&quot: {k: v.cpu() for k, v in model.state_dict().items()}, &quotfield&quot: field,
                                               &quotbest_decascore&quot: best_decascore}
                            save_opt_state_dict = opt.state_dict()
                            save_opt_state_dict.update({&quotstart_iteration&quot: iteration})

                            if world_size &gt; 1:
                                torch.distributed.barrier()
                            saver.save(save_model_state_dict, save_opt_state_dict, global_step=iteration)
                            if should_save_best:
                                logger.info(f&quot{args.timestamp}:{elapsed_time(logger)}:iteration_{iteration}:{round_progress}train_{task}:{task_progress}found new best model&quot)
                                torch.save(save_model_state_dict, os.path.join(args.log_dir, &quotbest.pth&quot))
                                if world_size &gt; 1:
                                    torch.distributed.barrier()
                                torch.save(save_opt_state_dict, os.path.join(args.log_dir, &quotbest_optim.pth&quot))
                                if world_size &gt; 1:
                                    torch.distributed.barrier()

                    &#47&#47 lr update
                    lr = opt.param_groups[0][&quotlr&quot] 
                    if args.warmup &gt; 0 and args.transformer_lr:
                        lr = get_learning_rate(iteration, args) 

                    &#47&#47 param update
                    loss, train_metric_dict = step(model, batch, opt, iteration, field, task, lr=lr, grad_clip=args.grad_clip, writer=writer, it=train_iter)

                    &#47&#47 train metrics
                    local_loss += loss
                    for metric_name, metric_val in train_metric_dict.items():
                        if metric_name in local_train_metric_dict:
                            local_train_metric_dict[metric_name] += metric_val / args.log_every
                        else:
                            local_train_metric_dict[metric_name] = metric_val / args.log_every

                    &#47&#47 train logs
                    num_examples += batch.context.size(0)
                    len_contexts += batch.context.size(1)
                    len_answers += batch.answer.size(1)

                    if log_every is not None and (iteration % log_every == 0 % log_every):
                        local_loss /= args.log_every
                        num_examples /= args.log_every
                        len_contexts /= args.log_every
                        len_answers /= args.log_every
                        avg_batch_size = f&quotavbatch_{num_examples:.0f}_{len_contexts:.0f}_{len_answers:.0f}:&quot
                        metric_entry = &quot&quot
                        for metric_key, metric_value in local_train_metric_dict.items():
                            metric_entry += f&quot{metric_key}_{metric_value:.2f}:&quot
                        metric_entry = f&quot{metric_entry[:-1]}&quot
                        logger.info(f&quot{args.timestamp}:{elapsed_time(logger)}:iteration_{iteration}:{round_progress}train_{task}:{task_progress}{avg_batch_size}loss_{local_loss:.4f}{metric_entry}&quot) 
                        num_examples = 0 
                        len_contexts = 0 
                        len_answers = 0  
    
                        if writer is not None:
                            writer.add_scalar(f&quotloss/{task}/train&quot, local_loss, iteration)
                            for metric_key, metric_value in local_train_metric_dict.items():
                                writer.add_scalar(f&quot{metric_key}/{task}/train&quot, metric_value, iteration)
                                writer.add_scalar(f&quot{task}/{metric_key}/train&quot, metric_value, iteration)

                        local_loss = 0
                        local_train_metric_dict = {}
                        num_examples = 0
                    
                &#47&#47 book keeping
                task_iteration[task] += 1
                iteration += 1

        &#47&#47 book keeping
        epoch += 1
        rnd += 1
        <a id="change">if </a>all(task_done.values()):
            logger.info(f&quottraining is done after {epoch} epochs&quot)
            break
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/stanford-oval/genienlp/commit/b6bc8988b23ef76b9172daaee7f6d3539d406b83#diff-605dd195f574ab979ac4e3d48e9ed7dc75e65560aec927db9feaf3b2ddf90858L247' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43569079</div><div id='project'> Project Name: stanford-oval/genienlp</div><div id='commit'> Commit Name: b6bc8988b23ef76b9172daaee7f6d3539d406b83</div><div id='time'> Time: 2019-03-12</div><div id='author'> Author: mehrad@stanford.edu</div><div id='file'> File Name: decanlp/train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train(18)</div><div id='n_method'> N Method Name: train(18)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: decanlp/train.py</div><div id='n_file'> N File Name: decanlp/train.py</div><div id='m_start'> M Start Line: 257</div><div id='m_end'> M End Line: 414</div><div id='n_start'> N Start Line: 250</div><div id='n_end'> N End Line: 425</div><BR>