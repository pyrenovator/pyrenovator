<html><h3>Pattern ID :30128
</h3><img src='89300306.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                 f&quotLoad with pandas.read_csv().&quot)
    DataFrame(results.evaluation).to_csv(eval_file)
    &#47&#47 Save ground-truth and predicted classes for further performance analysis
    <a id="change">logging.info(f&quotSaving ground truth (y_true) and predicted (y_pred) &quot
                 f&quotlabels (from training/validation) to {classes_file}... &quot
                 f&quotLoad with numpy.load().&quot</a><a id="change">)</a>
    np.savez(classes_file,
             y_train_true=results.y_train_true,
             y_train_pred=results.y_train_pred,
             y_val_true=results.y_val_true,</code></pre><h3>After Change</h3><pre><code class='java'>
    from deepnog.learning import fit
    from deepnog.utils import get_logger

    <a id="change">logger = </a><a id="change">get_logger(</a>__name__<a id="change">, verbose=args.verbose)</a>

    if args.n_epochs &lt;= 0:
        raise ValueError(f&quotNumber of epochs must be greater than or equal &quot
                         f&quotone. Got n_epochs = {args.n_epochs} instead.&quot)
    out_dir = Path(args.out)
    <a id="change">logger.info(f&quotOutput directory: {out_dir} (creating, if necessary)&quot</a><a id="change">)</a>
    out_dir.mkdir(parents=True, exist_ok=True)
    &#47&#47 Add random letters to files to avoid name collisions
    while True:
        random_letters = &quot&quot.join(random.sample(string.ascii_letters, 4))
        if not any([random_letters in str(f) for f in out_dir.iterdir()]):
            break  &#47&#47 if these letters were not used previously
    experiment_name = f&quotdeepnog_custom_model_{args.database}_{args.tax}_{random_letters}&quot
    model_file = out_dir/f&quot{experiment_name}_model.pt&quot
    eval_file = out_dir/f&quot{experiment_name}_eval.csv&quot
    classes_file = out_dir/f&quot{experiment_name}_labels.npz&quot

    results = fit(architecture=args.architecture,
                  training_sequences=args.training_sequences,
                  validation_sequences=args.validation_sequences,
                  data_loader_params={&quotbatch_size&quot: args.batch_size,
                                      &quotnum_workers&quot: args.num_workers},
                  learning_rate=args.learning_rate,
                  labels=args.labels,
                  device=args.device,
                  verbose=args.verbose,
                  n_epochs=args.n_epochs,
                  shuffle=args.shuffle,
                  random_seed=args.random_seed,
                  &#47&#47 TODO add the rest of the parameters to the client
                  )

    &#47&#47 Save model to output dir
    logger.info(f&quotSaving model to {model_file}...&quot)
    torch.save({&quotclasses&quot: results.training_dataset.label_encoder.classes_,
                &quotmodel_state_dict&quot: results.model.state_dict()},
               model_file)
    &#47&#47 Save a dataframe of several training/validation statistics
    <a id="change">logger.info(f&quotSaving evaluation statistics to {eval_file}... &quot
                f&quotLoad with pandas.read_csv().&quot</a><a id="change">)</a>
    DataFrame(results.evaluation).to_csv(eval_file)
    &#47&#47 Save ground-truth and predicted classes for further performance analysis
    logger.info(f&quotSaving ground truth (y_true) and predicted (y_pred) &quot
                f&quotlabels (from training/validation) to {classes_file}... &quot</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/univiecube/deepnog/commit/fb708b2e20a5291791bfcca05f911179ed1c5745#diff-86cf8881102340a02991064455bc3d2ec43e28c1fa00c0f4f963ddab395ff5baL337' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 89300306</div><div id='project'> Project Name: univiecube/deepnog</div><div id='commit'> Commit Name: fb708b2e20a5291791bfcca05f911179ed1c5745</div><div id='time'> Time: 2020-06-16</div><div id='author'> Author: sci@feldbauer.org</div><div id='file'> File Name: deepnog/client/client.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _start_training(1)</div><div id='n_method'> N Method Name: _start_training(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: deepnog/client/client.py</div><div id='n_file'> N File Name: deepnog/client/client.py</div><div id='m_start'> M Start Line: 337</div><div id='m_end'> M End Line: 383</div><div id='n_start'> N Start Line: 339</div><div id='n_end'> N End Line: 392</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                 f&quotLoad with pandas.read_csv().&quot)
    DataFrame(results.evaluation).to_csv(eval_file)
    &#47&#47 Save ground-truth and predicted classes for further performance analysis
    <a id="change">logging.info(f&quotSaving ground truth (y_true) and predicted (y_pred) &quot
                 f&quotlabels (from training/validation) to {classes_file}... &quot
                 f&quotLoad with numpy.load().&quot</a><a id="change">)</a>
    np.savez(classes_file,
             y_train_true=results.y_train_true,
             y_train_pred=results.y_train_pred,
             y_val_true=results.y_val_true,</code></pre><h3>After Change</h3><pre><code class='java'>
    from deepnog.learning import fit
    from deepnog.utils import get_logger

    <a id="change">logger = </a><a id="change">get_logger(</a>__name__<a id="change">, verbose=args.verbose)</a>

    if args.n_epochs &lt;= 0:
        raise ValueError(f&quotNumber of epochs must be greater than or equal &quot
                         f&quotone. Got n_epochs = {args.n_epochs} instead.&quot)
    out_dir = Path(args.out)
    logger.info(f&quotOutput directory: {out_dir} (creating, if necessary)&quot)
    out_dir.mkdir(parents=True, exist_ok=True)
    &#47&#47 Add random letters to files to avoid name collisions
    while True:
        random_letters = &quot&quot.join(random.sample(string.ascii_letters, 4))
        if not any([random_letters in str(f) for f in out_dir.iterdir()]):
            break  &#47&#47 if these letters were not used previously
    experiment_name = f&quotdeepnog_custom_model_{args.database}_{args.tax}_{random_letters}&quot
    model_file = out_dir/f&quot{experiment_name}_model.pt&quot
    eval_file = out_dir/f&quot{experiment_name}_eval.csv&quot
    classes_file = out_dir/f&quot{experiment_name}_labels.npz&quot

    results = fit(architecture=args.architecture,
                  training_sequences=args.training_sequences,
                  validation_sequences=args.validation_sequences,
                  data_loader_params={&quotbatch_size&quot: args.batch_size,
                                      &quotnum_workers&quot: args.num_workers},
                  learning_rate=args.learning_rate,
                  labels=args.labels,
                  device=args.device,
                  verbose=args.verbose,
                  n_epochs=args.n_epochs,
                  shuffle=args.shuffle,
                  random_seed=args.random_seed,
                  &#47&#47 TODO add the rest of the parameters to the client
                  )

    &#47&#47 Save model to output dir
    logger.info(f&quotSaving model to {model_file}...&quot)
    torch.save({&quotclasses&quot: results.training_dataset.label_encoder.classes_,
                &quotmodel_state_dict&quot: results.model.state_dict()},
               model_file)
    &#47&#47 Save a dataframe of several training/validation statistics
    <a id="change">logger.info(f&quotSaving evaluation statistics to {eval_file}... &quot
                f&quotLoad with pandas.read_csv().&quot</a><a id="change">)</a>
    DataFrame(results.evaluation).to_csv(eval_file)
    &#47&#47 Save ground-truth and predicted classes for further performance analysis
    logger.info(f&quotSaving ground truth (y_true) and predicted (y_pred) &quot
                f&quotlabels (from training/validation) to {classes_file}... &quot
                f&quotLoad with numpy.load().&quot)
    np.savez(classes_file,
             y_train_true=results.y_train_true,
             y_train_pred=results.y_train_pred,
             y_val_true=results.y_val_true,
             y_val_pred=results.y_val_pred)

    <a id="change">logger.info(&quotAll done.&quot</a><a id="change">)</a>
    return


def main():</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/univiecube/deepnog/commit/fb708b2e20a5291791bfcca05f911179ed1c5745#diff-86cf8881102340a02991064455bc3d2ec43e28c1fa00c0f4f963ddab395ff5baL324' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 89300307</div><div id='project'> Project Name: univiecube/deepnog</div><div id='commit'> Commit Name: fb708b2e20a5291791bfcca05f911179ed1c5745</div><div id='time'> Time: 2020-06-16</div><div id='author'> Author: sci@feldbauer.org</div><div id='file'> File Name: deepnog/client/client.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _start_training(1)</div><div id='n_method'> N Method Name: _start_training(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: deepnog/client/client.py</div><div id='n_file'> N File Name: deepnog/client/client.py</div><div id='m_start'> M Start Line: 337</div><div id='m_end'> M End Line: 383</div><div id='n_start'> N Start Line: 339</div><div id='n_end'> N End Line: 392</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        return

    &#47&#47 8. Start training and validation
    <a id="change">logging.info(f"Start training from epoch {last_epoch+1}."</a><a id="change">)</a>
    for epoch in range(last_epoch+1, config.TRAIN.NUM_EPOCHS+1):
        &#47&#47 train
        logging.info(f"Now training epoch {epoch}. LR={optimizer.get_lr():.6f}")
        train_loss_ce, train_loss_bbox, train_loss_giou, train_time = train(</code></pre><h3>After Change</h3><pre><code class='java'>
    np.random.seed(seed)
    random.seed(seed)

    <a id="change">logger = </a><a id="change">get_logger(filename=os.path.join(config.SAVE, &quotlog.txt&quot))</a>
    <a id="change">logger.info(f&quot\n{config}&quot</a><a id="change">)</a>

    &#47&#47 STEP 1: Create model and criterion
    model, criterion, postprocessors = build_detr(config)
    &#47&#47 STEP 2: Create train and val dataloader
    if not config.EVAL:
        dataset_train = build_coco(&quottrain&quot, config.DATA.DATA_PATH)
        dataloader_train = get_dataloader(dataset_train,
                                          batch_size=config.DATA.BATCH_SIZE,
                                          mode=&quottrain&quot, 
                                          multi_gpu=False)

    dataset_val = build_coco(&quotval&quot, config.DATA.DATA_PATH)
    dataloader_val = get_dataloader(dataset_val,
                                    batch_size=config.DATA.BATCH_SIZE_EVAL,
                                    mode=&quotval&quot, 
                                    multi_gpu=False)

    base_ds = dataset_val.coco   &#47&#47 pycocotools.coco.COCO(anno_file)
    &#47&#47 STEP 3: Define lr_scheduler
    scheduler = None
    if config.TRAIN.LR_SCHEDULER.NAME == "warmupcosine":
        scheduler = WarmupCosineScheduler(learning_rate=config.TRAIN.BASE_LR,
                                          warmup_start_lr=config.TRAIN.WARMUP_START_LR,
                                          start_lr=config.TRAIN.BASE_LR,
                                          end_lr=config.TRAIN.END_LR,
                                          warmup_epochs=config.TRAIN.WARMUP_EPOCHS,
                                          total_epochs=config.TRAIN.NUM_EPOCHS,
                                          last_epoch=config.TRAIN.LAST_EPOCH,
                                          )
    elif config.TRAIN.LR_SCHEDULER.NAME == "cosine":
        scheduler = paddle.optimizer.lr.CosineAnnealingDecay(learning_rate=config.TRAIN.BASE_LR, 
                                                             T_max=config.TRAIN.NUM_EPOCHS,
                                                             last_epoch=last_epoch)
    elif config.TRAIN.LR_SCHEDULER.NAME == "multi-step":
        milestones = [int(v.strip()) for v in config.TRAIN.LR_SCHEDULER.MILESTONES.split(",")]
        scheduler = paddle.optimizer.lr.MultiStepDecay(learning_rate=config.TRAIN.BASE_LR, 
                                                       milestones=milestons,
                                                       gamma=config.TRAIN.LR_SCHEDULER.DECAY_RATE,
                                                       last_epoch=last_epoch)
    elif config.TRAIN.LR_SCHEDULER.NAME == &quotstep&quot:
        scheduler = paddle.optimizer.lr.StepDecay(learning_rate=config.TRAIN.BASE_LR,
                                                  step_size=config.TRAIN.LR_SCHEDULER.DECAY_EPOCHS,
                                                  gamma=config.TRAIN.LR_SCHEDULER.DECAY_RATE,
                                                  last_epoch=last_epoch)
    else:
        logger.fatal(f"Unsupported Scheduler: {config.TRAIN.LR_SCHEDULER}.")
        raise NotImplementedError(f"Unsupported Scheduler: {config.TRAIN.LR_SCHEDULER}.")

    &#47&#47 STEP 4: Define optimizer
    if config.TRAIN.GRAD_CLIP:
        clip = paddle.nn.ClipGradByGlobalNorm(config.TRAIN.GRAD_CLIP)
    else:
        clip = None
    if config.TRAIN.OPTIMIZER.NAME == "SGD":
        optimizer = paddle.optimizer.Momentum(
            parameters=model.parameters(),
            learning_rate=scheduler if scheduler is not None else config.TRAIN.BASE_LR,
            weight_decay=config.TRAIN.WEIGHT_DECAY,
            momentum=config.TRAIN.OPTIMIZER.MOMENTUM,
            grad_clip=clip)
    elif config.TRAIN.OPTIMIZER.NAME == "AdamW":
        optimizer = paddle.optimizer.AdamW(
            parameters=model.parameters(),
            learning_rate=scheduler if scheduler is not None else config.TRAIN.BASE_LR,
            beta1=config.TRAIN.OPTIMIZER.BETAS[0],
            beta2=config.TRAIN.OPTIMIZER.BETAS[1],
            weight_decay=config.TRAIN.WEIGHT_DECAY,
            epsilon=config.TRAIN.OPTIMIZER.EPS,
            grad_clip=clip,
            )
    else:
        logger.fatal(f"Unsupported Optimizer: {config.TRAIN.OPTIMIZER.NAME}.")
        raise NotImplementedError(f"Unsupported Optimizer: {config.TRAIN.OPTIMIZER.NAME}.")

    &#47&#47 STEP 5: Load pretrained model or load resume model and optimizer states
    if config.MODEL.PRETRAINED:
        if (config.MODEL.PRETRAINED).endswith(&quot.pdparams&quot):
            raise ValueError(f&quot{config.MODEL.PRETRAINED} should not contain .pdparams&quot)
        assert os.path.isfile(config.MODEL.PRETRAINED + &quot.pdparams&quot) is True
        model_state = paddle.load(config.MODEL.PRETRAINED + &quot.pdparams&quot) 
        model.set_dict(model_state)
        logger.info(f"----- Pretrained: Load model state from {config.MODEL.PRETRAINED}")

    if config.MODEL.RESUME: 
        assert os.path.isfile(config.MODEL.RESUME + &quot.pdparams&quot) is True
        assert os.path.isfile(config.MODEL.RESUME + &quot.pdopt&quot) is True
        model_state = paddle.load(config.MODEL.RESUME + &quot.pdparams&quot) 
        model.set_dict(model_state)
        opt_state = paddle.load(config.MODEL.RESUME + &quot.pdopt&quot) 
        optimizer.set_dict(opt_state)
        logger.info(
            f"----- Resume Training: Load model and optmizer states from {config.MODEL.RESUME}")

    &#47&#47 STEP 6: Validation
    if config.EVAL:
        logger.info(f&quot----- Start Validating&quot)
        val_loss_ce, val_loss_bbox, val_loss_giou, val_time = validate(
            dataloader=dataloader_val,
            model=model,
            criterion=criterion,
            postprocessors=postprocessors,
            base_ds=base_ds,
            total_batch=len(dataloader_val),
            debug_steps=config.REPORT_FREQ)
        logger.info(f"Validation Loss ce: {val_loss_ce:.4f}, " +
                    f"Validation Loss bbox: {val_loss_bbox:.4f}, " +
                    f"Validation Loss giou: {val_loss_giou:.4f}, " +
                    f"time: {val_time:.2f}")
        return

    &#47&#47 STEP 7: Start training and validation
    <a id="change">logger.info(f"Start training from epoch {last_epoch+1}."</a><a id="change">)</a>
    for epoch in range(last_epoch+1, config.TRAIN.NUM_EPOCHS+1):
        &#47&#47 train
        logger.info(f"Now training epoch {epoch}. LR={optimizer.get_lr():.6f}")
        train_loss_ce, train_loss_bbox, train_loss_giou, train_time = train(</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/br-idl/paddlevit/commit/66ec7d581509e0d34a264da33f917b4a577b1996#diff-6047b77f2d39404b4ebdc6272dde8d2d23879f0995d2f8de13bcbe6e53c0cf4aL170' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 89300288</div><div id='project'> Project Name: br-idl/paddlevit</div><div id='commit'> Commit Name: 66ec7d581509e0d34a264da33f917b4a577b1996</div><div id='time'> Time: 2022-01-07</div><div id='author'> Author: xperzy@gmail.com</div><div id='file'> File Name: object_detection/DETR/main_single_gpu.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(0)</div><div id='n_method'> N Method Name: main(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: object_detection/DETR/main_single_gpu.py</div><div id='n_file'> N File Name: object_detection/DETR/main_single_gpu.py</div><div id='m_start'> M Start Line: 210</div><div id='m_end'> M End Line: 315</div><div id='n_start'> N Start Line: 231</div><div id='n_end'> N End Line: 405</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            ids.extend(batch.ids)
            indices.extend(batch.indices)

    <a id="change">logging.info(&quotInference complete.&quot</a><a id="change">)</a>
    &#47&#47 Collect skipped-sequences messages from workers in the case of
    &#47&#47 multi-process data-loading
    n_skipped = dataset.n_skipped
    &#47&#47 Check if sequences were skipped due to empty id</code></pre><h3>After Change</h3><pre><code class='java'>
        Stores the unique indices of sequences mapping to their position
        in the file
    
    <a id="change">logger = </a><a id="change">get_logger(</a>__name__<a id="change">, verbose=verbose)</a>

    <a id="change">logger.info(f&quotInference device: {device}&quot</a><a id="change">)</a>

    pred_l = []
    conf_l = []
    ids = []
    indices = []

    if num_workers &lt; 2:
        num_workers = 0
    &#47&#47 Create data-loader for protein dataset
    data_loader = DataLoader(dataset,
                             batch_size=batch_size,
                             num_workers=num_workers,
                             collate_fn=collate_sequences,
                             )
    &#47&#47 Disable tracking of gradients to increase performance
    with torch.no_grad():
        &#47&#47 Do prediction calculations
        disable_tqdm = verbose &lt; 3
        for i, batch in enumerate(tqdm(data_loader,
                                       disable=disable_tqdm,
                                       desc="deepnog inference")):
            &#47&#47 Push sequences on correct device
            sequences = batch.sequences.to(device)
            &#47&#47 Predict protein families
            output = model(sequences)
            output = model.softmax(output)

            conf, pred = torch.max(output, 1)
            &#47&#47 Store predictions
            pred_l.append(pred)
            conf_l.append(conf)
            ids.extend(batch.ids)
            indices.extend(batch.indices)

    <a id="change">logger.info(&quotInference complete.&quot</a><a id="change">)</a>
    &#47&#47 Collect skipped-sequences messages from workers in the case of
    &#47&#47 multi-process data-loading
    n_skipped = dataset.n_skipped
    &#47&#47 Check if sequences were skipped due to empty id</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/univiecube/deepnog/commit/fb708b2e20a5291791bfcca05f911179ed1c5745#diff-eb3825ce70bc08b63c53bcf5c7cce69426e7a5624de17cefe5d40b0fc50c29fdL23' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 89300292</div><div id='project'> Project Name: univiecube/deepnog</div><div id='commit'> Commit Name: fb708b2e20a5291791bfcca05f911179ed1c5745</div><div id='time'> Time: 2020-06-16</div><div id='author'> Author: sci@feldbauer.org</div><div id='file'> File Name: deepnog/learning/inference.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: predict(6)</div><div id='n_method'> N Method Name: predict(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: deepnog/learning/inference.py</div><div id='n_file'> N File Name: deepnog/learning/inference.py</div><div id='m_start'> M Start Line: 58</div><div id='m_end'> M End Line: 93</div><div id='n_start'> N Start Line: 58</div><div id='n_end'> N End Line: 95</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                    f"time: {val_time:.2f}")
        return
    &#47&#47 8. Start training and validation
    <a id="change">logging.info(f"Start training from epoch {last_epoch + 1}."</a><a id="change">)</a>
    for epoch in range(last_epoch + 1, config.TRAIN.NUM_EPOCHS + 1):
        &#47&#47 train
        logging.info(f"Now training epoch {epoch}. LR={optimizer.get_lr():.6f}")
        train_loss, train_acc, train_time = train(dataloader=dataloader_train,</code></pre><h3>After Change</h3><pre><code class='java'>
    paddle.seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    <a id="change">logger = </a><a id="change">get_logger(filename=os.path.join(config.SAVE, &quotlog.txt&quot))</a>
    <a id="change">logger.info(f&quot\n{config}&quot</a><a id="change">)</a>

    &#47&#47 STEP 1: Create model
    model = build_model(config)

    &#47&#47 STEP 2: Create train and val dataloader
    dataset_train = get_dataset(config, mode=&quottrain&quot)
    dataset_val = get_dataset(config, mode=&quotval&quot)
    dataloader_train = get_dataloader(config, dataset_train, &quottrain&quot, False)
    dataloader_val = get_dataloader(config, dataset_val, &quotval&quot, False)
    &#47&#47 3. Define criterion
    criterion = nn.CrossEntropyLoss()
    &#47&#47 4. Define lr_scheduler
    scheduler = None
    if config.TRAIN.LR_SCHEDULER.NAME == "warmupcosine":
        scheduler = WarmupCosineScheduler(learning_rate=config.TRAIN.BASE_LR,
                                          warmup_start_lr=config.TRAIN.WARMUP_START_LR,
                                          start_lr=config.TRAIN.BASE_LR,
                                          end_lr=config.TRAIN.END_LR,
                                          warmup_epochs=config.TRAIN.WARMUP_EPOCHS,
                                          total_epochs=config.TRAIN.NUM_EPOCHS,
                                          last_epoch=config.TRAIN.LAST_EPOCH)
    elif config.TRAIN.LR_SCHEDULER.NAME == "cosine":
        scheduler = paddle.optimizer.lr.CosineAnnealingDecay(learning_rate=config.TRAIN.BASE_LR,
                                                             T_max=config.TRAIN.NUM_EPOCHS,
                                                             last_epoch=last_epoch)
    elif config.scheduler == "multi-step":
        milestones = [int(v.strip()) for v in config.TRAIN.LR_SCHEDULER.MILESTONES.split(",")]
        scheduler = paddle.optimizer.lr.MultiStepDecay(learning_rate=config.TRAIN.BASE_LR,
                                                       milestones=milestones,
                                                       gamma=config.TRAIN.LR_SCHEDULER.DECAY_RATE,
                                                       last_epoch=last_epoch)
    else:
        logger.fatal(f"Unsupported Scheduler: {config.TRAIN.LR_SCHEDULER}.")
        raise NotImplementedError(f"Unsupported Scheduler: {config.TRAIN.LR_SCHEDULER}.")

    if config.TRAIN.OPTIMIZER.NAME == "SGD":
        if config.TRAIN.GRAD_CLIP:
            clip = paddle.nn.ClipGradByGlobalNorm(config.TRAIN.GRAD_CLIP)
        else:
            clip = None
        optimizer = paddle.optimizer.Momentum(
            parameters=model.parameters(),
            learning_rate=scheduler if scheduler is not None else config.TRAIN.BASE_LR,
            weight_decay=config.TRAIN.WEIGHT_DECAY,
            momentum=config.TRAIN.OPTIMIZER.MOMENTUM,
            grad_clip=clip)
    elif config.TRAIN.OPTIMIZER.NAME == "AdamW":
        if config.TRAIN.GRAD_CLIP:
            clip = paddle.nn.ClipGradByGlobalNorm(config.TRAIN.GRAD_CLIP)
        else:
            clip = None
        optimizer = paddle.optimizer.AdamW(
            parameters=model.parameters(),
            learning_rate=scheduler if scheduler is not None else config.TRAIN.BASE_LR,
            beta1=config.TRAIN.OPTIMIZER.BETAS[0],
            beta2=config.TRAIN.OPTIMIZER.BETAS[1],
            weight_decay=config.TRAIN.WEIGHT_DECAY,
            epsilon=config.TRAIN.OPTIMIZER.EPS,
            grad_clip=clip)
    else:
        logger.fatal(f"Unsupported Optimizer: {config.TRAIN.OPTIMIZER.NAME}.")
        raise NotImplementedError(f"Unsupported Optimizer: {config.TRAIN.OPTIMIZER.NAME}.")

    &#47&#47 STEP 6: Load pretrained model or load resume model and optimizer states
    if config.MODEL.PRETRAINED:
        if (config.MODEL.PRETRAINED).endswith(&quot.pdparams&quot):
            raise ValueError(f&quot{config.MODEL.PRETRAINED} should not contain .pdparams&quot)
        assert os.path.isfile(config.MODEL.PRETRAINED + &quot.pdparams&quot) is True
        model_state = paddle.load(config.MODEL.PRETRAINED+&quot.pdparams&quot)
        model.set_dict(model_state)
        logger.info(f"----- Pretrained: Load model state from {config.MODEL.PRETRAINED}")

    if config.MODEL.RESUME:
        assert os.path.isfile(config.MODEL.RESUME + &quot.pdparams&quot) is True
        assert os.path.isfile(config.MODEL.RESUME + &quot.pdopt&quot) is True
        model_state = paddle.load(config.MODEL.RESUME + &quot.pdparams&quot)
        model.set_dict(model_state)
        opt_state = paddle.load(config.MODEL.RESUME+&quot.pdopt&quot)
        optimizer.set_state_dict(opt_state)
        logger.info(
            f"----- Resume: Load model and optmizer from {config.MODEL.RESUME}")

    &#47&#47 STEP 7: Validation (eval mode)
    if config.EVAL:
        logger.info(&quot----- Start Validating&quot)
        val_loss, val_acc1, val_acc5, val_time = validate(
            dataloader=dataloader_val,
            model=model,
            criterion=criterion,
            total_batch=len(dataloader_val),
            debug_steps=config.REPORT_FREQ,
            logger=logger)
        logger.info(f"Validation Loss: {val_loss:.4f}, " +
                    f"Validation Acc@1: {val_acc1:.4f}, " +
                    f"Validation Acc@5: {val_acc5:.4f}, " +
                    f"time: {val_time:.2f}")
        return

    &#47&#47 STEP 8: Start training and validation (train mode)
    <a id="change">logger.info(f"Start training from epoch {last_epoch+1}."</a><a id="change">)</a>
    for epoch in range(last_epoch+1, config.TRAIN.NUM_EPOCHS+1):
        &#47&#47 train
        logger.info(f"Now training epoch {epoch}. LR={optimizer.get_lr():.6f}")
        train_loss, train_acc, train_time = train(dataloader=dataloader_train,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/br-idl/paddlevit/commit/e23569eededb7555508ac68a1244b94d153d3429#diff-51aff1a5e42639c26553409ebf1a90af8b3483a0de297dc9f9947b9feabe1da0L205' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 89300281</div><div id='project'> Project Name: br-idl/paddlevit</div><div id='commit'> Commit Name: e23569eededb7555508ac68a1244b94d153d3429</div><div id='time'> Time: 2021-12-09</div><div id='author'> Author: xperzy@gmail.com</div><div id='file'> File Name: image_classification/ViT/main_single_gpu.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(0)</div><div id='n_method'> N Method Name: main(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: image_classification/ViT/main_single_gpu.py</div><div id='n_file'> N File Name: image_classification/ViT/main_single_gpu.py</div><div id='m_start'> M Start Line: 244</div><div id='m_end'> M End Line: 308</div><div id='n_start'> N Start Line: 205</div><div id='n_end'> N End Line: 324</div><BR>