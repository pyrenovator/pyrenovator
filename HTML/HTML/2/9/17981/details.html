<html><h3>Pattern ID :17981
</h3><img src='59020212.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Run a forward chain to collect intermediate inputs
        &#47&#47 Note that we do not forward for the last module since we do not need its output
        inter_inputs = [inputs]
        <a id="change">for </a>backend in requested_backends[:-1]<a id="change">:
            </a><a id="change">assert </a>inputs.ndim == 3, f"inputs to {type(backend)} must be a single 3d tensor of hidden states"
            inputs = <a id="change">await </a><a id="change">backend.forward_pool.submit_task(</a>inputs<a id="change">)</a>
            <a id="change">assert </a>isinstance(inputs, (list, tuple)) and len(inputs) == 1
            inputs<a id="change"> = </a>inputs[0]
            inter_inputs.append(inputs)

        &#47&#47 Run a chain of requested backends</code></pre><h3>After Change</h3><pre><code class='java'>
        requested_uids = self._check_uids(request.uid)
        requested_backends = tuple(self.module_backends[uid] for uid in requested_uids)

        grads = <a id="change">await </a>_rpc_backward(*flat_tensors, requested_backends=requested_backends)

        &#47&#47 Modify grad_inputs_schema to support grad_prompts
        assert len(requested_backends[0].args_schema) == 1 and len(grads) in (1, 2)  &#47&#47 TODO generalize

        grad_inputs_schema_with_prompts = (
            requested_backends[0].args_schema * len(grads)<a id="change">,
            requested_backends[0].kwargs_schema</a>,
        )  &#47&#47 TODO generalize

        &#47&#47 Serialize the overall grad_input and respond</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/bigscience-workshop/distributed-bloom/commit/6095f58681bde14962c5023b75a2219c525cbe4c#diff-65ada620fb21ff82c2b88abea8bc2cf7fdd27241f03553f7e7d076339dbaab55L133' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 59020212</div><div id='project'> Project Name: bigscience-workshop/distributed-bloom</div><div id='commit'> Commit Name: 6095f58681bde14962c5023b75a2219c525cbe4c</div><div id='time'> Time: 2022-08-12</div><div id='author'> Author: dmitrybaranchuk@gmail.com</div><div id='file'> File Name: src/server/handler.py</div><div id='m_class'> M Class Name: TransformerConnectionHandler</div><div id='n_method'> N Class Name: TransformerConnectionHandler</div><div id='m_method'> M Method Name: rpc_backward(3)</div><div id='n_method'> N Method Name: rpc_backward(3)</div><div id='m_parent_class'> M Parent Class: ConnectionHandler</div><div id='n_parent_class'> N Parent Class: ConnectionHandler</div><div id='m_file'> M File Name: src/server/handler.py</div><div id='n_file'> N File Name: src/server/handler.py</div><div id='m_start'> M Start Line: 142</div><div id='m_end'> M End Line: 168</div><div id='n_start'> N Start Line: 133</div><div id='n_end'> N End Line: 145</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        hidden_states = [tensor.to(requested_backends[0].dtype) for tensor in hidden_states]

        &#47&#47 Run a chain of requested backends
        <a id="change">for backend</a> in requested_backends<a id="change">:
            </a><a id="change">assert </a>isinstance(hidden_states, (list, tuple))
            <a id="change">assert </a>(
                len(hidden_states) == 1 and hidden_states[0].ndim == 3
            ), f"inputs to {type(backend)} must be a list with a single 3d tensor of hidden states"
            hidden_states<a id="change"> = </a><a id="change">await </a><a id="change">backend.forward_pool.submit_task(</a>*<a id="change">hidden_states)</a>

        &#47&#47 Serialize the overall output
        assert len(hidden_states) == 1 and hidden_states[0].ndim == 3
        serialized_output = [</code></pre><h3>After Change</h3><pre><code class='java'>
        requested_uids = self._check_uids(uid_str)
        requested_backends = tuple(self.module_backends[uid] for uid in requested_uids)

        hidden_states = <a id="change">await </a>_rpc_forward(flat_inputs, requested_backends)
        assert isinstance(hidden_states, torch.Tensor) and hidden_states.ndim == 3

        &#47&#47 Serialize the overall output
        serialized_output = [
            serialize_torch_tensor(result.to(proto.dtype), proto.compression, allow_inplace=True)
            for result, proto in zip((hidden_states<a id="change"></a>,), nested_flatten(requested_backends[-1].outputs_schema))
        ]

        &#47&#47 Split the serialized_output for streaming and respond to client</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/bigscience-workshop/distributed-bloom/commit/6095f58681bde14962c5023b75a2219c525cbe4c#diff-65ada620fb21ff82c2b88abea8bc2cf7fdd27241f03553f7e7d076339dbaab55L107' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 59020213</div><div id='project'> Project Name: bigscience-workshop/distributed-bloom</div><div id='commit'> Commit Name: 6095f58681bde14962c5023b75a2219c525cbe4c</div><div id='time'> Time: 2022-08-12</div><div id='author'> Author: dmitrybaranchuk@gmail.com</div><div id='file'> File Name: src/server/handler.py</div><div id='m_class'> M Class Name: TransformerConnectionHandler</div><div id='n_method'> N Class Name: TransformerConnectionHandler</div><div id='m_method'> M Method Name: rpc_forward_stream(3)</div><div id='n_method'> N Method Name: rpc_forward_stream(3)</div><div id='m_parent_class'> M Parent Class: ConnectionHandler</div><div id='n_parent_class'> N Parent Class: ConnectionHandler</div><div id='m_file'> M File Name: src/server/handler.py</div><div id='n_file'> N File Name: src/server/handler.py</div><div id='m_start'> M Start Line: 111</div><div id='m_end'> M End Line: 130</div><div id='n_start'> N Start Line: 111</div><div id='n_end'> N End Line: 121</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Run a forward chain to collect intermediate inputs
        &#47&#47 Note that we do not forward for the last module since we do not need its outputs
        inter_inputs = [inputs]
        <a id="change">for backend</a> in requested_backends[:-1]<a id="change">:
            </a><a id="change">assert </a>inputs.ndim == 3, f"inputs to {type(backend)} must be a single 3d tensor of hidden states"
            inputs = <a id="change">await </a><a id="change">backend.forward_pool.submit_task(</a>inputs<a id="change">)</a>
            <a id="change">assert </a>isinstance(inputs, (list, tuple)) and len(inputs) == 1
            inputs<a id="change"> = </a>inputs[0]
            inter_inputs.append(inputs)

        &#47&#47 Run a backward chain for requested backends</code></pre><h3>After Change</h3><pre><code class='java'>
        requested_uids = self._check_uids(uids_header)
        requested_backends = tuple(self.module_backends[uid] for uid in requested_uids)

        grads = <a id="change">await </a>_rpc_backward(*flat_tensors, requested_backends=requested_backends)

        &#47&#47 Modify grad_inputs_schema to support grad_prompts
        assert len(requested_backends[0].args_schema) == 1 and len(grads) in (1, 2)  &#47&#47 TODO generalize
        grad_inputs_schema_with_prompts = (
            requested_backends[0].args_schema * len(grads)<a id="change">,
            requested_backends[0].kwargs_schema</a>,
        )  &#47&#47 TODO generalize

        &#47&#47 Serialize the overall grad_inputs</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/bigscience-workshop/distributed-bloom/commit/6095f58681bde14962c5023b75a2219c525cbe4c#diff-65ada620fb21ff82c2b88abea8bc2cf7fdd27241f03553f7e7d076339dbaab55L175' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 59020214</div><div id='project'> Project Name: bigscience-workshop/distributed-bloom</div><div id='commit'> Commit Name: 6095f58681bde14962c5023b75a2219c525cbe4c</div><div id='time'> Time: 2022-08-12</div><div id='author'> Author: dmitrybaranchuk@gmail.com</div><div id='file'> File Name: src/server/handler.py</div><div id='m_class'> M Class Name: TransformerConnectionHandler</div><div id='n_method'> N Class Name: TransformerConnectionHandler</div><div id='m_method'> M Method Name: rpc_backward_stream(3)</div><div id='n_method'> N Method Name: rpc_backward_stream(3)</div><div id='m_parent_class'> M Parent Class: ConnectionHandler</div><div id='n_parent_class'> N Parent Class: ConnectionHandler</div><div id='m_file'> M File Name: src/server/handler.py</div><div id='n_file'> N File Name: src/server/handler.py</div><div id='m_start'> M Start Line: 179</div><div id='m_end'> M End Line: 208</div><div id='n_start'> N Start Line: 159</div><div id='n_end'> N End Line: 175</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Run a forward chain to collect intermediate inputs
        &#47&#47 Note that we do not forward for the last module since we do not need its output
        inter_inputs = [inputs]
        <a id="change">for backend</a> in requested_backends[:-1]<a id="change">:
            assert </a>inputs.ndim == 3, f"inputs to {type(backend)} must be a single 3d tensor of hidden states"
            inputs<a id="change"> = </a><a id="change">await </a><a id="change">backend.forward_pool.submit_task(</a>inputs<a id="change">)</a>
            <a id="change">assert </a>isinstance(inputs, (list, tuple)) and len(inputs) == 1
            inputs = inputs[0]
            inter_inputs.append(inputs)
</code></pre><h3>After Change</h3><pre><code class='java'>
        requested_uids = self._check_uids(request.uid)
        requested_backends = tuple(self.module_backends[uid] for uid in requested_uids)

        grads = <a id="change">await </a>_rpc_backward(*flat_tensors, requested_backends=requested_backends)

        &#47&#47 Modify grad_inputs_schema to support grad_prompts
        assert len(requested_backends[0].args_schema) == 1 and len(grads) in (1, 2)  &#47&#47 TODO generalize

        grad_inputs_schema_with_prompts = (
            requested_backends[0].args_schema * len(grads)<a id="change">,
            requested_backends[0].kwargs_schema</a>,
        )  &#47&#47 TODO generalize

        &#47&#47 Serialize the overall grad_input and respond</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/bigscience-workshop/distributed-bloom/commit/6095f58681bde14962c5023b75a2219c525cbe4c#diff-65ada620fb21ff82c2b88abea8bc2cf7fdd27241f03553f7e7d076339dbaab55L140' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 59020208</div><div id='project'> Project Name: bigscience-workshop/distributed-bloom</div><div id='commit'> Commit Name: 6095f58681bde14962c5023b75a2219c525cbe4c</div><div id='time'> Time: 2022-08-12</div><div id='author'> Author: dmitrybaranchuk@gmail.com</div><div id='file'> File Name: src/server/handler.py</div><div id='m_class'> M Class Name: TransformerConnectionHandler</div><div id='n_method'> N Class Name: TransformerConnectionHandler</div><div id='m_method'> M Method Name: rpc_backward(3)</div><div id='n_method'> N Method Name: rpc_backward(3)</div><div id='m_parent_class'> M Parent Class: ConnectionHandler</div><div id='n_parent_class'> N Parent Class: ConnectionHandler</div><div id='m_file'> M File Name: src/server/handler.py</div><div id='n_file'> N File Name: src/server/handler.py</div><div id='m_start'> M Start Line: 142</div><div id='m_end'> M End Line: 168</div><div id='n_start'> N Start Line: 133</div><div id='n_end'> N End Line: 145</div><BR>