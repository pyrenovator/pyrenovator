<html><h3>Pattern ID :16862
</h3><img src='56375422.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    param_groups,
    states
):
    params<a id="change"> = list(</a>params<a id="change">)</a>
    for group_idx, group_mapping in enumerate(param_mapping):
        group = param_groups[group_idx]

        amsgrad = group[&quotamsgrad&quot]
        beta1, beta2 = group[&quotbetas&quot]

        for param_idx in group_mapping:
            p = params[param_idx]

            if p.gradient is None:
                continue
            grad = p.gradient

            p = p * (1 - group[&quotlr&quot] * group[&quotweight_decay&quot])
            state = states[param_idx]

            state[&quotstep&quot] += 1
            bias_correction1 = 1 - beta1**state[&quotstep&quot]
            bias_correction2 = 1 - beta2**state[&quotstep&quot]

            state[&quotexp_avg&quot] = state[&quotexp_avg&quot] * beta1 + (1 - beta1) * grad
            state[&quotexp_avg_sq&quot] = state[&quotexp_avg_sq&quot] * beta2 + (1 - beta2) * grad * grad

            if amsgrad:
                state[&quotmax_exp_avg_sq&quot] = torch.max(state[&quotmax_exp_avg_sq&quot], state[&quotexp_avg_sq&quot])
                denom = state[&quotmax_exp_avg_sq&quot] / math.sqrt(bias_correction2) + group[&quoteps&quot]
            else:
                denom = state[&quotexp_avg_sq&quot] / math.sqrt(bias_correction2) + group[&quoteps&quot]

            step_size = group[&quotlr&quot] / bias_correction1
            params[param_idx] = p - step_size * (state[&quotexp_avg&quot] / denom)

    <a id="change">return tuple(</a>params<a id="change">)</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
            step_size = group[&quotlr&quot] / bias_correction1
            p.update = step_size * (state[&quotexp_avg&quot] / denom)

    out<a id="change"> = tuple(</a><a id="change">p - p.update for p in params if hasattr(p, &quotupdate&quot))</a>
    for p in params:
        if hasattr(p, &quotupdate&quot):
            del p.update
    <a id="change">return </a>out
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 9</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/leopard-ai/betty/commit/6d731ca13bf887bb15eefc85c120c681d0d9a1d0#diff-14cb4988c9063d54b880c71701a8d37ef78cfbe2d6c604b241ce7ccf55dd6d48L11' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 56375422</div><div id='project'> Project Name: leopard-ai/betty</div><div id='commit'> Commit Name: 6d731ca13bf887bb15eefc85c120c681d0d9a1d0</div><div id='time'> Time: 2022-02-17</div><div id='author'> Author: sangkeuc@andrew.cmu.edu</div><div id='file'> File Name: betty/optim/adamw.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: fadamw(4)</div><div id='n_method'> N Method Name: fadamw(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: betty/optim/adamw.py</div><div id='n_file'> N File Name: betty/optim/adamw.py</div><div id='m_start'> M Start Line: 11</div><div id='m_end'> M End Line: 44</div><div id='n_start'> N Start Line: 24</div><div id='n_end'> N End Line: 47</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                args.append(self.storage_state)
                code = self.loader.code_per_stage[stage]
                result = code(*args)
                args<a id="change"> = list(</a>result<a id="change">)</a>
                if first_stage:
                    first_stage = False
                    self.memory_context.end_batch(b_ix)
        <a id="change">return tuple(</a>x<a id="change">[:len(batch_indices)] for x in args)</a>

    def __next__(self):
        result = self.output_queue.get()
        if result is None:</code></pre><h3>After Change</h3><pre><code class='java'>
                    args[f&quotresult_{node_id}&quot] = result
                pass

            result<a id="change"> = tuple(</a><a id="change">args[f&quotresult_{x}&quot] for x in outputs)</a>
            <a id="change">return </a>result

    def __next__(self):
        result = self.output_queue.get()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/libffcv/ffcv/commit/f8baf227d1243d4207c082d8ea11b89b5a73da32#diff-d1c1d17dd7f942f4223cf450209bcd6a3979e50b67e9c881848b09da0845f7ffL110' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 56375424</div><div id='project'> Project Name: libffcv/ffcv</div><div id='commit'> Commit Name: f8baf227d1243d4207c082d8ea11b89b5a73da32</div><div id='time'> Time: 2022-02-08</div><div id='author'> Author: leclerc@mit.edu</div><div id='file'> File Name: ffcv/loader/epoch_iterator.py</div><div id='m_class'> M Class Name: EpochIterator</div><div id='n_method'> N Class Name: EpochIterator</div><div id='m_method'> M Method Name: run_pipeline(5)</div><div id='n_method'> N Method Name: run_pipeline(5)</div><div id='m_parent_class'> M Parent Class: Thread</div><div id='n_parent_class'> N Parent Class: Thread</div><div id='m_file'> M File Name: ffcv/loader/epoch_iterator.py</div><div id='n_file'> N File Name: ffcv/loader/epoch_iterator.py</div><div id='m_start'> M Start Line: 113</div><div id='m_end'> M End Line: 142</div><div id='n_start'> N Start Line: 111</div><div id='n_end'> N End Line: 140</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    param_groups,
    states
):
    params<a id="change"> = list(</a>params<a id="change">)</a>
    for group_idx, group_mapping in enumerate(param_mapping):
        group = param_groups[group_idx]

        amsgrad = group[&quotamsgrad&quot]
        beta1, beta2 = group[&quotbetas&quot]
        weight_decay = group[&quotweight_decay&quot]

        for param_idx in group_mapping:
            p = params[param_idx]

            if p.gradient is None:
                continue
            grad = p.gradient
            print(&quotadam grad:&quot, grad)

            state = states[param_idx]

            state[&quotstep&quot] += 1
            bias_correction1 = 1 - beta1**state[&quotstep&quot]
            bias_correction2 = 1 - beta2**state[&quotstep&quot]

            if weight_decay != 0:
                grad = grad + (weight_decay * p)

            state[&quotexp_avg&quot] = state[&quotexp_avg&quot] * beta1 + (1 - beta1) * grad
            state[&quotexp_avg_sq&quot] =  state[&quotexp_avg_sq&quot] * beta2 + (1 - beta2) * grad * grad

            if amsgrad:
                state[&quotmax_exp_avg_sq&quot] = torch.max(state[&quotmax_exp_avg_sq&quot], state[&quotexp_avg_sq&quot])
                denom = state[&quotmax_exp_avg_sq&quot].sqrt() / math.sqrt(bias_correction2) + group[&quoteps&quot]
            else:
                denom = state[&quotexp_avg_sq&quot].sqrt() / math.sqrt(bias_correction2) + group[&quoteps&quot]

            step_size = group[&quotlr&quot] / bias_correction1
            params[param_idx] = p - step_size * (state[&quotexp_avg&quot] / denom)

    <a id="change">return tuple(</a>params<a id="change">)</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
            step_size = group[&quotlr&quot] / bias_correction1
            p.update = step_size * (state[&quotexp_avg&quot] / denom)

    out<a id="change"> = tuple(</a><a id="change">p - p.update for p in params if hasattr(p, &quotupdate&quot))</a>
    for p in params:
        if hasattr(p, &quotupdate&quot):
            del p.update
    <a id="change">return </a>out
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leopard-ai/betty/commit/6d731ca13bf887bb15eefc85c120c681d0d9a1d0#diff-02175463e720b8e991266c8411306e432bbc787fd5644b2cd38a122de76fa8cbL6' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 56375425</div><div id='project'> Project Name: leopard-ai/betty</div><div id='commit'> Commit Name: 6d731ca13bf887bb15eefc85c120c681d0d9a1d0</div><div id='time'> Time: 2022-02-17</div><div id='author'> Author: sangkeuc@andrew.cmu.edu</div><div id='file'> File Name: betty/optim/adam.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: fadam(4)</div><div id='n_method'> N Method Name: fadam(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: betty/optim/adam.py</div><div id='n_file'> N File Name: betty/optim/adam.py</div><div id='m_start'> M Start Line: 12</div><div id='m_end'> M End Line: 49</div><div id='n_start'> N Start Line: 20</div><div id='n_end'> N End Line: 51</div><BR>