<html><h3>Pattern ID :27571
</h3><img src='81927906.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        get_attn = lambda: LSHSelfAttention(dim, heads, bucket_size, n_hashes, causal = causal, dropout = lsh_dropout, post_attn_dropout = post_attn_dropout, attn_chunks = attn_chunks, allow_duplicate_attention = lsh_allow_duplicate_attention, attend_across_buckets = lsh_attend_across_buckets, random_rotations_per_head = random_rotations_per_head, num_mem_kv = num_mem_kv, use_full_attn = use_full_attn, full_attn_thres = full_attn_thres, one_value_head = one_value_head, n_local_attn_heads = n_local_attn_heads)
        get_ff = lambda: Chunk(ff_chunks, FeedForward(dim, dropout = ff_dropout, activation = ff_activation, mult = ff_mult, glu = ff_glu), along_dim = -2)

        <a id="change">if weight_tie</a>:
            get_attn<a id="change"> = </a>cache_fn(get_attn)
            get_ff<a id="change"> = </a>cache_fn(get_ff)

        blocks = []
</code></pre><h3>After Change</h3><pre><code class='java'>
        get_ff = lambda: Chunk(ff_chunks, FeedForward(dim, dropout = ff_dropout, activation = ff_activation, mult = ff_mult, glu = ff_glu), along_dim = -2)
        get_pkm = lambda: PKM(dim, num_keys = pkm_num_keys)

        if <a id="change">weight_tie</a>:
            get_attn<a id="change">, get_ff, get_pkm = </a><a id="change">map(</a>cache_fn, (get_attn, get_ff, get_pkm)<a id="change">)</a>

        blocks = []

        norm_type = ScaleNorm if use_scale_norm else nn.LayerNorm</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/reformer-pytorch/commit/fbae34221f4e2c2d777551a5e92b8bba5ae2385c#diff-7e740488ff2ce47996cfdd5356981ed3e6ac2bb38739a71b400275ac91d1c3bfL751' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 81927906</div><div id='project'> Project Name: lucidrains/reformer-pytorch</div><div id='commit'> Commit Name: fbae34221f4e2c2d777551a5e92b8bba5ae2385c</div><div id='time'> Time: 2020-06-06</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: reformer_pytorch/reformer_pytorch.py</div><div id='m_class'> M Class Name: Reformer</div><div id='n_method'> N Class Name: Reformer</div><div id='m_method'> M Method Name: __init__(32)</div><div id='n_method'> N Method Name: __init__(30)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: reformer_pytorch/reformer_pytorch.py</div><div id='n_file'> N File Name: reformer_pytorch/reformer_pytorch.py</div><div id='m_start'> M Start Line: 757</div><div id='m_end'> M End Line: 772</div><div id='n_start'> N Start Line: 751</div><div id='n_end'> N End Line: 789</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        get_attn = lambda: SinkhornSelfAttention(dim, causal = causal, heads = heads, buckets = buckets, kv_buckets = kv_buckets, non_permutative = non_permutative, sinkhorn_iter = sinkhorn_iter, n_sortcut = n_sortcut, temperature = temperature, attn_dropout = attn_dropout, dropout = attn_layer_dropout, attn_sort_net = attn_sort_net)
        get_ff = lambda: FeedForward(dim, dropout = ff_dropout, glu = ff_glu)

        <a id="change">if weight_tie</a>:
            get_attn<a id="change"> = </a>cache_fn(get_attn)
            get_ff<a id="change"> = </a>cache_fn(get_ff)

        for _ in range(depth):
            layers.append(nn.ModuleList([</code></pre><h3>After Change</h3><pre><code class='java'>
        get_attn_context = lambda: SinkhornSelfAttention(dim, context_only = True, heads = heads, buckets = buckets, kv_buckets = kv_buckets, non_permutative = non_permutative, sinkhorn_iter = sinkhorn_iter, n_sortcut = n_sortcut, temperature = temperature, attn_dropout = attn_dropout, dropout = attn_layer_dropout, attn_sort_net = attn_sort_net)
        get_ff_context = lambda: FeedForward(dim, dropout = ff_dropout, glu = ff_glu)

        if <a id="change">weight_tie</a>:
            get_attn<a id="change">, get_attn_context, get_ff, get_ff_context = </a><a id="change">map(</a>cache_fn, (get_attn, get_attn_context, get_ff, get_ff_context)<a id="change">)</a>

        for _ in range(depth):
            layers.append(nn.ModuleList([
                PreNorm(nn.LayerNorm, dim, get_attn()),</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/sinkhorn-transformer/commit/c2662a20cb783efd3351936cfabc83131060a2a6#diff-2ed19ae7feb552dc52b4c7a7c89a078c0fd371c922789f894af09ff01f35eccbL543' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 81927874</div><div id='project'> Project Name: lucidrains/sinkhorn-transformer</div><div id='commit'> Commit Name: c2662a20cb783efd3351936cfabc83131060a2a6</div><div id='time'> Time: 2020-04-10</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='m_class'> M Class Name: SinkhornTransformer</div><div id='n_method'> N Class Name: SinkhornTransformer</div><div id='m_method'> M Method Name: __init__(20)</div><div id='n_method'> N Method Name: __init__(19)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='n_file'> N File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='m_start'> M Start Line: 547</div><div id='m_end'> M End Line: 554</div><div id='n_start'> N Start Line: 546</div><div id='n_end'> N End Line: 576</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                shuffle_after_epoch=random_shuffle,
                labels=labels,
            )
        elif <a id="change">encode_indexes_into_labels</a>:
            labels = sorted(Path(entry.name) for entry in os.scandir(data_path) if entry.is_dir())

            data = [
                (data_path / label / file, label_idx)
                for label_idx, label in enumerate(labels)
                for file in sorted(os.listdir(data_path / label))
            ]

            files<a id="change"> = </a>[]
            labels<a id="change"> = </a>[]
            &#47&#47 for debugging
            true_labels = []
</code></pre><h3>After Change</h3><pre><code class='java'>
                for label_idx, label in enumerate(labels)
                for file in sorted(os.listdir(data_path / label))
            ]
            files<a id="change">, labels = </a><a id="change">map(</a>list, zip(*data)<a id="change">)</a>

        if data_fraction &gt; 0:
            assert data_fraction &lt; 1, "Only use data_fraction for values smaller than 1."
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/vturrisi/contrastive-learning/commit/5a1c1e3e03b99bfedfc219fb6eed2b8661264aa3#diff-81ba21dc69a962eb663dea39a286c39f62d9fa16b21c6471b25eb7a7d42ad291L502' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 81927905</div><div id='project'> Project Name: vturrisi/contrastive-learning</div><div id='commit'> Commit Name: 5a1c1e3e03b99bfedfc219fb6eed2b8661264aa3</div><div id='time'> Time: 2022-04-12</div><div id='author'> Author: vt.turrisi@gmail.com</div><div id='file'> File Name: solo/utils/dali_dataloader.py</div><div id='m_class'> M Class Name: PretrainPipeline</div><div id='n_method'> N Class Name: PretrainPipeline</div><div id='m_method'> M Method Name: __init__(15)</div><div id='n_method'> N Method Name: __init__(14)</div><div id='m_parent_class'> M Parent Class: Pipeline</div><div id='n_parent_class'> N Parent Class: Pipeline</div><div id='m_file'> M File Name: solo/utils/dali_dataloader.py</div><div id='n_file'> N File Name: solo/utils/dali_dataloader.py</div><div id='m_start'> M Start Line: 554</div><div id='m_end'> M End Line: 598</div><div id='n_start'> N Start Line: 540</div><div id='n_end'> N End Line: 633</div><BR>