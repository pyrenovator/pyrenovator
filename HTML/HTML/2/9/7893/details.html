<html><h3>Pattern ID :7893
</h3><img src='28089996.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    elif lang == "de":
        sentence = "Dies ist ein ungesehener Satz."
    text = tf.string_to_tensor(sentence).long().squeeze(0).to(device)
    spec = <a id="change">net.inference(text=text, speaker_embeddings=reference_speaker_embedding_for_plot).transpose(0</a>, <a id="change">1</a><a id="change">)</a>.to("cpu").numpy()
    if not os.path.exists(os.path.join(save_dir, "spec")):
        os.makedirs(os.path.join(save_dir, "spec"))
    fig, ax = plt.subplots(nrows=1, ncols=1)</code></pre><h3>After Change</h3><pre><code class='java'>
    elif lang == "de":
        sentence = "Dies ist ein ungesehener Satz."
    phoneme_vector = tf.string_to_tensor(sentence).long().squeeze(0).to(device)
    spec<a id="change">, durations, *_</a> = net.inference(text=phoneme_vector, speaker_embeddings=reference_speaker_embedding_for_plot, return_duration_pitch_energy=True)
    spec = spec.transpose(0, 1).to("cpu").numpy()
    duration_splits<a id="change">, label_positions = cumsum_durations(durations</a><a id="change">.cpu().numpy())</a>
    if not os.path.exists(os.path.join(save_dir, "spec")):
        os.makedirs(os.path.join(save_dir, "spec"))
    fig, ax = plt.subplots(nrows=1, ncols=1)
    lbd.specshow(spec,</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 5</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/4bbfcda47cf77f65ad0e557f8a06a174197d1bfc#diff-1988a4cedcd0412bf08be68eb840ca996e1dbe491b7519d2153870a127712118L21' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 28089996</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 4bbfcda47cf77f65ad0e557f8a06a174197d1bfc</div><div id='time'> Time: 2021-07-24</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: FastSpeech2/fastspeech2_train_loop.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: plot_progress_spec(6)</div><div id='n_method'> N Method Name: plot_progress_spec(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: FastSpeech2/fastspeech2_train_loop.py</div><div id='n_file'> N File Name: FastSpeech2/fastspeech2_train_loop.py</div><div id='m_start'> M Start Line: 27</div><div id='m_end'> M End Line: 27</div><div id='n_start'> N Start Line: 21</div><div id='n_end'> N End Line: 46</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def forward(self, text, view=False):
        with torch.no_grad():
            phones = self.text2phone.string_to_tensor(text).squeeze(0).long().to(torch.device(self.device))
            mel = <a id="change">self.phone2mel(phones, speaker_embedding=self.speaker_embedding).transpose(0</a>, <a id="change">1</a><a id="change">)</a>
            wave = self.mel2wav(mel.unsqueeze(0)).squeeze(0).squeeze(0)
        if view:
            import matplotlib.pyplot as plt
            import librosa.display as lbd</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward(self, text, view=False):
        with torch.no_grad():
            phones = self.text2phone.string_to_tensor(text).squeeze(0).long().to(torch.device(self.device))
            mel<a id="change">, durations, pitch, energy</a> = self.phone2mel(phones, speaker_embedding=self.speaker_embedding, return_duration_pitch_energy=True)
            mel = mel.transpose(0, 1)
            wave = self.mel2wav(mel.unsqueeze(0)).squeeze(0).squeeze(0)
        if view:
            import matplotlib.pyplot as plt
            import librosa.display as lbd
            from Utility.utils import cumsum_durations
            fig, ax = plt.subplots(nrows=2, ncols=1)
            ax[0].plot(wave.cpu().numpy())
            lbd.specshow(mel.cpu().numpy(),
                         ax=ax[1],
                         sr=16000,
                         cmap=&quotGnBu&quot,
                         y_axis=&quotmel&quot,
                         x_axis=None,
                         hop_length=256)
            ax[0].yaxis.set_visible(False)
            ax[1].yaxis.set_visible(False)
            duration_splits<a id="change">, label_positions = cumsum_durations(</a><a id="change">durations.cpu().numpy())</a>
            ax[1].set_xticks(duration_splits, minor=True)
            ax[1].xaxis.grid(True, which=&quotminor&quot)
            ax[1].set_xticks(label_positions, minor=False)
            ax[1].set_xticklabels(self.text2phone.get_phone_string(text))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/4bbfcda47cf77f65ad0e557f8a06a174197d1bfc#diff-7d3b87a93b0bc1f791ef878d1df587515e570f34cd1a868873e0fd6e7f941af5L31' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 28089992</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 4bbfcda47cf77f65ad0e557f8a06a174197d1bfc</div><div id='time'> Time: 2021-07-24</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: InferenceInterfaces/Nancy_FastSpeechInference.py</div><div id='m_class'> M Class Name: Nancy_FastSpeechInference</div><div id='n_method'> N Class Name: Nancy_FastSpeechInference</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: InferenceInterfaces/Nancy_FastSpeechInference.py</div><div id='n_file'> N File Name: InferenceInterfaces/Nancy_FastSpeechInference.py</div><div id='m_start'> M Start Line: 33</div><div id='m_end'> M End Line: 43</div><div id='n_start'> N Start Line: 33</div><div id='n_end'> N End Line: 57</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def forward(self, text, view=False):
        with torch.no_grad():
            phones = self.text2phone.string_to_tensor(text).squeeze(0).long().to(torch.device(self.device))
            mel = <a id="change">self.phone2mel(phones, speaker_embedding=self.speaker_embedding).transpose(0</a>, <a id="change">1</a><a id="change">)</a>
            wave = self.mel2wav(mel.unsqueeze(0)).squeeze(0).squeeze(0)
        if view:
            import matplotlib.pyplot as plt
            import librosa.display as lbd</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward(self, text, view=False):
        with torch.no_grad():
            phones = self.text2phone.string_to_tensor(text).squeeze(0).long().to(torch.device(self.device))
            mel<a id="change">, durations, pitch, energy</a> = self.phone2mel(phones, speaker_embedding=self.speaker_embedding, return_duration_pitch_energy=True)
            mel = mel.transpose(0, 1)
            wave = self.mel2wav(mel.unsqueeze(0)).squeeze(0).squeeze(0)
        if view:
            import matplotlib.pyplot as plt
            import librosa.display as lbd
            from Utility.utils import cumsum_durations
            fig, ax = plt.subplots(nrows=2, ncols=1)
            ax[0].plot(wave.cpu().numpy())
            lbd.specshow(mel.cpu().numpy(),
                         ax=ax[1],
                         sr=16000,
                         cmap=&quotGnBu&quot,
                         y_axis=&quotmel&quot,
                         x_axis=None,
                         hop_length=256)
            ax[0].yaxis.set_visible(False)
            ax[1].yaxis.set_visible(False)
            duration_splits<a id="change">, label_positions = cumsum_durations(</a><a id="change">durations.cpu().numpy())</a>
            ax[1].set_xticks(duration_splits, minor=True)
            ax[1].xaxis.grid(True, which=&quotminor&quot)
            ax[1].set_xticks(label_positions, minor=False)
            ax[1].set_xticklabels(self.text2phone.get_phone_string(text))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/4bbfcda47cf77f65ad0e557f8a06a174197d1bfc#diff-74abc2e7bebe273ec5fa359ca3a9ef79f13947e109a095575f6b818f4e7f438cL32' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 28089993</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 4bbfcda47cf77f65ad0e557f8a06a174197d1bfc</div><div id='time'> Time: 2021-07-24</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: InferenceInterfaces/LibriTTS_FastSpeechInference.py</div><div id='m_class'> M Class Name: LibriTTS_FastSpeechInference</div><div id='n_method'> N Class Name: LibriTTS_FastSpeechInference</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: InferenceInterfaces/LibriTTS_FastSpeechInference.py</div><div id='n_file'> N File Name: InferenceInterfaces/LibriTTS_FastSpeechInference.py</div><div id='m_start'> M Start Line: 34</div><div id='m_end'> M End Line: 43</div><div id='n_start'> N Start Line: 34</div><div id='n_end'> N End Line: 58</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def forward(self, text, view=False):
        with torch.no_grad():
            phones = self.text2phone.string_to_tensor(text).squeeze(0).long().to(torch.device(self.device))
            mel = <a id="change">self.phone2mel(phones, speaker_embedding=self.speaker_embedding).transpose(0</a>, <a id="change">1</a><a id="change">)</a>
            wave = self.mel2wav(mel.unsqueeze(0)).squeeze(0).squeeze(0)
        if view:
            import matplotlib.pyplot as plt
            import librosa.display as lbd</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward(self, text, view=False):
        with torch.no_grad():
            phones = self.text2phone.string_to_tensor(text).squeeze(0).long().to(torch.device(self.device))
            mel<a id="change">, durations, pitch, energy</a> = self.phone2mel(phones, speaker_embedding=self.speaker_embedding, return_duration_pitch_energy=True)
            mel = mel.transpose(0, 1)
            wave = self.mel2wav(mel.unsqueeze(0)).squeeze(0).squeeze(0)
        if view:
            import matplotlib.pyplot as plt
            import librosa.display as lbd
            from Utility.utils import cumsum_durations
            fig, ax = plt.subplots(nrows=2, ncols=1)
            ax[0].plot(wave.cpu().numpy())
            lbd.specshow(mel.cpu().numpy(),
                         ax=ax[1],
                         sr=16000,
                         cmap=&quotGnBu&quot,
                         y_axis=&quotmel&quot,
                         x_axis=None,
                         hop_length=256)
            ax[0].yaxis.set_visible(False)
            ax[1].yaxis.set_visible(False)
            duration_splits<a id="change">, label_positions = cumsum_durations(</a><a id="change">durations.cpu().numpy())</a>
            ax[1].set_xticks(duration_splits, minor=True)
            ax[1].xaxis.grid(True, which=&quotminor&quot)
            ax[1].set_xticks(label_positions, minor=False)
            ax[1].set_xticklabels(self.text2phone.get_phone_string(text))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/4bbfcda47cf77f65ad0e557f8a06a174197d1bfc#diff-4b2f26f3d3287b567ffaec37b852ede237af7e76f93c389c33396f41ee6a1cf0L30' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 28089995</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 4bbfcda47cf77f65ad0e557f8a06a174197d1bfc</div><div id='time'> Time: 2021-07-24</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: InferenceInterfaces/Thorsten_FastSpeechInference.py</div><div id='m_class'> M Class Name: Thorsten_FastSpeechInference</div><div id='n_method'> N Class Name: Thorsten_FastSpeechInference</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: InferenceInterfaces/Thorsten_FastSpeechInference.py</div><div id='n_file'> N File Name: InferenceInterfaces/Thorsten_FastSpeechInference.py</div><div id='m_start'> M Start Line: 32</div><div id='m_end'> M End Line: 41</div><div id='n_start'> N Start Line: 32</div><div id='n_end'> N End Line: 56</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    elif lang == "de":
        sentence = "Dies ist ein ungesehener Satz."
    text = tf.string_to_tensor(sentence).long().squeeze(0).to(device)
    spec = <a id="change">net.inference(text=text, speaker_embeddings=reference_speaker_embedding_for_plot).transpose(0</a>, <a id="change">1</a><a id="change">)</a>.to("cpu").numpy()
    if not os.path.exists(os.path.join(save_dir, "spec")):
        os.makedirs(os.path.join(save_dir, "spec"))
    fig, ax = plt.subplots(nrows=1, ncols=1)</code></pre><h3>After Change</h3><pre><code class='java'>
    elif lang == "de":
        sentence = "Dies ist ein ungesehener Satz."
    phoneme_vector = tf.string_to_tensor(sentence).long().squeeze(0).to(device)
    spec<a id="change">, durations, *_</a> = net.inference(text=phoneme_vector, speaker_embeddings=reference_speaker_embedding_for_plot, return_duration_pitch_energy=True)
    spec = spec.transpose(0, 1).to("cpu").numpy()
    duration_splits<a id="change">, label_positions = cumsum_durations(</a><a id="change">durations.cpu().numpy())</a>
    if not os.path.exists(os.path.join(save_dir, "spec")):
        os.makedirs(os.path.join(save_dir, "spec"))
    fig, ax = plt.subplots(nrows=1, ncols=1)
    lbd.specshow(spec,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/4bbfcda47cf77f65ad0e557f8a06a174197d1bfc#diff-1988a4cedcd0412bf08be68eb840ca996e1dbe491b7519d2153870a127712118L19' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 28089988</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 4bbfcda47cf77f65ad0e557f8a06a174197d1bfc</div><div id='time'> Time: 2021-07-24</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: FastSpeech2/fastspeech2_train_loop.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: plot_progress_spec(6)</div><div id='n_method'> N Method Name: plot_progress_spec(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: FastSpeech2/fastspeech2_train_loop.py</div><div id='n_file'> N File Name: FastSpeech2/fastspeech2_train_loop.py</div><div id='m_start'> M Start Line: 27</div><div id='m_end'> M End Line: 27</div><div id='n_start'> N Start Line: 21</div><div id='n_end'> N End Line: 46</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def forward(self, text, view=False):
        with torch.no_grad():
            phones = self.text2phone.string_to_tensor(text).squeeze(0).long().to(torch.device(self.device))
            mel = <a id="change">self.phone2mel(phones, speaker_embedding=self.speaker_embedding).transpose(0</a>, <a id="change">1</a><a id="change">)</a>
            wave = self.mel2wav(mel.unsqueeze(0)).squeeze(0).squeeze(0)
        if view:
            import matplotlib.pyplot as plt
            import librosa.display as lbd</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward(self, text, view=False):
        with torch.no_grad():
            phones = self.text2phone.string_to_tensor(text).squeeze(0).long().to(torch.device(self.device))
            mel<a id="change">, durations, pitch, energy</a> = self.phone2mel(phones, speaker_embedding=self.speaker_embedding, return_duration_pitch_energy=True)
            mel = mel.transpose(0, 1)
            wave = self.mel2wav(mel.unsqueeze(0)).squeeze(0).squeeze(0)
        if view:
            import matplotlib.pyplot as plt
            import librosa.display as lbd
            from Utility.utils import cumsum_durations
            fig, ax = plt.subplots(nrows=2, ncols=1)
            ax[0].plot(wave.cpu().numpy())
            lbd.specshow(mel.cpu().numpy(),
                         ax=ax[1],
                         sr=16000,
                         cmap=&quotGnBu&quot,
                         y_axis=&quotmel&quot,
                         x_axis=None,
                         hop_length=256)
            ax[0].yaxis.set_visible(False)
            ax[1].yaxis.set_visible(False)
            duration_splits<a id="change">, label_positions = cumsum_durations(</a><a id="change">durations.cpu().numpy())</a>
            ax[1].set_xticks(duration_splits, minor=True)
            ax[1].xaxis.grid(True, which=&quotminor&quot)
            ax[1].set_xticks(label_positions, minor=False)
            ax[1].set_xticklabels(self.text2phone.get_phone_string(text))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/4bbfcda47cf77f65ad0e557f8a06a174197d1bfc#diff-98c0a7845dd35e306bac5af5963f246159ffc54c9cc93cdac53373929d69df73L31' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 28089990</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 4bbfcda47cf77f65ad0e557f8a06a174197d1bfc</div><div id='time'> Time: 2021-07-24</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: InferenceInterfaces/LJSpeech_FastSpeechInference.py</div><div id='m_class'> M Class Name: LJSpeech_FastSpeechInference</div><div id='n_method'> N Class Name: LJSpeech_FastSpeechInference</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: InferenceInterfaces/LJSpeech_FastSpeechInference.py</div><div id='n_file'> N File Name: InferenceInterfaces/LJSpeech_FastSpeechInference.py</div><div id='m_start'> M Start Line: 33</div><div id='m_end'> M End Line: 43</div><div id='n_start'> N Start Line: 33</div><div id='n_end'> N End Line: 57</div><BR>