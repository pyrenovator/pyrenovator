<html><h3>Pattern ID :5574
</h3><img src='19702670.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 Note - this mismatch is by design! We return sharded_grad_output
        &#47&#47 to allow PyTorch shape matching to proceed correctly.
        <a id="change">return </a>(None<a id="change">, None, sharded_grad_output.view(-1)</a>)


class All2Allv_Req(Function):</code></pre><h3>After Change</h3><pre><code class='java'>
        input_splits = a2ai.output_splits
        output_splits = a2ai.input_splits

        <a id="change">if a2ai.codecs is not None</a>:
            sharded_grad_output<a id="change"> = </a><a id="change">a2ai.codecs.backward.encode(</a>sharded_grad_output<a id="change">)</a>

        sharded_grad_input = torch.empty(
            sum(output_splits),
            device=sharded_grad_output.device,
            dtype=sharded_grad_output.dtype,
        )

        with record_function("&#47&#47&#47&#47 alltoall_seq_embedding_bwd_single &#47&#47&#47&#47"):
            req = dist.all_to_all_single(
                output=sharded_grad_input,
                input=sharded_grad_output.view(-1),
                output_split_sizes=output_splits,
                input_split_sizes=input_splits,
                group=pg,
                async_op=True,
            )
        myreq.req = req
        myreq.tensor = sharded_grad_input

        <a id="change">return </a>(<a id="change">None</a><a id="change">, None, myreq.dummy_tensor</a>)


class All2Allv_Req(Function):</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 6</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/fdb901e2becf3de3339370cdae3333956f644fa0#diff-233b62fdfbb8e887bafc580edd784c08250a8cc36a3b206dfb03524e49e098dbL793' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 19702670</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: fdb901e2becf3de3339370cdae3333956f644fa0</div><div id='time'> Time: 2022-07-27</div><div id='author'> Author: yingliufb@fb.com</div><div id='file'> File Name: torchrec/distributed/comm_ops.py</div><div id='m_class'> M Class Name: All2All_Seq_Req_Wait</div><div id='n_method'> N Class Name: All2All_Seq_Req_Wait</div><div id='m_method'> M Method Name: backward(2)</div><div id='n_method'> N Method Name: backward(2)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: torchrec/distributed/comm_ops.py</div><div id='n_file'> N File Name: torchrec/distributed/comm_ops.py</div><div id='m_start'> M Start Line: 793</div><div id='m_end'> M End Line: 793</div><div id='n_start'> N Start Line: 836</div><div id='n_end'> N End Line: 863</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 Note - this mismatch is by design! We return sharded_grad_output
        &#47&#47 to allow PyTorch shape matching to proceed correctly.
        <a id="change">return </a>(None<a id="change">, None, sharded_grad_output.view(-1)</a>)


class All2Allv_Req(Function):</code></pre><h3>After Change</h3><pre><code class='java'>
        input_splits = a2ai.output_splits
        output_splits = a2ai.input_splits

        <a id="change">if a2ai.codecs is not None</a>:
            sharded_grad_output<a id="change"> = </a><a id="change">a2ai.codecs.backward.encode(</a>sharded_grad_output<a id="change">)</a>

        sharded_grad_input = torch.empty(
            sum(output_splits),
            device=sharded_grad_output.device,
            dtype=sharded_grad_output.dtype,
        )

        with record_function("&#47&#47&#47&#47 alltoall_seq_embedding_bwd_single &#47&#47&#47&#47"):
            req = dist.all_to_all_single(
                output=sharded_grad_input,
                input=sharded_grad_output.view(-1),
                output_split_sizes=output_splits,
                input_split_sizes=input_splits,
                group=pg,
                async_op=True,
            )
        myreq.req = req
        myreq.tensor = sharded_grad_input

        <a id="change">return </a>(None<a id="change">, None, myreq.dummy_tensor</a>)


class All2Allv_Req(Function):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/fdb901e2becf3de3339370cdae3333956f644fa0#diff-233b62fdfbb8e887bafc580edd784c08250a8cc36a3b206dfb03524e49e098dbL768' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 19702671</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: fdb901e2becf3de3339370cdae3333956f644fa0</div><div id='time'> Time: 2022-07-27</div><div id='author'> Author: yingliufb@fb.com</div><div id='file'> File Name: torchrec/distributed/comm_ops.py</div><div id='m_class'> M Class Name: All2All_Seq_Req_Wait</div><div id='n_method'> N Class Name: All2All_Seq_Req_Wait</div><div id='m_method'> M Method Name: backward(2)</div><div id='n_method'> N Method Name: backward(2)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: torchrec/distributed/comm_ops.py</div><div id='n_file'> N File Name: torchrec/distributed/comm_ops.py</div><div id='m_start'> M Start Line: 793</div><div id='m_end'> M End Line: 793</div><div id='n_start'> N Start Line: 836</div><div id='n_end'> N End Line: 863</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            )
        myreq.req = req
        myreq.tensor = grad_inputs
        <a id="change">return </a>(None<a id="change">, None, grad_output</a>)
</code></pre><h3>After Change</h3><pre><code class='java'>
        myreq = ctx.myreq
        rsi = myreq.rsi

        <a id="change">if rsi.codecs is not None</a>:
            grad_output<a id="change"> = </a><a id="change">rsi.codecs.backward.encode(</a>grad_output<a id="change">)</a>
        grad_inputs = grad_output.new_empty(rsi.input_sizes)

        with record_function("&#47&#47&#47&#47 reduce_scatter_base_bw (all_gather) &#47&#47&#47&#47"):
            req = dist._all_gather_base(
                grad_inputs,
                grad_output.contiguous(),
                group=ctx.pg,
                async_op=True,
            )
        myreq.req = req
        myreq.tensor = grad_inputs
        <a id="change">return </a>(None<a id="change">, None, myreq.dummy_tensor</a>)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/fdb901e2becf3de3339370cdae3333956f644fa0#diff-233b62fdfbb8e887bafc580edd784c08250a8cc36a3b206dfb03524e49e098dbL1035' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 19702672</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: fdb901e2becf3de3339370cdae3333956f644fa0</div><div id='time'> Time: 2022-07-27</div><div id='author'> Author: yingliufb@fb.com</div><div id='file'> File Name: torchrec/distributed/comm_ops.py</div><div id='m_class'> M Class Name: ReduceScatterBase_Wait</div><div id='n_method'> N Class Name: ReduceScatterBase_Wait</div><div id='m_method'> M Method Name: backward(2)</div><div id='n_method'> N Method Name: backward(2)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: torchrec/distributed/comm_ops.py</div><div id='n_file'> N File Name: torchrec/distributed/comm_ops.py</div><div id='m_start'> M Start Line: 1048</div><div id='m_end'> M End Line: 1048</div><div id='n_start'> N Start Line: 1171</div><div id='n_end'> N End Line: 1187</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            )
        myreq.req = req
        myreq.tensor = grad_inputs
        <a id="change">return </a>(None<a id="change">, None, grad_output</a>)


class ReduceScatterBase_Req(Function):</code></pre><h3>After Change</h3><pre><code class='java'>
    def backward(ctx, grad_output: Tensor) -&gt; Tuple[None, None, Tensor]:
        myreq = ctx.myreq
        rsi = myreq.rsi
        <a id="change">if rsi.codecs is not None</a>:
            grad_output<a id="change"> = </a><a id="change">rsi.codecs.backward.encode(</a>grad_output<a id="change">)</a>
        grad_inputs = [
            grad_output.new_empty(
                in_size,
                dtype=grad_output.dtype,
                device=grad_output.device,
            )
            for in_size in rsi.input_sizes
        ]
        with record_function("&#47&#47&#47&#47 reduce_scatter_bw (all_gather) &#47&#47&#47&#47"):
            req = dist.all_gather(
                grad_inputs,
                grad_output.contiguous(),
                group=ctx.pg,
                async_op=True,
            )

        myreq.req = req
        myreq.tensor = grad_inputs
        <a id="change">return </a>(None<a id="change">, None, myreq.dummy_tensor</a>)


class ReduceScatterBase_Req(Function):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/fdb901e2becf3de3339370cdae3333956f644fa0#diff-233b62fdfbb8e887bafc580edd784c08250a8cc36a3b206dfb03524e49e098dbL954' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 19702673</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: fdb901e2becf3de3339370cdae3333956f644fa0</div><div id='time'> Time: 2022-07-27</div><div id='author'> Author: yingliufb@fb.com</div><div id='file'> File Name: torchrec/distributed/comm_ops.py</div><div id='m_class'> M Class Name: ReduceScatter_Wait</div><div id='n_method'> N Class Name: ReduceScatter_Wait</div><div id='m_method'> M Method Name: backward(2)</div><div id='n_method'> N Method Name: backward(2)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: torchrec/distributed/comm_ops.py</div><div id='n_file'> N File Name: torchrec/distributed/comm_ops.py</div><div id='m_start'> M Start Line: 974</div><div id='m_end'> M End Line: 974</div><div id='n_start'> N Start Line: 1064</div><div id='n_end'> N End Line: 1086</div><BR>