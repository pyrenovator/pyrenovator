<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 Note - this mismatch is by design! We return sharded_grad_output
        &#47&#47 to allow PyTorch shape matching to proceed correctly.
        <a id="change">return </a>(None<a id="change">, None, sharded_grad_output.view(-1)</a>)


class All2Allv_Req(Function):</code></pre><h3>After Change</h3><pre><code class='java'>
        input_splits = a2ai.output_splits
        output_splits = a2ai.input_splits

        <a id="change">if a2ai.codecs is not None</a>:
            sharded_grad_output<a id="change"> = </a><a id="change">a2ai.codecs.backward.encode(</a>sharded_grad_output<a id="change">)</a>

        sharded_grad_input = torch.empty(
            sum(output_splits),
            device=sharded_grad_output.device,
            dtype=sharded_grad_output.dtype,
        )

        with record_function("&#47&#47&#47&#47 alltoall_seq_embedding_bwd_single &#47&#47&#47&#47"):
            req = dist.all_to_all_single(
                output=sharded_grad_input,
                input=sharded_grad_output.view(-1),
                output_split_sizes=output_splits,
                input_split_sizes=input_splits,
                group=pg,
                async_op=True,
            )
        myreq.req = req
        myreq.tensor = sharded_grad_input

        <a id="change">return </a>(<a id="change">None</a><a id="change">, None, myreq.dummy_tensor</a>)


class All2Allv_Req(Function):</code></pre>