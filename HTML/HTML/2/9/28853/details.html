<html><h3>Pattern ID :28853
</h3><img src='84840174.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                if run_condition.blocking:
                    for recv_port in self.service_to_runtime:
                        data = recv_port.recv()
                        <a id="change">if not enum_equal(data, MGMT_RESPONSE.DONE)</a>:
                            if enum_equal(data, MGMT_RESPONSE.ERROR):
                                &#47&#47 Receive all errors from the ProcessModels
                                error_cnt<a id="change"> = </a>0
                                for actors in \
                                        self._messaging_infrastructure.actors:
                                    actors.join()
                                    if actors.exception:
                                        _<a id="change">, traceback = </a>actors.exception
                                        <a id="change">print(</a>traceback<a id="change">)</a>
                                        error_cnt += 1

                                raise RuntimeError(
                                    f"{error_cnt} Exception(s) occurred. See "
                                    f"output above for details.")
                            else:
                                <a id="change">raise </a><a id="change">RuntimeError(f"Runtime Received {data}"</a><a id="change">)</a>
                if run_condition.blocking:
                    self._is_running = False
            elif isinstance(run_condition, RunContinuous):
                pass</code></pre><h3>After Change</h3><pre><code class='java'>
                    self._get_resp_for_run()
            elif isinstance(run_condition, RunContinuous):
                self.num_steps = sys.maxsize
                <a id="change">for </a>send_port in self.runtime_to_service<a id="change">:
                    </a>send_port.send(enum_to_np(self.num_steps))
            else:
                raise ValueError(f"Wrong type of run_condition : "
                                 f"{run_condition.__class__}")</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lava-nc/lava/commit/4956db0191ef2d0b51c026fb9c1bb50ea57e6c90#diff-c65c489aca051f74137b2f1f1e8c67e393132ddb0fefa29aafc5d4cec47b7ceeL285' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 84840174</div><div id='project'> Project Name: lava-nc/lava</div><div id='commit'> Commit Name: 4956db0191ef2d0b51c026fb9c1bb50ea57e6c90</div><div id='time'> Time: 2022-02-17</div><div id='author'> Author: yashwardhan.singh@intel.com</div><div id='file'> File Name: src/lava/magma/runtime/runtime.py</div><div id='m_class'> M Class Name: Runtime</div><div id='n_method'> N Class Name: Runtime</div><div id='m_method'> M Method Name: _run(2)</div><div id='n_method'> N Method Name: _run(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/lava/magma/runtime/runtime.py</div><div id='n_file'> N File Name: src/lava/magma/runtime/runtime.py</div><div id='m_start'> M Start Line: 285</div><div id='m_end'> M End Line: 308</div><div id='n_start'> N Start Line: 326</div><div id='n_end'> N End Line: 332</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    conf_data = Config(config_filepath=conf_data_filepath)[&quotdataset&quot]
    storage_name = conf_data[&quotstorage_name&quot]
    tar_filepath = os.path.join(pt_data_dir, storage_name + &quot.tar&quot)
    <a id="change">if not os.path.isfile(tar_filepath)</a>:
        <a id="change">raise </a><a id="change">RuntimeError(f&quotTar file for dataset at {tar_filepath} was not found&quot</a><a id="change">)</a>

    tar_size<a id="change"> = </a>pathlib.Path(tar_filepath).stat().st_size
    print(&quottar_filepath:&quot, tar_filepath, &quottar_size:&quot, tar_size)

    local_dataroot<a id="change"> = </a>utils.full_path(args.dataroot)
    print(&quotlocal_dataroot:&quot, local_dataroot)
    _create_ram_disk(tar_size, local_dataroot)
    &#47&#47 os.makedirs(local_dataroot, exist_ok=True)

    utils.exec_shell_command(f&quottar -xf "{tar_filepath}" -C "{local_dataroot}"&quot)

    <a id="change">print(</a>f&quotdataset copied from {tar_filepath} to {local_dataroot} sucessfully&quot<a id="change">)</a>

if __name__ == &quot__main__&quot:
    main()</code></pre><h3>After Change</h3><pre><code class='java'>
    print(&quotconf_data_filepath:&quot, conf_data_filepath)

    conf = Config(config_filepath=conf_data_filepath)
    <a id="change">for </a>dataset_key in [&quotdataset&quot, &quotdataset_search&quot, &quotdataset_eval&quot]<a id="change">:
        </a>if dataset_key in conf:
            conf_data = conf[dataset_key]
            untar_dataset(pt_data_dir, conf_data, args.dataroot)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/archai/commit/cec9c167805699f4590f3f81c8584e8c419bb0d8#diff-573aaddafe4bceb6f60ec5bce02c75bcf2fd238ba7cb07550a690ab023066a97L32' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 84840172</div><div id='project'> Project Name: microsoft/archai</div><div id='commit'> Commit Name: cec9c167805699f4590f3f81c8584e8c419bb0d8</div><div id='time'> Time: 2020-05-18</div><div id='author'> Author: shitals@microsoft.com</div><div id='file'> File Name: scripts/datasets/pt_install.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(0)</div><div id='n_method'> N Method Name: main(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: scripts/datasets/pt_install.py</div><div id='n_file'> N File Name: scripts/datasets/pt_install.py</div><div id='m_start'> M Start Line: 43</div><div id='m_end'> M End Line: 65</div><div id='n_start'> N Start Line: 62</div><div id='n_end'> N End Line: 74</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        try:
            print("OPENED RPC_INFERENCE")
            request = await anext(requests)
            <a id="change">if not request.uid</a>:
                <a id="change">raise </a><a id="change">RuntimeError("User did not provide any uids."</a><a id="change">)</a>
            backend = self.module_backends[request.uid]
            assert isinstance(backend, TransformerBackend)

            &#47&#47 prepare attention cache
            num_heads<a id="change"> = </a>backend.module.self_attention.num_heads
            head_dim<a id="change"> = </a>backend.module.self_attention.head_dim
            cache_metadata = torch.tensor([[-1, -1]], dtype=torch.int64)  &#47&#47 [cache_handle, prefix_length]
            cache_descriptor = TensorDescriptor(size=(2, 1, MAX_LENGTH, num_heads, head_dim), dtype=torch.float32)
            prefix_length = 0

            async with backend.memory_cache.allocate_cache(cache_descriptor) as cache_handle:
                while request.uid or request.tensors:  &#47&#47 iterate while user is willing to supply tensors
                    inputs = [cache_metadata, *(deserialize_torch_tensor(tensor) for tensor in request.tensors)]
                    <a id="change">print(</a>"INPUTS:", inputs<a id="change">)</a>
                    assert len(inputs) == 2 and inputs[1].ndim == 3, "send only hidden states for now"
                    cache_metadata[0, 0], cache_metadata[0, 1] = cache_handle, prefix_length
                    outputs = await self._process_inputs(inputs, backend.inference_pool, backend.outputs_schema)
                    yield runtime_pb2.ExpertResponse(tensors=outputs)</code></pre><h3>After Change</h3><pre><code class='java'>
                    hidden_states = [deserialize_torch_tensor(tensor) for tensor in request.tensors]

                    &#47&#47 run request tensors through all requested modules, update caches
                    <a id="change">for </a>backend, cache_handle in zip(requested_backends, cache_handles)<a id="change">:
                        </a>cache_metadata[0, 0], cache_metadata[0, 1] = cache_handle, prefix_length
                        assert len(hidden_states) == 1 and hidden_states[0].ndim == 3, \
                            f"inputs to {type(backend)} must be a list with a single 3d tensor of hidden states"
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/bigscience-workshop/distributed-bloom/commit/1cdf8a77fb49c72cf778f390c621d447b350fa2d#diff-65ada620fb21ff82c2b88abea8bc2cf7fdd27241f03553f7e7d076339dbaab55L20' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 84840151</div><div id='project'> Project Name: bigscience-workshop/distributed-bloom</div><div id='commit'> Commit Name: 1cdf8a77fb49c72cf778f390c621d447b350fa2d</div><div id='time'> Time: 2022-06-23</div><div id='author'> Author: justheuristic@gmail.com</div><div id='file'> File Name: src/server/handler.py</div><div id='m_class'> M Class Name: TransformerConnectionHandler</div><div id='n_method'> N Class Name: TransformerConnectionHandler</div><div id='m_method'> M Method Name: rpc_inference(3)</div><div id='n_method'> N Method Name: rpc_inference(3)</div><div id='m_parent_class'> M Parent Class: ConnectionHandler</div><div id='n_parent_class'> N Parent Class: ConnectionHandler</div><div id='m_file'> M File Name: src/server/handler.py</div><div id='n_file'> N File Name: src/server/handler.py</div><div id='m_start'> M Start Line: 26</div><div id='m_end'> M End Line: 50</div><div id='n_start'> N Start Line: 29</div><div id='n_end'> N End Line: 60</div><BR>