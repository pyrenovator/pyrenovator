<html><h3>Pattern ID :41631
</h3><img src='116928253.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    @staticmethod
    def add_file_to_dictionary(filename, dict, tokenize):
        <a id="change">with open</a><a id="change">(filename, &quotr&quot) as f:
            </a>for line in f:
                for word in tokenize(line):
                    dict.add_symbol(word)
                dict.add_symbol(dict.eos_word)</code></pre><h3>After Change</h3><pre><code class='java'>
                dict.add_symbol(w, c)
        if num_workers &gt; 1:
            pool = Pool(processes=num_workers)
            results<a id="change"> = </a><a id="change">[]</a>
            <a id="change">for </a>worker_id in range(num_workers)<a id="change">:
                </a><a id="change">results.append(</a>pool.apply_async(
                    Tokenizer.add_file_to_dictionary_single_worker,
                    (filename, tokenize, dict.eos_word, worker_id, num_workers)
                )<a id="change">)</a>
            pool.close()
            pool.join()
            for r in results:
                merge_result(r.get())</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/kssteven418/i-bert/commit/862cad112ac51be1b20eed9efc5e40ef865a9457#diff-3461dadae7b1752387f4f0222a3cd879682d91f526ef1c2a8944457285b329f3L27' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116928253</div><div id='project'> Project Name: kssteven418/i-bert</div><div id='commit'> Commit Name: 862cad112ac51be1b20eed9efc5e40ef865a9457</div><div id='time'> Time: 2018-09-25</div><div id='author'> Author: edunov@apache.org</div><div id='file'> File Name: fairseq/tokenizer.py</div><div id='m_class'> M Class Name: Tokenizer</div><div id='n_method'> N Class Name: Tokenizer</div><div id='m_method'> M Method Name: add_file_to_dictionary(4)</div><div id='n_method'> N Method Name: add_file_to_dictionary(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: fairseq/tokenizer.py</div><div id='n_file'> N File Name: fairseq/tokenizer.py</div><div id='m_start'> M Start Line: 27</div><div id='m_end'> M End Line: 33</div><div id='n_start'> N Start Line: 56</div><div id='n_end'> N End Line: 75</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        Args:
            data_path (list): path to pickle file.
        
        <a id="change">with open</a><a id="change">(data_path, &quotrb&quot) as data_file:
            </a>data = pickle.load(data_file)

        self.data = [[torch.from_numpy(u).cuda() for u in s] for s in data]
        self.seg_len = seg_len</code></pre><h3>After Change</h3><pre><code class='java'>

        assert os.path.isdir(data_dir)

        self.data<a id="change"> = </a><a id="change">[]</a>
        self.n_uttrances = n_utterances
        self.seg_len = seg_len

        <a id="change">for </a><a id="change">data_file</a> in os.listdir(data_dir)<a id="change">:
            </a>data_path = os.path.join(data_dir, data_file)
            raw = pickle.load(open(data_path, "rb"))
            data = [torch.from_numpy(d) for d in raw if len(d) &gt; seg_len]
            if len(data) &lt; n_utterances:
                continue
            <a id="change">self.data.append(</a>data<a id="change">)</a>

    def __len__(self):
        return len(self.data)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/yistlin/dvector/commit/5e47d5f6e72c99fbd81923da2a611ae87d439032#diff-7a2001189ef2a3d5140a37f4c968015a248f0ddb1af1c3388980ea184c6031fcL14' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116928256</div><div id='project'> Project Name: yistlin/dvector</div><div id='commit'> Commit Name: 5e47d5f6e72c99fbd81923da2a611ae87d439032</div><div id='time'> Time: 2020-03-28</div><div id='author'> Author: yishen992@gmail.com</div><div id='file'> File Name: modules/utterances.py</div><div id='m_class'> M Class Name: Utterances</div><div id='n_method'> N Class Name: Utterances</div><div id='m_method'> M Method Name: __init__(4)</div><div id='n_method'> N Method Name: __init__(4)</div><div id='m_parent_class'> M Parent Class: Dataset</div><div id='n_parent_class'> N Parent Class: Dataset</div><div id='m_file'> M File Name: modules/utterances.py</div><div id='n_file'> N File Name: modules/utterances.py</div><div id='m_start'> M Start Line: 18</div><div id='m_end'> M End Line: 23</div><div id='n_start'> N Start Line: 20</div><div id='n_end'> N End Line: 34</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    @staticmethod
    def add_file_to_dictionary(filename, dict, tokenize):
        <a id="change">with open</a><a id="change">(filename, &quotr&quot) as f:
            </a>for line in f:
                for word in tokenize(line):
                    dict.add_symbol(word)
                dict.add_symbol(dict.eos_word)</code></pre><h3>After Change</h3><pre><code class='java'>
                dict.add_symbol(w, c)
        if num_workers &gt; 1:
            pool = Pool(processes=num_workers)
            results<a id="change"> = </a><a id="change">[]</a>
            <a id="change">for </a><a id="change">worker_id</a> in range(num_workers)<a id="change">:
                </a><a id="change">results.append(</a>pool.apply_async(
                    Tokenizer.add_file_to_dictionary_single_worker,
                    (filename, tokenize, dict.eos_word, worker_id, num_workers)
                )<a id="change">)</a>
            pool.close()
            pool.join()
            for r in results:
                merge_result(r.get())</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/neulab/retomaton/commit/862cad112ac51be1b20eed9efc5e40ef865a9457#diff-3461dadae7b1752387f4f0222a3cd879682d91f526ef1c2a8944457285b329f3L26' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116928259</div><div id='project'> Project Name: neulab/retomaton</div><div id='commit'> Commit Name: 862cad112ac51be1b20eed9efc5e40ef865a9457</div><div id='time'> Time: 2018-09-25</div><div id='author'> Author: edunov@apache.org</div><div id='file'> File Name: fairseq/tokenizer.py</div><div id='m_class'> M Class Name: Tokenizer</div><div id='n_method'> N Class Name: Tokenizer</div><div id='m_method'> M Method Name: add_file_to_dictionary(4)</div><div id='n_method'> N Method Name: add_file_to_dictionary(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: fairseq/tokenizer.py</div><div id='n_file'> N File Name: fairseq/tokenizer.py</div><div id='m_start'> M Start Line: 27</div><div id='m_end'> M End Line: 33</div><div id='n_start'> N Start Line: 56</div><div id='n_end'> N End Line: 75</div><BR>