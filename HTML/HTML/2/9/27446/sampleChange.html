<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        )
        target_dict.update(identity_target)

        tokenised_text<a id="change"> = </a><a id="change">self.tokenizer(
            </a>text<a id="change">, return_tensors="pt", padding=True, truncation=True
        )</a>
        tokenised_text<a id="change"> = </a>tokenised_text.data
        meta["multi_target"] = torch.tensor(
            list(target_dict.values()), dtype=torch.int32
        )
        meta["text_id"] = text_id
        if self.train:
            meta["weights"] = self.weights[index]
        else:
            meta["weights"] = torch.tensor([1], dtype=torch.int32)

        meta["toxicity_ids"] = torch.tensor(
            [True] * len(self.classes) + [False] * len(self.identity_classes)
        )
        <a id="change">return </a>tokenised_text<a id="change">, meta</a>

    def compute_weigths(self, train_df):
        Inspired from 2nd solution.
        Source: https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/100661</code></pre><h3>After Change</h3><pre><code class='java'>
        else:
            meta["weights"] = torch.tensor([1], dtype=torch.int32)

        <a id="change">meta["toxicity_ids"]</a> = torch.tensor(
            [True] * len(self.classes) + [False] * len(self.identity_classes)
        )
        <a id="change">return </a>text<a id="change">, meta</a>

    def compute_weigths(self, train_df):
        Inspired from 2nd solution.
        Source: https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/100661</code></pre>