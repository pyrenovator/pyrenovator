<html><h3>Pattern ID :29133
</h3><img src='85807549.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            if done:
                num_collected_episodes += 1
                self._episode_num += 1
                <a id="change">episode_rewards.append(</a>episode_reward<a id="change">)</a>
                <a id="change">total_timesteps.append(</a>episode_timesteps<a id="change">)</a>

                if action_noise is not None:
                    action_noise.reset()
</code></pre><h3>After Change</h3><pre><code class='java'>
            assert train_freq.unit == TrainFrequencyUnit.STEP, "You must use only one env when doing episodic training."

        &#47&#47 Vectorize action noise if needed
        <a id="change">if action_noise is not None</a><a id="change"> and env.num_envs &gt; 1 and not isinstance(action_noise, VectorizedActionNoise)</a>:
            action_noise<a id="change"> = </a>VectorizedActionNoise(action_noise, env.num_envs)

        if self.use_sde:
            self.actor.reset_noise(env.num_envs)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/dlr-rm/stable-baselines3/commit/507ed1762e62bd6c4e85ea572ba166b69116b1ac#diff-7809dfd549054549abed96b27661447862894a94e6873c67f8f96f4b6842debdL544' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 85807549</div><div id='project'> Project Name: dlr-rm/stable-baselines3</div><div id='commit'> Commit Name: 507ed1762e62bd6c4e85ea572ba166b69116b1ac</div><div id='time'> Time: 2021-12-01</div><div id='author'> Author: antonin.raffin@ensta.org</div><div id='file'> File Name: stable_baselines3/common/off_policy_algorithm.py</div><div id='m_class'> M Class Name: OffPolicyAlgorithm</div><div id='n_method'> N Class Name: OffPolicyAlgorithm</div><div id='m_method'> M Method Name: collect_rollouts(8)</div><div id='n_method'> N Method Name: collect_rollouts(8)</div><div id='m_parent_class'> M Parent Class: BaseAlgorithm</div><div id='n_parent_class'> N Parent Class: BaseAlgorithm</div><div id='m_file'> M File Name: stable_baselines3/common/off_policy_algorithm.py</div><div id='n_file'> N File Name: stable_baselines3/common/off_policy_algorithm.py</div><div id='m_start'> M Start Line: 544</div><div id='m_end'> M End Line: 619</div><div id='n_start'> N Start Line: 565</div><div id='n_end'> N End Line: 628</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        if self.context &gt; 1:
            z = [x]
            for <a id="change">d</a> in range(1, self.context // 2 + 1):
                z_u = torch.zeros_like(x)
                z_u[:, d:, :] = x[:, :-d, :] &#47&#47 i-d
                <a id="change">z.append(</a>z_u<a id="change">)</a>
                z_d = torch.zeros_like(x)
                z_d[:, :-d, :] = x[:, d:, :] &#47&#47 i+d
                <a id="change">z.append(</a>z_d<a id="change">)</a>
            x = torch.cat(z, dim=2) &#47&#47 (B, N, C*width)

        x = x.view(B*N, -1) &#47&#47 (B*N, C*width)
        for fc in self.fc[:-1]:</code></pre><h3>After Change</h3><pre><code class='java'>
        B, N, C = x.shape

        x = self.contextize(x, self.context) &#47&#47 (B, N, C*context)
        <a id="change">if self.mix_base &gt; 0</a><a id="change"> and x_base is not None</a>:
            x_base<a id="change"> = </a>self.contextize(x_base, self.mix_base) &#47&#47 (B, N, 4*mix_base)
            x = torch.cat((x_base, x), dim=2)

        x = x.view(B*N, -1) &#47&#47 (B*N, C*width)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/keio-bioinformatics/mxfold2/commit/998dd44f054b5851de1d3b751eb20091d5bf5628#diff-c9a664b6ab969299efeb624a60d9f43a2f26d097fa73e50c22da2a6220cbabfaL209' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 85807548</div><div id='project'> Project Name: keio-bioinformatics/mxfold2</div><div id='commit'> Commit Name: 998dd44f054b5851de1d3b751eb20091d5bf5628</div><div id='time'> Time: 2019-11-11</div><div id='author'> Author: satoken@bio.keio.ac.jp</div><div id='file'> File Name: dnnfold/fold/layers.py</div><div id='m_class'> M Class Name: FCUnpairedLayer</div><div id='n_method'> N Class Name: FCUnpairedLayer</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dnnfold/fold/layers.py</div><div id='n_file'> N File Name: dnnfold/fold/layers.py</div><div id='m_start'> M Start Line: 212</div><div id='m_end'> M End Line: 223</div><div id='n_start'> N Start Line: 255</div><div id='n_end'> N End Line: 263</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        continue_training = True

        while should_collect_more_steps(train_freq, num_collected_steps, num_collected_episodes):
            <a id="change">done</a> = False
            episode_reward, episode_timesteps = 0.0, 0

            while not done:

                if self.use_sde and self.sde_sample_freq &gt; 0 and num_collected_steps % self.sde_sample_freq == 0:
                    &#47&#47 Sample a new noise matrix
                    self.actor.reset_noise()

                &#47&#47 Select action randomly or according to policy
                action, buffer_action = self._sample_action(learning_starts, action_noise)

                &#47&#47 Rescale and perform action
                new_obs, reward, done, infos = env.step(action)

                self.num_timesteps += 1
                episode_timesteps += 1
                num_collected_steps += 1

                &#47&#47 Give access to local variables
                callback.update_locals(locals())
                &#47&#47 Only stop training if return value is False, not when it is None.
                if callback.on_step() is False:
                    return RolloutReturn(0.0, num_collected_steps, num_collected_episodes, continue_training=False)

                episode_reward += reward

                &#47&#47 Retrieve reward and episode length if using Monitor wrapper
                self._update_info_buffer(infos, done)

                &#47&#47 Store data in replay buffer (normalized action and unnormalized observation)
                self._store_transition(replay_buffer, buffer_action, new_obs, reward, done, infos)

                self._update_current_progress_remaining(self.num_timesteps, self._total_timesteps)

                &#47&#47 For DQN, check if the target network should be updated
                &#47&#47 and update the exploration schedule
                &#47&#47 For SAC/TD3, the update is done as the same time as the gradient update
                &#47&#47 see https://github.com/hill-a/stable-baselines/issues/900
                self._on_step()

                if not should_collect_more_steps(train_freq, num_collected_steps, num_collected_episodes):
                    break

            if done:
                num_collected_episodes += 1
                self._episode_num += 1
                <a id="change">episode_rewards.append(</a>episode_reward<a id="change">)</a>
                <a id="change">total_timesteps.append(</a>episode_timesteps<a id="change">)</a>

                if action_noise is not None:
                    action_noise.reset()
</code></pre><h3>After Change</h3><pre><code class='java'>
            assert train_freq.unit == TrainFrequencyUnit.STEP, "You must use only one env when doing episodic training."

        &#47&#47 Vectorize action noise if needed
        <a id="change">if action_noise is not None</a><a id="change"> and env.num_envs &gt; 1 and not isinstance(action_noise, VectorizedActionNoise)</a>:
            action_noise<a id="change"> = </a>VectorizedActionNoise(action_noise, env.num_envs)

        if self.use_sde:
            self.actor.reset_noise(env.num_envs)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/dlr-rm/stable-baselines3/commit/507ed1762e62bd6c4e85ea572ba166b69116b1ac#diff-7809dfd549054549abed96b27661447862894a94e6873c67f8f96f4b6842debdL512' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 85807546</div><div id='project'> Project Name: dlr-rm/stable-baselines3</div><div id='commit'> Commit Name: 507ed1762e62bd6c4e85ea572ba166b69116b1ac</div><div id='time'> Time: 2021-12-01</div><div id='author'> Author: antonin.raffin@ensta.org</div><div id='file'> File Name: stable_baselines3/common/off_policy_algorithm.py</div><div id='m_class'> M Class Name: OffPolicyAlgorithm</div><div id='n_method'> N Class Name: OffPolicyAlgorithm</div><div id='m_method'> M Method Name: collect_rollouts(8)</div><div id='n_method'> N Method Name: collect_rollouts(8)</div><div id='m_parent_class'> M Parent Class: BaseAlgorithm</div><div id='n_parent_class'> N Parent Class: BaseAlgorithm</div><div id='m_file'> M File Name: stable_baselines3/common/off_policy_algorithm.py</div><div id='n_file'> N File Name: stable_baselines3/common/off_policy_algorithm.py</div><div id='m_start'> M Start Line: 544</div><div id='m_end'> M End Line: 619</div><div id='n_start'> N Start Line: 565</div><div id='n_end'> N End Line: 628</div><BR>