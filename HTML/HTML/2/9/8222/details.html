<html><h3>Pattern ID :8222
</h3><img src='29032234.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>


def run(gpu_id, resume_checkpoint, finetune, model_dir, resume):
    <a id="change">if </a>gpu_id == "cpu":
        os.environ["CUDA_VISIBLE_DEVICES"] = ""
        device<a id="change"> = </a>torch.device("cpu")

    else:
        os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
        <a id="change">os.environ["CUDA_VISIBLE_DEVICES"] = </a>"{}".format(gpu_id)
        device<a id="change"> = </a>torch.device("cuda")

    torch.manual_seed(131714)
    random.seed(131714)</code></pre><h3>After Change</h3><pre><code class='java'>
        for index, train_set in enumerate(datasets):
            instance_save_dir = model_save_dirs[index] + f"_iteration_{iteration}"
            os.makedirs(instance_save_dir, exist_ok=True)
            <a id="change">processes.append(</a><a id="change">torch.multiprocessing.Process(target=train_loop,
                                                           kwargs={
                                                               "net"               : individual_models[index],
                                                               "train_dataset"     : train_set,
                                                               "device"            : torch.device(f"cuda:{gpus_available[-1]}"),
                                                               "save_directory"    : instance_save_dir,
                                                               "steps"             : 1,
                                                               "batch_size"        : 64,
                                                               "epochs_per_save"   : 1,
                                                               "lang"              : languages[index],
                                                               "lr"                : 0.001,
                                                               "path_to_checkpoint": meta_save_dir + "/best.pt",
                                                               "fine_tune"         : True,
                                                               "resume"            : resume
                                                               }))</a>
            processes[-1].start()
            print(f"Starting {instance_save_dir} on cuda:{gpus_available[-1]}")
            gpus_in_use.append(gpus_available.pop())
            while len(gpus_available) == 0:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/c2d6763d59533aadfe9096a70cf208455e3aee9c#diff-35ce52bf4206f5886ecd8e11eec900fc41f488e1fef7bb51d5e1e75213981b52L12' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 29032234</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: c2d6763d59533aadfe9096a70cf208455e3aee9c</div><div id='time'> Time: 2021-10-16</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: TrainingInterfaces/TrainingPipelines/Tacotron2_MetaCheckpoint.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: run(5)</div><div id='n_method'> N Method Name: run(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: TrainingInterfaces/TrainingPipelines/Tacotron2_MetaCheckpoint.py</div><div id='n_file'> N File Name: TrainingInterfaces/TrainingPipelines/Tacotron2_MetaCheckpoint.py</div><div id='m_start'> M Start Line: 12</div><div id='m_end'> M End Line: 175</div><div id='n_start'> N Start Line: 152</div><div id='n_end'> N End Line: 198</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>


def run_test_locally(fn):
    <a id="change">if </a>not torch.cuda.is_available():
        print("skip tests since cuda is not available")
        return []

    nprocs = torch.cuda.device_count()
    <a id="change">os.environ["WORLD_SIZE"]</a> = str(nprocs)
    <a id="change">os.environ["LOCAL_WORLD_SIZE"] = </a>str(nprocs)
    os.environ["MASTER_ADDR"] = "127.0.0.1"
    os.environ["MASTER_PORT"]<a id="change"> = </a>str(find_free_port())
    os.environ["BAGUA_SERVICE_PORT"]<a id="change"> = </a>str(find_free_port())

    results = [Result() for _ in range(nprocs)]
    mp.spawn(</code></pre><h3>After Change</h3><pre><code class='java'>
    results = [Result() for _ in range(nprocs)]
    processes = []
    for i in range(nprocs):
        p = <a id="change">mp.Process(
            target=fn,
            args=(i, nprocs, results, env),
        )</a>
        p.start()
        <a id="change">processes.append(</a>p<a id="change">)</a>

    for p in processes:
        p.join(timeout=60)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/baguasys/bagua/commit/a9529bef66e367884316a2b2ebc917ff35bf6334#diff-7a16c2809e9e060c1dc2b8f83387654f4a97459627405dbf5948e60b05bcea7dL105' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 29032233</div><div id='project'> Project Name: baguasys/bagua</div><div id='commit'> Commit Name: a9529bef66e367884316a2b2ebc917ff35bf6334</div><div id='time'> Time: 2021-08-26</div><div id='author'> Author: 45031995+wangraying@users.noreply.github.com</div><div id='file'> File Name: tests/comm/test_communicator.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: run_test_locally(1)</div><div id='n_method'> N Method Name: run_test_locally(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: tests/comm/test_communicator.py</div><div id='n_file'> N File Name: tests/comm/test_communicator.py</div><div id='m_start'> M Start Line: 106</div><div id='m_end'> M End Line: 122</div><div id='n_start'> N Start Line: 115</div><div id='n_end'> N End Line: 138</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

class TestAsyncModelAverage(unittest.TestCase):
    def test_algorithm(self):
        <a id="change">if </a>not torch.cuda.is_available():
            print("skip tests since cuda is not available")
            return

        nprocs = torch.cuda.device_count()
        os.environ["WORLD_SIZE"] = str(nprocs)
        <a id="change">os.environ["LOCAL_WORLD_SIZE"]</a> = str(nprocs)
        <a id="change">os.environ["MASTER_ADDR"] = </a>"127.0.0.1"
        os.environ["MASTER_PORT"]<a id="change"> = </a>str(find_free_port())
        os.environ["BAGUA_SERVICE_PORT"]<a id="change"> = </a>str(find_free_port())

        mp.spawn(
            run_model,</code></pre><h3>After Change</h3><pre><code class='java'>
        mp = multiprocessing.get_context("spawn")
        processes = []
        for i in range(nprocs):
            p = <a id="change">mp.Process(target=run_model, args=(i, env))</a>
            p.start()
            <a id="change">processes.append(</a>p<a id="change">)</a>

        for p in processes:
            p.join(timeout=60)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/baguasys/bagua/commit/a9529bef66e367884316a2b2ebc917ff35bf6334#diff-c46b7f4c320ba27d2783f4d47a513380419a948da235a60e6b47ccf8be3f194bL61' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 29032224</div><div id='project'> Project Name: baguasys/bagua</div><div id='commit'> Commit Name: a9529bef66e367884316a2b2ebc917ff35bf6334</div><div id='time'> Time: 2021-08-26</div><div id='author'> Author: 45031995+wangraying@users.noreply.github.com</div><div id='file'> File Name: tests/torch_api/test_async_model_average.py</div><div id='m_class'> M Class Name: TestAsyncModelAverage</div><div id='n_method'> N Class Name: TestAsyncModelAverage</div><div id='m_method'> M Method Name: test_algorithm(1)</div><div id='n_method'> N Method Name: test_algorithm(1)</div><div id='m_parent_class'> M Parent Class: unittest.TestCase</div><div id='n_parent_class'> N Parent Class: unittest.TestCase</div><div id='m_file'> M File Name: tests/torch_api/test_async_model_average.py</div><div id='n_file'> N File Name: tests/torch_api/test_async_model_average.py</div><div id='m_start'> M Start Line: 62</div><div id='m_end'> M End Line: 76</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 87</div><BR>