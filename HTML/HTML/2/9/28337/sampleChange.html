<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    batched_outputs = self._force_decode(encoded_inputs)

    &#47&#47 Convert to numpy for post-processing ... this doesn&quott work for jax
    detached_outputs<a id="change"> = </a><a id="change">{k: v.numpy() for k, v in batched_outputs.items()}</a>
    &#47&#47 Instead of the above I am trying to implement something like this below for jax
    &#47&#47 ----&gt;
    &#47&#47 predict_answer_tokens = results.input_ids[0, answer_start_index : answer_end_index + 1]
    &#47&#47    &#47&#47 generates answer text</code></pre><h3>After Change</h3><pre><code class='java'>
        question.append(i[&quotquestion&quot])
        context.append(i[&quotcontext&quot])

    prediction = <a id="change">[]</a>
    <a id="change">for </a>i in range(len(inputs))<a id="change">:
        </a>model = FlaxBertForQuestionAnswering.from_pretrained("mrm8488/bert-multi-cased-finedtuned-xquad-tydiqa-goldp")
        inputs = self.tokenizer(question[i], context[i], return_tensors="jax",padding=True)
        outputs = model(**inputs)

        answer_start_index = outputs.start_logits.argmax()

        answer_end_index = outputs.end_logits.argmax()

        predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index<a id="change"> + </a>1]

        <a id="change">prediction.append(</a>self.tokenizer.decode(predict_answer_tokens)<a id="change">)</a>
    print(&quotPredictions for the data is----&gt;/n&quot)
    print(prediction)
    &#47&#47 returning list of prediction
    return prediction</code></pre>