<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                bin_mask = segmask_array_iter == int(_class)
                bin_mask = bin_mask.long()
            one_hot_stack.append(bin_mask)
        one_hot_stack<a id="change"> = </a><a id="change">torch.stack(</a>one_hot_stack<a id="change">)</a>
        batch_stack.append(one_hot_stack)
    batch_stack = torch.stack(batch_stack)
    if batch_stack.shape[-1] == 1:
        batch_stack = batch_stack.squeeze(-1)</code></pre><h3>After Change</h3><pre><code class='java'>
                &#47&#47 we always ensure the append happens in dim 0, which is blank
                bin_mask = bin_mask.unsqueeze(0)
                
            <a id="change">if one_hot_stack is None</a>:
                one_hot_stack<a id="change"> = </a>bin_mask
            else:
                one_hot_stack<a id="change"> = </a>torch.cat((one_hot_stack, bin_mask))
                
        <a id="change">if batch_stack is None</a>:
            batch_stack<a id="change"> = </a>one_hot_stack
        else:
            batch_stack = torch.stack([batch_stack, one_hot_stack])
    &#47&#47 always ensure we are returning a tensor with batch_size encoded</code></pre>