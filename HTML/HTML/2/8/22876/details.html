<html><h3>Pattern ID :22876
</h3><img src='72718960.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.dim_sort = dim_sort

        self.linear_sort_q = nn.Parameter(torch.randn(1, heads, dim, dim_sort))
        self.linear_sort_k = <a id="change">nn.Parameter(torch.randn(</a>1, heads, dim, dim_sort<a id="change">)</a><a id="change">)</a>
        self.null_sort_k = nn.Parameter(torch.randn(1, heads, 1, dim_sort))

    def forward(self, q, k):
        bh, *_, h, buckets, dim, dim_sort = *q.shape, self.heads, self.buckets, self.dim, self.dim_sort</code></pre><h3>After Change</h3><pre><code class='java'>
        self.dim_sort = dim_sort

        self.q_pos_emb = nn.Parameter(torch.randn(1, heads, buckets, dim))
        self.k_pos_emb<a id="change"> = </a><a id="change">nn.Parameter(torch.randn(</a>1, heads, buckets, dim<a id="change">)</a><a id="change">)</a>

        self.linear_sort_q = nn.Parameter(torch.randn(1, heads, dim * 2, dim_sort))
        self.linear_sort_k = nn.Parameter(torch.randn(1, heads, <a id="change">dim</a><a id="change"> * 2</a>, dim_sort))

    def forward(self, q, k):
        bh, *_, h, buckets, dim, dim_sort = *q.shape, self.heads, self.buckets, self.dim, self.dim_sort</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/sinkhorn-transformer/commit/a3d36f7cdf5b9a134787caf42ce18b5b28b0ddc5#diff-2ed19ae7feb552dc52b4c7a7c89a078c0fd371c922789f894af09ff01f35eccbL348' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 72718960</div><div id='project'> Project Name: lucidrains/sinkhorn-transformer</div><div id='commit'> Commit Name: a3d36f7cdf5b9a134787caf42ce18b5b28b0ddc5</div><div id='time'> Time: 2020-04-10</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='m_class'> M Class Name: CausalAttentionSortNet</div><div id='n_method'> N Class Name: CausalAttentionSortNet</div><div id='m_method'> M Method Name: __init__(5)</div><div id='n_method'> N Method Name: __init__(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='n_file'> N File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='m_start'> M Start Line: 348</div><div id='m_end'> M End Line: 350</div><div id='n_start'> N Start Line: 357</div><div id='n_end'> N End Line: 361</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.n_sortcut = n_sortcut

        self.linear_sort_q = nn.Parameter(torch.randn(1, heads, dim, dim_sort))
        self.linear_sort_k = <a id="change">nn.Parameter(torch.randn(</a>1, heads, dim, dim_sort<a id="change">)</a><a id="change">)</a>

    def forward(self, q, k):
        bh, *_, buckets, dim, dim_sort = *q.shape, self.buckets, self.dim, self.dim_sort
        b = bh // self.heads</code></pre><h3>After Change</h3><pre><code class='java'>
        self.n_sortcut = n_sortcut

        self.q_pos_emb = nn.Parameter(torch.randn(1, heads, buckets if n_sortcut == 0 else 1, dim))
        self.k_pos_emb<a id="change"> = </a><a id="change">nn.Parameter(torch.randn(</a>1, heads, buckets, dim<a id="change">)</a><a id="change">)</a>

        self.linear_sort_q = nn.Parameter(torch.randn(1, heads, dim * 2, dim_sort))
        self.linear_sort_k = nn.Parameter(torch.randn(1, heads, dim<a id="change"> * 2</a>, dim_sort))

    def forward(self, q, k):
        bh, *_, buckets, dim, dim_sort = *q.shape, self.buckets, self.dim, self.dim_sort</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/sinkhorn-transformer/commit/a3d36f7cdf5b9a134787caf42ce18b5b28b0ddc5#diff-2ed19ae7feb552dc52b4c7a7c89a078c0fd371c922789f894af09ff01f35eccbL187' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 72718957</div><div id='project'> Project Name: lucidrains/sinkhorn-transformer</div><div id='commit'> Commit Name: a3d36f7cdf5b9a134787caf42ce18b5b28b0ddc5</div><div id='time'> Time: 2020-04-10</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='m_class'> M Class Name: AttentionSortNet</div><div id='n_method'> N Class Name: AttentionSortNet</div><div id='m_method'> M Method Name: __init__(9)</div><div id='n_method'> N Method Name: __init__(9)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='n_file'> N File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='m_start'> M Start Line: 198</div><div id='m_end'> M End Line: 199</div><div id='n_start'> N Start Line: 204</div><div id='n_end'> N End Line: 208</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.to_q = nn.Conv2d(dim, dim_k * heads, 1, bias = False)
        self.to_k = nn.Conv2d(dim, dim_k * dim_u, 1, bias = False)
        self.to_v = nn.Conv2d(dim, dim_v * dim_u, 1, bias = False)
        self.pos_emb = <a id="change">nn.Parameter(torch.randn(</a>n, m, dim_k, dim_u<a id="change">)</a><a id="change">)</a>

        self.norm_q = nn.BatchNorm2d(dim_k * heads)
        self.norm_v = nn.BatchNorm2d(dim_v * dim_u)
</code></pre><h3>After Change</h3><pre><code class='java'>
        self.local_contexts = exists(r)
        if exists(r):
            assert (r % 2) == 1, &quotReceptive kernel size should be odd&quot
            self.padding = r<a id="change"> // 2</a>
            self.R<a id="change"> = </a><a id="change">nn.Parameter(torch.randn(</a>dim_k, dim_u, 1, r, r<a id="change">)</a><a id="change">)</a>
        else:
            assert exists(n), &quotYou must specify the total sequence length (h x w)&quot
            self.pos_emb = nn.Parameter(torch.randn(n, n, dim_k, dim_u))
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/lambda-networks/commit/07a7fa070743bdf0e7a0fe80b98fc415caa86495#diff-6d46aff567baa42dfd4c6b8d2f5c956d3ea84b2811ca663ea67159e323344ccbL17' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 72718955</div><div id='project'> Project Name: lucidrains/lambda-networks</div><div id='commit'> Commit Name: 07a7fa070743bdf0e7a0fe80b98fc415caa86495</div><div id='time'> Time: 2020-10-08</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: lambda_networks/lambda_networks.py</div><div id='m_class'> M Class Name: LambdaLayer</div><div id='n_method'> N Class Name: LambdaLayer</div><div id='m_method'> M Method Name: __init__(2)</div><div id='n_method'> N Method Name: __init__(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: lambda_networks/lambda_networks.py</div><div id='n_file'> N File Name: lambda_networks/lambda_networks.py</div><div id='m_start'> M Start Line: 38</div><div id='m_end'> M End Line: 38</div><div id='n_start'> N Start Line: 42</div><div id='n_end'> N End Line: 52</div><BR>