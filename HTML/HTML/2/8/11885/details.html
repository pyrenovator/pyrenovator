<html><h3>Pattern ID :11885
</h3><img src='39974211.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    sz = 50
    for patch_size in POSSIBLE_PATCH_SIZE:
        <a id="change">if image_size[1] % patch_size == 0</a>:
            sz<a id="change"> = </a>patch_size
            patch_num = (image_size[0] // sz, image_size[1] // sz)
            break

    is_ref_model = type(network) == RefNeRF
    for k in range(patch_num[0]):
        for j in range(patch_num[1]):
            camera_rays = torch.cat((render_pose[:, -1].expand(sz, sz, -1), ray_raw[(sz * k):(sz * (k + 1)), (sz * j):(sz * (j + 1))]), dim = -1).reshape(-1, 6)        &#47&#47 shape (2500, 6)
            sampled_lengths = (all_lengths + torch.rand((sz, sz, 64)).cuda() * resolution).view(-1, 64)        &#47&#47 shape (2500, sample_num)
            pts = render_pose[:, -1].unsqueeze(0) + sampled_lengths[..., None] * camera_rays[:, None, 3:]
            density = prop_net.forward(pts)
            prop_weights_raw = ProposalNetwork.get_weights(density, sampled_lengths, camera_rays[:, 3:])      &#47&#47 (ray_num, num of proposal interval)
            prop_weights = maxBlurFilter(prop_weights_raw, 0.01)
            fine_lengths, _ = inverseSample(prop_weights, sampled_lengths, sample_num + 1, sort = True)
            fine_samples, fine_lengths = NeRF.coarseFineMerge(camera_rays, sampled_lengths, fine_lengths)
            &#47&#47 fine_samples = NeRF.length2pts(camera_rays, fine_lengths)
            output_rgbo<a id="change"> = </a>network.forward(fine_samples)
            if is_ref_model == True:
                output_rgbo, normal = output_rgbo
                output_rgbo[..., -1] = softplus(output_rgbo[..., -1] + 0.5)</code></pre><h3>After Change</h3><pre><code class='java'>
            prop_weights_raw = ProposalNetwork.get_weights(density, sampled_lengths, camera_rays[:, 3:])      &#47&#47 (ray_num, num of proposal interval)
            prop_weights = maxBlurFilter(prop_weights_raw, 0.01)
            fine_lengths, _ = inverseSample(prop_weights, sampled_lengths, sample_num + 1, sort = True)
            <a id="change">if </a><a id="change">is_ref_model</a>:
                fine_samples, fine_lengths = NeRF.coarseFineMerge(camera_rays, sampled_lengths, fine_lengths)
                output_rgbo, normal = <a id="change">network.forward(</a>fine_samples<a id="change">)</a>
                output_rgbo[..., -1] = softplus(output_rgbo[..., -1] + 0.5)
            else:
                fine_samples = NeRF.length2pts(camera_rays, fine_lengths)
                output_rgbo<a id="change"> = </a>network.forward(fine_samples)

            part_image, _, extras = NeRF.render(
                output_rgbo, fine_lengths, camera_rays[..., 3:], </code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/enigmatisms/nerf/commit/a6436d76592f3832a4c96ca1aa768f1d55e7801c#diff-c1437858ac9404bcb3ba7aac1188dad521081e6f8d45a4cb87432af354a3ebc6L28' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 39974211</div><div id='project'> Project Name: enigmatisms/nerf</div><div id='commit'> Commit Name: a6436d76592f3832a4c96ca1aa768f1d55e7801c</div><div id='time'> Time: 2022-08-19</div><div id='author'> Author: 984041003@qq.com</div><div id='file'> File Name: py/procedures.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: render_image(11)</div><div id='n_method'> N Method Name: render_image(11)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: py/procedures.py</div><div id='n_file'> N File Name: py/procedures.py</div><div id='m_start'> M Start Line: 28</div><div id='m_end'> M End Line: 73</div><div id='n_start'> N Start Line: 38</div><div id='n_end'> N End Line: 78</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        sequence_output = output[0]

        <a id="change">if labels is not None</a>:
            &#47&#47 Select only the masked tokens for the classifier
            max_number_of_masked_tokens = int(labels.size(1) * 0.25)
            masked_lm_labels, masked_lm_positions = torch.topk(labels, k=max_number_of_masked_tokens, dim=1)
            masked_output<a id="change"> = </a>self.gather_indices(sequence_output, masked_lm_positions)
        else:
            &#47&#47 This case should never happen during training
            masked_output<a id="change"> = </a>sequence_output

        prediction_scores = self.cls(masked_output)
        output = (prediction_scores,) + output[2:]</code></pre><h3>After Change</h3><pre><code class='java'>
        return self

    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):
        <a id="change">if </a><a id="change">self.training</a>:
            output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
            sequence_output = output[0]

            &#47&#47 Select only the masked tokens for the classifier
            max_number_of_masked_tokens = int(labels.size(1) * 0.25)
            masked_lm_labels, masked_lm_positions = torch.topk(labels, k=max_number_of_masked_tokens, dim=1)
            masked_output<a id="change"> = </a>self.gather_indices(sequence_output, masked_lm_positions)

            prediction_scores = self.cls(masked_output)
            output = (prediction_scores,) + output[2:]

            masked_lm_loss = F.cross_entropy(
                prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1)
            ).float()
            return (masked_lm_loss,)

        else:
            return <a id="change">super().forward(
                input_ids=input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids,
                labels=labels,
                return_dict=False,
            )</a>


class BertPipelineMixin(PipelineMixin):
    def parallelize(self):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/huggingface/optimum-graphcore/commit/66eefe1acd3c6f70badd77131320450e2606d5c6#diff-042125c098af719464a54fc080dad6fd6394b6575787c445a5f9c55927dd7dc2L266' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 39974215</div><div id='project'> Project Name: huggingface/optimum-graphcore</div><div id='commit'> Commit Name: 66eefe1acd3c6f70badd77131320450e2606d5c6</div><div id='time'> Time: 2022-03-21</div><div id='author'> Author: jamesbr@graphcore.ai</div><div id='file'> File Name: optimum/graphcore/models/bert/modeling_bert.py</div><div id='m_class'> M Class Name: PipelinedBertForMaskedLM</div><div id='n_method'> N Class Name: PipelinedBertForMaskedLM</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: PipelineMixin,BertForMaskedLM</div><div id='n_parent_class'> N Parent Class: PipelineMixin,BertForMaskedLM</div><div id='m_file'> M File Name: optimum/graphcore/models/bert/modeling_bert.py</div><div id='n_file'> N File Name: optimum/graphcore/models/bert/modeling_bert.py</div><div id='m_start'> M Start Line: 267</div><div id='m_end'> M End Line: 288</div><div id='n_start'> N Start Line: 267</div><div id='n_end'> N End Line: 293</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        outputs = self.roberta(input_ids, attention_mask=attention_mask)
        sequence_output = outputs[0]

        <a id="change">if labels is not None</a>:
            &#47&#47 Select only the masked tokens for the classifier
            max_number_of_masked_tokens = int(labels.size(1) * 0.25)
            masked_lm_labels<a id="change">, masked_lm_positions = </a>torch.topk(labels, k=max_number_of_masked_tokens, dim=1)
            masked_output = self.gather_indices(sequence_output, masked_lm_positions)
        else:
            &#47&#47 This case should never happen during training
            masked_output<a id="change"> = </a>sequence_output

        prediction_scores = self.lm_head(masked_output)
        outputs = (prediction_scores,) + outputs[2:]</code></pre><h3>After Change</h3><pre><code class='java'>
        return self

    def forward(self, input_ids, attention_mask, labels=None):
        <a id="change">if </a><a id="change">self.training</a>:
            outputs = self.roberta(input_ids, attention_mask=attention_mask)
            sequence_output = outputs[0]

            &#47&#47 Select only the masked tokens for the classifier
            max_number_of_masked_tokens = int(labels.size(1) * 0.25)
            masked_lm_labels<a id="change">, masked_lm_positions = </a>torch.topk(labels, k=max_number_of_masked_tokens, dim=1)
            masked_output = self.gather_indices(sequence_output, masked_lm_positions)

            prediction_scores = self.lm_head(masked_output)

            masked_lm_loss = F.cross_entropy(
                prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1)
            ).float()
            return (masked_lm_loss,)
        else:
            return <a id="change">super().forward(
                input_ids=input_ids, attention_mask=attention_mask, labels=labels, return_dict=False
            )</a>


@register(RobertaForSequenceClassification)
class PipelinedRobertaForSequenceClassification(RobertaForSequenceClassification, RobertaPipelineMixin):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/huggingface/optimum-graphcore/commit/66eefe1acd3c6f70badd77131320450e2606d5c6#diff-e6bb8260cba471d97c88459de99e2cd076bc2b8ccd368430dd4d393ae914cdd1L143' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 39974219</div><div id='project'> Project Name: huggingface/optimum-graphcore</div><div id='commit'> Commit Name: 66eefe1acd3c6f70badd77131320450e2606d5c6</div><div id='time'> Time: 2022-03-21</div><div id='author'> Author: jamesbr@graphcore.ai</div><div id='file'> File Name: optimum/graphcore/models/roberta/modeling_roberta.py</div><div id='m_class'> M Class Name: PipelinedRobertaForMaskedLM</div><div id='n_method'> N Class Name: PipelinedRobertaForMaskedLM</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: PipelineMixin,RobertaForMaskedLM</div><div id='n_parent_class'> N Parent Class: PipelineMixin,RobertaForMaskedLM</div><div id='m_file'> M File Name: optimum/graphcore/models/roberta/modeling_roberta.py</div><div id='n_file'> N File Name: optimum/graphcore/models/roberta/modeling_roberta.py</div><div id='m_start'> M Start Line: 144</div><div id='m_end'> M End Line: 165</div><div id='n_start'> N Start Line: 144</div><div id='n_end'> N End Line: 164</div><BR>