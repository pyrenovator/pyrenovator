<html><h3>Pattern ID :25908
</h3><img src='78311154.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        skip_dims = []

        for ind, (dim_in, dim_out) in enumerate(<a id="change">in_out</a>):
            is_last = ind &gt;= (num_resolutions - 1)
            skip_dims.append(dim_in)

            self.downs.append(nn.ModuleList([
                blocks(dim_in, dim_in),
                blocks(dim_in, dim_in),
                Downsample(dim_in, dim_out)
            ]))

        mid_dim = dims[-1]
        self.mid = blocks(mid_dim, mid_dim)
        self.mid_attn = Attention(mid_dim)
        self.mid_after = blocks(mid_dim, mid_dim)
        self.mid_upsample = Upsample(mid_dim, dims[-2])

        for ind, (dim_in, dim_out) in enumerate(<a id="change">reversed(in_out[:-1]</a><a id="change">)</a>):
            is_last = ind &gt;= (num_resolutions - 1)

            self.ups.append(nn.ModuleList([</code></pre><h3>After Change</h3><pre><code class='java'>
        self.init_conv = nn.Conv3d(channels, init_dim, (1, 7, 7), padding = (0, 3, 3))

        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]
        <a id="change">in_out</a> = list(zip(dims[:-1], dims[1:]))

        self.downs = nn.ModuleList([])
        self.ups = nn.ModuleList([])

        num_resolutions = len(in_out)

        &#47&#47 resnet or convnext

        blocks = partial(ConvNextBlock) if use_convnext else partial(ResnetBlock, groups = resnet_groups)

        &#47&#47 whether to use nested unet, as in unet squared paper

        nested_unet_depths = cast_tuple(nested_unet_depths, num_resolutions)

        &#47&#47 modules for all layers

        skip_dims = []

        for ind, ((dim_in, dim_out), nested_unet_depth) in enumerate(<a id="change">zip(in_out</a>, <a id="change">nested_unet_depths</a><a id="change">)</a>):
            is_last = ind &gt;= (num_resolutions - 1)
            skip_dims.append(dim_in)

            self.downs.append(nn.ModuleList([
                blocks(dim_in, dim_in, nested_unet_depth = nested_unet_depth, nested_unet_dim = nested_unet_dim),
                blocks(dim_in, dim_in, nested_unet_depth = nested_unet_depth, nested_unet_dim = nested_unet_dim),
                Downsample(dim_in, dim_out)
            ]))

        mid_dim = dims[-1]
        self.mid = blocks(mid_dim, mid_dim)
        self.mid_attn = Attention(mid_dim)
        self.mid_after = blocks(mid_dim, mid_dim)
        self.mid_upsample = Upsample(mid_dim, dims[-2])

        for ind, ((dim_in, dim_out), nested_unet_depth) in enumerate(<a id="change">zip(</a><a id="change">reversed(in_out[:-1]</a><a id="change">)</a>, <a id="change">reversed(nested_unet_depths[:-1]</a><a id="change">)</a><a id="change">)</a>):
            is_last = ind &gt;= (num_resolutions - 1)

            self.ups.append(nn.ModuleList([</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/x-unet/commit/b3baf3ac77328c1fea7bce83524cbfd3c17cc9f6#diff-6e9e7a6d8d7a8c116b8bdae64f3e845f516adfec59d0379fc01849d294784966L201' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 78311154</div><div id='project'> Project Name: lucidrains/x-unet</div><div id='commit'> Commit Name: b3baf3ac77328c1fea7bce83524cbfd3c17cc9f6</div><div id='time'> Time: 2022-07-25</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: x_unet/x_unet.py</div><div id='m_class'> M Class Name: XUnet</div><div id='n_method'> N Class Name: XUnet</div><div id='m_method'> M Method Name: __init__(11)</div><div id='n_method'> N Method Name: __init__(9)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: x_unet/x_unet.py</div><div id='n_file'> N File Name: x_unet/x_unet.py</div><div id='m_start'> M Start Line: 201</div><div id='m_end'> M End Line: 232</div><div id='n_start'> N Start Line: 216</div><div id='n_end'> N End Line: 265</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.cond_init_conv = nn.Conv2d(channels, init_dim, 7, padding = 3)

        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]
        <a id="change">in_out</a> = list(zip(dims[:-1], dims[1:]))

        block_klass = partial(ResnetBlock, groups = resnet_block_groups)

        &#47&#47 time embeddings

        time_dim = dim * 4

        self.time_mlp = nn.Sequential(
            SinusoidalPosEmb(dim),
            nn.Linear(dim, time_dim),
            nn.GELU(),
            nn.Linear(time_dim, time_dim)
        )

        &#47&#47 layers

        num_resolutions = len(in_out)

        self.conditioners = nn.ModuleList([])

        self.skip_connect_condition_fmaps = skip_connect_condition_fmaps

        &#47&#47 downsampling encoding blocks

        self.downs = nn.ModuleList([])

        curr_fmap_size = image_size

        for ind, (dim_in, dim_out) in enumerate(in_out):
            is_last = ind &gt;= (num_resolutions - 1)

            self.conditioners.append(conditioning_klass(curr_fmap_size, dim_in))

            self.downs.append(nn.ModuleList([
                block_klass(dim_in, dim_in, time_emb_dim = time_dim),
                block_klass(dim_in, dim_in, time_emb_dim = time_dim),
                Residual(LinearAttention(dim_in)),
                Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding = 1)
            ]))

            if not is_last:
                curr_fmap_size //= 2

        &#47&#47 middle blocks

        mid_dim = dims[-1]
        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)
        self.mid_attn = Residual(Attention(mid_dim))
        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)

        &#47&#47 condition encoding path will be the same as the main encoding path

        self.cond_downs = copy.deepcopy(self.downs)
        self.cond_mid_block1 = copy.deepcopy(self.mid_block1)

        &#47&#47 upsampling decoding blocks

        self.ups = nn.ModuleList([])

        for ind, (dim_in, dim_out) in enumerate(<a id="change">reversed(</a>in_out<a id="change">)</a>):
            is_last = ind == (len(in_out) - 1)

            skip_connect_dim = dim_in * (2 if self.skip_connect_condition_fmaps else 1)</code></pre><h3>After Change</h3><pre><code class='java'>
        self.cond_init_conv = nn.Conv2d(channels, init_dim, 7, padding = 3)

        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]
        <a id="change">in_out</a> = list(zip(dims[:-1], dims[1:]))

        block_klass = partial(ResnetBlock, groups = resnet_block_groups)

        &#47&#47 time embeddings

        time_dim = dim * 4

        self.time_mlp = nn.Sequential(
            SinusoidalPosEmb(dim),
            nn.Linear(dim, time_dim),
            nn.GELU(),
            nn.Linear(time_dim, time_dim)
        )

        &#47&#47 layers

        num_resolutions = len(in_out)
        assert len(full_self_attn) == num_resolutions

        self.conditioners = nn.ModuleList([])

        self.skip_connect_condition_fmaps = skip_connect_condition_fmaps

        &#47&#47 downsampling encoding blocks

        self.downs = nn.ModuleList([])

        curr_fmap_size = image_size

        for ind, ((dim_in, dim_out), full_attn) in enumerate(<a id="change">zip(</a>in_out, full_self_attn<a id="change">)</a>):
            is_last = ind &gt;= (num_resolutions - 1)
            attn_klass = Attention if full_attn else LinearAttention

            self.conditioners.append(conditioning_klass(curr_fmap_size, dim_in))


            self.downs.append(nn.ModuleList([
                block_klass(dim_in, dim_in, time_emb_dim = time_dim),
                block_klass(dim_in, dim_in, time_emb_dim = time_dim),
                Residual(attn_klass(dim_in)),
                Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding = 1)
            ]))

            if not is_last:
                curr_fmap_size //= 2

        &#47&#47 middle blocks

        mid_dim = dims[-1]
        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)
        self.mid_attn = Residual(Attention(mid_dim))
        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)

        &#47&#47 condition encoding path will be the same as the main encoding path

        self.cond_downs = copy.deepcopy(self.downs)
        self.cond_mid_block1 = copy.deepcopy(self.mid_block1)

        &#47&#47 upsampling decoding blocks

        self.ups = nn.ModuleList([])

        for ind, ((dim_in, dim_out), full_attn) in enumerate(<a id="change">zip(</a><a id="change">reversed(</a>in_out<a id="change">)</a>, <a id="change">reversed(</a>full_self_attn<a id="change">)</a><a id="change">)</a>):
            is_last = ind == (len(in_out) - 1)
            attn_klass = Attention if full_attn else LinearAttention
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/med-seg-diff-pytorch/commit/efbb6cd8fb9e012d242a872f4862f3e95929709f#diff-844b38754380e73816a9a883698d0be58ade3c98292bfe9b915c180f02596398L228' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 78311152</div><div id='project'> Project Name: lucidrains/med-seg-diff-pytorch</div><div id='commit'> Commit Name: efbb6cd8fb9e012d242a872f4862f3e95929709f</div><div id='time'> Time: 2022-11-30</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: med_seg_diff_pytorch/med_seg_diff_pytorch.py</div><div id='m_class'> M Class Name: Unet</div><div id='n_method'> N Class Name: Unet</div><div id='m_method'> M Method Name: __init__(12)</div><div id='n_method'> N Method Name: __init__(11)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: med_seg_diff_pytorch/med_seg_diff_pytorch.py</div><div id='n_file'> N File Name: med_seg_diff_pytorch/med_seg_diff_pytorch.py</div><div id='m_start'> M Start Line: 234</div><div id='m_end'> M End Line: 317</div><div id='n_start'> N Start Line: 237</div><div id='n_end'> N End Line: 326</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.init_conv = nn.Conv2d(input_channels, init_dim, 7, padding = 3)

        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]
        <a id="change">in_out</a> = list(zip(dims[:-1], dims[1:]))

        resnet_block = partial(ResnetBlock, groups = resnet_block_groups)

        &#47&#47 time embeddings

        time_dim = dim * 4

        sinu_pos_emb = RandomOrLearnedSinusoidalPosEmb(learned_sinusoidal_dim, random_fourier_features)
        fourier_dim = learned_sinusoidal_dim + 1

        self.time_mlp = nn.Sequential(
            sinu_pos_emb,
            nn.Linear(fourier_dim, time_dim),
            nn.GELU(),
            nn.Linear(time_dim, time_dim)
        )

        &#47&#47 layers

        self.downs = nn.ModuleList([])
        self.ups = nn.ModuleList([])
        num_resolutions = len(in_out)

        for ind, (dim_in, dim_out) in enumerate(in_out):
            is_last = ind &gt;= (num_resolutions - 1)

            self.downs.append(nn.ModuleList([
                resnet_block(dim_in, dim_in, time_emb_dim = time_dim),
                resnet_block(dim_in, dim_in, time_emb_dim = time_dim),
                LinearAttention(dim_in),
                Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding = 1)
            ]))

        mid_dim = dims[-1]

        self.vit = Transformer(
            dim = mid_dim,
            time_cond_dim = time_dim,
            depth = vit_depth,
            dim_head = attn_dim_head,
            heads = attn_heads,
            ff_mult = ff_mult,
            dropout = vit_dropout
        )

        for ind, (dim_in, dim_out) in enumerate(<a id="change">reversed(</a>in_out<a id="change">)</a>):
            is_last = ind == (len(in_out) - 1)

            self.ups.append(nn.ModuleList([</code></pre><h3>After Change</h3><pre><code class='java'>
        self.init_conv = nn.Conv2d(input_channels, init_dim, 7, padding = 3)

        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]
        <a id="change">in_out</a> = list(zip(dims[:-1], dims[1:]))

        resnet_block = partial(ResnetBlock, groups = resnet_block_groups)

        &#47&#47 time embeddings

        time_dim = dim * 4

        sinu_pos_emb = LearnedSinusoidalPosEmb(learned_sinusoidal_dim)
        fourier_dim = learned_sinusoidal_dim + 1

        self.time_mlp = nn.Sequential(
            sinu_pos_emb,
            nn.Linear(fourier_dim, time_dim),
            nn.GELU(),
            nn.Linear(time_dim, time_dim)
        )

        &#47&#47 downsample factors

        <a id="change">downsample_factor</a> = cast_tuple(downsample_factor, len(dim_mults))
        assert len(downsample_factor) == len(dim_mults)

        &#47&#47 layers

        self.downs = nn.ModuleList([])
        self.ups = nn.ModuleList([])
        num_resolutions = len(in_out)

        for ind, ((dim_in, dim_out), factor) in enumerate(<a id="change">zip(</a>in_out, downsample_factor<a id="change">)</a>):
            is_last = ind &gt;= (num_resolutions - 1)

            self.downs.append(nn.ModuleList([
                resnet_block(dim_in, dim_in, time_emb_dim = time_dim),
                resnet_block(dim_in, dim_in, time_emb_dim = time_dim),
                LinearAttention(dim_in),
                Downsample(dim_in, dim_out, factor = factor)
            ]))

        mid_dim = dims[-1]

        self.vit = Transformer(
            dim = mid_dim,
            time_cond_dim = time_dim,
            depth = vit_depth,
            dim_head = attn_dim_head,
            heads = attn_heads,
            ff_mult = ff_mult,
            dropout = vit_dropout
        )

        for ind, ((dim_in, dim_out), factor) in enumerate(<a id="change">zip(</a><a id="change">reversed(</a>in_out<a id="change">)</a>, <a id="change">reversed(</a>downsample_factor<a id="change">)</a><a id="change">)</a>):
            is_last = ind == (len(in_out) - 1)

            self.ups.append(nn.ModuleList([</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/denoising-diffusion-pytorch/commit/54557120880bb9adbea9f93a29a2c432b67991c1#diff-932ae956742e2e7e4ca0d46053f9844b2504bc2cf2dca187505413a2f363e4a3L282' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 78311153</div><div id='project'> Project Name: lucidrains/denoising-diffusion-pytorch</div><div id='commit'> Commit Name: 54557120880bb9adbea9f93a29a2c432b67991c1</div><div id='time'> Time: 2023-02-01</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: denoising_diffusion_pytorch/simple_diffusion.py</div><div id='m_class'> M Class Name: UViT</div><div id='n_method'> N Class Name: UViT</div><div id='m_method'> M Method Name: __init__(17)</div><div id='n_method'> N Method Name: __init__(17)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: denoising_diffusion_pytorch/simple_diffusion.py</div><div id='n_file'> N File Name: denoising_diffusion_pytorch/simple_diffusion.py</div><div id='m_start'> M Start Line: 295</div><div id='m_end'> M End Line: 383</div><div id='n_start'> N Start Line: 309</div><div id='n_end'> N End Line: 409</div><BR>