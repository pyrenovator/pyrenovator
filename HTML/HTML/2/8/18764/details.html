<html><h3>Pattern ID :18764
</h3><img src='61069171.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

class MonotonicAttention(torch.nn.Module):
    def __init__(self):
        <a id="change">pass</a>


class LuongMonotonicAttention(torch.nn.Module):
    def __init__(self):</code></pre><h3>After Change</h3><pre><code class='java'>
class MonotonicAttention(torch.nn.Module):
    def __init__(self, source_size, target_size, init_r=-4):
        super(MonotonicAttention, self).__init__()
        self.source_size<a id="change"> = source_size</a>
        self.target_size = target_size

        self.w_linear<a id="change"> = nn</a><a id="change">.Linear(source_size</a>, target_size<a id="change">)</a>
        self.v_linear = nn.Linear(target_size, target_size)
        self.bias = nn.Parameter(torch.Tensor(target_size).normal_())

        self.v = nn.utils.weight_norm(nn.Linear(target_size, 1))
        self.v.weight_g.data = torch.Tensor([1 / target_size]).sqrt()

        self.r<a id="change"> = nn</a><a id="change">.Parameter(</a>torch.Tensor([init_r])<a id="change">)</a>

    def gaussian_noise(self, *size):
        Additive gaussian nosie to encourage discreteness
        return torch.FloatTensor(*size).normal_()</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/rucaibox/textbox/commit/a58fdf4b51c70b10472233bfb9a06dea305ee9cb#diff-d9e15dc978d5b328aa30538e301dd8e56d5b98120cfda66705f8f322cd15c30fL24' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 61069171</div><div id='project'> Project Name: rucaibox/textbox</div><div id='commit'> Commit Name: a58fdf4b51c70b10472233bfb9a06dea305ee9cb</div><div id='time'> Time: 2020-12-03</div><div id='author'> Author: lijunyi@ruc.edu.cn</div><div id='file'> File Name: textbox/module/Attention/attention_mechanism.py</div><div id='m_class'> M Class Name: MonotonicAttention</div><div id='n_method'> N Class Name: MonotonicAttention</div><div id='m_method'> M Method Name: __init__(4)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: textbox/module/Attention/attention_mechanism.py</div><div id='n_file'> N File Name: textbox/module/Attention/attention_mechanism.py</div><div id='m_start'> M Start Line: 24</div><div id='m_end'> M End Line: 25</div><div id='n_start'> N Start Line: 104</div><div id='n_end'> N End Line: 116</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

class MonotonicAttention(torch.nn.Module):
    def __init__(self):
        <a id="change">pass</a>


class LuongMonotonicAttention(torch.nn.Module):
    def __init__(self):</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self, source_size, target_size, init_r=-4):
        super(MonotonicAttention, self).__init__()
        self.source_size = source_size
        self.target_size<a id="change"> = </a>target_size

        self.w_linear<a id="change"> = </a><a id="change">nn.Linear(</a>source_size, target_size<a id="change">)</a>
        self.v_linear = nn.Linear(target_size, target_size)
        self.bias<a id="change"> = </a><a id="change">nn.Parameter(</a>torch.Tensor(target_size).normal_()<a id="change">)</a>

        self.v = nn.utils.weight_norm(nn.Linear(target_size, 1))
        self.v.weight_g.data = torch.Tensor([1 / target_size]).sqrt()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/rucaibox/textbox/commit/a58fdf4b51c70b10472233bfb9a06dea305ee9cb#diff-d9e15dc978d5b328aa30538e301dd8e56d5b98120cfda66705f8f322cd15c30fL24' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 61069170</div><div id='project'> Project Name: rucaibox/textbox</div><div id='commit'> Commit Name: a58fdf4b51c70b10472233bfb9a06dea305ee9cb</div><div id='time'> Time: 2020-12-03</div><div id='author'> Author: lijunyi@ruc.edu.cn</div><div id='file'> File Name: textbox/module/Attention/attention_mechanism.py</div><div id='m_class'> M Class Name: MonotonicAttention</div><div id='n_method'> N Class Name: MonotonicAttention</div><div id='m_method'> M Method Name: __init__(4)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: textbox/module/Attention/attention_mechanism.py</div><div id='n_file'> N File Name: textbox/module/Attention/attention_mechanism.py</div><div id='m_start'> M Start Line: 24</div><div id='m_end'> M End Line: 25</div><div id='n_start'> N Start Line: 104</div><div id='n_end'> N End Line: 116</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

class LuongAttention(torch.nn.Module):
    def __init__(self):
        <a id="change">pass</a>


class BahdanauAttention(torch.nn.Module):
    def __init__(self):</code></pre><h3>After Change</h3><pre><code class='java'>
class LuongAttention(torch.nn.Module):
    def __init__(self, source_size, target_size, alignment_method=&quotconcat&quot):
        super(LuongAttention, self).__init__()
        self.source_size<a id="change"> = </a>source_size
        self.target_size = target_size
        self.alignment_method = alignment_method

        if self.alignment_method == &quotgeneral&quot:
            self.energy_linear<a id="change"> = </a><a id="change">nn.Linear(</a>target_size, source_size<a id="change">)</a>
        elif self.alignment_method == &quotconcat&quot:
            self.energy_linear = nn.Linear(source_size + target_size, target_size)
            self.v<a id="change"> = </a><a id="change">nn.Parameter(</a>torch.FloatTensor(target_size)<a id="change">)</a>
        elif self.alignment_method == &quotdot&quot:
            assert self.source_size == target_size
        else:
            raise ValueError("The alignment method for Luong Attention must be in [&quotgeneral&quot, &quotconcat&quot, &quotdot&quot].")</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/rucaibox/textbox/commit/a58fdf4b51c70b10472233bfb9a06dea305ee9cb#diff-d9e15dc978d5b328aa30538e301dd8e56d5b98120cfda66705f8f322cd15c30fL14' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 61069168</div><div id='project'> Project Name: rucaibox/textbox</div><div id='commit'> Commit Name: a58fdf4b51c70b10472233bfb9a06dea305ee9cb</div><div id='time'> Time: 2020-12-03</div><div id='author'> Author: lijunyi@ruc.edu.cn</div><div id='file'> File Name: textbox/module/Attention/attention_mechanism.py</div><div id='m_class'> M Class Name: LuongAttention</div><div id='n_method'> N Class Name: LuongAttention</div><div id='m_method'> M Method Name: __init__(4)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: textbox/module/Attention/attention_mechanism.py</div><div id='n_file'> N File Name: textbox/module/Attention/attention_mechanism.py</div><div id='m_start'> M Start Line: 14</div><div id='m_end'> M End Line: 15</div><div id='n_start'> N Start Line: 14</div><div id='n_end'> N End Line: 30</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

class BahdanauAttention(torch.nn.Module):
    def __init__(self):
        <a id="change">pass</a>


class MonotonicAttention(torch.nn.Module):
    def __init__(self):</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self, source_size, target_size):
        super(BahdanauAttention, self).__init__()
        self.source_size = source_size
        self.target_size<a id="change"> = </a>target_size

        self.energy_linear<a id="change"> = </a><a id="change">nn.Linear(</a>source_size + target_size, target_size<a id="change">)</a>
        self.v<a id="change"> = </a><a id="change">nn.Parameter(</a>torch.FloatTensor(target_size)<a id="change">)</a>

    def score(self, hidden_states, encoder_outputs):
        src_len = encoder_outputs.size(1)
        hidden_states = hidden_states.unsqueeze(1).repeat(1, src_len, 1)  &#47&#47 B * src_len * target_size</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/rucaibox/textbox/commit/a58fdf4b51c70b10472233bfb9a06dea305ee9cb#diff-d9e15dc978d5b328aa30538e301dd8e56d5b98120cfda66705f8f322cd15c30fL19' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 61069172</div><div id='project'> Project Name: rucaibox/textbox</div><div id='commit'> Commit Name: a58fdf4b51c70b10472233bfb9a06dea305ee9cb</div><div id='time'> Time: 2020-12-03</div><div id='author'> Author: lijunyi@ruc.edu.cn</div><div id='file'> File Name: textbox/module/Attention/attention_mechanism.py</div><div id='m_class'> M Class Name: BahdanauAttention</div><div id='n_method'> N Class Name: BahdanauAttention</div><div id='m_method'> M Method Name: __init__(3)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: textbox/module/Attention/attention_mechanism.py</div><div id='n_file'> N File Name: textbox/module/Attention/attention_mechanism.py</div><div id='m_start'> M Start Line: 19</div><div id='m_end'> M End Line: 20</div><div id='n_start'> N Start Line: 70</div><div id='n_end'> N End Line: 76</div><BR>