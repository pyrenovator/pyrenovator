<html><h3>Pattern ID :34039
</h3><img src='97396804.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            )
            if path:
                leaf_module = self._dmp_wrapped_module
                split_path<a id="change"> = </a>path.split(".")
                <a id="change">for </a><a id="change">name</a> in split_path[:-1]<a id="change">:
                    </a>leaf_module<a id="change"> = </a><a id="change">getattr(</a>leaf_module, <a id="change">name</a><a id="change">)</a>
                setattr(leaf_module, split_path[-1], sharded_module)
            else:
                self._dmp_wrapped_module = sharded_module
            if isinstance(sharded_module, FusedOptimizerModule):</code></pre><h3>After Change</h3><pre><code class='java'>
    ) -&gt; nn.Module:

        &#47&#47 pre-sharded module
        <a id="change">if </a>isinstance(module, ShardedModule):
            return module

        &#47&#47 shardable module
        module_sharding_plan = self._plan.get_plan_for_module(path)
        if module_sharding_plan:
            sharder_key = sharder_name(type(module))
            module = self._sharder_map[sharder_key].shard(
                module,
                module_sharding_plan,
                self._env,
                self.device,
            )
            return module

        for name, child in module.named_children():
            child<a id="change"> = </a>self._shard_modules_impl(
                child,
                path + "." + name if path else name,
            )
            <a id="change">setattr(</a>module, name, child<a id="change">)</a>

        return module

    def _init_parameters(self, module: nn.Module) -&gt; None:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/f13d0c06f6492b058beeb9fe7e2b9e2bbaa5f0f2#diff-088378575010e7b9e27ea06507ba52fcc8604bcf9cb107f4939a9a49b0d52c89L342' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 97396804</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: f13d0c06f6492b058beeb9fe7e2b9e2bbaa5f0f2</div><div id='time'> Time: 2022-11-07</div><div id='author'> Author: dstaay@meta.com</div><div id='file'> File Name: torchrec/distributed/model_parallel.py</div><div id='m_class'> M Class Name: DistributedModelParallel</div><div id='n_method'> N Class Name: DistributedModelParallel</div><div id='m_method'> M Method Name: _shard_modules_impl(3)</div><div id='n_method'> N Method Name: _shard_modules_impl(4)</div><div id='m_parent_class'> M Parent Class: FusedOptimizerModule,nn.Module</div><div id='n_parent_class'> N Parent Class: FusedOptimizerModule,nn.Module</div><div id='m_file'> M File Name: torchrec/distributed/model_parallel.py</div><div id='n_file'> N File Name: torchrec/distributed/model_parallel.py</div><div id='m_start'> M Start Line: 342</div><div id='m_end'> M End Line: 371</div><div id='n_start'> N Start Line: 352</div><div id='n_end'> N End Line: 374</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        
        with ExitStack() as stack:

            port_args<a id="change"> = </a>[&quotport_in&quot, &quotport_out&quot, &quotport_ctrl&quot]
            port_mapping = defaultdict(dict)

            &#47&#47 set env vars
            stack.enter_context(change_env(&quotJINA_FULL_CLI&quot, &quottrue&quot))
            if self.envs:
                for key, val in self.envs.items():
                    stack.enter_context(change_env(key, val))

            &#47&#47 change directory to `workspace`
            stack.enter_context(change_cwd(get_workspace_path(self.workspace_id)))

            &#47&#47 load and build
            f: Flow = Flow.load_config(str(self.localpath())).build()

            &#47&#47 get & set the ports mapping, set `runs_in_docker`
            port_mapping[&quotgateway&quot][&quotport_expose&quot] = f.port_expose
            for pod_name, pod in f._pod_nodes.items():
                runtime_cls = update_runtime_cls(pod.args, copy=True).runtime_cls
                if runtime_cls in [&quotZEDRuntime&quot] + list(GATEWAY_RUNTIME_DICT.values()):
                    if isinstance(pod, CompoundPod):
                        &#47&#47 For a `CompoundPod`, publish ports only for head Pea & tail Pea
                        &#47&#47 Check daemon.stores.partial.PartialFlowStore.add() for addtional logic
                        &#47&#47 to handle `CompoundPod` inside partial-daemon.
                        for pea_args in [pod.head_args, pod.tail_args]:
                            pea_args.runs_in_docker = False
                            <a id="change">for </a><a id="change">port_arg</a> in port_args<a id="change">:
                                </a>port_mapping[pea_args.name][port_arg]<a id="change"> = </a><a id="change">getattr(
                                    </a>pea_args, port_arg<a id="change">
                                )</a>
                    else:
                        pod.args.runs_in_docker = True
                        for port_arg in port_args:
                            port_mapping[pod_name][port_arg] = getattr(</code></pre><h3>After Change</h3><pre><code class='java'>
                    &#47&#47 For a `CompoundPod`, publish ports only for head Pea & tail Pea
                    &#47&#47 Check daemon.stores.partial.PartialFlowStore.add() for addtional logic
                    &#47&#47 to handle `CompoundPod` inside partial-daemon.
                    <a id="change">if </a>runtime_cls in [&quotZEDRuntime&quot, &quotContainerRuntime&quot] + list(
                        GATEWAY_RUNTIME_DICT.values()
                    ):
                        for pea_args in [pod.head_args, pod.tail_args]:
                            pea_args.runs_in_docker = False
                            current_ports<a id="change"> = </a>Ports()
                            for port_name in Ports.__fields__:
                                &#47&#47 Get port from Namespace args & set to PortMappings
                                <a id="change">setattr(
                                    </a>current_ports,
                                    port_name,
                                    getattr(pea_args, port_name, None)<a id="change">,
                                )</a>
                            port_mapping.append(
                                PortMapping(
                                    pod_name=pod_name,
                                    pea_name=pea_args.name,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/jina-ai/jina/commit/4221b25862efb8287eb7e30158e18ce2cfab0b31#diff-e92de852b8c9c6715e2a21527889a87bcc3994af6909ad17fe576f1e9dad42cfL97' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 97396805</div><div id='project'> Project Name: jina-ai/jina</div><div id='commit'> Commit Name: 4221b25862efb8287eb7e30158e18ce2cfab0b31</div><div id='time'> Time: 2021-08-31</div><div id='author'> Author: deepankar.mahapatro@jina.ai</div><div id='file'> File Name: daemon/api/dependencies.py</div><div id='m_class'> M Class Name: FlowDepends</div><div id='n_method'> N Class Name: FlowDepends</div><div id='m_method'> M Method Name: load_and_dump(1)</div><div id='n_method'> N Method Name: load_and_dump(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: daemon/api/dependencies.py</div><div id='n_file'> N File Name: daemon/api/dependencies.py</div><div id='m_start'> M Start Line: 118</div><div id='m_end'> M End Line: 153</div><div id='n_start'> N Start Line: 130</div><div id='n_end'> N End Line: 180</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            )
            if path:
                leaf_module = self._dmp_wrapped_module
                split_path<a id="change"> = </a>path.split(".")
                <a id="change">for </a><a id="change">name</a> in split_path[:-1]<a id="change">:
                    </a>leaf_module<a id="change"> = </a><a id="change">getattr(</a>leaf_module, name<a id="change">)</a>
                setattr(leaf_module, split_path[-1], sharded_module)
            else:
                self._dmp_wrapped_module = sharded_module
            if isinstance(sharded_module, FusedOptimizerModule):</code></pre><h3>After Change</h3><pre><code class='java'>
    ) -&gt; nn.Module:

        &#47&#47 pre-sharded module
        <a id="change">if </a>isinstance(module, ShardedModule):
            return module

        &#47&#47 shardable module
        module_sharding_plan = self._plan.get_plan_for_module(path)
        if module_sharding_plan:
            sharder_key = sharder_name(type(module))
            module = self._sharder_map[sharder_key].shard(
                module,
                module_sharding_plan,
                self._env,
                self.device,
            )
            return module

        for name, child in module.named_children():
            child<a id="change"> = </a>self._shard_modules_impl(
                child,
                path + "." + name if path else name,
            )
            <a id="change">setattr(</a>module, name, child<a id="change">)</a>

        return module

    def _init_parameters(self, module: nn.Module) -&gt; None:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/f13d0c06f6492b058beeb9fe7e2b9e2bbaa5f0f2#diff-088378575010e7b9e27ea06507ba52fcc8604bcf9cb107f4939a9a49b0d52c89L338' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 97396802</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: f13d0c06f6492b058beeb9fe7e2b9e2bbaa5f0f2</div><div id='time'> Time: 2022-11-07</div><div id='author'> Author: dstaay@meta.com</div><div id='file'> File Name: torchrec/distributed/model_parallel.py</div><div id='m_class'> M Class Name: DistributedModelParallel</div><div id='n_method'> N Class Name: DistributedModelParallel</div><div id='m_method'> M Method Name: _shard_modules_impl(3)</div><div id='n_method'> N Method Name: _shard_modules_impl(4)</div><div id='m_parent_class'> M Parent Class: FusedOptimizerModule,nn.Module</div><div id='n_parent_class'> N Parent Class: FusedOptimizerModule,nn.Module</div><div id='m_file'> M File Name: torchrec/distributed/model_parallel.py</div><div id='n_file'> N File Name: torchrec/distributed/model_parallel.py</div><div id='m_start'> M Start Line: 342</div><div id='m_end'> M End Line: 371</div><div id='n_start'> N Start Line: 352</div><div id='n_end'> N End Line: 374</div><BR>