<html><h3>Pattern ID :35334
</h3><img src='100419260.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 delay in the masking stage.
        if not self.pad_specf:
            spec = self.pad_spec(spec)
        m<a id="change"> = </a><a id="change">self.pad_out(m.unsqueeze(-1)).squeeze(-1</a><a id="change">)</a>
        spec = self.mask(spec, m)

        if self.run_df:
            df_coefs, _ = self.df_dec(emb, c0)
            df_coefs = self.pad_out(df_coefs)

            if self.pad_specf:
                &#47&#47 Only pad the lower part of the spectrum.
                spec_f<a id="change"> = </a>self.pad_spec(spec)
                spec_f = self.df_op(spec_f, df_coefs)
                spec[..., : self.nb_df, :] = spec_f[..., : self.nb_df, :]
            else:</code></pre><h3>After Change</h3><pre><code class='java'>
        spec = self.mask(spec, m)
        df_coefs, df_alpha = self.df_dec(emb, c0)

        <a id="change">for </a>_ in <a id="change">range(</a>self.df_iter<a id="change">):
            </a>if self.use_alpha:
                spec = self.df_op(spec, df_coefs, df_alpha)
            else:
                spec = self.df_op(spec, df_coefs)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/rikorose/deepfilternet/commit/3e554333e5a3f0802f8df5a00e07de517dfe6561#diff-c68fcf7e3e857dc9d67e36d42a07b6bdcc0eafa25601537f8f70149f37a07bccL428' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 100419260</div><div id='project'> Project Name: rikorose/deepfilternet</div><div id='commit'> Commit Name: 3e554333e5a3f0802f8df5a00e07de517dfe6561</div><div id='time'> Time: 2022-10-18</div><div id='author'> Author: h.schroeter@pm.me</div><div id='file'> File Name: DeepFilterNet/df/deepfilternet2.py</div><div id='m_class'> M Class Name: DfNet</div><div id='n_method'> N Class Name: DfNet</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: DeepFilterNet/df/deepfilternet2.py</div><div id='n_file'> N File Name: DeepFilterNet/df/deepfilternet2.py</div><div id='m_start'> M Start Line: 428</div><div id='m_end'> M End Line: 455</div><div id='n_start'> N Start Line: 486</div><div id='n_end'> N End Line: 502</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        xyz, point_features = torch.split(points, [3, 1], dim=-1)
        out = [(point_features, xyz)] + out

        xyz = <a id="change">xyz.unsqueeze(0</a><a id="change">)</a>.contiguous()
        indices<a id="change"> = </a><a id="change">furthest_point_sample(xyz, cfg.n_keypoints).squeeze(0</a><a id="change">)</a>.long()
        keypoints<a id="change"> = </a>points[indices]
        keypoints_xyz, keypoints_features = torch.split(keypoints, [3, 1], dim=-1)
        voxel_features_i, voxel_coords_i = out[2]
</code></pre><h3>After Change</h3><pre><code class='java'>
        xyz, point_features = torch.split(points, [3, 1], dim=-1)
        keypoints_xyz, keypoints_features = self.sample_keypoints(xyz, point_features)
        out = [(point_features, xyz)] + out
        <a id="change">for </a>i in <a id="change">range(</a>len(self.cfg.strides)<a id="change">):
            </a>voxel_features_i, voxel_coords_i = out[i]
            voxel_coords_i = voxel_coords_i.unsqueeze(0).contiguous()
            voxel_features_i = voxel_features_i.unsqueeze(0).permute(0, 2, 1).contiguous()
            keypoints_xyz = keypoints_xyz.unsqueeze(0).contiguous()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/jhultman/vision3d/commit/b58d5a394a6cc04a99daeaa04b0ef90f050f8582#diff-acdbb7a8541d282e4e6bd0147d753b4b222401d39382c8be5223bd3ab78db7fcL109' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 100419262</div><div id='project'> Project Name: jhultman/vision3d</div><div id='commit'> Commit Name: b58d5a394a6cc04a99daeaa04b0ef90f050f8582</div><div id='time'> Time: 2020-01-17</div><div id='author'> Author: 27909223+jhultman@users.noreply.github.com</div><div id='file'> File Name: pvrcnn/main.py</div><div id='m_class'> M Class Name: PV_RCNN</div><div id='n_method'> N Class Name: PV_RCNN</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: pvrcnn/main.py</div><div id='n_file'> N File Name: pvrcnn/main.py</div><div id='m_start'> M Start Line: 120</div><div id='m_end'> M End Line: 130</div><div id='n_start'> N Start Line: 143</div><div id='n_end'> N End Line: 151</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            &#47&#47 Get cropped video
            input, target, vid_path = data.__getitem__(i)   &#47&#47 4 channels x num_frames x H x W
            num_frames = input.shape[1]
            rgb_center_img_tensor = <a id="change">input[:3, num_frames // 2, :, :].unsqueeze(1</a><a id="change">)</a>  &#47&#47 3 x 1 x H x W
            masks = input[3]  
            mask = torch.mean(masks, dim=0)  &#47&#47 H x W
            &#47&#47mask = torch.ceil(masks)
            &#47&#47mask = torch.round(masks)

            center_img_salient = rgb_center_img_tensor*mask     &#47&#47 3 channels x 1 frame x H x W
            center_img_salient<a id="change"> = </a><a id="change">center_img_salient.squeeze(1</a><a id="change">)</a>

            &#47&#47 Put image values into range [0, 1] and then normalize using 
            &#47&#47 mean and std for ImageNet
            &#47&#47 https://pytorch.org/docs/stable/torchvision/models.html

            center_img_salient = tensor_min_max_normalize(center_img_salient)
            normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
            center_img_salient = normalize(center_img_salient)

            &#47&#47print (&quotInput size&quot, input.size())      
            &#47&#47print (&quotsalient&quot, center_img_salient.size())    
            &#47&#47print(vid_path)

            &#47&#47 Visualize mask region
            &#47&#47center_img = vid_tensor_to_numpy(center_img_salient.unsqueeze(1))[0]
            &#47&#47center_img = cv2.cvtColor(center_img, cv2.COLOR_RGB2BGR)
            &#47&#47center_img = cv_f32_to_u8(center_img)
            &#47&#47cv2.imshow(&quotinput&quot, center_img)
            &#47&#47cv2.waitKey()

            &#47&#47&#47&#47 Visual mask
            &#47&#47&#47&#47mask = vid_tensor_to_numpy(input[3].unsqueeze(0))[0]
            &#47&#47&#47&#47print(mask)
            &#47&#47&#47&#47mask = cv2.merge([mask, mask, mask])

            center_img_salient<a id="change"> = </a>center_img_salient.unsqueeze(0)
            if cuda:
                center_img_salient = center_img_salient.to(device)
</code></pre><h3>After Change</h3><pre><code class='java'>
            batch_size = inputs.size(0)

            center_img_salient_batch = []
            <a id="change">for </a>i in <a id="change">range(</a>batch_size<a id="change">):
                </a>center_img_salient = rgb_center_img_tensor[i] * mask[i] 
                &#47&#47 Put image values into range [0, 1] and then normalize using 
                &#47&#47 mean and std for ImageNet
                &#47&#47 https://pytorch.org/docs/stable/torchvision/models.html</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/rvl-lab-utoronto/video_similarity_search/commit/18ec38547447c76ceed935f971282fce78095ca3#diff-98f1ab4b8aa34886d0f0a9d861b630e102aafdb9b6fed0569e138b1facf9d4efL49' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 100419254</div><div id='project'> Project Name: rvl-lab-utoronto/video_similarity_search</div><div id='commit'> Commit Name: 18ec38547447c76ceed935f971282fce78095ca3</div><div id='time'> Time: 2020-08-28</div><div id='author'> Author: salar77h@gmail.com</div><div id='file'> File Name: clustering/cluster_masks.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: get_embeddings_mask_regions(4)</div><div id='n_method'> N Method Name: get_embeddings_mask_regions(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: clustering/cluster_masks.py</div><div id='n_file'> N File Name: clustering/cluster_masks.py</div><div id='m_start'> M Start Line: 60</div><div id='m_end'> M End Line: 125</div><div id='n_start'> N Start Line: 49</div><div id='n_end'> N End Line: 111</div><BR>