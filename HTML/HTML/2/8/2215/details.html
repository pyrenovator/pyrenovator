<html><h3>Pattern ID :2215
</h3><img src='9526001.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if self.pos_emb is not None:
            tensor += self.pos_emb(tensor).type(tensor.type())

        <a id="change">for </a>layer in self.layers<a id="change">:
            </a>module = layer[0]
            norm<a id="change"> = layer[1]</a>

            before_module_tensor = <a id="change">tensor.clone().detach()</a>
            if self.checkpoint_level == "C1" or self.checkpoint_level == "C2":
                tensor = checkpoint(module, tensor)
            else:
                tensor = module(tensor, **kwargs)
            tensor<a id="change"> += </a>before_module_tensor
            tensor<a id="change"> = </a>norm(tensor)

        return tensor
</code></pre><h3>After Change</h3><pre><code class='java'>
        assert c == self.channels, "This tensor is of the wrong size. Dimension 2 has to match the `channels` flag"
        assert self.checkpoint_level == "C0" if kwargs else True, "Cannot run checkpointing when visualizing. Please set the checkpoint level to `C0`"

        <a id="change">for </a>layer in self.seq<a id="change">:
            </a>if self.checkpoint_level != "C0":
                tensor = checkpoint(layer, tensor)
            else:
                tensor = layer(tensor, **kwargs)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tatp22/linformer-pytorch/commit/8f3208acf42096adf54ab8fca11b5a509d8b4eb9#diff-1ce692e4c944ce1a9eafe989300fabd5878d3e81dff51127062721b098ce8962L216' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 9526001</div><div id='project'> Project Name: tatp22/linformer-pytorch</div><div id='commit'> Commit Name: 8f3208acf42096adf54ab8fca11b5a509d8b4eb9</div><div id='time'> Time: 2020-07-04</div><div id='author'> Author: tatp22@gmail.com</div><div id='file'> File Name: linformer_pytorch/linformer_pytorch.py</div><div id='m_class'> M Class Name: Linformer</div><div id='n_method'> N Class Name: Linformer</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: linformer_pytorch/linformer_pytorch.py</div><div id='n_file'> N File Name: linformer_pytorch/linformer_pytorch.py</div><div id='m_start'> M Start Line: 216</div><div id='m_end'> M End Line: 231</div><div id='n_start'> N Start Line: 249</div><div id='n_end'> N End Line: 254</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

            &#47&#47 group.setdefault(&quotparams_old&quot, group[&quotparams&quot])

            <a id="change">for </a>p in group[&quotparams&quot]<a id="change">:
                </a>if p.grad is None:
                    continue

                p_t = p
                param_state = self.state[p]
                if &quotparam_old&quot not in param_state:
                    param_state[&quotparam_old&quot]<a id="change"> = torch.clone(p).detach()</a>
                else:
                    p_t<a id="change"> = param_state[&quotparam_old&quot]</a>

                d_p = p.grad.data + lamda*(p-p_t)
                p.add_(d_p, alpha=-lr)
</code></pre><h3>After Change</h3><pre><code class='java'>
    @torch.no_grad()
    def step(self, global_params, device):
        for group in self.param_groups:
            <a id="change">for </a>p, g in zip(group[&quotparams&quot], global_params)<a id="change">:
                </a>g<a id="change"> = </a>g.to(device)
                d_p = p.grad.data + group[&quotmu&quot] * (p.data - g.data)
                p.data.add_(d_p, alpha=-group[&quotlr&quot])
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tsingz0/pfl-non-iid/commit/330ce2d6169fd4a54cec28895418bd38c206b6a4#diff-a99c19163c19c03eede69bdda22f1a2d923b8eb7d0f4b0463d797d66d5fc8e87L89' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 9526003</div><div id='project'> Project Name: tsingz0/pfl-non-iid</div><div id='commit'> Commit Name: 330ce2d6169fd4a54cec28895418bd38c206b6a4</div><div id='time'> Time: 2021-03-24</div><div id='author'> Author: 2719584131@qq.com</div><div id='file'> File Name: system/flcore/optimizers/fedoptimizer.py</div><div id='m_class'> M Class Name: PerturbedGradientDescent</div><div id='n_method'> N Class Name: PerturbedGradientDescent</div><div id='m_method'> M Method Name: step(3)</div><div id='n_method'> N Method Name: step(1)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: system/flcore/optimizers/fedoptimizer.py</div><div id='n_file'> N File Name: system/flcore/optimizers/fedoptimizer.py</div><div id='m_start'> M Start Line: 89</div><div id='m_end'> M End Line: 108</div><div id='n_start'> N Start Line: 116</div><div id='n_end'> N End Line: 121</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            dampening = group[&quotdampening&quot]
            nesterov = group[&quotnesterov&quot]

            <a id="change">for </a>p in group[&quotparams&quot]<a id="change">:
                </a>if p.grad is None:
                    continue
                d_p = p.grad
                if weight_decay != 0:
                    d_p = d_p.add(p, alpha=weight_decay)
                if momentum != 0:
                    param_state = self.state[p]
                    if &quotmomentum_buffer&quot not in param_state:
                        buf = param_state[&quotmomentum_buffer&quot]<a id="change"> = torch.clone(d_p).detach()</a>
                    else:
                        buf = param_state[&quotmomentum_buffer&quot]
                        buf.mul_(momentum).add_(d_p, alpha=1 - dampening)
                    if nesterov:
                        d_p = d_p.add(buf, alpha=momentum)
                    else:
                        d_p<a id="change"> = </a>buf

                p.add_(d_p, alpha=-group[&quotlr&quot])
</code></pre><h3>After Change</h3><pre><code class='java'>
                  nesterov)

            &#47&#47 update momentum_buffers in state
            <a id="change">for </a>p, momentum_buffer in zip(params_with_grad, momentum_buffer_list)<a id="change">:
                </a>state<a id="change"> = self.state[p]</a>
                state[&quotmomentum_buffer&quot] = momentum_buffer

        return loss
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/a0cf5566d88533c5caa7a490beb6eb0760eee9b4#diff-e5ea47a2193f1cfb039210c5c0ff83e8175739afc0551866052f6ad31bb91482L76' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 9526002</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: a0cf5566d88533c5caa7a490beb6eb0760eee9b4</div><div id='time'> Time: 2021-01-21</div><div id='author'> Author: wanchaol@users.noreply.github.com</div><div id='file'> File Name: torch/optim/sgd.py</div><div id='m_class'> M Class Name: SGD</div><div id='n_method'> N Class Name: SGD</div><div id='m_method'> M Method Name: step(2)</div><div id='n_method'> N Method Name: step(2)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: torch/optim/sgd.py</div><div id='n_file'> N File Name: torch/optim/sgd.py</div><div id='m_start'> M Start Line: 88</div><div id='m_end'> M End Line: 114</div><div id='n_start'> N Start Line: 89</div><div id='n_end'> N End Line: 124</div><BR>