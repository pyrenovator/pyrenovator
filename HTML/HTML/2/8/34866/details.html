<html><h3>Pattern ID :34866
</h3><img src='99914871.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def update_running_stats(self):
        &#47&#47 TODO: remove after getting step from optimizer works.
        super().inc_step_count()
        <a id="change">pass</a>


&#47&#47 TODO: add to adam...
def get_adamw_gap_aware_cls() -&gt; AdamWGapAware:</code></pre><h3>After Change</h3><pre><code class='java'>
        opt_s = self.optimizer.state
        ra = self.running_avg_step

        <a id="change">with torch</a><a id="change">.no_grad():
            for </a>pg in self.optimizer.param_groups<a id="change">:
                </a>beta1<a id="change">, beta2</a> = pg[&quotbetas&quot]

                if beta1 != 0:
                    for p in pg[&quotparams&quot]:
                        ra[id(p)].data<a id="change"> = </a>beta2 * ra[id(p)].data + \
                            (1 - beta2) * \
                            (opt_s[p]["exp_avg"].data ** 2)
                else:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/5ab23421874eadce8bed648b3eebccce714c7271#diff-22e803f6848fe13ddfd6473f05a519c62fd0e31bafe04851f4fcb25b161fa432L174' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 99914871</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 5ab23421874eadce8bed648b3eebccce714c7271</div><div id='time'> Time: 2020-03-17</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: pipeline/gap_aware/adamw_gap_aware.py</div><div id='m_class'> M Class Name: AdamWGapAware</div><div id='n_method'> N Class Name: AdamWGapAware</div><div id='m_method'> M Method Name: update_running_stats(1)</div><div id='n_method'> N Method Name: update_running_stats(1)</div><div id='m_parent_class'> M Parent Class: GapAwareBase</div><div id='n_parent_class'> N Parent Class: GapAwareBase</div><div id='m_file'> M File Name: pipeline/gap_aware/adamw_gap_aware.py</div><div id='n_file'> N File Name: pipeline/gap_aware/adamw_gap_aware.py</div><div id='m_start'> M Start Line: 188</div><div id='m_end'> M End Line: 189</div><div id='n_start'> N Start Line: 174</div><div id='n_end'> N End Line: 191</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def update_running_stats(self):
        &#47&#47 TODO: remove after getting step from optimizer works.
        super().inc_step_count()
        <a id="change">pass</a>


&#47&#47 TODO: add to adam...
def get_adamw_gap_aware_cls() -&gt; AdamWGapAware:</code></pre><h3>After Change</h3><pre><code class='java'>
        opt_s = self.optimizer.state
        ra = self.running_avg_step

        <a id="change">with torch</a><a id="change">.no_grad():
            for pg</a> in self.optimizer.param_groups<a id="change">:
                </a>beta1<a id="change">, beta2</a> = pg[&quotbetas&quot]

                if beta1 != 0:
                    for p in pg[&quotparams&quot]:
                        ra[id(p)].data = beta2 * ra[id(p)].data + \
                            (1 - beta2) * \
                            (opt_s[p]["exp_avg"].data ** 2)
                else:
                    for p in pg[&quotparams&quot]:
                        ra[id(p)].data<a id="change"> = </a>opt_s[p][&quotexp_avg_sq&quot].data


&#47&#47 TODO: add to adam...</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/5ab23421874eadce8bed648b3eebccce714c7271#diff-22e803f6848fe13ddfd6473f05a519c62fd0e31bafe04851f4fcb25b161fa432L186' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 99914876</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 5ab23421874eadce8bed648b3eebccce714c7271</div><div id='time'> Time: 2020-03-17</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: pipeline/gap_aware/adamw_gap_aware.py</div><div id='m_class'> M Class Name: AdamWGapAware</div><div id='n_method'> N Class Name: AdamWGapAware</div><div id='m_method'> M Method Name: update_running_stats(1)</div><div id='n_method'> N Method Name: update_running_stats(1)</div><div id='m_parent_class'> M Parent Class: GapAwareBase</div><div id='n_parent_class'> N Parent Class: GapAwareBase</div><div id='m_file'> M File Name: pipeline/gap_aware/adamw_gap_aware.py</div><div id='n_file'> N File Name: pipeline/gap_aware/adamw_gap_aware.py</div><div id='m_start'> M Start Line: 188</div><div id='m_end'> M End Line: 189</div><div id='n_start'> N Start Line: 174</div><div id='n_end'> N End Line: 191</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def update_running_stats(self):
        &#47&#47 TODO: remove after getting step from optimizer works.
        super().inc_step_count()
        <a id="change">pass</a>


def get_adam_gap_aware_cls() -&gt; AdamGapAware:
    gap_aware_cls = AdamGapAware</code></pre><h3>After Change</h3><pre><code class='java'>
        opt_s = self.optimizer.state
        ra = self.running_avg_step

        <a id="change">with torch</a><a id="change">.no_grad():
            for pg</a> in self.optimizer.param_groups<a id="change">:
                </a>beta1<a id="change">, beta2</a> = pg[&quotbetas&quot]

                if beta1 != 0:
                    for p in pg[&quotparams&quot]:
                        ra[id(p)].data<a id="change"> = </a>beta2 * ra[id(p)].data + \
                            (1 - beta2) * \
                            (opt_s[p]["exp_avg"].data ** 2)
                else:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/5ab23421874eadce8bed648b3eebccce714c7271#diff-65d217c40d70981a1b6f6ed49a6deb9244789b70496f926d6c2a87a525ab3980L150' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 99914877</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 5ab23421874eadce8bed648b3eebccce714c7271</div><div id='time'> Time: 2020-03-17</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: pipeline/gap_aware/adam_gap_aware.py</div><div id='m_class'> M Class Name: AdamGapAware</div><div id='n_method'> N Class Name: AdamGapAware</div><div id='m_method'> M Method Name: update_running_stats(1)</div><div id='n_method'> N Method Name: update_running_stats(1)</div><div id='m_parent_class'> M Parent Class: GapAwareBase</div><div id='n_parent_class'> N Parent Class: GapAwareBase</div><div id='m_file'> M File Name: pipeline/gap_aware/adam_gap_aware.py</div><div id='n_file'> N File Name: pipeline/gap_aware/adam_gap_aware.py</div><div id='m_start'> M Start Line: 152</div><div id='m_end'> M End Line: 153</div><div id='n_start'> N Start Line: 137</div><div id='n_end'> N End Line: 153</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        length = t.LongTensor([1] * batch_size, device=device)
        &#47&#47         length = t.full((batch_size), fill_value=1, dtype=t.long, device=device)
        probs = t.Tensor().to(device)
        <a id="change">pass</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47         length = t.full((batch_size), fill_value=1, dtype=t.long, device=device)
        probs = t.Tensor().to(device)
        &#47&#47 count = 0
        <a id="change">with t</a><a id="change">.no_grad():
            for i</a> in range(self.max_length)<a id="change">:
                </a>try:
                    token_mask = Masker.get_mask(length)
                    self_attention_mask = Masker.get_dot_mask(token_mask, token_mask)
                    last_prob<a id="change">, last_token_id = </a>self.decode_step(
                        token_id, encoder_output, token_mask, self_attention_mask, dot_attention_mask,
                        topk=beam_size, return_last=True)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tongjinle123/speech-transformer-pytorch_lightning/commit/d2bd78d0e9c23170f3b92013e76003b394fd4ff8#diff-ba6fc3d9a321d0d12af1d28f3c639883d825471f5e8ebe97e13f583cb3998da3L86' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 99914872</div><div id='project'> Project Name: tongjinle123/speech-transformer-pytorch_lightning</div><div id='commit'> Commit Name: d2bd78d0e9c23170f3b92013e76003b394fd4ff8</div><div id='time'> Time: 2020-02-17</div><div id='author'> Author: lancertong@live.com</div><div id='file'> File Name: src/model/modules/token_decoder.py</div><div id='m_class'> M Class Name: TokenDecoder</div><div id='n_method'> N Class Name: TokenDecoder</div><div id='m_method'> M Method Name: beam_search_decode(4)</div><div id='n_method'> N Method Name: beam_search_decode(3)</div><div id='m_parent_class'> M Parent Class: t.nn.Module</div><div id='n_parent_class'> N Parent Class: t.nn.Module</div><div id='m_file'> M File Name: src/model/modules/token_decoder.py</div><div id='n_file'> N File Name: src/model/modules/token_decoder.py</div><div id='m_start'> M Start Line: 93</div><div id='m_end'> M End Line: 93</div><div id='n_start'> N Start Line: 35</div><div id='n_end'> N End Line: 56</div><BR>