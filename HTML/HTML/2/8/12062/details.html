<html><h3>Pattern ID :12062
</h3><img src='40753133.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>


def run_test_locally(fn):
    <a id="change">if </a>not <a id="change">torch.cuda.is_available()</a>:
        print("skip tests since cuda is not available")
        return []

    nprocs = torch.cuda.device_count()
    os.environ["WORLD_SIZE"] = str(nprocs)
    os.environ["LOCAL_WORLD_SIZE"]<a id="change"> = </a>str(nprocs)
    os.environ["MASTER_ADDR"] = "127.0.0.1"
    os.environ["MASTER_PORT"]<a id="change"> = </a>str(find_free_port())
    os.environ["BAGUA_SERVICE_PORT"]<a id="change"> = </a>str(find_free_port())

    results = [Result() for _ in range(nprocs)]
    mp.spawn(</code></pre><h3>After Change</h3><pre><code class='java'>
    mp = multiprocessing.get_context("spawn")
    results = [Result() for _ in range(nprocs)]
    processes = []
    <a id="change">for i</a> in range(nprocs)<a id="change">:
        </a>p<a id="change"> = </a>mp.Process(
            target=fn,
            args=(i, nprocs, results, env),
        )</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/baguasys/bagua/commit/a9529bef66e367884316a2b2ebc917ff35bf6334#diff-7a16c2809e9e060c1dc2b8f83387654f4a97459627405dbf5948e60b05bcea7dL106' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 40753133</div><div id='project'> Project Name: baguasys/bagua</div><div id='commit'> Commit Name: a9529bef66e367884316a2b2ebc917ff35bf6334</div><div id='time'> Time: 2021-08-26</div><div id='author'> Author: 45031995+wangraying@users.noreply.github.com</div><div id='file'> File Name: tests/comm/test_communicator.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: run_test_locally(1)</div><div id='n_method'> N Method Name: run_test_locally(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: tests/comm/test_communicator.py</div><div id='n_file'> N File Name: tests/comm/test_communicator.py</div><div id='m_start'> M Start Line: 106</div><div id='m_end'> M End Line: 122</div><div id='n_start'> N Start Line: 115</div><div id='n_end'> N End Line: 138</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            &#47&#47 p = flow.cuda.get_device_properties(i)
            &#47&#47 s += f"{&quot&quot if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / (1 &lt;&lt; 20):.0f}MiB)\n"  &#47&#47 bytes to MB
        arg = &quotcuda:0&quot
    elif <a id="change"></a>mps and getattr(flow, &quothas_mps&quot, False) and <a id="change">flow.backends.mps.is_available()</a>:  &#47&#47 prefer MPS if available
        s<a id="change"> += </a>&quotMPS\n&quot
        arg<a id="change"> = </a>&quotmps&quot
    else:  &#47&#47 revert to CPU
        s += &quotCPU\n&quot
        arg<a id="change"> = </a>&quotcpu&quot

    if not newline:
        s = s.rstrip()</code></pre><h3>After Change</h3><pre><code class='java'>
        if n &gt; 1 and batch_size &gt; 0:  &#47&#47 check batch_size is divisible by device_count
            assert batch_size % n == 0, f&quotbatch-size {batch_size} not multiple of GPU count {n}&quot
        space = &quot &quot * (len(s) + 1)
        <a id="change">for </a>i, <a id="change">d</a> in enumerate(devices)<a id="change">:
            </a>s<a id="change"> += </a>f"{&quot&quot if i == 0 else space}CUDA:{d}\n"  &#47&#47 bytes to MB
    else:
        s += &quotCPU\n&quot
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/oneflow-inc/one-yolov5/commit/4f0088a5e5c32f67560514fcff66884e515f7138#diff-887acdc86b965c7bd7350cbe9458a749d11766e0a2551c353cf8a1f56caa7a75L63' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 40753134</div><div id='project'> Project Name: oneflow-inc/one-yolov5</div><div id='commit'> Commit Name: 4f0088a5e5c32f67560514fcff66884e515f7138</div><div id='time'> Time: 2022-07-31</div><div id='author'> Author: ccsuwen@gmail.com</div><div id='file'> File Name: utils/flow_utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: select_device(3)</div><div id='n_method'> N Method Name: select_device(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: utils/flow_utils.py</div><div id='n_file'> N File Name: utils/flow_utils.py</div><div id='m_start'> M Start Line: 65</div><div id='m_end'> M End Line: 96</div><div id='n_start'> N Start Line: 42</div><div id='n_end'> N End Line: 66</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

class TestAsyncModelAverage(unittest.TestCase):
    def test_algorithm(self):
        <a id="change">if </a>not <a id="change">torch.cuda.is_available()</a>:
            print("skip tests since cuda is not available")
            return

        nprocs = torch.cuda.device_count()
        os.environ["WORLD_SIZE"] = str(nprocs)
        os.environ["LOCAL_WORLD_SIZE"] = str(nprocs)
        os.environ["MASTER_ADDR"]<a id="change"> = </a>"127.0.0.1"
        os.environ["MASTER_PORT"]<a id="change"> = </a>str(find_free_port())
        os.environ["BAGUA_SERVICE_PORT"]<a id="change"> = </a>str(find_free_port())

        mp.spawn(
            run_model,</code></pre><h3>After Change</h3><pre><code class='java'>

        mp = multiprocessing.get_context("spawn")
        processes = []
        <a id="change">for i</a> in range(nprocs)<a id="change">:
            </a>p<a id="change"> = </a>mp.Process(target=run_model, args=(i, env))
            p.start()
            processes.append(p)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/baguasys/bagua/commit/a9529bef66e367884316a2b2ebc917ff35bf6334#diff-c46b7f4c320ba27d2783f4d47a513380419a948da235a60e6b47ccf8be3f194bL61' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 40753126</div><div id='project'> Project Name: baguasys/bagua</div><div id='commit'> Commit Name: a9529bef66e367884316a2b2ebc917ff35bf6334</div><div id='time'> Time: 2021-08-26</div><div id='author'> Author: 45031995+wangraying@users.noreply.github.com</div><div id='file'> File Name: tests/torch_api/test_async_model_average.py</div><div id='m_class'> M Class Name: TestAsyncModelAverage</div><div id='n_method'> N Class Name: TestAsyncModelAverage</div><div id='m_method'> M Method Name: test_algorithm(1)</div><div id='n_method'> N Method Name: test_algorithm(1)</div><div id='m_parent_class'> M Parent Class: unittest.TestCase</div><div id='n_parent_class'> N Parent Class: unittest.TestCase</div><div id='m_file'> M File Name: tests/torch_api/test_async_model_average.py</div><div id='n_file'> N File Name: tests/torch_api/test_async_model_average.py</div><div id='m_start'> M Start Line: 62</div><div id='m_end'> M End Line: 76</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 87</div><BR>