<html><h3>Pattern ID :5672
</h3><img src='19898351.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                Residual(<a id="change">PreNorm(dim</a>, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)<a id="change">)</a>),
                Residual(<a id="change">PreNorm(dim</a>, <a id="change">FeedForward(</a>dim, mlp_dim<a id="change">, dropout = dropout))</a>)
            ]))
    def forward(self, x):
        for attn, ff in self.layers:</code></pre><h3>After Change</h3><pre><code class='java'>
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                <a id="change">PreNorm(dim</a>, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)<a id="change">)</a>,
                <a id="change">PreNorm(dim</a>, <a id="change">FeedForward(</a>dim, mlp_dim<a id="change">, dropout = dropout))</a>
            ]))
    def forward(self, x, **kwargs):
        for attn, ff in self.layers:
            x = attn(x, **kwargs) + x</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/vit-pytorch/commit/3067155cea0c360f8a411404b8b27a6da9885f81#diff-597064871eb788c3c68b53fa983984ace3a6fbb451ba40e4c078c9fe4c3d4431L71' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 19898351</div><div id='project'> Project Name: lucidrains/vit-pytorch</div><div id='commit'> Commit Name: 3067155cea0c360f8a411404b8b27a6da9885f81</div><div id='time'> Time: 2021-03-29</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: vit_pytorch/vit.py</div><div id='m_class'> M Class Name: Transformer</div><div id='n_method'> N Class Name: Transformer</div><div id='m_method'> M Method Name: __init__(7)</div><div id='n_method'> N Method Name: __init__(7)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: vit_pytorch/vit.py</div><div id='n_file'> N File Name: vit_pytorch/vit.py</div><div id='m_start'> M Start Line: 72</div><div id='m_end'> M End Line: 73</div><div id='n_start'> N Start Line: 71</div><div id='n_end'> N End Line: 72</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                Residual(<a id="change">PreNorm(</a>dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)<a id="change">)</a>),
                Residual(<a id="change">PreNorm(</a>dim, <a id="change">FeedForward(</a>dim, mlp_dim<a id="change">, dropout = dropout))</a>)
            ]))
    def forward(self, x):
        for attn, ff in self.layers:</code></pre><h3>After Change</h3><pre><code class='java'>
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                <a id="change">PreNorm(</a>dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)<a id="change">)</a>,
                <a id="change">PreNorm(</a>dim, <a id="change">FeedForward(</a>dim, mlp_dim<a id="change">, dropout = dropout))</a>
            ]))
    def forward(self, x, **kwargs):
        for attn, ff in self.layers:
            x = attn(x, **kwargs) + x</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/vit-pytorch/commit/3067155cea0c360f8a411404b8b27a6da9885f81#diff-597064871eb788c3c68b53fa983984ace3a6fbb451ba40e4c078c9fe4c3d4431L67' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 19898350</div><div id='project'> Project Name: lucidrains/vit-pytorch</div><div id='commit'> Commit Name: 3067155cea0c360f8a411404b8b27a6da9885f81</div><div id='time'> Time: 2021-03-29</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: vit_pytorch/vit.py</div><div id='m_class'> M Class Name: Transformer</div><div id='n_method'> N Class Name: Transformer</div><div id='m_method'> M Method Name: __init__(7)</div><div id='n_method'> N Method Name: __init__(7)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: vit_pytorch/vit.py</div><div id='n_file'> N File Name: vit_pytorch/vit.py</div><div id='m_start'> M Start Line: 72</div><div id='m_end'> M End Line: 73</div><div id='n_start'> N Start Line: 71</div><div id='n_end'> N End Line: 72</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                raise ValueError(f&quotattention type "{attn_type}" is not valid&quot)

            layers.append(nn.ModuleList([
                <a id="change">PreNorm(</a>dim, attn_class(dim, causal = causal, seq_len = seq_len, heads = heads, dim_head = dim_head, dropout = attn_dropout)<a id="change">)</a>,
                <a id="change">PreNorm(</a>dim, <a id="change">FeedForward(</a>dim<a id="change">, mult = ff_mult, dropout = ff_dropout))</a>
            ]))

        execute_type = ReversibleSequence if reversible else SequentialSequence
        route_attn = ((True, False),) * depth</code></pre><h3>After Change</h3><pre><code class='java'>
                raise ValueError(f&quotattention type "{attn_type}" is not valid&quot)

            layers.append(nn.ModuleList([
                LayerScale(dim, ind + 1, <a id="change">PreNorm(</a>dim, attn_class(dim, causal = causal, seq_len = seq_len, heads = heads, dim_head = dim_head, dropout = attn_dropout)<a id="change">)</a>),
                LayerScale(dim, ind + 1, <a id="change">PreNorm(</a>dim, <a id="change">FeedForward(</a>dim<a id="change">, mult = ff_mult, dropout = ff_dropout))</a>)
            ]))

        execute_type = ReversibleSequence if reversible else SequentialSequence</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/dalle-pytorch/commit/b6ebcebea691f0ace45b27d6233deba75230e3f0#diff-ee397916e2ccc5603bb55b6ea44f20d581aae722f1e26d17108234ff0411b1beL55' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 19898349</div><div id='project'> Project Name: lucidrains/dalle-pytorch</div><div id='commit'> Commit Name: b6ebcebea691f0ace45b27d6233deba75230e3f0</div><div id='time'> Time: 2021-04-01</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: dalle_pytorch/transformer.py</div><div id='m_class'> M Class Name: Transformer</div><div id='n_method'> N Class Name: Transformer</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dalle_pytorch/transformer.py</div><div id='n_file'> N File Name: dalle_pytorch/transformer.py</div><div id='m_start'> M Start Line: 95</div><div id='m_end'> M End Line: 96</div><div id='n_start'> N Start Line: 97</div><div id='n_end'> N End Line: 113</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
class LinearAttentionTransformer(nn.Module):
    def __init__(self, dim, depth, max_seq_len, heads = 8, bucket_size = 64, causal = False, one_kv_head = False):
        super().__init__()
        self.attn_layers = nn.ModuleList([<a id="change">PreNorm(</a>dim, SelfAttention(dim, heads, causal, one_kv_head = one_kv_head)<a id="change">)</a> for _ in range(depth)])
        self.ff_layers = nn.ModuleList([<a id="change">PreNorm(</a>dim, <a id="change">FeedForward(</a>dim<a id="change">))</a> for _ in range(depth)])

    def forward(self, x, **kwargs):
        for attn, ff in zip(self.attn_layers, self.ff_layers):</code></pre><h3>After Change</h3><pre><code class='java'>

        for _ in range(depth):
            layer = nn.ModuleList([
                <a id="change">PreNorm(</a>dim, SelfAttention(dim, heads, causal, one_kv_head = one_kv_head)<a id="change">)</a>,
                <a id="change">PreNorm(</a>dim, <a id="change">FeedForward(</a>dim<a id="change">))</a>
            ])
            layers.append(layer)

        self.layers = ReversibleSequence(layers)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/linear-attention-transformer/commit/a0a35e7e9727b6428c7527ec34f5192529ab5e82#diff-eee6cf669463e9683bcf9643c4b19a493056f98ded32854287144d47f02a7032L189' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 19898355</div><div id='project'> Project Name: lucidrains/linear-attention-transformer</div><div id='commit'> Commit Name: a0a35e7e9727b6428c7527ec34f5192529ab5e82</div><div id='time'> Time: 2020-06-04</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='m_class'> M Class Name: LinearAttentionTransformer</div><div id='n_method'> N Class Name: LinearAttentionTransformer</div><div id='m_method'> M Method Name: __init__(8)</div><div id='n_method'> N Method Name: __init__(8)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='n_file'> N File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='m_start'> M Start Line: 191</div><div id='m_end'> M End Line: 192</div><div id='n_start'> N Start Line: 193</div><div id='n_end'> N End Line: 202</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                Residual(<a id="change">PreNorm(</a>dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)<a id="change">)</a>),
                Residual(<a id="change">PreNorm(</a>dim, <a id="change">FeedForward(</a>dim<a id="change">, dropout = ff_dropout))</a>),
            ]))

    def forward(self, x):</code></pre><h3>After Change</h3><pre><code class='java'>

        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                <a id="change">PreNorm(</a>dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)<a id="change">)</a>,
                <a id="change">PreNorm(</a>dim, <a id="change">FeedForward(</a>dim<a id="change">, dropout = ff_dropout))</a>,
            ]))

    def forward(self, x, return_attn = False):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/tab-transformer-pytorch/commit/e2e8b58e7d1b453e47bc4638164e10c19fce271b#diff-d87dc12538c5ed85349330c1ea686aed85875355e3b485c6d2ab4ae1d54231c6L88' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 19898353</div><div id='project'> Project Name: lucidrains/tab-transformer-pytorch</div><div id='commit'> Commit Name: e2e8b58e7d1b453e47bc4638164e10c19fce271b</div><div id='time'> Time: 2023-04-05</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: tab_transformer_pytorch/tab_transformer_pytorch.py</div><div id='m_class'> M Class Name: Transformer</div><div id='n_method'> N Class Name: Transformer</div><div id='m_method'> M Method Name: __init__(8)</div><div id='n_method'> N Method Name: __init__(8)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: tab_transformer_pytorch/tab_transformer_pytorch.py</div><div id='n_file'> N File Name: tab_transformer_pytorch/tab_transformer_pytorch.py</div><div id='m_start'> M Start Line: 95</div><div id='m_end'> M End Line: 96</div><div id='n_start'> N Start Line: 95</div><div id='n_end'> N End Line: 96</div><BR>