<html><h3>Pattern ID :21944
</h3><img src='69786312.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    if matched:
        return _all_gather_cpu(data)
    else:
        <a id="change">return </a>_all_gather_gpu(data)


def _all_gather_gpu(data):</code></pre><h3>After Change</h3><pre><code class='java'>
    Returns:
        gathered (list[data]): a list of data gathered from each rank
    
    <a id="change">if </a>get_world_size(group=group) == 1:
        return [data]

    <a id="change">tensor</a><a id="change">, group</a> = _serialize_to_tensor(data, group)
    size_list, tensor = _pad_tensors(tensor, group)
    max_size = max(size_list)

    tensor_list<a id="change"> = [tensor.new_empty([max_size]) for _ in size_list]</a>
    <a id="change">dist.all_gather(</a>tensor_list, <a id="change">tensor</a><a id="change">, group=group)</a>

    gathered = []
    for size, tensor in zip(size_list, tensor_list):
        buffer = tensor.cpu().numpy().tobytes()[:size]</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/yeliudev/nncore/commit/c60692996f4316b5befe701efbc18a9ad7cc7ac1#diff-fdf562e6b578fc92f4c74e59141f61b874fc8722e8eb5d843ef38c4c81d99321L47' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 69786312</div><div id='project'> Project Name: yeliudev/nncore</div><div id='commit'> Commit Name: c60692996f4316b5befe701efbc18a9ad7cc7ac1</div><div id='time'> Time: 2020-03-11</div><div id='author'> Author: goolhanrry@gmail.com</div><div id='file'> File Name: nncore/engine/comm.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: all_gather(2)</div><div id='n_method'> N Method Name: all_gather(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: nncore/engine/comm.py</div><div id='n_file'> N File Name: nncore/engine/comm.py</div><div id='m_start'> M Start Line: 47</div><div id='m_end'> M End Line: 78</div><div id='n_start'> N Start Line: 86</div><div id='n_end'> N End Line: 112</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        hidden_states, present = self.gpt_neox(
            input_ids, position_ids, cu_seqlens, max_s, past_key_values
        )
        <a id="change">return </a>self.embed_out(hidden_states), present
</code></pre><h3>After Change</h3><pre><code class='java'>
        hidden_states, present = self.gpt_neox(
            input_ids, position_ids, cu_seqlens, max_s, past_key_values
        )
        <a id="change">logits</a> = self.embed_out(hidden_states)

        <a id="change">if </a>self.gpt_neox.tp_embeddings:
            &#47&#47 Logits are sharded, so we need to gather them
            world_logits<a id="change"> = [torch.empty_like(logits) for _ in range(self.world_size)]</a>
            <a id="change">torch.distributed.all_gather(</a>world_logits, logits<a id="change">, group=self.process_group)</a>
            world_logits = torch.cat(world_logits, dim=1)

            return world_logits, present
        return logits<a id="change">, present</a>
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/huggingface/text-generation-inference/commit/9987960062e40de2deae030ab7e4ad6f57de0b20#diff-e24db7a565766a50b02649a0a1ac904f9bacff6ae58a2901fefc092befcbd980L660' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 69786307</div><div id='project'> Project Name: huggingface/text-generation-inference</div><div id='commit'> Commit Name: 9987960062e40de2deae030ab7e4ad6f57de0b20</div><div id='time'> Time: 2023-04-09</div><div id='author'> Author: olivier@huggingface.co</div><div id='file'> File Name: server/text_generation_server/models/custom_modeling/flash_neox_modeling.py</div><div id='m_class'> M Class Name: FlashGPTNeoXForCausalLM</div><div id='n_method'> N Class Name: FlashGPTNeoXForCausalLM</div><div id='m_method'> M Method Name: forward(6)</div><div id='n_method'> N Method Name: forward(6)</div><div id='m_parent_class'> M Parent Class: FlashGPTNeoXPreTrainedModel</div><div id='n_parent_class'> N Parent Class: FlashGPTNeoXPreTrainedModel</div><div id='m_file'> M File Name: server/text_generation_server/models/custom_modeling/flash_neox_modeling.py</div><div id='n_file'> N File Name: server/text_generation_server/models/custom_modeling/flash_neox_modeling.py</div><div id='m_start'> M Start Line: 671</div><div id='m_end'> M End Line: 671</div><div id='n_start'> N Start Line: 674</div><div id='n_end'> N End Line: 683</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        hidden_states, present = self.transformer(
            input_ids, position_ids, cu_seqlens, max_s, past_key_values
        )
        <a id="change">return </a>self.lm_head(hidden_states), present
</code></pre><h3>After Change</h3><pre><code class='java'>
        hidden_states, present = self.transformer(
            input_ids, position_ids, cu_seqlens, max_s, past_key_values
        )
        <a id="change">logits</a> = self.lm_head(hidden_states)

        <a id="change">if </a>self.transformer.tp_embeddings:
            &#47&#47 Logits are sharded, so we need to gather them
            world_logits<a id="change"> = [
                torch.empty_like(logits) for _ in range(self.transformer.tp_world_size)
            ]</a>
            <a id="change">torch.distributed.all_gather(
                </a>world_logits, logits<a id="change">, group=self.transformer.process_group
            )</a>
            world_logits = torch.cat(world_logits, dim=1)

            return world_logits, present

        return logits<a id="change">, present</a>
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/huggingface/text-generation-inference/commit/880a76eed5f058043367d9643be8a498b286bde2#diff-4294535c61ba845a58c3811602b0f1895f3ce53ab074172e37be99a81e32129fL344' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 69786308</div><div id='project'> Project Name: huggingface/text-generation-inference</div><div id='commit'> Commit Name: 880a76eed5f058043367d9643be8a498b286bde2</div><div id='time'> Time: 2023-04-12</div><div id='author'> Author: olivier@huggingface.co</div><div id='file'> File Name: server/text_generation_server/models/custom_modeling/flash_santacoder_modeling.py</div><div id='m_class'> M Class Name: FlashSantacoderForCausalLM</div><div id='n_method'> N Class Name: FlashSantacoderForCausalLM</div><div id='m_method'> M Method Name: forward(6)</div><div id='n_method'> N Method Name: forward(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: server/text_generation_server/models/custom_modeling/flash_santacoder_modeling.py</div><div id='n_file'> N File Name: server/text_generation_server/models/custom_modeling/flash_santacoder_modeling.py</div><div id='m_start'> M Start Line: 355</div><div id='m_end'> M End Line: 355</div><div id='n_start'> N Start Line: 526</div><div id='n_end'> N End Line: 540</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            break

    if matched:
        <a id="change">return </a>_all_gather_cpu(data)
    else:
        return _all_gather_gpu(data)
</code></pre><h3>After Change</h3><pre><code class='java'>
    Returns:
        gathered (list[data]): a list of data gathered from each rank
    
    <a id="change">if </a>get_world_size(group=group) == 1:
        return [data]

    tensor, group = _serialize_to_tensor(data, group)
    size_list<a id="change">, tensor</a> = _pad_tensors(tensor, group)
    max_size = max(size_list)

    tensor_list<a id="change"> = [tensor.new_empty([max_size]) for _ in size_list]</a>
    <a id="change">dist.all_gather(</a>tensor_list, tensor<a id="change">, group=group)</a>

    gathered = []
    for size, tensor in zip(size_list, tensor_list):
        buffer = tensor.cpu().numpy().tobytes()[:size]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/yeliudev/nncore/commit/c60692996f4316b5befe701efbc18a9ad7cc7ac1#diff-fdf562e6b578fc92f4c74e59141f61b874fc8722e8eb5d843ef38c4c81d99321L47' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 69786309</div><div id='project'> Project Name: yeliudev/nncore</div><div id='commit'> Commit Name: c60692996f4316b5befe701efbc18a9ad7cc7ac1</div><div id='time'> Time: 2020-03-11</div><div id='author'> Author: goolhanrry@gmail.com</div><div id='file'> File Name: nncore/engine/comm.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: all_gather(2)</div><div id='n_method'> N Method Name: all_gather(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: nncore/engine/comm.py</div><div id='n_file'> N File Name: nncore/engine/comm.py</div><div id='m_start'> M Start Line: 47</div><div id='m_end'> M End Line: 78</div><div id='n_start'> N Start Line: 86</div><div id='n_end'> N End Line: 112</div><BR>