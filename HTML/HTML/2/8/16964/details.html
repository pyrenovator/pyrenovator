<html><h3>Pattern ID :16964
</h3><img src='56996804.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            [sample.file_name for sample in samples],
            custom_metadata,
        )
        <a id="change">if </a>len(filename_to_metadata) != len(custom_metadata[COCO_ANNOTATION_KEYS.images]):
            <a id="change">raise </a><a id="change">ValueError(
                f&quotThere is a mismatch between the number of images &quot
                f&quotin the metadata file ({len(filename_to_metadata)}) and on the &quot
                f&quotserver ({len(custom_metadata[COCO_ANNOTATION_KEYS.images])}).&quot</a><a id="change">
            )</a>

        &#47&#47 retry upload if it times out
        def upload_sample_metadata(args):
            request, sample = args</code></pre><h3>After Change</h3><pre><code class='java'>
        upload_requests = []
        for metadata in custom_metadata[COCO_ANNOTATION_KEYS.custom_metadata]:
            image_id = metadata[COCO_ANNOTATION_KEYS.custom_metadata_image_id]
            filename<a id="change"> = </a>image_id_to_filename.get(image_id, None)
            <a id="change">if </a>filename is None:
                print_as_warning(
                    f&quotNo image found for custom metadata annotation &quot
                    f&quotwith image_id {image_id}. &quot
                    f&quotThis custom metadata annotation is skipped. &quot,
                    InvalidCustomMetadataWarning
                )
                <a id="change">continue</a>
            sample_id = filename_to_sample_id.get(filename, None)
            if sample_id is None:
                print_as_warning(
                    f&quotYou tried to upload custom metadata for a sample with &quot</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lightly-ai/lightly/commit/ad42377093f1168889faa601bcf86c637aabeb05#diff-be57b233f5bf6063e59932f261fbe614c60035e59b7f7355d4a19914570fff36L156' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 56996804</div><div id='project'> Project Name: lightly-ai/lightly</div><div id='commit'> Commit Name: ad42377093f1168889faa601bcf86c637aabeb05</div><div id='time'> Time: 2022-03-11</div><div id='author'> Author: malte.ebner@gmail.com</div><div id='file'> File Name: lightly/api/api_workflow_upload_metadata.py</div><div id='m_class'> M Class Name: _UploadCustomMetadataMixin</div><div id='n_method'> N Class Name: _UploadCustomMetadataMixin</div><div id='m_method'> M Method Name: upload_custom_metadata(4)</div><div id='n_method'> N Method Name: upload_custom_metadata(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: lightly/api/api_workflow_upload_metadata.py</div><div id='n_file'> N File Name: lightly/api/api_workflow_upload_metadata.py</div><div id='m_start'> M Start Line: 172</div><div id='m_end'> M End Line: 205</div><div id='n_start'> N Start Line: 156</div><div id='n_end'> N End Line: 195</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            [sample.file_name for sample in samples],
            custom_metadata,
        )
        <a id="change">if </a>len(filename_to_metadata) != len(custom_metadata[COCO_ANNOTATION_KEYS.images]):
            <a id="change">raise </a><a id="change">ValueError(
                f&quotThere is a mismatch between the number of images &quot
                f&quotin the metadata file ({len(filename_to_metadata)}) and on the &quot
                f&quotserver ({len(custom_metadata[COCO_ANNOTATION_KEYS.images])}).&quot</a><a id="change">
            )</a>

        &#47&#47 retry upload if it times out
        def upload_sample_metadata(args):
            request, sample = args</code></pre><h3>After Change</h3><pre><code class='java'>
        }

        upload_requests = []
        for <a id="change">metadata</a> in custom_metadata[COCO_ANNOTATION_KEYS.custom_metadata]:
            image_id = metadata[COCO_ANNOTATION_KEYS.custom_metadata_image_id]
            filename = image_id_to_filename.get(image_id, None)
            if filename is None:
                print_as_warning(
                    f&quotNo image found for custom metadata annotation &quot
                    f&quotwith image_id {image_id}. &quot
                    f&quotThis custom metadata annotation is skipped. &quot,
                    InvalidCustomMetadataWarning
                )
                continue
            sample_id<a id="change"> = </a>filename_to_sample_id.get(filename, None)
            <a id="change">if </a>sample_id is None:
                print_as_warning(
                    f&quotYou tried to upload custom metadata for a sample with &quot
                    f&quotfilename {{{filename}}}, &quot
                    f&quotbut a sample with this filename &quot
                    f&quotdoes not exist on the server. &quot
                    f&quotThis custom metadata annotation is skipped. &quot,
                    InvalidCustomMetadataWarning
                )
                <a id="change">continue</a>
            upload_request = (metadata, sample_id)
            upload_requests.append(upload_request)

        &#47&#47 retry upload if it times out</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lightly-ai/lightly/commit/ad42377093f1168889faa601bcf86c637aabeb05#diff-be57b233f5bf6063e59932f261fbe614c60035e59b7f7355d4a19914570fff36L116' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 56996801</div><div id='project'> Project Name: lightly-ai/lightly</div><div id='commit'> Commit Name: ad42377093f1168889faa601bcf86c637aabeb05</div><div id='time'> Time: 2022-03-11</div><div id='author'> Author: malte.ebner@gmail.com</div><div id='file'> File Name: lightly/api/api_workflow_upload_metadata.py</div><div id='m_class'> M Class Name: _UploadCustomMetadataMixin</div><div id='n_method'> N Class Name: _UploadCustomMetadataMixin</div><div id='m_method'> M Method Name: upload_custom_metadata(4)</div><div id='n_method'> N Method Name: upload_custom_metadata(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: lightly/api/api_workflow_upload_metadata.py</div><div id='n_file'> N File Name: lightly/api/api_workflow_upload_metadata.py</div><div id='m_start'> M Start Line: 172</div><div id='m_end'> M End Line: 205</div><div id='n_start'> N Start Line: 156</div><div id='n_end'> N End Line: 195</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                                phase, LoihiPyRuntimeService.Phase.HOST):

                            for rsp in self._get_pm_resp():
                                <a id="change">if </a>not enum_equal(rsp, MGMT_RESPONSE.DONE):
                                    if enum_equal(rsp, MGMT_RESPONSE.ERROR):
                                        &#47&#47 Forward error to runtime
                                        self.service_to_runtime.send(
                                            MGMT_RESPONSE.ERROR)
                                        &#47&#47 stop all other pm
                                        self._send_pm_cmd(MGMT_COMMAND.STOP)
                                        return
                                    else:
                                        <a id="change">raise </a><a id="change">ValueError(
                                            f"Wrong Response Received : {rsp}"</a><a id="change">)</a>

                        &#47&#47 If HOST phase (last time step ended) break the loop
                        if enum_equal(
                                phase, LoihiPyRuntimeService.Phase.HOST):</code></pre><h3>After Change</h3><pre><code class='java'>
                        enum_equal(command, MGMT_COMMAND.SET_DATA):
                    self._handle_get_set(phase, command)
                else:
                    self.paused<a id="change"> = </a>False
                    &#47&#47 The number of time steps was received ("command")
                    &#47&#47 Start iterating through Loihi phases
                    curr_time_step = 0
                    phase = LoihiPyRuntimeService.Phase.HOST
                    while True:
                        &#47&#47 Check if it is the last time step
                        is_last_ts = enum_equal(enum_to_np(curr_time_step),
                                                command)
                        &#47&#47 Advance to the next phase
                        phase = self._next_phase(is_last_ts)
                        if enum_equal(phase, MGMT_COMMAND.STOP):
                            self.service_to_runtime.send(MGMT_RESPONSE.REQ_STOP)
                            break
                        if enum_equal(phase, MGMT_COMMAND.PAUSE):
                            self.service_to_runtime.send(
                                MGMT_RESPONSE.REQ_PAUSE)
                            break
                        &#47&#47 Increase time step if spiking phase
                        if enum_equal(phase, LoihiPyRuntimeService.Phase.SPK):
                            curr_time_step += 1
                        &#47&#47 Inform ProcessModels about current phase
                        self._send_pm_cmd(phase)
                        &#47&#47 ProcessModels respond with DONE if not HOST phase
                        if not enum_equal(
                                phase, LoihiPyRuntimeService.Phase.HOST):
                            self._get_pm_resp()
                            if self._error:
                                &#47&#47 Forward error to runtime
                                self.service_to_runtime.send(
                                    MGMT_RESPONSE.ERROR)
                                &#47&#47 stop all other pm
                                self._send_pm_cmd(MGMT_COMMAND.STOP)
                                return
                        &#47&#47 Check if pause or stop received from Runtime
                        &#47&#47 TODO: Do we actualy need to wait for PMs to be in
                        &#47&#47 HOST or MGMT phase to stop or pause them?
                        if self.runtime_to_service.probe():
                            cmd = self.runtime_to_service.peek()
                            if enum_equal(cmd, MGMT_COMMAND.STOP):
                                self.runtime_to_service.recv()
                                self._handle_stop()
                                return
                            if enum_equal(cmd, MGMT_COMMAND.PAUSE):
                                self.runtime_to_service.recv()
                                self._handle_pause()
                                self.paused = True
                                break

                        &#47&#47 If HOST phase (last time step ended) break the loop
                        if enum_equal(
                                phase, LoihiPyRuntimeService.Phase.HOST):
                            break
                    <a id="change">if </a>self.paused or enum_equal(phase, MGMT_COMMAND.STOP) or \
                            enum_equal(phase, MGMT_COMMAND.PAUSE):
                        <a id="change">continue</a>
                    &#47&#47 Inform the runtime that last time step was reached
                    self.service_to_runtime.send(MGMT_RESPONSE.DONE)
            else:
                self.service_to_runtime.send(MGMT_RESPONSE.ERROR)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lava-nc/lava/commit/4956db0191ef2d0b51c026fb9c1bb50ea57e6c90#diff-92a2d9166185e8fd9185c65d0c6ed303a14113cdb72e19495a3a2f4fe32d2978L190' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 56996800</div><div id='project'> Project Name: lava-nc/lava</div><div id='commit'> Commit Name: 4956db0191ef2d0b51c026fb9c1bb50ea57e6c90</div><div id='time'> Time: 2022-02-17</div><div id='author'> Author: yashwardhan.singh@intel.com</div><div id='file'> File Name: src/lava/magma/runtime/runtime_service.py</div><div id='m_class'> M Class Name: LoihiPyRuntimeService</div><div id='n_method'> N Class Name: LoihiPyRuntimeService</div><div id='m_method'> M Method Name: run(1)</div><div id='n_method'> N Method Name: run(1)</div><div id='m_parent_class'> M Parent Class: PyRuntimeService</div><div id='n_parent_class'> N Parent Class: PyRuntimeService</div><div id='m_file'> M File Name: src/lava/magma/runtime/runtime_service.py</div><div id='n_file'> N File Name: src/lava/magma/runtime/runtime_service.py</div><div id='m_start'> M Start Line: 207</div><div id='m_end'> M End Line: 266</div><div id='n_start'> N Start Line: 296</div><div id='n_end'> N End Line: 360</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            probs_per_label = probs
            if score_fct == &quotllr&quot:
                s = self._log_likelihood_ratio(probs_per_label, label_mask, normalize)
            elif <a id="change"></a>score_fct == &quotce&quot:
                s = self._cross_entropy(probs_per_label, label_mask, normalize)
            else:
                <a id="change">raise </a><a id="change">ValueError(f"Score function &quot{score_fct}&quot not implemented"</a><a id="change">)</a>
            sorted_ids  = torch.argsort(s, descending=True)[:words_per_label]
            label_words_ids.append(sorted_ids.cpu().numpy().tolist())
        return label_words_ids
</code></pre><h3>After Change</h3><pre><code class='java'>
        for label_id in torch.unique(self.labels_buffer):
            scores = self.probs_buffer[self.labels_buffer==label_id].mean(axis=0).cpu().numpy()
            kept = []
            for <a id="change">i</a> in np.argsort(-scores):
                word<a id="change"> = </a>self.tokenizer.convert_ids_to_tokens([i])[0]
                <a id="change">if </a>self.invalid_label_word(word):
                    <a id="change">continue</a>
                kept.append(i)
            label_words_ids.append(kept[:self.label_word_num_per_class])
        return label_words_ids
    </code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thunlp/openprompt/commit/35f140a0928724204ab887f5ed0d45d469aecc8b#diff-0060b001b7bcb59bcad42e574d4b6d2e4b7a1b8fdea8d3082342eeb80c703efdL309' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 56996803</div><div id='project'> Project Name: thunlp/openprompt</div><div id='commit'> Commit Name: 35f140a0928724204ab887f5ed0d45d469aecc8b</div><div id='time'> Time: 2021-10-23</div><div id='author'> Author: yl-chen17@mails.tsinghua.edu.cn</div><div id='file'> File Name: openprompt/prompts/prompt_generator.py</div><div id='m_class'> M Class Name: VerbalizerGenerator</div><div id='n_method'> N Class Name: VerbalizerGenerator</div><div id='m_method'> M Method Name: _get_top_words(1)</div><div id='n_method'> N Method Name: _get_top_words(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: openprompt/prompts/prompt_generator.py</div><div id='n_file'> N File Name: openprompt/prompts/prompt_generator.py</div><div id='m_start'> M Start Line: 309</div><div id='m_end'> M End Line: 325</div><div id='n_start'> N Start Line: 308</div><div id='n_end'> N End Line: 318</div><BR>