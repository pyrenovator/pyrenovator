<html><h3>Pattern ID :21466
</h3><img src='68790601.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    )

                    loss.backward(retain_graph=True)
                    <a id="change">opt_discriminator.step()</a>

                    if opt_attribute_discriminator is not None:
                        opt_attribute_discriminator.zero_grad()
                        &#47&#47 Exclude features (last element of batches) for</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 Set torch modules to training mode
        self._set_mode(True)
        <a id="change">scaler = </a><a id="change">torch.cuda.amp.GradScaler(enabled=self.config.mixed_precision_training)</a>

        for epoch in range(self.config.epochs):
            logger.info(f"epoch: {epoch}")

            for real_batch in loader:
                global_step += 1
                with torch.cuda.amp.autocast(
                    enabled=self.config.mixed_precision_training
                ):
                    attribute_noise = self.attribute_noise_func(
                        real_batch[0].shape[0]
                    ).to(self.device, non_blocking=True)
                    feature_noise = self.feature_noise_func(real_batch[0].shape[0]).to(
                        self.device, non_blocking=True
                    )

                    &#47&#47 Both real and generated batch are always three element tuple of
                    &#47&#47 tensors. The tuple is structured as follows: (attribute_output,
                    &#47&#47 additional_attribute_output, feature_output). If self.attribute_output
                    &#47&#47 and/or self.additional_attribute_output is empty, the respective
                    &#47&#47 tuple index will be filled with a placeholder nan-filled tensor.
                    &#47&#47 These nan-filled tensors get filtered out in the _discriminate,
                    &#47&#47 _get_gradient_penalty, and _discriminate_attributes functions.

                    generated_batch = self.generator(attribute_noise, feature_noise)
                    real_batch = [
                        x.to(self.device, non_blocking=True) for x in real_batch
                    ]

                for _ in range(self.config.discriminator_rounds):
                    opt_discriminator.zero_grad(
                        set_to_none=self.config.mixed_precision_training
                    )
                    with torch.cuda.amp.autocast(enabled=True):
                        generated_output = self._discriminate(generated_batch)
                        real_output = self._discriminate(real_batch)

                        loss_generated = torch.mean(generated_output)
                        loss_real = -torch.mean(real_output)
                        loss_gradient_penalty = self._get_gradient_penalty(
                            generated_batch, real_batch, self._discriminate
                        )

                        loss = (
                            loss_generated
                            + loss_real
                            + self.config.gradient_penalty_coef * loss_gradient_penalty
                        )

                    scaler.scale(loss).backward(retain_graph=True)
                    scaler.step(opt_discriminator)
                    <a id="change">scaler.update()</a>

                    if opt_attribute_discriminator is not None:
                        opt_attribute_discriminator.zero_grad(set_to_none=False)
                        &#47&#47 Exclude features (last element of batches) for
                        &#47&#47 attribute discriminator
                        with torch.cuda.amp.autocast(
                            enabled=self.config.mixed_precision_training
                        ):
                            generated_output = self._discriminate_attributes(
                                generated_batch[:-1]
                            )
                            real_output = self._discriminate_attributes(real_batch[:-1])

                            loss_generated = torch.mean(generated_output)
                            loss_real = -torch.mean(real_output)
                            loss_gradient_penalty = self._get_gradient_penalty(
                                generated_batch[:-1],
                                real_batch[:-1],
                                self._discriminate_attributes,
                            )

                            attribute_loss = (
                                loss_generated
                                + loss_real
                                + self.config.attribute_gradient_penalty_coef
                                * loss_gradient_penalty
                            )

                        scaler.scale(attribute_loss).backward(retain_graph=True)
                        <a id="change">scaler.step(</a>opt_attribute_discriminator<a id="change">)</a>
                        scaler.update()

                for _ in range(self.config.generator_rounds):
                    opt_generator.zero_grad(set_to_none=False)
                    with torch.cuda.amp.autocast(
                        enabled=self.config.mixed_precision_training
                    ):
                        generated_output = self._discriminate(generated_batch)

                        if self.attribute_discriminator:
                            &#47&#47 Exclude features (last element of batch) before
                            &#47&#47 calling attribute discriminator
                            attribute_generated_output = self._discriminate_attributes(
                                generated_batch[:-1]
                            )

                            loss = -torch.mean(
                                generated_output
                            ) + self.config.attribute_loss_coef * -torch.mean(
                                attribute_generated_output
                            )
                        else:
                            loss = -torch.mean(generated_output)

                    <a id="change">scaler.scale(loss).backward()</a>
                    scaler.step(opt_generator)
                    scaler.update()

    def _generate(</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/gretelai/gretel-synthetics/commit/6e6bbfc93aa69a7ea98907eb2e8e7dcc8350fbc7#diff-9728e4048dbb5458e1e874742fccf009eeab8ca1b80d1627f0f1d27af60ac3d0L535' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 68790601</div><div id='project'> Project Name: gretelai/gretel-synthetics</div><div id='commit'> Commit Name: 6e6bbfc93aa69a7ea98907eb2e8e7dcc8350fbc7</div><div id='time'> Time: 2022-06-23</div><div id='author'> Author: santhoshsubramanian101@gmail.com</div><div id='file'> File Name: src/gretel_synthetics/timeseries_dgan/dgan.py</div><div id='m_class'> M Class Name: DGAN</div><div id='n_method'> N Class Name: DGAN</div><div id='m_method'> M Method Name: _train(2)</div><div id='n_method'> N Method Name: _train(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/gretel_synthetics/timeseries_dgan/dgan.py</div><div id='n_file'> N File Name: src/gretel_synthetics/timeseries_dgan/dgan.py</div><div id='m_start'> M Start Line: 535</div><div id='m_end'> M End Line: 646</div><div id='n_start'> N Start Line: 543</div><div id='n_end'> N End Line: 676</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                        if n == "bias":
                            p.grad.mul_(module.weight_mask[:, 0, 0, 0])
        
        <a id="change">optimizer.step()</a>
        scheduler.step()
        
        &#47&#47 Test
        model.eval()</code></pre><h3>After Change</h3><pre><code class='java'>
    
    train_loader, test_loader = get_data_loaders(os.path.join(config.root, "ImageNet"), 256, 256, 0, True, 8, False)
    optimizer = SGD(model.parameters(), lr=0.1, weight_decay=1e-4)
    <a id="change">scaler = </a><a id="change">torch.cuda.amp.GradScaler()</a>

    scheduler = CosineAnnealingLR(optimizer, train_iteration, 1e-3)
    criterion = CrossEntropyLoss()
    
    total_neurons = 0
    remaining_neurons = 0
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d):
            total_neurons += module.weight.shape[0]
            remaining_neurons += module.weight.shape[0]
    
    wandb.init()
    wandb.watch(model)
    
    &#47&#47 Train
    for i, (images, target) in enumerate(tqdm(train_loader)):
        
        model.train()
        images, target = images.to(device), target.to(device)

        &#47&#47 Prune the network by 5% at each pass
        if i + 1 % prune_iteration == 0:
            print("Pruning")
            
            remaining_neurons = 0
            for name, module in model.named_modules():
                if isinstance(module, nn.Conv2d):
                    prune.ln_structured(module, &quotweight&quot, amount=0.05, n=2, dim=0)
                    ch_sum = module.weight.sum(dim=(1, 2, 3))
                    remaining_neurons += ch_sum[ch_sum != 0].shape[0]
            
            print(f"The current model has {(remaining_neurons / total_neurons) * 100} % of the original neurons")
            
            optimizer = SGD(model.parameters(), lr=0.1, weight_decay=1e-4)
            scheduler = CosineAnnealingLR(optimizer, train_iteration, 1e-3, last_epoch=i-1)
        
        with torch.cuda.amp.autocast():
            start = time.time()
            output = model(images)
            forward_time = time.time() - start
            
            loss = criterion(output, target)
            optimizer.zero_grad()
            
            start = time.time()
            <a id="change">scaler.scale(loss).backward()</a>
            backward_time = time.time() - start
        
        &#47&#47 Set to 0 the gradient of pruned neurons
        with torch.no_grad():
            for name, module in model.named_modules():
                if isinstance(module, nn.Conv2d):
                    for n, p in module.named_parameters():
                        if n == "weight_orig":
                            p.grad.mul_(module.weight_mask)
                        if n == "bias":
                            p.grad.mul_(module.weight_mask[:, 0, 0, 0])
        
        <a id="change">scaler.step(</a>optimizer<a id="change">)</a>
        <a id="change">scaler.update()</a>
        scheduler.step()
        
        &#47&#47 Test
        model.eval()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/eidoslab/simplify/commit/8c1cf93f9a2545c52c01379b934939040a76cc77#diff-41474220006d3f313aca51ff34722bbf4cb675023f4106ffd78a318ae920faccL39' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 68790600</div><div id='project'> Project Name: eidoslab/simplify</div><div id='commit'> Commit Name: 8c1cf93f9a2545c52c01379b934939040a76cc77</div><div id='time'> Time: 2021-07-01</div><div id='author'> Author: carlo.alberto.barbano@outlook.com</div><div id='file'> File Name: training/train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: training/train.py</div><div id='n_file'> N File Name: training/train.py</div><div id='m_start'> M Start Line: 55</div><div id='m_end'> M End Line: 112</div><div id='n_start'> N Start Line: 55</div><div id='n_end'> N End Line: 116</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                    )

                    loss.backward(retain_graph=True)
                    <a id="change">opt_discriminator.step()</a>

                    if opt_attribute_discriminator is not None:
                        opt_attribute_discriminator.zero_grad()
                        &#47&#47 Exclude features (last element of batches) for</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 Set torch modules to training mode
        self._set_mode(True)
        <a id="change">scaler = </a><a id="change">torch.cuda.amp.GradScaler(enabled=self.config.mixed_precision_training)</a>

        for epoch in range(self.config.epochs):
            logger.info(f"epoch: {epoch}")

            for real_batch in loader:
                global_step += 1
                with torch.cuda.amp.autocast(
                    enabled=self.config.mixed_precision_training
                ):
                    attribute_noise = self.attribute_noise_func(
                        real_batch[0].shape[0]
                    ).to(self.device, non_blocking=True)
                    feature_noise = self.feature_noise_func(real_batch[0].shape[0]).to(
                        self.device, non_blocking=True
                    )

                    &#47&#47 Both real and generated batch are always three element tuple of
                    &#47&#47 tensors. The tuple is structured as follows: (attribute_output,
                    &#47&#47 additional_attribute_output, feature_output). If self.attribute_output
                    &#47&#47 and/or self.additional_attribute_output is empty, the respective
                    &#47&#47 tuple index will be filled with a placeholder nan-filled tensor.
                    &#47&#47 These nan-filled tensors get filtered out in the _discriminate,
                    &#47&#47 _get_gradient_penalty, and _discriminate_attributes functions.

                    generated_batch = self.generator(attribute_noise, feature_noise)
                    real_batch = [
                        x.to(self.device, non_blocking=True) for x in real_batch
                    ]

                for _ in range(self.config.discriminator_rounds):
                    opt_discriminator.zero_grad(
                        set_to_none=self.config.mixed_precision_training
                    )
                    with torch.cuda.amp.autocast(enabled=True):
                        generated_output = self._discriminate(generated_batch)
                        real_output = self._discriminate(real_batch)

                        loss_generated = torch.mean(generated_output)
                        loss_real = -torch.mean(real_output)
                        loss_gradient_penalty = self._get_gradient_penalty(
                            generated_batch, real_batch, self._discriminate
                        )

                        loss = (
                            loss_generated
                            + loss_real
                            + self.config.gradient_penalty_coef * loss_gradient_penalty
                        )

                    scaler.scale(loss).backward(retain_graph=True)
                    <a id="change">scaler.step(</a>opt_discriminator<a id="change">)</a>
                    <a id="change">scaler.update()</a>

                    if opt_attribute_discriminator is not None:
                        opt_attribute_discriminator.zero_grad(set_to_none=False)
                        &#47&#47 Exclude features (last element of batches) for
                        &#47&#47 attribute discriminator
                        with torch.cuda.amp.autocast(
                            enabled=self.config.mixed_precision_training
                        ):
                            generated_output = self._discriminate_attributes(
                                generated_batch[:-1]
                            )
                            real_output = self._discriminate_attributes(real_batch[:-1])

                            loss_generated = torch.mean(generated_output)
                            loss_real = -torch.mean(real_output)
                            loss_gradient_penalty = self._get_gradient_penalty(
                                generated_batch[:-1],
                                real_batch[:-1],
                                self._discriminate_attributes,
                            )

                            attribute_loss = (
                                loss_generated
                                + loss_real
                                + self.config.attribute_gradient_penalty_coef
                                * loss_gradient_penalty
                            )

                        scaler.scale(attribute_loss).backward(retain_graph=True)
                        scaler.step(opt_attribute_discriminator)
                        scaler.update()

                for _ in range(self.config.generator_rounds):
                    opt_generator.zero_grad(set_to_none=False)
                    with torch.cuda.amp.autocast(
                        enabled=self.config.mixed_precision_training
                    ):
                        generated_output = self._discriminate(generated_batch)

                        if self.attribute_discriminator:
                            &#47&#47 Exclude features (last element of batch) before
                            &#47&#47 calling attribute discriminator
                            attribute_generated_output = self._discriminate_attributes(
                                generated_batch[:-1]
                            )

                            loss = -torch.mean(
                                generated_output
                            ) + self.config.attribute_loss_coef * -torch.mean(
                                attribute_generated_output
                            )
                        else:
                            loss = -torch.mean(generated_output)

                    <a id="change">scaler.scale(loss).backward()</a>
                    scaler.step(opt_generator)
                    scaler.update()

    def _generate(</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/gretelai/gretel-synthetics/commit/6e6bbfc93aa69a7ea98907eb2e8e7dcc8350fbc7#diff-9728e4048dbb5458e1e874742fccf009eeab8ca1b80d1627f0f1d27af60ac3d0L514' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 68790603</div><div id='project'> Project Name: gretelai/gretel-synthetics</div><div id='commit'> Commit Name: 6e6bbfc93aa69a7ea98907eb2e8e7dcc8350fbc7</div><div id='time'> Time: 2022-06-23</div><div id='author'> Author: santhoshsubramanian101@gmail.com</div><div id='file'> File Name: src/gretel_synthetics/timeseries_dgan/dgan.py</div><div id='m_class'> M Class Name: DGAN</div><div id='n_method'> N Class Name: DGAN</div><div id='m_method'> M Method Name: _train(2)</div><div id='n_method'> N Method Name: _train(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/gretel_synthetics/timeseries_dgan/dgan.py</div><div id='n_file'> N File Name: src/gretel_synthetics/timeseries_dgan/dgan.py</div><div id='m_start'> M Start Line: 535</div><div id='m_end'> M End Line: 646</div><div id='n_start'> N Start Line: 543</div><div id='n_end'> N End Line: 676</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

            loss = seg_loss(pred, labels) + 0.4 * seg_loss(aux, labels)
            loss.backward()
            <a id="change">optimizer.step()</a>

            print(
                f"epoch = {epoch:2d}, iter = {i_iter:6d}/{args.num_epochs * len(trainloader.dataset):6d}, {i_iter/(args.num_epochs * len(trainloader.dataset)):2.2%}, loss_seg = {loss:.3f}, lr = {lr:.6f}")</code></pre><h3>After Change</h3><pre><code class='java'>
    interp = nn.Upsample(size=(input_size, input_size),
                         mode=&quotbilinear&quot, align_corners=True)

    <a id="change">scaler = </a><a id="change">torch.cuda.amp.GradScaler()</a>

    i_iter = 0
    print(len(trainloader))
    for epoch in range(args.num_epochs):
        for images, labels, _, _, _ in trainloader:

            optimizer.zero_grad()

            i_iter += images.shape[0]
            lr = adjust_learning_rate(
                args, optimizer, i_iter, args.num_epochs * len(trainloader.dataset))

            with torch.cuda.amp.autocast():
                images = images.cuda()
                labels = labels.cuda()

                aux, pred = model(images)
                pred = interp(pred)
                aux = interp(aux)

                loss = seg_loss(pred, labels) + 0.4 * seg_loss(aux, labels)

                <a id="change">scaler.scale(loss).backward()</a>
                <a id="change">scaler.step(</a>optimizer<a id="change">)</a>

                <a id="change">scaler.update()</a>

            print(
                f"epoch = {epoch:2d}, iter = {i_iter:6d}/{args.num_epochs * len(trainloader.dataset):6d}, {i_iter/(args.num_epochs * len(trainloader.dataset)):2.2%}, loss_seg = {loss:.3f}, lr = {lr:.6f}")</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/smhassanerfani/atlantis/commit/35a88733c2d4ab90e1ab4a38c2b07faa89eb1d50#diff-ed183d67207df065a11e1289f19d34cc2abbc5448dea952683cfe9728c342b95L65' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 68790602</div><div id='project'> Project Name: smhassanerfani/atlantis</div><div id='commit'> Commit Name: 35a88733c2d4ab90e1ab4a38c2b07faa89eb1d50</div><div id='time'> Time: 2021-09-13</div><div id='author'> Author: serfani@email.sc.edu</div><div id='file'> File Name: train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(0)</div><div id='n_method'> N Method Name: main(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: train.py</div><div id='n_file'> N File Name: train.py</div><div id='m_start'> M Start Line: 111</div><div id='m_end'> M End Line: 138</div><div id='n_start'> N Start Line: 111</div><div id='n_end'> N End Line: 146</div><BR>