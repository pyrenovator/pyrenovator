<html><h3>Pattern ID :40963
</h3><img src='115470433.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        **kwargs: Any,
    ) -&gt; Tuple[tf.Tensor, tf.Tensor]:

        seq_len = <a id="change">tf.shape(x)[1]</a>

        x = self.embedding(x, **kwargs)  &#47&#47 (batch_size, target_seq_len, d_model)
        x *= tf.math.sqrt(tf.cast(self.d_model, x.dtype))
        x += tf.cast(self.pos_encoding[:, :seq_len, :], dtype=x.dtype)</code></pre><h3>After Change</h3><pre><code class='java'>
    ) -&gt; tf.Tensor:

        tgt = self.embed(tgt, **kwargs) * math.sqrt(self.d_model)
        pos_enc_tgt<a id="change"> = </a>self.positional_encoding(tgt, **kwargs)
        output = pos_enc_tgt

        for i in range(self.num_layers):
            normed_output = self.layer_norm(output, **kwargs)
            output = output + self.dropout(
                self.attention[i](normed_output, normed_output, normed_output, target_mask, **kwargs),
                **kwargs,
            )
            normed_output = self.layer_norm(output, **kwargs)
            output<a id="change"> = </a>output + <a id="change">self.dropout(
                self.source_attention[i](</a>normed_output, memory, memory, source_mask<a id="change">, **kwargs)</a><a id="change">,
                **kwargs,
            )</a>
            normed_output = self.layer_norm(output, **kwargs)
            output<a id="change"> = </a>output + self.dropout(self.position_feed_forward[i](normed_output, **kwargs), **kwargs)

        &#47&#47 (batch_size, seq_len, d_model)
        <a id="change">return </a>self.layer_norm(output, **kwargs)
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/mindee/doctr/commit/9530f81d15395006b4844299236bdadba11c1dde#diff-e1106a3628d5365c5fe2e4fec7304ea8e40c58a5439f23d8f20513e722955178L160' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 115470433</div><div id='project'> Project Name: mindee/doctr</div><div id='commit'> Commit Name: 9530f81d15395006b4844299236bdadba11c1dde</div><div id='time'> Time: 2022-07-01</div><div id='author'> Author: felixdittrich92@gmail.com</div><div id='file'> File Name: doctr/models/recognition/transformer/tensorflow.py</div><div id='m_class'> M Class Name: Decoder</div><div id='n_method'> N Class Name: Decoder</div><div id='m_method'> M Method Name: call(5)</div><div id='n_method'> N Method Name: call(5)</div><div id='m_parent_class'> M Parent Class: NestedObject,layers.Layer</div><div id='n_parent_class'> N Parent Class: tf.keras.layers.Layer</div><div id='m_file'> M File Name: doctr/models/recognition/transformer/tensorflow.py</div><div id='n_file'> N File Name: doctr/models/recognition/transformer/tensorflow.py</div><div id='m_start'> M Start Line: 251</div><div id='m_end'> M End Line: 265</div><div id='n_start'> N Start Line: 160</div><div id='n_end'> N End Line: 179</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        padding_mask: Optional[torch.Tensor] = None,
    ) -&gt; torch.Tensor:

        seq_len = <a id="change">x.shape[1]</a>  &#47&#47 Batch first = True

        x = self.embedding(x)  &#47&#47 (batch_size, target_seq_len, d_model)
        x *= math.sqrt(self.d_model)</code></pre><h3>After Change</h3><pre><code class='java'>

        tgt = self.embed(tgt) * math.sqrt(self.d_model)
        pos_enc_tgt = self.positional_encoding(tgt)
        output<a id="change"> = </a>pos_enc_tgt

        for i in range(self.num_layers):
            normed_output = self.layer_norm(output)
            output = output + self.dropout(
                self.attention[i](normed_output, normed_output, normed_output, target_mask)
            )
            normed_output = self.layer_norm(output)
            output = output + <a id="change">self.dropout(
                self.source_attention[i](</a>normed_output, memory, memory, source_mask<a id="change">)</a><a id="change">
            )</a>
            normed_output<a id="change"> = </a>self.layer_norm(output)
            output<a id="change"> = </a>output + self.dropout(self.position_feed_forward[i](normed_output))

        <a id="change">return </a>self.layer_norm(output)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/mindee/doctr/commit/fddceba7bee5098b4219b7ba6a0bdf4f4a98adfe#diff-5eb4b6b7a215017ac91351a0c5e665af04fa644ddf1ecd5037545ba096ec1337L69' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 115470401</div><div id='project'> Project Name: mindee/doctr</div><div id='commit'> Commit Name: fddceba7bee5098b4219b7ba6a0bdf4f4a98adfe</div><div id='time'> Time: 2022-06-09</div><div id='author'> Author: felixdittrich92@gmail.com</div><div id='file'> File Name: doctr/models/recognition/transformer/pytorch.py</div><div id='m_class'> M Class Name: Decoder</div><div id='n_method'> N Class Name: Decoder</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: doctr/models/recognition/transformer/pytorch.py</div><div id='n_file'> N File Name: doctr/models/recognition/transformer/pytorch.py</div><div id='m_start'> M Start Line: 74</div><div id='m_end'> M End Line: 91</div><div id='n_start'> N Start Line: 147</div><div id='n_end'> N End Line: 167</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        for l in range(self.n_layers - 1):
            h = self.layers[l](blocks[l], h).flatten(1)

        logits = self.layers[-1](<a id="change">blocks[-1]</a>, h).mean(1)

        return logits
</code></pre><h3>After Change</h3><pre><code class='java'>

    def forward(self, blocks, features):
        h = features
        h<a id="change"> = </a>self.input_drop(h)

        h_last = None

        for l in range(self.n_layers):
            h = self.layers[l](blocks[l], h).flatten(1, -1)

            if h_last is not None:
                h += h_last[: h.shape[0], :]

            h_last = h

            h = <a id="change">self.norms[l](</a>h<a id="change">)</a>
            h = self.activation(h)
            h<a id="change"> = </a><a id="change">self.dropout(</a>h<a id="change">)</a>

        &#47&#47 logits = self.layers[-1](blocks[-1], h).mean(1)
        h<a id="change"> = </a>self.pred_linear(h)

        <a id="change">return </a>h
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/ytchx1999/maxp_dgl_graph/commit/880fef62dc7b31a2f406895038f16195662ebab0#diff-79e8e0d24a77796bb1fbc9cf4b5504d090f72acbca6cd3dc4db16e6b1b096fa6L219' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 115470439</div><div id='project'> Project Name: ytchx1999/maxp_dgl_graph</div><div id='commit'> Commit Name: 880fef62dc7b31a2f406895038f16195662ebab0</div><div id='time'> Time: 2021-10-13</div><div id='author'> Author: 54234005+ytchx1999@users.noreply.github.com</div><div id='file'> File Name: gnn/models.py</div><div id='m_class'> M Class Name: GraphAttnModel</div><div id='n_method'> N Class Name: GraphAttnModel</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: thnn.Module</div><div id='n_parent_class'> N Parent Class: thnn.Module</div><div id='m_file'> M File Name: gnn/models.py</div><div id='n_file'> N File Name: gnn/models.py</div><div id='m_start'> M Start Line: 222</div><div id='m_end'> M End Line: 227</div><div id='n_start'> N Start Line: 229</div><div id='n_end'> N End Line: 249</div><BR>