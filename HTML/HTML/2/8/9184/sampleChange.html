<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 setup model
    loss_record, most_recent_encoder_path = model.learn(dataset_dict, pretrain_epochs, pretrain_batches)
    <a id="change">if </a>ppo_finetune and not isinstance(model, algos.RecurrentCPC):
        encoder_checkpoint = model.encoder_checkpoints_path
        all_checkpoints = glob(os.path.join(encoder_checkpoint, &quot*&quot))
        latest_checkpoint<a id="change"> = </a><a id="change">max(</a>all_checkpoints<a id="change">, key=os.path.getctime)</a>
        encoder_feature_extractor_kwargs = {&quotfeatures_dim&quot: algo_params["representation_dim"],
                                            &quotencoder_path&quot: os.path.abspath(latest_checkpoint)}

        &#47&#47 TODO figure out how to not have to set `ortho_init` to False for the whole policy
        policy_kwargs<a id="change"> = </a>{&quotfeatures_extractor_class&quot: EncoderFeatureExtractor,
                         &quotfeatures_extractor_kwargs&quot: encoder_feature_extractor_kwargs,
                         &quotortho_init&quot: False}
        ppo_model = PPO(policy=ActorCriticPolicy, env=venv,</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 setup model
    loss_record, most_recent_encoder_path = model.learn(
        webdataset, pretrain_epochs, pretrain_batches)
    <a id="change">if </a>ppo_finetune and not isinstance(model, algos.RecurrentCPC):
        <a id="change">raise NotImplementedError("PPO fine-tuning is not currently supported"</a><a id="change">)</a>

        &#47&#47 encoder_checkpoint = model.encoder_checkpoints_path
        &#47&#47 all_checkpoints = glob(os.path.join(encoder_checkpoint, &quot*&quot))
        &#47&#47 latest_checkpoint = max(all_checkpoints, key=os.path.getctime)</code></pre>