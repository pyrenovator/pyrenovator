<html><h3>Pattern ID :18421
</h3><img src='60198326.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>



    <a id="change">for </a>name,w in  model.named_parameters()<a id="change">:
        </a>if &quotbias&quot not in name  and w.trainable:
            w_data = w.value().detach()
            norms = sqrt(reduce_sum(square(w_data), axis=axis, keepdims=True))
            desired = (rate * clip(norms, min_value, max_value) + (1 - rate) * norms)
            <a id="change">w.assign(</a>w_data<a id="change"> * </a>(desired / (epsilon() + norms))<a id="change">)</a>



</code></pre><h3>After Change</h3><pre><code class='java'>
        axis (int): axis along which to calculate weight norms
    

    <a id="change">for </a><a id="change">module</a> in <a id="change">model.children():
        </a><a id="change">for </a>key, <a id="change">param</a> in module._parameters.items()<a id="change">:
            </a>if <a id="change">param is not None</a>  and <a id="change">param.trainable==True</a> and key!="bias":

                &#47&#47 Tensors stored in modules are graph leaves, and we don&quott want to
                &#47&#47 track autograd history of `param_applied`, so we have to use
                w_data = param.value().detach()
                reduce_axis=list(range(len(w_data.shape)))
                reduce_axis.remove(reduce_axis[-1])
                norms = sqrt(reduce_sum(square(w_data), axis=reduce_axis, keepdims=True))
                desired = (rate * clip(norms, min_value, max_value) + (1 - rate) * norms)
                <a id="change">param_applied</a><a id="change"> =</a>w_data<a id="change"> * </a>(desired / (epsilon() + norms))
                <a id="change">if isinstance(param</a>, tf.Variable<a id="change">)</a>:
                    <a id="change">module._parameters[key]</a><a id="change"> = Parameter(param_applied</a><a id="change">, trainable=param.trainable)</a>
                else:
                    param<a id="change"> = param_applied</a>



</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 16</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/allanyiin/trident/commit/b2f780799b10d1d116e460e98accbd67f6f882ab#diff-bc94bbb66df4dc80907d09466a0e48df4870a4cc5d50707925c74e2239d7c1d4L74' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60198326</div><div id='project'> Project Name: allanyiin/trident</div><div id='commit'> Commit Name: b2f780799b10d1d116e460e98accbd67f6f882ab</div><div id='time'> Time: 2020-12-31</div><div id='author'> Author: allan@asiaminer.com.tw</div><div id='file'> File Name: trident/optims/tensorflow_constraints.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: min_max_norm(5)</div><div id='n_method'> N Method Name: min_max_norm(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: trident/optims/tensorflow_constraints.py</div><div id='n_file'> N File Name: trident/optims/tensorflow_constraints.py</div><div id='m_start'> M Start Line: 74</div><div id='m_end'> M End Line: 85</div><div id='n_start'> N Start Line: 103</div><div id='n_end'> N End Line: 124</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        axis (int):axis along which to calculate weight norms.

    
    <a id="change">for </a>name,<a id="change">w</a> in  model.named_parameters()<a id="change">:
        </a>if &quotbias&quot not in name  and w.trainable:
            w_data=w.value().detach()
            norms = sqrt(reduce_sum(square(w_data), axis=axis, keepdims=True))
            desired = clip(norms, 0, max_value)
            <a id="change">w.assign(</a>w_data * (desired<a id="change"> / </a>(epsilon() + norms))<a id="change">)</a>

def  non_neg_norm(model):
    
    Constrains the weights to be non-negative.</code></pre><h3>After Change</h3><pre><code class='java'>

    

    <a id="change">for </a><a id="change">module</a> in <a id="change">model.children():
        </a><a id="change">for </a>key, <a id="change">param</a> in module._parameters.items()<a id="change">:
            </a>if <a id="change">param is not None</a>  and <a id="change">param.trainable==True</a>  and key!="bias":
                &#47&#47 Tensors stored in modules are graph leaves, and we don&quott want to
                &#47&#47 track autograd history of `param_applied`, so we have to use
                w_data = param.value().detach()
                reduce_axis = list(range(len(w_data.shape)))
                reduce_axis.remove(reduce_axis[-1])
                norms = sqrt(reduce_sum(square(w_data), axis=reduce_axis, keepdims=True))
                desired = clip(norms, 0, max_value)
                <a id="change">param_applied</a><a id="change"> =</a>w_data * (desired<a id="change"> / </a>(epsilon() + norms))
                <a id="change">if isinstance(</a>param, tf.Variable<a id="change">)</a>:
                    <a id="change">module._parameters[key]</a><a id="change"> = Parameter(</a>param_applied<a id="change">, trainable=param.trainable)</a>
                else:
                    param<a id="change"> = </a>param_applied


def  non_neg_norm(model):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/allanyiin/trident/commit/b2f780799b10d1d116e460e98accbd67f6f882ab#diff-bc94bbb66df4dc80907d09466a0e48df4870a4cc5d50707925c74e2239d7c1d4L17' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60198327</div><div id='project'> Project Name: allanyiin/trident</div><div id='commit'> Commit Name: b2f780799b10d1d116e460e98accbd67f6f882ab</div><div id='time'> Time: 2020-12-31</div><div id='author'> Author: allan@asiaminer.com.tw</div><div id='file'> File Name: trident/optims/tensorflow_constraints.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: max_norm(3)</div><div id='n_method'> N Method Name: max_norm(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: trident/optims/tensorflow_constraints.py</div><div id='n_file'> N File Name: trident/optims/tensorflow_constraints.py</div><div id='m_start'> M Start Line: 27</div><div id='m_end'> M End Line: 33</div><div id='n_start'> N Start Line: 30</div><div id='n_end'> N End Line: 46</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>



    <a id="change">for </a>name,<a id="change">w</a> in  model.named_parameters()<a id="change">:
        </a>if &quotbias&quot not in name  and w.trainable:
            w_data = w.value().detach()
            norms = sqrt(reduce_sum(square(w_data), axis=axis, keepdims=True))
            desired = (rate * clip(norms, min_value, max_value) + (1 - rate) * norms)
            <a id="change">w.assign(</a>w_data<a id="change"> * </a>(desired / (epsilon() + norms))<a id="change">)</a>



</code></pre><h3>After Change</h3><pre><code class='java'>
        axis (int): axis along which to calculate weight norms
    

    <a id="change">for </a><a id="change">module</a> in <a id="change">model.children():
        </a><a id="change">for </a>key, <a id="change">param</a> in module._parameters.items()<a id="change">:
            </a>if <a id="change">param is not None</a>  and <a id="change">param.trainable==True</a> and key!="bias":

                &#47&#47 Tensors stored in modules are graph leaves, and we don&quott want to
                &#47&#47 track autograd history of `param_applied`, so we have to use
                w_data = param.value().detach()
                reduce_axis=list(range(len(w_data.shape)))
                reduce_axis.remove(reduce_axis[-1])
                norms = sqrt(reduce_sum(square(w_data), axis=reduce_axis, keepdims=True))
                desired = (rate * clip(norms, min_value, max_value) + (1 - rate) * norms)
                <a id="change">param_applied</a><a id="change"> =</a>w_data<a id="change"> * </a>(desired / (epsilon() + norms))
                <a id="change">if isinstance(</a>param, tf.Variable<a id="change">)</a>:
                    <a id="change">module._parameters[key]</a><a id="change"> = Parameter(</a>param_applied<a id="change">, trainable=param.trainable)</a>
                else:
                    param<a id="change"> = </a>param_applied


</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/allanyiin/trident/commit/b2f780799b10d1d116e460e98accbd67f6f882ab#diff-bc94bbb66df4dc80907d09466a0e48df4870a4cc5d50707925c74e2239d7c1d4L59' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60198325</div><div id='project'> Project Name: allanyiin/trident</div><div id='commit'> Commit Name: b2f780799b10d1d116e460e98accbd67f6f882ab</div><div id='time'> Time: 2020-12-31</div><div id='author'> Author: allan@asiaminer.com.tw</div><div id='file'> File Name: trident/optims/tensorflow_constraints.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: min_max_norm(5)</div><div id='n_method'> N Method Name: min_max_norm(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: trident/optims/tensorflow_constraints.py</div><div id='n_file'> N File Name: trident/optims/tensorflow_constraints.py</div><div id='m_start'> M Start Line: 74</div><div id='m_end'> M End Line: 85</div><div id='n_start'> N Start Line: 103</div><div id='n_end'> N End Line: 124</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        model : the model contains  weights need to setting the constraints.

    
    <a id="change">for </a>name,<a id="change">w</a> in  model.named_parameters()<a id="change">:
        </a>if &quotbias&quot not in name and w.trainable:
            w_data=w.value().detach()
            <a id="change">w.assign(</a>w_data<a id="change"> * </a>tf.cast(greater_equal(w, 0.), tf.float32)<a id="change">)</a>

def  unit_norm(model,axis=0):
    
    Constrains the weights incident to each hidden unit to have unit norm.</code></pre><h3>After Change</h3><pre><code class='java'>

    

    <a id="change">for </a><a id="change">module</a> in <a id="change">model.children():
        </a><a id="change">for </a>key, <a id="change">param</a> in module._parameters.items()<a id="change">:
            </a>if <a id="change">param is not None</a>  and <a id="change">param.trainable==True</a> and key!="bias":
                &#47&#47 Tensors stored in modules are graph leaves, and we don&quott want to
                &#47&#47 track autograd history of `param_applied`, so we have to use
                w_data = param.value().detach()
                <a id="change">param_applied</a><a id="change"> =</a>w_data<a id="change"> * </a>tf.cast(greater_equal(param, 0.), tf.float32)
                <a id="change">if isinstance(</a>param, tf.Variable<a id="change">)</a>:
                    <a id="change">module._parameters[key]</a><a id="change"> = Parameter(</a>param_applied<a id="change">, trainable=param.trainable)</a>
                else:
                    param<a id="change"> = </a>param_applied

def  unit_norm(model,axis=0):
    </code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/allanyiin/trident/commit/b2f780799b10d1d116e460e98accbd67f6f882ab#diff-bc94bbb66df4dc80907d09466a0e48df4870a4cc5d50707925c74e2239d7c1d4L34' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60198323</div><div id='project'> Project Name: allanyiin/trident</div><div id='commit'> Commit Name: b2f780799b10d1d116e460e98accbd67f6f882ab</div><div id='time'> Time: 2020-12-31</div><div id='author'> Author: allan@asiaminer.com.tw</div><div id='file'> File Name: trident/optims/tensorflow_constraints.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: non_neg_norm(1)</div><div id='n_method'> N Method Name: non_neg_norm(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: trident/optims/tensorflow_constraints.py</div><div id='n_file'> N File Name: trident/optims/tensorflow_constraints.py</div><div id='m_start'> M Start Line: 41</div><div id='m_end'> M End Line: 45</div><div id='n_start'> N Start Line: 55</div><div id='n_end'> N End Line: 66</div><BR>