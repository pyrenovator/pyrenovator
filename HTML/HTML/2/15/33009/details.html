<html><h3>Pattern ID :33009
</h3><img src='95485554.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    if not isinstance(preprocessor, ViTFeatureExtractor):
        raise ValueError(f"Unknown preprocessor: {preprocessor}")

    wrapper = <a id="change">Wrapper(</a>torch_model<a id="change">)</a>

    scale = 1.0 / (preprocessor.image_std[0] * 255)
    bias = [
        -preprocessor.image_mean[0] / preprocessor.image_std[0],</code></pre><h3>After Change</h3><pre><code class='java'>
    if not isinstance(feature_extractor, ViTFeatureExtractor):
        raise ValueError(f"Unknown feature extractor: {feature_extractor}")

    wrapper = <a id="change">Wrapper(torch_model).eval()</a>

    scale = 1.0 / (feature_extractor.image_std[0] * 255)
    bias = [
        -feature_extractor.image_mean[0] / feature_extractor.image_std[0],
        -feature_extractor.image_mean[1] / feature_extractor.image_std[1],
        -feature_extractor.image_mean[2] / feature_extractor.image_std[2],
    ]

    image_size = feature_extractor.size
    image_shape = (1, 3, image_size, image_size)
    example_input = torch.rand(image_shape) * 2.0 - 1.0

    traced_model = torch.jit.trace(wrapper, example_input, strict=False)

    convert_kwargs = { }
    <a id="change">if </a><a id="change">not legacy</a>:
        convert_kwargs["compute_precision"] = ct.precision.FLOAT16<a id="change"> if </a><a id="change">quantize == "float16" else </a>ct.precision.FLOAT32

    if isinstance(torch_model, ViTForImageClassification):
        class_labels = [torch_model.config.id2label[x] for x in range(torch_model.config.num_labels)]
        classifier_config = ct.ClassifierConfig(class_labels)
        convert_kwargs[&quotclassifier_config&quot] = classifier_config

    mlmodel = ct.convert(
        traced_model,
        inputs=[ct.ImageType(name="image", shape=image_shape, scale=scale, bias=bias, 
                             color_layout="RGB", channel_first=True)],
        convert_to="neuralnetwork" if legacy else "mlprogram",
        **convert_kwargs,
    )

    spec = mlmodel._spec

    user_defined_metadata = {}
    if torch_model.config.transformers_version:
        user_defined_metadata["transformers_version"] = torch_model.config.transformers_version

    if isinstance(torch_model, ViTForImageClassification):
        probs_output_name = spec.description.predictedProbabilitiesName
        ct.utils.rename_feature(spec, probs_output_name, "probabilities")
        spec.description.predictedProbabilitiesName = "probabilities"

        mlmodel.input_description["image"] = "Image to be classified"
        mlmodel.output_description["probabilities"] = "Probability of each category"
        mlmodel.output_description["classLabel"] = "Category with the highest score"

    if isinstance(torch_model, ViTModel):
        &#47&#47 Rename the output from the pooler.
        if torch_model.pooler is not None:
            output_names = get_output_names(spec)
            for output_name in output_names:
                if output_name != "hidden_states":
                    ct.utils.rename_feature(spec, output_name, "pooler_output")

        &#47&#47 Fill in the shapes for the output tensors.
        with torch.no_grad():
            temp = traced_model(example_input)

        if torch_model.pooler is not None:
            hidden_shape = temp[0].shape
            pooler_shape = temp[1].shape
            set_multiarray_shape(get_output_named(spec, "hidden_states"), hidden_shape)
            set_multiarray_shape(get_output_named(spec, "pooler_output"), pooler_shape)            
            mlmodel.output_description["pooler_output"] = "Output from the global pooling layer"
        else:
            hidden_shape = temp.shape
            set_multiarray_shape(get_output_named(spec, "hidden_states"), hidden_shape)

        mlmodel.input_description["image"] = "Image to be classified"
        mlmodel.output_description["hidden_states"] = "Hidden states from the last layer"

    if len(user_defined_metadata) &gt; 0:
        spec.description.metadata.userDefined.update(user_defined_metadata)

    &#47&#47 Reload the model in case any input / output names were changed.
    mlmodel = ct.models.MLModel(mlmodel._spec, weights_dir=mlmodel.weights_dir)

    <a id="change">if legacy and quantize == "float16"</a>:
        mlmodel<a id="change"> = </a><a id="change">quantization_utils.quantize_weights(</a>mlmodel<a id="change">, nbits=16)</a>

    return mlmodel
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 12</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/huggingface/exporters/commit/19b95438aefa065f60a0c0c98a547a9b47e0986e#diff-1fb2e83ba8babf8846d748478a5eedd7e7518667807ff0659c6ee9a6fd26f497L55' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95485554</div><div id='project'> Project Name: huggingface/exporters</div><div id='commit'> Commit Name: 19b95438aefa065f60a0c0c98a547a9b47e0986e</div><div id='time'> Time: 2022-05-25</div><div id='author'> Author: mail@hollance.com</div><div id='file'> File Name: coreml/models/vit.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: export(4)</div><div id='n_method'> N Method Name: export(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: coreml/models/vit.py</div><div id='n_file'> N File Name: coreml/models/vit.py</div><div id='m_start'> M Start Line: 55</div><div id='m_end'> M End Line: 55</div><div id='n_start'> N Start Line: 56</div><div id='n_end'> N End Line: 142</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    example_input = torch.randint(tokenizer.vocab_size, (1, sequence_length))

    wrapper = <a id="change">Wrapper(</a>torch_model<a id="change">)</a>
    traced_model = torch.jit.trace(wrapper, example_input, strict=False)

    convert_kwargs = {}
</code></pre><h3>After Change</h3><pre><code class='java'>

    example_input = torch.randint(tokenizer.vocab_size, (1, sequence_length))

    wrapper = <a id="change">Wrapper(torch_model).eval()</a>
    traced_model = torch.jit.trace(wrapper, example_input, strict=False)

    convert_kwargs = {}
    <a id="change">if </a><a id="change">not legacy</a>:
        convert_kwargs["compute_precision"] = ct.precision.FLOAT16<a id="change"> if </a><a id="change">quantize == "float16" else </a>ct.precision.FLOAT32

    mlmodel = ct.convert(
        traced_model,
        inputs=[ct.TensorType(name="input_ids", shape=example_input.shape, dtype=np.int32)],
        convert_to="neuralnetwork" if legacy else "mlprogram",
        **convert_kwargs,
    )

    spec = mlmodel._spec

    user_defined_metadata = {}
    if torch_model.config.transformers_version:
        user_defined_metadata["transformers_version"] = torch_model.config.transformers_version

    if is_any_instance(torch_model, [BertForQuestionAnswering, DistilBertForQuestionAnswering]):
        &#47&#47 Rename the outputs and fill in their shapes.
        output = spec.description.output[0]
        ct.utils.rename_feature(spec, output.name, "start_scores")
        set_multiarray_shape(output, (1, sequence_length))
        
        output = spec.description.output[1]
        ct.utils.rename_feature(spec, output.name, "end_scores")
        set_multiarray_shape(output, (1, sequence_length))

        mlmodel.input_description["input_ids"] = "Indices of input sequence tokens in the vocabulary"
        mlmodel.output_description["start_scores"] = "Span-start scores (after softmax)"
        mlmodel.output_description["end_scores"] = "Span-end scores (after softmax)"

    if len(user_defined_metadata) &gt; 0:
        spec.description.metadata.userDefined.update(user_defined_metadata)

    &#47&#47 Reload the model in case any input / output names were changed.
    mlmodel = ct.models.MLModel(mlmodel._spec, weights_dir=mlmodel.weights_dir)

    <a id="change">if legacy and quantize == "float16"</a>:
        mlmodel<a id="change"> = </a><a id="change">quantization_utils.quantize_weights(</a>mlmodel<a id="change">, nbits=16)</a>

    return mlmodel
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/huggingface/exporters/commit/19b95438aefa065f60a0c0c98a547a9b47e0986e#diff-de98d1d2c23f630fad941c1181c00df0fd78152c02d9dd115e31c086cae7977dL42' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95485556</div><div id='project'> Project Name: huggingface/exporters</div><div id='commit'> Commit Name: 19b95438aefa065f60a0c0c98a547a9b47e0986e</div><div id='time'> Time: 2022-05-25</div><div id='author'> Author: mail@hollance.com</div><div id='file'> File Name: coreml/models/distilbert.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: export(5)</div><div id='n_method'> N Method Name: export(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: coreml/models/distilbert.py</div><div id='n_file'> N File Name: coreml/models/distilbert.py</div><div id='m_start'> M Start Line: 53</div><div id='m_end'> M End Line: 53</div><div id='n_start'> N Start Line: 48</div><div id='n_end'> N End Line: 98</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    if not isinstance(preprocessor, MobileViTFeatureExtractor):
        raise ValueError(f"Unknown preprocessor: {preprocessor}")

    wrapper = <a id="change">Wrapper(</a>torch_model<a id="change">)</a>

    scale = 1.0 / 255
    bias = [ 0.0, 0.0, 0.0 ]
</code></pre><h3>After Change</h3><pre><code class='java'>
    if not isinstance(feature_extractor, MobileViTFeatureExtractor):
        raise ValueError(f"Unknown feature extractor: {feature_extractor}")

    wrapper = <a id="change">Wrapper(torch_model).eval()</a>

    scale = 1.0 / 255
    bias = [ 0.0, 0.0, 0.0 ]

    image_size = feature_extractor.crop_size
    image_shape = (1, 3, image_size, image_size)
    example_input = torch.rand(image_shape) * 2.0 - 1.0

    traced_model = torch.jit.trace(wrapper, example_input, strict=False)

    convert_kwargs = { }
    <a id="change">if </a><a id="change">not legacy</a>:
        convert_kwargs["compute_precision"] = ct.precision.FLOAT16<a id="change"> if </a><a id="change">quantize == "float16" else </a>ct.precision.FLOAT32

    if isinstance(torch_model, MobileViTForImageClassification):
        class_labels = [torch_model.config.id2label[x] for x in range(torch_model.config.num_labels)]
        classifier_config = ct.ClassifierConfig(class_labels)
        convert_kwargs[&quotclassifier_config&quot] = classifier_config

    mlmodel = ct.convert(
        traced_model,
        inputs=[ct.ImageType(name="image", shape=image_shape, scale=scale, bias=bias, 
                             color_layout="BGR", channel_first=True)],
        convert_to="neuralnetwork" if legacy else "mlprogram",
        **convert_kwargs,
    )

    spec = mlmodel._spec

    user_defined_metadata = {}
    if torch_model.config.transformers_version:
        user_defined_metadata["transformers_version"] = torch_model.config.transformers_version

    if isinstance(torch_model, MobileViTForImageClassification):
        probs_output_name = spec.description.predictedProbabilitiesName
        ct.utils.rename_feature(spec, probs_output_name, "probabilities")
        spec.description.predictedProbabilitiesName = "probabilities"

        mlmodel.input_description["image"] = "Image to be classified"
        mlmodel.output_description["probabilities"] = "Probability of each category"
        mlmodel.output_description["classLabel"] = "Category with the highest score"

    if isinstance(torch_model, MobileViTForSemanticSegmentation):
        &#47&#47 Rename the segmentation output and fill in its shape.
        output = spec.description.output[0]
        ct.utils.rename_feature(spec, output.name, "classLabels")
        set_multiarray_shape(output, (1, image_size, image_size))

        mlmodel.input_description["image"] = "Image input"
        mlmodel.output_description["classLabels"] = "Segmentation map"

        labels = get_labels_as_list(torch_model)
        user_defined_metadata["classes"] = ",".join(labels)

    if isinstance(torch_model, MobileViTModel):
        &#47&#47 Rename the pooled output.
        output_names = get_output_names(spec)
        for output_name in output_names:
            if output_name != "last_hidden_state":
                ct.utils.rename_feature(spec, output_name, "pooler_output")

        mlmodel.input_description["image"] = "Image input"
        mlmodel.output_description["last_hidden_state"] = "Hidden states from the last layer"
        mlmodel.output_description["pooler_output"] = "Output from the global pooling layer"

        &#47&#47 Fill in the shapes for the output tensors.
        with torch.no_grad():
            temp = traced_model(example_input)

        hidden_shape = temp[0].shape
        pooler_shape = temp[1].shape
        set_multiarray_shape(get_output_named(spec, "last_hidden_state"), hidden_shape)
        set_multiarray_shape(get_output_named(spec, "pooler_output"), pooler_shape)

    if len(user_defined_metadata) &gt; 0:
        spec.description.metadata.userDefined.update(user_defined_metadata)

    &#47&#47 Reload the model in case any input / output names were changed.
    mlmodel = ct.models.MLModel(mlmodel._spec, weights_dir=mlmodel.weights_dir)

    <a id="change">if legacy and quantize == "float16"</a>:
        mlmodel<a id="change"> = </a><a id="change">quantization_utils.quantize_weights(</a>mlmodel<a id="change">, nbits=16)</a>

    return mlmodel
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/huggingface/exporters/commit/19b95438aefa065f60a0c0c98a547a9b47e0986e#diff-5e13e0782e208bc7ab38bd1943841996adbd8fe0b2ccc94fa019de3adfafd4f9L53' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95485557</div><div id='project'> Project Name: huggingface/exporters</div><div id='commit'> Commit Name: 19b95438aefa065f60a0c0c98a547a9b47e0986e</div><div id='time'> Time: 2022-05-25</div><div id='author'> Author: mail@hollance.com</div><div id='file'> File Name: coreml/models/mobilevit.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: export(4)</div><div id='n_method'> N Method Name: export(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: coreml/models/mobilevit.py</div><div id='n_file'> N File Name: coreml/models/mobilevit.py</div><div id='m_start'> M Start Line: 57</div><div id='m_end'> M End Line: 57</div><div id='n_start'> N Start Line: 58</div><div id='n_end'> N End Line: 147</div><BR>