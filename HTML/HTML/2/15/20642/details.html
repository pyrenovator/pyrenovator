<html><h3>Pattern ID :20642
</h3><img src='66498454.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            self.trg_embedding = FactorizedEmbedding(trg_vocab_size, 
                                                     embedding_size,
                                                     d_model)
        self.dropout = <a id="change">Dropout(</a>dropout<a id="change">)</a>
        self.pos_embedding = PositionEmbedding(trg_max_len, d_model)
        
        if share_layer_params == False:
            self.layers = nn.ModuleList([</code></pre><h3>After Change</h3><pre><code class='java'>
        self.scale_embedding = scale_embedding
        self.use_pre_norm = use_pre_norm
        self.norm_before_pred = norm_before_pred
        self.norm_after_embedding<a id="change"> = </a>norm_after_embedding
        
        if embedding_size is None:
            self.trg_embedding = Embedding(trg_vocab_size, 
                                           d_model, 
                                           scale_embedding)
        else:
            self.trg_embedding = FactorizedEmbedding(trg_vocab_size, 
                                                     embedding_size,
                                                     d_model)
        self.emb_dropout = Dropout(emb_dropout)
        self.pos_embedding = PositionEmbedding(trg_max_len,
                                               d_model,
                                               pos_need_train)
        <a id="change">if self.norm_after_embedding == True</a>:
            self.norm_emb<a id="change"> = </a><a id="change">LayerNorm(</a>self.d_model<a id="change">)</a>
            
        if share_layer_params == False:
            self.layers = nn.ModuleList([
                    LMDecoderLayer(n_heads, 
                                   d_model, 
                                   d_ff, 
                                   d_qk, 
                                   d_v,
                                   dropout=dropout,
                                   attn_dropout=attn_dropout,
                                   ln_eps=ln_eps,
                                   use_pre_norm=use_pre_norm,
                                   activation=activation)
                    for _ in range(n_layers)])
        else:
            layers = []
            for i in range(n_layers):
                if i % n_share_across_layers == 0:
                    layer = LMDecoderLayer(n_heads, 
                                           d_model,
                                           d_ff, 
                                           d_qk,
                                           d_v,
                                           dropout=dropout,
                                           attn_dropout=attn_dropout,
                                           ln_eps=ln_eps,
                                           use_pre_norm=use_pre_norm,
                                           activation=activation)

                    layers.append(layer)
            self.layers = nn.ModuleList(layers)
        
        self.share_emb_out_proj = share_emb_out_proj
        if share_emb_out_proj == False: 
            self.W = nn.Parameter(torch.Tensor(trg_vocab_size, d_model))
        
        self.use_proj_bias<a id="change"> = use_proj_bias</a>
        <a id="change">if use_proj_bias</a><a id="change"> == True</a>:
            self.b = nn.Parameter(torch.Tensor(trg_vocab_size))
        
        <a id="change">if </a><a id="change">self.norm_before_pred == True</a>:
            self.norm<a id="change"> = </a><a id="change">LayerNorm(</a>d_model<a id="change">)</a>
        
        self.reset_parameters()
    
    </code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 13</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/yaoxiaoyuan/mimix/commit/59b6082bb310a2a17c2ac30225e300124904cc2f#diff-a64c2aeac9b9657dde1bded0c4c95edecd9cf7a70d5b67f9fcf61c9864f85f9aL1717' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 66498454</div><div id='project'> Project Name: yaoxiaoyuan/mimix</div><div id='commit'> Commit Name: 59b6082bb310a2a17c2ac30225e300124904cc2f</div><div id='time'> Time: 2022-06-21</div><div id='author'> Author: sbsbsbsbsb945@gmail.com</div><div id='file'> File Name: src/layers.py</div><div id='m_class'> M Class Name: LMDecoder</div><div id='n_method'> N Class Name: LMDecoder</div><div id='m_method'> M Method Name: __init__(24)</div><div id='n_method'> N Method Name: __init__(17)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/layers.py</div><div id='n_file'> N File Name: src/layers.py</div><div id='m_start'> M Start Line: 1717</div><div id='m_end'> M End Line: 1743</div><div id='n_start'> N Start Line: 1882</div><div id='n_end'> N End Line: 1967</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                                           d_model, 
                                           scale_embedding)
            
        self.dropout = <a id="change">Dropout(</a>dropout<a id="change">)</a>
        self.pos_embedding = PositionEmbedding(trg_max_len, d_model)
        
        if share_layer_params == False:
            self.layers = nn.ModuleList([</code></pre><h3>After Change</h3><pre><code class='java'>
        self.share_layer_params = share_layer_params
        self.n_share_across_layers = n_share_across_layers
        self.scale_embedding = scale_embedding
        self.norm_before_pred<a id="change"> = </a>norm_before_pred
        self.norm_after_embedding = norm_after_embedding
        
        if share_src_trg_emb == False:
            self.trg_embedding = Embedding(trg_vocab_size, 
                                           d_model, 
                                           scale_embedding)
            
        self.emb_dropout = Dropout(emb_dropout)
        self.pos_embedding = PositionEmbedding(trg_max_len, 
                                               d_model, 
                                               pos_need_train)
        
        <a id="change">if self.norm_after_embedding == True</a>:
            self.norm_emb<a id="change"> = </a><a id="change">LayerNorm(</a>self.d_model<a id="change">)</a>
        
        if share_layer_params == False:
            self.layers = nn.ModuleList([
                    DecoderLayer(n_heads, 
                                 d_model, 
                                 d_ff, 
                                 d_qk, 
                                 d_v,
                                 dropout=dropout,
                                 attn_dropout=attn_dropout,
                                 ln_eps=ln_eps,
                                 use_pre_norm=use_pre_norm,
                                 activation=activation)
                    for _ in range(n_layers)])
        else:
            layers = []
            for i in range(n_layers):
                if i % n_share_across_layers == 0:
                    layer = DecoderLayer(n_heads,
                                         d_model, 
                                         d_ff, 
                                         d_qk, 
                                         d_v,
                                         dropout=dropout,
                                         attn_dropout=attn_dropout,
                                         ln_eps=ln_eps,
                                         use_pre_norm=use_pre_norm,
                                         activation=activation)
                    layers.append(layer)
            self.layers = nn.ModuleList(layers)
    
        self.share_emb_out_proj = share_emb_out_proj
        if share_emb_out_proj == False: 
            self.W = nn.Parameter(torch.Tensor(trg_vocab_size, d_model))
        self.use_proj_bias<a id="change"> = </a>use_proj_bias
        <a id="change">if </a><a id="change">use_proj_bias == True</a>:
            self.b = nn.Parameter(torch.Tensor(trg_vocab_size))

        <a id="change">if </a><a id="change">self.norm_before_pred == True</a>:
            self.norm<a id="change"> = </a><a id="change">LayerNorm(</a>d_model<a id="change">)</a>
        
        self.reset_parameters()
    
    </code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/yaoxiaoyuan/mimix/commit/59b6082bb310a2a17c2ac30225e300124904cc2f#diff-a64c2aeac9b9657dde1bded0c4c95edecd9cf7a70d5b67f9fcf61c9864f85f9aL1446' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 66498455</div><div id='project'> Project Name: yaoxiaoyuan/mimix</div><div id='commit'> Commit Name: 59b6082bb310a2a17c2ac30225e300124904cc2f</div><div id='time'> Time: 2022-06-21</div><div id='author'> Author: sbsbsbsbsb945@gmail.com</div><div id='file'> File Name: src/layers.py</div><div id='m_class'> M Class Name: Decoder</div><div id='n_method'> N Class Name: Decoder</div><div id='m_method'> M Method Name: __init__(25)</div><div id='n_method'> N Method Name: __init__(18)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/layers.py</div><div id='n_file'> N File Name: src/layers.py</div><div id='m_start'> M Start Line: 1468</div><div id='m_end'> M End Line: 1492</div><div id='n_start'> N Start Line: 1566</div><div id='n_end'> N End Line: 1647</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                                                     d_model)

        self.pos_embedding = PositionEmbedding(src_max_len, d_model)
        self.dropout = <a id="change">Dropout(</a>dropout<a id="change">)</a>
        
        if share_layer_params == False:
            self.layers = nn.ModuleList([
                    EncoderLayer(n_heads, d_model, d_ff, d_qk, d_v,</code></pre><h3>After Change</h3><pre><code class='java'>
        self.n_share_across_layers = n_share_across_layers
        self.scale_embedding = scale_embedding
        self.norm_before_pred = norm_before_pred
        self.norm_after_embedding<a id="change"> = </a>norm_after_embedding
        
        if embedding_size is None:
            self.src_embedding = Embedding(src_vocab_size, 
                                           d_model, 
                                           scale_embedding)
        else:
            self.src_embedding = FactorizedEmbedding(src_vocab_size, 
                                                     embedding_size,
                                                     d_model)

        self.pos_embedding = PositionEmbedding(src_max_len, 
                                               d_model, 
                                               pos_need_train)
        
        self.add_segment_embedding<a id="change"> = </a>add_segment_embedding
        self.n_types = n_types
        <a id="change">if </a><a id="change">add_segment_embedding == True</a>:
            self.type_embedding = Embedding(self.n_types, self.d_model)
        
        <a id="change">if </a><a id="change">self.norm_after_embedding == True</a>:
            self.norm_emb<a id="change"> = </a><a id="change">LayerNorm(</a>self.d_model<a id="change">)</a>
        
        self.emb_dropout = Dropout(emb_dropout)
        
        if share_layer_params == False:
            self.layers = nn.ModuleList([
                    EncoderLayer(n_heads, 
                                 d_model, 
                                 d_ff, 
                                 d_qk, 
                                 d_v,
                                 dropout=dropout,
                                 attn_dropout=attn_dropout,
                                 ln_eps=ln_eps,
                                 use_pre_norm=use_pre_norm,                                 
                                 activation=activation)
                    for _ in range(n_layers)])
        else:
            layers = []
            for i in range(n_layers):
                if i % n_share_across_layers == 0:
                    layer = EncoderLayer(n_heads,
                                         d_model, 
                                         d_ff, 
                                         d_qk, 
                                         d_v,
                                         dropout=dropout, 
                                         attn_dropout=attn_dropout,
                                         ln_eps=ln_eps,
                                         use_pre_norm=use_pre_norm,
                                         activation=activation)
                    layers.append(layer)
            self.layers = nn.ModuleList(layers)
    
        <a id="change">if self.norm_before_pred == True</a>:
            self.norm<a id="change"> = </a><a id="change">LayerNorm(</a>self.d_model<a id="change">)</a>
    
    
    def forward(self, x, attn_mask, x_type=None, return_states=False):
        </code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/yaoxiaoyuan/mimix/commit/59b6082bb310a2a17c2ac30225e300124904cc2f#diff-a64c2aeac9b9657dde1bded0c4c95edecd9cf7a70d5b67f9fcf61c9864f85f9aL1269' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 66498421</div><div id='project'> Project Name: yaoxiaoyuan/mimix</div><div id='commit'> Commit Name: 59b6082bb310a2a17c2ac30225e300124904cc2f</div><div id='time'> Time: 2022-06-21</div><div id='author'> Author: sbsbsbsbsb945@gmail.com</div><div id='file'> File Name: src/layers.py</div><div id='m_class'> M Class Name: Encoder</div><div id='n_method'> N Class Name: Encoder</div><div id='m_method'> M Method Name: __init__(24)</div><div id='n_method'> N Method Name: __init__(16)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/layers.py</div><div id='n_file'> N File Name: src/layers.py</div><div id='m_start'> M Start Line: 1293</div><div id='m_end'> M End Line: 1293</div><div id='n_start'> N Start Line: 1311</div><div id='n_end'> N End Line: 1394</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            self.trg_embedding = FactorizedEmbedding(trg_vocab_size, 
                                                     embedding_size,
                                                     d_model)
        self.dropout = <a id="change">Dropout(</a>dropout<a id="change">)</a>
        self.pos_embedding = PositionEmbedding(trg_max_len, d_model)
        
        if share_layer_params == False:
            self.layers = nn.ModuleList([</code></pre><h3>After Change</h3><pre><code class='java'>
        self.scale_embedding = scale_embedding
        self.use_pre_norm = use_pre_norm
        self.norm_before_pred = norm_before_pred
        self.norm_after_embedding<a id="change"> = </a>norm_after_embedding
        
        if embedding_size is None:
            self.trg_embedding = Embedding(trg_vocab_size, 
                                           d_model, 
                                           scale_embedding)
        else:
            self.trg_embedding = FactorizedEmbedding(trg_vocab_size, 
                                                     embedding_size,
                                                     d_model)
        self.emb_dropout = Dropout(emb_dropout)
        self.pos_embedding = PositionEmbedding(trg_max_len,
                                               d_model,
                                               pos_need_train)
        <a id="change">if </a><a id="change">self.norm_after_embedding == True</a>:
            self.norm_emb<a id="change"> = </a><a id="change">LayerNorm(</a>self.d_model<a id="change">)</a>
            
        if share_layer_params == False:
            self.layers = nn.ModuleList([
                    LMDecoderLayer(n_heads, 
                                   d_model, 
                                   d_ff, 
                                   d_qk, 
                                   d_v,
                                   dropout=dropout,
                                   attn_dropout=attn_dropout,
                                   ln_eps=ln_eps,
                                   use_pre_norm=use_pre_norm,
                                   activation=activation)
                    for _ in range(n_layers)])
        else:
            layers = []
            for i in range(n_layers):
                if i % n_share_across_layers == 0:
                    layer = LMDecoderLayer(n_heads, 
                                           d_model,
                                           d_ff, 
                                           d_qk,
                                           d_v,
                                           dropout=dropout,
                                           attn_dropout=attn_dropout,
                                           ln_eps=ln_eps,
                                           use_pre_norm=use_pre_norm,
                                           activation=activation)

                    layers.append(layer)
            self.layers = nn.ModuleList(layers)
        
        self.share_emb_out_proj = share_emb_out_proj
        if share_emb_out_proj == False: 
            self.W = nn.Parameter(torch.Tensor(trg_vocab_size, d_model))
        
        self.use_proj_bias<a id="change"> = </a>use_proj_bias
        <a id="change">if </a><a id="change">use_proj_bias == True</a>:
            self.b = nn.Parameter(torch.Tensor(trg_vocab_size))
        
        <a id="change">if self.norm_before_pred == True</a>:
            self.norm<a id="change"> = </a><a id="change">LayerNorm(</a>d_model<a id="change">)</a>
        
        self.reset_parameters()
    
    </code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/yaoxiaoyuan/mimix/commit/59b6082bb310a2a17c2ac30225e300124904cc2f#diff-a64c2aeac9b9657dde1bded0c4c95edecd9cf7a70d5b67f9fcf61c9864f85f9aL1693' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 66498456</div><div id='project'> Project Name: yaoxiaoyuan/mimix</div><div id='commit'> Commit Name: 59b6082bb310a2a17c2ac30225e300124904cc2f</div><div id='time'> Time: 2022-06-21</div><div id='author'> Author: sbsbsbsbsb945@gmail.com</div><div id='file'> File Name: src/layers.py</div><div id='m_class'> M Class Name: LMDecoder</div><div id='n_method'> N Class Name: LMDecoder</div><div id='m_method'> M Method Name: __init__(24)</div><div id='n_method'> N Method Name: __init__(17)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/layers.py</div><div id='n_file'> N File Name: src/layers.py</div><div id='m_start'> M Start Line: 1717</div><div id='m_end'> M End Line: 1743</div><div id='n_start'> N Start Line: 1882</div><div id='n_end'> N End Line: 1967</div><BR>