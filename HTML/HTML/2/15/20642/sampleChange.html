<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            self.trg_embedding = FactorizedEmbedding(trg_vocab_size, 
                                                     embedding_size,
                                                     d_model)
        self.dropout = <a id="change">Dropout(</a>dropout<a id="change">)</a>
        self.pos_embedding = PositionEmbedding(trg_max_len, d_model)
        
        if share_layer_params == False:
            self.layers = nn.ModuleList([</code></pre><h3>After Change</h3><pre><code class='java'>
        self.scale_embedding = scale_embedding
        self.use_pre_norm = use_pre_norm
        self.norm_before_pred = norm_before_pred
        self.norm_after_embedding<a id="change"> = </a>norm_after_embedding
        
        if embedding_size is None:
            self.trg_embedding = Embedding(trg_vocab_size, 
                                           d_model, 
                                           scale_embedding)
        else:
            self.trg_embedding = FactorizedEmbedding(trg_vocab_size, 
                                                     embedding_size,
                                                     d_model)
        self.emb_dropout = Dropout(emb_dropout)
        self.pos_embedding = PositionEmbedding(trg_max_len,
                                               d_model,
                                               pos_need_train)
        <a id="change">if self.norm_after_embedding == True</a>:
            self.norm_emb<a id="change"> = </a><a id="change">LayerNorm(</a>self.d_model<a id="change">)</a>
            
        if share_layer_params == False:
            self.layers = nn.ModuleList([
                    LMDecoderLayer(n_heads, 
                                   d_model, 
                                   d_ff, 
                                   d_qk, 
                                   d_v,
                                   dropout=dropout,
                                   attn_dropout=attn_dropout,
                                   ln_eps=ln_eps,
                                   use_pre_norm=use_pre_norm,
                                   activation=activation)
                    for _ in range(n_layers)])
        else:
            layers = []
            for i in range(n_layers):
                if i % n_share_across_layers == 0:
                    layer = LMDecoderLayer(n_heads, 
                                           d_model,
                                           d_ff, 
                                           d_qk,
                                           d_v,
                                           dropout=dropout,
                                           attn_dropout=attn_dropout,
                                           ln_eps=ln_eps,
                                           use_pre_norm=use_pre_norm,
                                           activation=activation)

                    layers.append(layer)
            self.layers = nn.ModuleList(layers)
        
        self.share_emb_out_proj = share_emb_out_proj
        if share_emb_out_proj == False: 
            self.W = nn.Parameter(torch.Tensor(trg_vocab_size, d_model))
        
        self.use_proj_bias<a id="change"> = use_proj_bias</a>
        <a id="change">if use_proj_bias</a><a id="change"> == True</a>:
            self.b = nn.Parameter(torch.Tensor(trg_vocab_size))
        
        <a id="change">if </a><a id="change">self.norm_before_pred == True</a>:
            self.norm<a id="change"> = </a><a id="change">LayerNorm(</a>d_model<a id="change">)</a>
        
        self.reset_parameters()
    
    </code></pre>