<html><h3>Pattern ID :40188
</h3><img src='114172622.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        super().__init__()

    def forward(self, x):
        <a id="change">return </a>x
</code></pre><h3>After Change</h3><pre><code class='java'>
        )

    def forward(self, x, mask = None):
        b<a id="change">, n, device</a> = *x.shape, x.device
        x<a id="change"> = self.token_emb(</a>x<a id="change">)</a>

        pos_emb<a id="change"> = self.pos_emb(</a><a id="change">torch.arange(n</a><a id="change">, device = device))</a>
        pos_emb = <a id="change">rearrange(</a>pos_emb, <a id="change">&quotn d -&gt; () n d&quot</a><a id="change">)</a>

        x = x<a id="change"> + </a>pos_emb
        x<a id="change"> = </a>rearrange(x, &quotb n d -&gt; b d n&quot)

        for attn, ff in self.layers:
            x = attn(x) + x
            x = ff(x) + x

        <a id="change">return self.to_logits(</a>x<a id="change">)</a>
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 5</div><BR><div id='size'>Non-data size: 12</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/multistream-transformers/commit/86dac4546b311f7fe7192762abb8979f45722d53#diff-e81141685efabebecc465bdbe0fb5f026c1b47e7c282daeaaf1d717b13d5d981L19' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 114172622</div><div id='project'> Project Name: lucidrains/multistream-transformers</div><div id='commit'> Commit Name: 86dac4546b311f7fe7192762abb8979f45722d53</div><div id='time'> Time: 2021-07-29</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: multistream_transformers/multistream_transformers.py</div><div id='m_class'> M Class Name: MultistreamTransformer</div><div id='n_method'> N Class Name: MultistreamTransformer</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: multistream_transformers/multistream_transformers.py</div><div id='n_file'> N File Name: multistream_transformers/multistream_transformers.py</div><div id='m_start'> M Start Line: 19</div><div id='m_end'> M End Line: 19</div><div id='n_start'> N Start Line: 120</div><div id='n_end'> N End Line: 134</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        super().__init__()

    def forward(self, x):
        <a id="change">return </a>x
</code></pre><h3>After Change</h3><pre><code class='java'>
        )

    def forward(self, x, mask = None):
        b<a id="change">, n, device</a> = *x.shape, x.device
        x<a id="change"> = self.token_emb(</a>x<a id="change">)</a>

        pos_emb<a id="change"> = self.pos_emb(</a><a id="change">torch.arange(</a>n<a id="change">, device = device))</a>
        pos_emb = <a id="change">rearrange(</a>pos_emb, <a id="change">&quotn d -&gt; () n d&quot</a><a id="change">)</a>

        x<a id="change"> = </a>x<a id="change"> + </a>pos_emb
        x = rearrange(x, &quotb n d -&gt; b d n&quot)

        for attn, ff in self.layers:
            x = attn(x) + x
            x = ff(x) + x

        <a id="change">return self.to_logits(</a>x<a id="change">)</a>
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/multistream-transformers/commit/86dac4546b311f7fe7192762abb8979f45722d53#diff-e81141685efabebecc465bdbe0fb5f026c1b47e7c282daeaaf1d717b13d5d981L18' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 114172623</div><div id='project'> Project Name: lucidrains/multistream-transformers</div><div id='commit'> Commit Name: 86dac4546b311f7fe7192762abb8979f45722d53</div><div id='time'> Time: 2021-07-29</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: multistream_transformers/multistream_transformers.py</div><div id='m_class'> M Class Name: MultistreamTransformer</div><div id='n_method'> N Class Name: MultistreamTransformer</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: multistream_transformers/multistream_transformers.py</div><div id='n_file'> N File Name: multistream_transformers/multistream_transformers.py</div><div id='m_start'> M Start Line: 19</div><div id='m_end'> M End Line: 19</div><div id='n_start'> N Start Line: 120</div><div id='n_end'> N End Line: 134</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        super().__init__()

    def forward(self, x):
        <a id="change">return </a>x
</code></pre><h3>After Change</h3><pre><code class='java'>
        x,
        mask = None
    ):
        n<a id="change">, device</a> = x.shape[1], x.device
        x<a id="change"> = self.token_emb(</a>x<a id="change">)</a>
        pos_emb<a id="change"> = self.pos_emb(</a><a id="change">torch.arange(</a>n<a id="change">, device = device))</a>
        x = x<a id="change"> + rearrange(</a>pos_emb, <a id="change">&quotn d -&gt; () n d&quot</a><a id="change">)</a>

        for attn, ff in self.layers:
            x<a id="change"> = </a>attn(x, mask = mask) + x
            x = ff(x) + x

        <a id="change">return self.to_logits(</a>x<a id="change">)</a>
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/fast-transformer-pytorch/commit/a67157f6405ad4bdce53b0219027cd90a0d3a7b1#diff-baf9a81f13acc33e891fd7b4e107bcf15ac301e0a4b12d120a33f224ec154333L17' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 114172618</div><div id='project'> Project Name: lucidrains/fast-transformer-pytorch</div><div id='commit'> Commit Name: a67157f6405ad4bdce53b0219027cd90a0d3a7b1</div><div id='time'> Time: 2021-08-23</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: fast_transformer_pytorch/fast_transformer_pytorch.py</div><div id='m_class'> M Class Name: FastTransformer</div><div id='n_method'> N Class Name: FastTransformer</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: fast_transformer_pytorch/fast_transformer_pytorch.py</div><div id='n_file'> N File Name: fast_transformer_pytorch/fast_transformer_pytorch.py</div><div id='m_start'> M Start Line: 18</div><div id='m_end'> M End Line: 18</div><div id='n_start'> N Start Line: 145</div><div id='n_end'> N End Line: 156</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        super().__init__()

    def forward(self, x):
        <a id="change">return </a>x
</code></pre><h3>After Change</h3><pre><code class='java'>
        )

    def forward(self, x, mask = None):
        n<a id="change">, device</a> = x.shape[1], x.device
        x<a id="change"> = self.token_emb(</a>x<a id="change">)</a>
        pos_emb<a id="change"> = self.pos_emb(</a><a id="change">torch.arange(</a>n<a id="change">, device = device))</a>
        x<a id="change"> = </a>x<a id="change"> + rearrange(</a>pos_emb, <a id="change">&quotn d -&gt; () n d&quot</a><a id="change">)</a>

        if self.training:
            &#47&#47 training mode

            for _ in range(self.train_max_steps):
                x, halt_logits = self.block(x)

            <a id="change">return self.to_logits(</a>x<a id="change">)</a>
        else:
            &#47&#47 evaluation mode

            for _ in range(self.train_max_steps):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/ponder-transformer/commit/8ff8c53083298ba5809d55a8b36173d8b010138f#diff-0a16a60c9ef4ac6aaae6b561bb8a5eba450a2f07d88250b572859c5e4e567215L11' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 114172619</div><div id='project'> Project Name: lucidrains/ponder-transformer</div><div id='commit'> Commit Name: 8ff8c53083298ba5809d55a8b36173d8b010138f</div><div id='time'> Time: 2021-08-26</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: ponder_transformer/ponder_transformer.py</div><div id='m_class'> M Class Name: PonderTransformer</div><div id='n_method'> N Class Name: PonderTransformer</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: ponder_transformer/ponder_transformer.py</div><div id='n_file'> N File Name: ponder_transformer/ponder_transformer.py</div><div id='m_start'> M Start Line: 12</div><div id='m_end'> M End Line: 12</div><div id='n_start'> N Start Line: 153</div><div id='n_end'> N End Line: 172</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        super().__init__()

    def forward(self, x):
        <a id="change">return </a>x
</code></pre><h3>After Change</h3><pre><code class='java'>
        )

    def forward(self, x):
        n<a id="change">, device</a> = x.shape[1], x.device

        x<a id="change"> = self.token_emb(</a>x<a id="change">)</a>

        for attn, ff in self.layers:
            x = attn(x) + x
            x = ff(x) + x

        pos_emb<a id="change"> = self.pos_emb(</a><a id="change">torch.arange(</a>n<a id="change">, device = device))</a>
        x<a id="change"> = </a>x<a id="change"> + rearrange(</a>pos_emb, <a id="change">&quotn d -&gt; () n d&quot</a><a id="change">)</a>

        <a id="change">return self.to_logits(</a>x<a id="change">)</a>
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/triton-transformer/commit/4251b68569f927c7b1f9429cc5ba656cbf7b57a4#diff-efb464e8953d3171bdd3355d448443781f005f551f7281446030316b4157bb28L13' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 114172612</div><div id='project'> Project Name: lucidrains/triton-transformer</div><div id='commit'> Commit Name: 4251b68569f927c7b1f9429cc5ba656cbf7b57a4</div><div id='time'> Time: 2021-09-15</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: triton_transformer/triton_transformer.py</div><div id='m_class'> M Class Name: Transformer</div><div id='n_method'> N Class Name: Transformer</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: triton_transformer/triton_transformer.py</div><div id='n_file'> N File Name: triton_transformer/triton_transformer.py</div><div id='m_start'> M Start Line: 14</div><div id='m_end'> M End Line: 14</div><div id='n_start'> N Start Line: 79</div><div id='n_end'> N End Line: 90</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        super().__init__()

    def forward(self, x):
        <a id="change">return </a>x
</code></pre><h3>After Change</h3><pre><code class='java'>
    ):
        assert not (return_loss and not self.training), &quotmust be training if returning loss&quot

        n<a id="change">, device</a> = seq.shape[-1], seq.device

        &#47&#47 use 0 as bos

        seq = F.pad(seq, (1, 0), value = 0)

        &#47&#47 if training, derive labels

        if return_loss:
            seq, labels = seq[:, :-1], seq[:, 1:]

        &#47&#47 embed both sequence and retrieved chunks

        embed<a id="change"> = self.token_emb(</a>seq<a id="change">)</a>
        retrieved = self.token_emb(retrieved)

        &#47&#47 get positional embedding

        pos_emb<a id="change"> = self.pos_emb(</a><a id="change">torch.arange(</a>n<a id="change">, device = device))</a>
        pos_emb = <a id="change">rearrange(</a>pos_emb, <a id="change">&quotn d -&gt; 1 n d&quot</a><a id="change">)</a>
        embed<a id="change"> = </a>embed<a id="change"> + </a>pos_emb

        logits = <a id="change">self.to_logits(</a>embed<a id="change">)</a>

        if not return_loss:
            <a id="change">return </a>logits

        loss = F.cross_entropy(rearrange(logits, &quotb n c -&gt; b c n&quot), labels)
        return loss</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/retro-pytorch/commit/e737b0c407799969e66abe02d9071e6459c629a0#diff-fedd9cc24fef747e4ffbe9ce88b42fa6cdd64bd399aeb8589d4b304931bfcc64L15' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 114172628</div><div id='project'> Project Name: lucidrains/retro-pytorch</div><div id='commit'> Commit Name: e737b0c407799969e66abe02d9071e6459c629a0</div><div id='time'> Time: 2022-01-17</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: retro_pytorch/retro_pytorch.py</div><div id='m_class'> M Class Name: RETRO</div><div id='n_method'> N Class Name: RETRO</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: retro_pytorch/retro_pytorch.py</div><div id='n_file'> N File Name: retro_pytorch/retro_pytorch.py</div><div id='m_start'> M Start Line: 15</div><div id='m_end'> M End Line: 16</div><div id='n_start'> N Start Line: 66</div><div id='n_end'> N End Line: 101</div><BR>