<html><h3>Pattern ID :2317
</h3><img src='9872662.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 implicit grad
    v1 = torch.autograd.grad(loss,
                             <a id="change">child.trainable_parameters()</a>,
                             retain_graph=False)

    &#47&#47 ! Mabye replace with child.loss by adding self.loss attribute to save computation
    in_loss = <a id="change">child.training_step(</a>child.cur_batch<a id="change">)</a>
    in_grad = torch.autograd.grad(in_loss, child.trainable_parameters(), create_graph=True)
    v2 = approx_inverse_hvp(v1, in_grad, <a id="change">child.trainable_parameters()</a>,
                            iterations=config.neumann_iterations,
                            alpha=config.neumann_alpha)
    implicit_grad<a id="change"> = </a>torch.autograd.grad(in_grad, params, grad_outputs=v2)

    return [sub_with_none(dg, ig) for dg, ig in zip(direct_grad, implicit_grad)]
</code></pre><h3>After Change</h3><pre><code class='java'>

    &#47&#47 implicit grad
    implicit_grad = torch.autograd.grad(loss,
                                        <a id="change">path[1].trainable_parameters()</a>,
                                        retain_graph=False)
    <a id="change">for i</a> in <a id="change">range(1</a>, <a id="change">len(path</a><a id="change">)-1</a><a id="change">):
        </a>implicit_grad<a id="change"> = </a>neumann_helper(implicit_grad, <a id="change">path[i]</a>, <a id="change">path[i+1]</a>, config)

    return [sub_with_none(dg, ig) for dg, ig in zip(direct_grad, implicit_grad)]
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 14</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/leopard-ai/betty/commit/5576beb63f931a42bedc2b2397d20b80515cee1e#diff-2eaca33a3d9bd6167a3c00b16773071ae29c98853a5cda4ed9bf5fb18cb47cbfL15' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 9872662</div><div id='project'> Project Name: leopard-ai/betty</div><div id='commit'> Commit Name: 5576beb63f931a42bedc2b2397d20b80515cee1e</div><div id='time'> Time: 2022-04-23</div><div id='author'> Author: sangkeuc@andrew.cmu.edu</div><div id='file'> File Name: betty/hypergradient/neumann.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: neumann(7)</div><div id='n_method'> N Method Name: neumann(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: betty/hypergradient/neumann.py</div><div id='n_file'> N File Name: betty/hypergradient/neumann.py</div><div id='m_start'> M Start Line: 15</div><div id='m_end'> M End Line: 25</div><div id='n_start'> N Start Line: 15</div><div id='n_end'> N End Line: 21</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 implicit grad
    v1 = torch.autograd.grad(loss,
                             <a id="change">child.trainable_parameters()</a>,
                             retain_graph=False)

    &#47&#47 ! Mabye replace with child.loss by adding self.loss attribute to save computation
    in_loss<a id="change"> = </a><a id="change">child.training_step(</a>child.cur_batch<a id="change">)</a>
    in_grad = torch.autograd.grad(in_loss, child.trainable_parameters(), create_graph=True)
    v2 = approx_inverse_hvp(v1, in_grad, <a id="change">child.trainable_parameters()</a>,
                            iterations=config.neumann_iterations,
                            alpha=config.neumann_alpha)
    implicit_grad = torch.autograd.grad(in_grad, params, grad_outputs=v2)</code></pre><h3>After Change</h3><pre><code class='java'>

    &#47&#47 implicit grad
    implicit_grad = torch.autograd.grad(loss,
                                        <a id="change">path[1].trainable_parameters()</a>,
                                        retain_graph=False)
    <a id="change">for i</a> in <a id="change">range(1</a>, <a id="change">len(</a>path<a id="change">)-1</a><a id="change">):
        </a>implicit_grad<a id="change"> = </a>neumann_helper(implicit_grad, <a id="change">path[i]</a>, <a id="change">path[i+1]</a>, config)

    return [sub_with_none(dg, ig) for dg, ig in zip(direct_grad, implicit_grad)]
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leopard-ai/betty/commit/5576beb63f931a42bedc2b2397d20b80515cee1e#diff-2eaca33a3d9bd6167a3c00b16773071ae29c98853a5cda4ed9bf5fb18cb47cbfL6' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 9872596</div><div id='project'> Project Name: leopard-ai/betty</div><div id='commit'> Commit Name: 5576beb63f931a42bedc2b2397d20b80515cee1e</div><div id='time'> Time: 2022-04-23</div><div id='author'> Author: sangkeuc@andrew.cmu.edu</div><div id='file'> File Name: betty/hypergradient/neumann.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: neumann(7)</div><div id='n_method'> N Method Name: neumann(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: betty/hypergradient/neumann.py</div><div id='n_file'> N File Name: betty/hypergradient/neumann.py</div><div id='m_start'> M Start Line: 15</div><div id='m_end'> M End Line: 25</div><div id='n_start'> N Start Line: 15</div><div id='n_end'> N End Line: 21</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                                      allow_unused=allow_unused)

    &#47&#47 implicit grad
    implicit_grad = torch.autograd.grad(loss, <a id="change">path[1].trainable_parameters()</a>)
    <a id="change">for i</a> in <a id="change">range(1</a>, <a id="change">len(</a>path<a id="change">)-1</a><a id="change">):
        </a>implicit_grad<a id="change"> = </a>darts_helper(implicit_grad, <a id="change">path[i]</a>, <a id="change">path[i+1]</a>, config)

    return [add_with_none(dg, ig) for dg, ig in zip(direct_grad, implicit_grad)]
</code></pre><h3>After Change</h3><pre><code class='java'>
    grad_p = torch.autograd.grad(loss_p, prev.trainable_parameters())

    &#47&#47 negative
    for p, v in zip(<a id="change">curr.trainable_parameters()</a>, vector):
        p.data.add_(v.data, alpha=-2*eps)
    loss_n<a id="change"> = </a><a id="change">curr.training_step(</a>curr.cur_batch<a id="change">)</a>
    grad_n = torch.autograd.grad(loss_n, prev.trainable_parameters())

    &#47&#47 reverse weight change
    for p, v in zip(<a id="change">curr.trainable_parameters()</a>, vector):
        p.data.add(v.data, alpha=eps)

    implicit_grad = [(x - y).div_(2 * eps) for x, y in zip(grad_n, grad_p)]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leopard-ai/betty/commit/5542eb6dd30fc3b008a0626760a339f3fe31088e#diff-fee71c6e11eca0a047fb000e6ce9c0986274faf27ef8f81ac06ee49870b9ef48L6' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 9872658</div><div id='project'> Project Name: leopard-ai/betty</div><div id='commit'> Commit Name: 5542eb6dd30fc3b008a0626760a339f3fe31088e</div><div id='time'> Time: 2022-06-03</div><div id='author'> Author: sangkeuc@andrew.cmu.edu</div><div id='file'> File Name: betty/hypergradient/darts.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: darts(3)</div><div id='n_method'> N Method Name: darts(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: betty/hypergradient/darts.py</div><div id='n_file'> N File Name: betty/hypergradient/darts.py</div><div id='m_start'> M Start Line: 6</div><div id='m_end'> M End Line: 49</div><div id='n_start'> N Start Line: 6</div><div id='n_end'> N End Line: 36</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 implicit grad
    implicit_grad = torch.autograd.grad(loss,
                                        <a id="change">path[1].trainable_parameters()</a>,
                                        retain_graph=False)
    <a id="change">for i</a> in <a id="change">range(1</a>, <a id="change">len(</a>path<a id="change">)-1</a><a id="change">):
        </a>implicit_grad<a id="change"> = </a>neumann_helper(implicit_grad, <a id="change">path[i]</a>, <a id="change">path[i+1]</a>, config)

    return [add_with_none(dg, ig) for dg, ig in zip(direct_grad, implicit_grad)]
</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 ! Mabye replace with child.loss by adding self.loss attribute to save computation
    assert len(curr.paths) == 0, &quotneumann method is not supported for higher order MLO!&quot
    config = curr.config
    in_loss<a id="change"> = </a><a id="change">curr.training_step(</a>curr.cur_batch<a id="change">)</a>
    in_grad = torch.autograd.grad(in_loss, <a id="change">curr.trainable_parameters()</a>, create_graph=True)
    v2 = approx_inverse_hvp(vector, in_grad, <a id="change">curr.trainable_parameters()</a>,
                            iterations=config.neumann_iterations,
                            alpha=config.neumann_alpha)
    implicit_grad = torch.autograd.grad(in_grad, prev.trainable_parameters(), grad_outputs=v2)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leopard-ai/betty/commit/5542eb6dd30fc3b008a0626760a339f3fe31088e#diff-2eaca33a3d9bd6167a3c00b16773071ae29c98853a5cda4ed9bf5fb18cb47cbfL6' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 9872670</div><div id='project'> Project Name: leopard-ai/betty</div><div id='commit'> Commit Name: 5542eb6dd30fc3b008a0626760a339f3fe31088e</div><div id='time'> Time: 2022-06-03</div><div id='author'> Author: sangkeuc@andrew.cmu.edu</div><div id='file'> File Name: betty/hypergradient/neumann.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: neumann(3)</div><div id='n_method'> N Method Name: neumann(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: betty/hypergradient/neumann.py</div><div id='n_file'> N File Name: betty/hypergradient/neumann.py</div><div id='m_start'> M Start Line: 6</div><div id='m_end'> M End Line: 52</div><div id='n_start'> N Start Line: 6</div><div id='n_end'> N End Line: 26</div><BR>