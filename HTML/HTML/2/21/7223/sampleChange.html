<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        for config in self._config.global_embedding_tables:
            key = prefix + f"{config.name}.weight"
            <a id="change">if config in self._config.local_embedding_tables</a>:
                config_idx<a id="change"> = </a><a id="change">self._config.local_embedding_tables.index(config</a><a id="change">)</a>
                param<a id="change"> = </a><a id="change">self.split_embedding_weights()[config_idx]</a>
                <a id="change">assert </a>config.local_rows == param.size(0)
                assert config.local_cols == param.size(1)
                <a id="change">if </a>config.local_metadata is not None:
                    destination[key] = sharded_tensor.init_from_local_shards(
                        <a id="change">[</a><a id="change">Shard(</a>param, config.local_metadata<a id="change">)</a>],
                        <a id="change">[</a>config.num_embeddings, config.embedding_dim<a id="change"></a>],
                        process_group=self._pg,
                    )
                else:
                    destination[key] = param
            else:
                &#47&#47 just an handler for tw-related sharding on the rank that
                &#47&#47 those tables aren&quott exist, this is to comply with SPMD
                <a id="change">sharded_tensor.init_from_local_shards(
                    </a><a id="change">[]</a>,
                    <a id="change">[</a>config.num_embeddings, config.embedding_dim<a id="change"></a>]<a id="change">,
                    process_group=self._pg,
                )</a>
        return destination

    def split_embedding_weights(self) -&gt; List[torch.Tensor]:
        return self.emb_module.split_embedding_weights()</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 pyre-ignore [16]
            destination._metadata = OrderedDict()

        for config, <a id="change">param</a> in <a id="change">zip(
            </a>self._config.embedding_tables,
            self.split_embedding_weights()<a id="change">,
        )</a>:
            key = prefix + f"{config.name}.weight"
            <a id="change">assert </a>config.local_rows == param.size(0)
            assert config.local_cols == param.size(1)
            <a id="change">if </a>config.global_metadata is not None:
                &#47&#47 set additional field of sharded tensor based on local tensor properties
                config.global_metadata.tensor_properties.dtype<a id="change"> = </a>param.dtype
                config.global_metadata.tensor_properties.requires_grad<a id="change"> = </a>(
                    param.requires_grad
                )
                destination[</code></pre>