<html><h3>Pattern ID :4904
</h3><img src='17271876.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        For a single table with multiple shards (i.e CW) these are combined into one table/weight.
        Used in composability.
        
        <a id="change">table_name_to_count = </a><a id="change">self.table_name_to_count.copy()</a>
        emb_module = self._emb_module
        &#47&#47 TODO: move logic to FBGEMM to avoid accessing fbgemm internals
        <a id="change">for </a>t_idx, (rows, dim, location, _) in enumerate(emb_module.embedding_specs)<a id="change">:
            table_name = </a><a id="change">self._config.embedding_tables[t_idx]</a>.name
            <a id="change">if table_name not in table_name_to_count</a>:
                <a id="change">continue</a>
            table_count<a id="change"> = table_name_to_count</a><a id="change">.pop(table_name</a><a id="change">)</a>
            if emb_module.weights_precision == SparseType.INT8:
                dim += emb_module.int8_emb_row_dim_offset
            &#47&#47 pyre-ignore[29]
            offset<a id="change"> = </a><a id="change">emb_module.weights_physical_offsets[t_idx]</a>
            weights: torch.Tensor
            if location == EmbeddingLocation.DEVICE.value:
                &#47&#47 pyre-ignore
                weights = emb_module.weights_dev
            elif location == EmbeddingLocation.HOST.value:
                &#47&#47 pyre-ignore
                weights = emb_module.weights_host
            else:
                &#47&#47 pyre-ignore
                weights = emb_module.weights_uvm
            weight<a id="change"> = </a><a id="change">TableBatchedEmbeddingSlice(
                original_tensor=weights,
                start_offset=offset,
                end_offset=offset + table_count * rows * dim,
                num_embeddings=-1,
                embedding_dim=dim,
            )</a>
            &#47&#47 TODO: use InBackwardOptimizer
            &#47&#47 pyre-ignore
            weight._overlapped_optimizer = self._optim_per_table[table_name]
            <a id="change">yield </a>(<a id="change">table_name</a><a id="change">, weight</a>)


class BatchedDenseEmbeddingBag(BaseBatchedEmbeddingBag):</code></pre><h3>After Change</h3><pre><code class='java'>
        For a single table with multiple shards (i.e CW) these are combined into one table/weight.
        Used in composability.
        
        <a id="change">return </a>_named_parameters_by_table_fused(
            self._emb_module,
            <a id="change">self.table_name_to_count.copy()</a>,
            self._config,
            self._optim_per_table,
        )</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 18</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/0ffa1c7616403c416c73118107bec307baa3abfb#diff-f743d49eaf5a79885b5b70ca6c2790355c1cbadf8aab5605549ab11c68545109L760' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 17271876</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: 0ffa1c7616403c416c73118107bec307baa3abfb</div><div id='time'> Time: 2022-12-21</div><div id='author'> Author: colin2328@meta.com</div><div id='file'> File Name: torchrec/distributed/batched_embedding_kernel.py</div><div id='m_class'> M Class Name: BatchedFusedEmbeddingBag</div><div id='n_method'> N Class Name: BatchedFusedEmbeddingBag</div><div id='m_method'> M Method Name: named_parameters_by_table(1)</div><div id='n_method'> N Method Name: named_parameters_by_table(1)</div><div id='m_parent_class'> M Parent Class: FusedOptimizerModule,BaseBatchedEmbeddingBag</div><div id='n_parent_class'> N Parent Class: FusedOptimizerModule,BaseBatchedEmbeddingBag</div><div id='m_file'> M File Name: torchrec/distributed/batched_embedding_kernel.py</div><div id='n_file'> N File Name: torchrec/distributed/batched_embedding_kernel.py</div><div id='m_start'> M Start Line: 760</div><div id='m_end'> M End Line: 794</div><div id='n_start'> N Start Line: 833</div><div id='n_end'> N End Line: 838</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        For a single table with multiple shards (i.e CW) these are combined into one table/weight.
        Used in composability.
        
        <a id="change">table_name_to_count = </a><a id="change">self.table_name_to_count.copy()</a>
        emb_module = self._emb_module
        &#47&#47 TODO: move logic to FBGEMM to avoid accessing fbgemm internals
        <a id="change">for t_idx</a>, (rows, dim, location, _) in enumerate(emb_module.embedding_specs)<a id="change">:
            table_name = </a><a id="change">self._config.embedding_tables[t_idx]</a>.name
            <a id="change">if table_name not in table_name_to_count</a>:
                <a id="change">continue</a>
            table_count<a id="change"> = </a><a id="change">table_name_to_count.pop(</a>table_name<a id="change">)</a>
            if emb_module.weights_precision == SparseType.INT8:
                dim += emb_module.int8_emb_row_dim_offset
            &#47&#47 pyre-ignore[29]
            offset<a id="change"> = </a><a id="change">emb_module.weights_physical_offsets[t_idx]</a>
            weights: torch.Tensor
            if location == EmbeddingLocation.DEVICE.value:
                &#47&#47 pyre-ignore
                weights = emb_module.weights_dev
            elif location == EmbeddingLocation.HOST.value:
                &#47&#47 pyre-ignore
                weights = emb_module.weights_host
            else:
                &#47&#47 pyre-ignore
                weights = emb_module.weights_uvm
            weight<a id="change"> = </a><a id="change">TableBatchedEmbeddingSlice(
                original_tensor=weights,
                start_offset=offset,
                end_offset=offset + table_count * rows * dim,
                num_embeddings=-1,
                embedding_dim=dim,
            )</a>
            &#47&#47 TODO: use InBackwardOptimizer
            &#47&#47 pyre-ignore
            weight._overlapped_optimizer = self._optim_per_table[table_name]
            <a id="change">yield </a>(table_name<a id="change">, weight</a>)


class BatchedDenseEmbeddingBag(BaseBatchedEmbeddingBag):</code></pre><h3>After Change</h3><pre><code class='java'>
        For a single table with multiple shards (i.e CW) these are combined into one table/weight.
        Used in composability.
        
        <a id="change">return </a>_named_parameters_by_table_fused(
            self._emb_module,
            <a id="change">self.table_name_to_count.copy()</a>,
            self._config,
            self._optim_per_table,
        )</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/0ffa1c7616403c416c73118107bec307baa3abfb#diff-f743d49eaf5a79885b5b70ca6c2790355c1cbadf8aab5605549ab11c68545109L752' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 17271891</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: 0ffa1c7616403c416c73118107bec307baa3abfb</div><div id='time'> Time: 2022-12-21</div><div id='author'> Author: colin2328@meta.com</div><div id='file'> File Name: torchrec/distributed/batched_embedding_kernel.py</div><div id='m_class'> M Class Name: BatchedFusedEmbeddingBag</div><div id='n_method'> N Class Name: BatchedFusedEmbeddingBag</div><div id='m_method'> M Method Name: named_parameters_by_table(1)</div><div id='n_method'> N Method Name: named_parameters_by_table(1)</div><div id='m_parent_class'> M Parent Class: FusedOptimizerModule,BaseBatchedEmbeddingBag</div><div id='n_parent_class'> N Parent Class: FusedOptimizerModule,BaseBatchedEmbeddingBag</div><div id='m_file'> M File Name: torchrec/distributed/batched_embedding_kernel.py</div><div id='n_file'> N File Name: torchrec/distributed/batched_embedding_kernel.py</div><div id='m_start'> M Start Line: 760</div><div id='m_end'> M End Line: 794</div><div id='n_start'> N Start Line: 833</div><div id='n_end'> N End Line: 838</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        For a single table with multiple shards (i.e CW) these are combined into one table/weight.
        Used in composability.
        
        <a id="change">table_name_to_count = </a><a id="change">self.table_name_to_count.copy()</a>
        &#47&#47 TODO: move logic to FBGEMM to avoid accessing fbgemm internals
        <a id="change">for t_idx</a>, (rows, dim) in enumerate(self._emb_module.embedding_specs)<a id="change">:
            table_name = </a><a id="change">self._config.embedding_tables[t_idx]</a>.name
            <a id="change">if table_name not in table_name_to_count</a>:
                <a id="change">continue</a>
            table_count<a id="change"> = </a><a id="change">table_name_to_count.pop(</a>table_name<a id="change">)</a>
            offset<a id="change"> = </a><a id="change">self._emb_module.weights_physical_offsets[t_idx]</a>
            weight<a id="change"> = </a><a id="change">TableBatchedEmbeddingSlice(
                original_tensor=self._emb_module.weights,
                start_offset=offset,
                end_offset=offset + table_count * rows * dim,
                num_embeddings=-1,
                embedding_dim=dim,
            )</a>
            <a id="change">yield </a>(table_name<a id="change">, weight</a>)
</code></pre><h3>After Change</h3><pre><code class='java'>
        For a single table with multiple shards (i.e CW) these are combined into one table/weight.
        Used in composability.
        
        <a id="change">return </a>_named_parameters_by_table_dense(
            self._emb_module, <a id="change">self.table_name_to_count.copy()</a>, self._config
        )
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/0ffa1c7616403c416c73118107bec307baa3abfb#diff-f743d49eaf5a79885b5b70ca6c2790355c1cbadf8aab5605549ab11c68545109L840' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 17271874</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: 0ffa1c7616403c416c73118107bec307baa3abfb</div><div id='time'> Time: 2022-12-21</div><div id='author'> Author: colin2328@meta.com</div><div id='file'> File Name: torchrec/distributed/batched_embedding_kernel.py</div><div id='m_class'> M Class Name: BatchedDenseEmbeddingBag</div><div id='n_method'> N Class Name: BatchedDenseEmbeddingBag</div><div id='m_method'> M Method Name: named_parameters_by_table(1)</div><div id='n_method'> N Method Name: named_parameters_by_table(1)</div><div id='m_parent_class'> M Parent Class: BaseBatchedEmbeddingBag</div><div id='n_parent_class'> N Parent Class: BaseBatchedEmbeddingBag</div><div id='m_file'> M File Name: torchrec/distributed/batched_embedding_kernel.py</div><div id='n_file'> N File Name: torchrec/distributed/batched_embedding_kernel.py</div><div id='m_start'> M Start Line: 848</div><div id='m_end'> M End Line: 863</div><div id='n_start'> N Start Line: 894</div><div id='n_end'> N End Line: 896</div><BR>