<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 Forward through models
    target = trans(example_clip)

    <a id="change">outputs</a> = <a id="change">[]</a>
    &#47&#47 Manual zero pad (due to padding=1 in trans.b Conv3d)
    zeros = torch.zeros_like(example_clip[:, :, 0])
    outputs.append(cotrans.forward(zeros))
    <a id="change">for </a><a id="change">i</a> in <a id="change">range(example_clip.shape[2]</a><a id="change">):
        outputs</a><a id="change">.append(cotrans.forward(</a><a id="change">example_clip[:, :, i])</a><a id="change">)</a>
    outputs.append(cotrans.forward(zeros))
    outputs.append(cotrans.forward(zeros))
    <a id="change">outputs.append(</a>cotrans.forward(zeros)<a id="change">)</a>

    &#47&#47 For debugging:
    &#47&#47 close = []
    &#47&#47 for t in range(target.shape[2]):
    &#47&#47     for i in range(len(outputs)):
    &#47&#47         if torch.allclose(target[:, :, t], outputs[i], atol=5e-2):
    &#47&#47             close.append(f"t = {t}, o = {i}")

    shift = example_clip.shape[2]<a id="change"> - 1</a>  &#47&#47 From SE

    &#47&#47 Not very precise because SE block still initializes
    for t in range(1, target.shape[2]):
        assert torch.allclose(target[:, :, t], outputs[t + shift], atol=5e-2)

    &#47&#47 After temporal_window_size inputs it also produces precise computation
    <a id="change">torch.allclose(target[:, :, 3]</a>, <a id="change">outputs[3 + shift]</a><a id="change">, atol=1e-9)</a>

    &#47&#47 forward_steps also works as expected
    outputs2 = cotrans.forward_steps(example_clip)
    assert torch.allclose(target, outputs2)</code></pre><h3>After Change</h3><pre><code class='java'>

    &#47&#47 Broken up
    cotrans.clean_state()
    nothing = cotrans.forward_steps(<a id="change">sample[:, :, :-1]</a>, pad_end=False)  &#47&#47 init
    assert isinstance(nothing, TensorPlaceholder)

    lasts = cotrans.forward_steps(sample[:, :, -1:], pad_end=True)</code></pre>