<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                            )

                        &#47&#47 run request tensors through all requested modules, update caches
                        <a id="change">for </a>backend, backend_cache_handles, prompt in zip(requested_backends, cache_handles, prompts)<a id="change">:
                            </a>if not is_dummy(prompt):
                                hidden_states[:, : prompt.shape[1]] += prompt
                            <a id="change">if </a>hidden_states.numel() == 0:
                                <a id="change">continue</a>  &#47&#47 user passed a tensor with 0 tokens. This is a special case that occurs, e.g.
                                &#47&#47 when user wants to pre-allocate cache or check that server *can* allocate that cache

                            metadata<a id="change"> = </a>InferenceMetadata(prefix_length, tuple(backend_cache_handles))
                            assert isinstance(
                                hidden_states, torch.Tensor
                            ), f"hidden states must be tensor, got {type(hidden_states)}"</code></pre><h3>After Change</h3><pre><code class='java'>
                            prompts = [None] * len(requested_backends)
                        else:
                            prompts = [p.squeeze(0) for p in prompts.to(requested_backends[0].dtype).split(1, dim=0)]
                            prompts = [prompt<a id="change"> if </a>not is_dummy(prompt)<a id="change"> else </a>None for prompt in prompts]

                        if not (len(requested_backends) == len(prompts)):
                            raise ValueError(f"Received {len(prompts)} prompts for {len(requested_backends)} backends")</code></pre>