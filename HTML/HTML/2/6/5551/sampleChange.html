<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        hidden_with_time_axis = hidden.permute(1, 0, 2)
        print("HiddenTimeaxis:", hidden_with_time_axis.shape)
        &#47&#47 score: (batch_size, max_length, hidden_dim)
        score = <a id="change">torch.tanh(</a>self.W1(enc_output) + self.W2(hidden_with_time_axis)<a id="change">)</a>

        &#47&#47 attention_weights shape == (batch_size, max_length, 1)
        &#47&#47 we get 1 at the last axis because we are applying score to self.V
        attention_weights = torch.softmax(self.V(score), dim=1)

        &#47&#47 context_vector shape after sum == (batch_size, hidden_dim)
        context_vector<a id="change"> = </a>attention_weights<a id="change"> * </a>enc_output
        context_vector = torch.sum(context_vector, dim=1)
        &#47&#47 context_vector: batch_size, 1, hidden_dim
        context_vector = context_vector.unsqueeze(1)
        &#47&#47&#47&#47 -------------------------

        &#47&#47 x shape after embedding == (batch_size, 1, dec_embed_dim)
        x = self.embedding(x)

        &#47&#47 x shape after concatenation == (batch_size, 1, dec_embed_dim + hidden_size)
        x = torch.cat((context_vector<a id="change">, x</a>), -1)

        &#47&#47 passing the concatenated vector to the GRU
        &#47&#47 output: (batch_size, 1, hidden_size)</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 output shape == (batch_size * 1, output_dim)
        output = self.fc(output)

        <a id="change">return </a>output<a id="change">, hidden</a>


class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device,</code></pre>