<html><h3>Pattern ID :16675
</h3><img src='55896212.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        
        &#47&#47 some checks before starting
        <a id="change">if </a><a id="change">max_length is None or max_length == 0</a>:
            max_length = self.huggingface_tokenizer.model_max_length
        if return_tensors is True:
            return_tensors = "pt"</code></pre><h3>After Change</h3><pre><code class='java'>
            model_inputs.update(additional_inputs)
            &#47&#47 check if there is a padding strategy
            if padding:
                missing_keys<a id="change"> = </a>set(additional_inputs.keys()) - set(<a id="change">self.padding_ops.keys()</a>)
                <a id="change">if </a>missing_keys:
                    <a id="change">raise </a>ValueError(
                        f"There are no padding strategy for the following keys: {missing_keys}. "
                        "Please add one with `tokenizer.add_padding_ops()`."
                    )</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/riccorl/transformer-embedder/commit/de13183c5a0b5e0d39145e0e13831b33b3bb34d3#diff-e1362114c5ca41a7c33efa84bbf1f37cf18ed86e11a3db3d7fe5b859be03df1cL63' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 55896212</div><div id='project'> Project Name: riccorl/transformer-embedder</div><div id='commit'> Commit Name: de13183c5a0b5e0d39145e0e13831b33b3bb34d3</div><div id='time'> Time: 2022-03-03</div><div id='author'> Author: orlandoricc@gmail.com</div><div id='file'> File Name: transformers_embedder/tokenizer.py</div><div id='m_class'> M Class Name: Tokenizer</div><div id='n_method'> N Class Name: Tokenizer</div><div id='m_method'> M Method Name: __call__(9)</div><div id='n_method'> N Method Name: __call__(8)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: transformers_embedder/tokenizer.py</div><div id='n_file'> N File Name: transformers_embedder/tokenizer.py</div><div id='m_start'> M Start Line: 118</div><div id='m_end'> M End Line: 137</div><div id='n_start'> N Start Line: 63</div><div id='n_end'> N End Line: 135</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

            is_number = isinstance(metric_value, numbers.Number)
            &#47&#47 If not numeric type.
            <a id="change">if </a>not <a id="change">(is_number or isinstance(metric_value, Tensor) or isinstance(metric_value, np.ndarray))</a>:
                raise ValueError(f&quot{metric_with_utils.log_name} must compute number value, &quot
                                 f&quotnot numpy array element with dtype {metric_value.dtype}.&quot)
</code></pre><h3>After Change</h3><pre><code class='java'>
        for metric_with_utils in self.phase2metrics[phase.name]:
            metric_value = metric_with_utils.compute()
            if isinstance(metric_value, dict):
                metric_keys<a id="change"> = </a>list(<a id="change">metric_value.keys()</a>)
                for metric_name_d in metric_keys:
                    metric_value_d = metric_value.pop(metric_name_d)
                    if self.is_number(metric_value_d):
                        metric_value[f&quot{phase.value}/{metric_with_utils.log_name}_{metric_name_d}&quot] = metric_value_d
                &#47&#47 If there is no numeric value
                <a id="change">if </a>len(metric_value) == 0:
                    <a id="change">raise </a>ValueError(f&quotMetric manager on_epoch_end method. Metric {metric_with_utils.log_name}&quot
                                     f&quotreturn dict with has no numeric values.&quot)
                log.update(metric_value)
            elif self.is_number(metric_value):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/eora-ai/torchok/commit/c2fc24fbea4374c237f2164b01b38d617ba7b685#diff-1e414ece74f1d86149bd7914491e9ee38f9de42b30af9d90d2014826477d3087L110' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 55896214</div><div id='project'> Project Name: eora-ai/torchok</div><div id='commit'> Commit Name: c2fc24fbea4374c237f2164b01b38d617ba7b685</div><div id='time'> Time: 2022-09-01</div><div id='author'> Author: Sitcebelly</div><div id='file'> File Name: torchok/metrics/metrics_manager.py</div><div id='m_class'> M Class Name: MetricsManager</div><div id='n_method'> N Class Name: MetricsManager</div><div id='m_method'> M Method Name: on_epoch_end(2)</div><div id='n_method'> N Method Name: on_epoch_end(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: torchok/metrics/metrics_manager.py</div><div id='n_file'> N File Name: torchok/metrics/metrics_manager.py</div><div id='m_start'> M Start Line: 125</div><div id='m_end'> M End Line: 147</div><div id='n_start'> N Start Line: 132</div><div id='n_end'> N End Line: 151</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        
        num_moidifiers = len(self.modifiers)
        modifier_keys = [int(idx) for idx in state_dict.keys()]
        <a id="change">if </a>any(<a id="change">idx &lt; 0 or idx &gt;= num_moidifiers</a> for idx in modifier_keys):
            raise RuntimeError(
                "Invalid modifier index in state dict for ScheduledModifierManager for"
                "ScheduledModifierManager with {} modifiers. Given indices: {}".format(</code></pre><h3>After Change</h3><pre><code class='java'>
        modifiers_index = {mod.identifier(): mod for mod in self.modifiers}

        if strict:
            modifier_keys = {key for key in <a id="change">modifiers_index.keys()</a>}
            state_dict_keys = {key for key in state_dict.keys()}
            diff<a id="change"> = </a>modifier_keys.symmetric_difference(state_dict_keys)
            <a id="change">if </a>diff:
                <a id="change">raise </a>IndexError(
                    f"Found extra keys: {state_dict_keys - modifier_keys} "
                    f"and missing keys: {modifier_keys - state_dict_keys}"
                )</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/neuralmagic/sparseml/commit/35be148dfed1e7ea46e5fbc4e12972d7ccce121a#diff-647ea3e761b58578229a60caddc837e1c4f459270eb665da94723f1fac40ab5fL117' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 55896209</div><div id='project'> Project Name: neuralmagic/sparseml</div><div id='commit'> Commit Name: 35be148dfed1e7ea46e5fbc4e12972d7ccce121a</div><div id='time'> Time: 2021-05-12</div><div id='author'> Author: mark@neuralmagic.com</div><div id='file'> File Name: src/sparseml/pytorch/optim/manager.py</div><div id='m_class'> M Class Name: ScheduledModifierManager</div><div id='n_method'> N Class Name: ScheduledModifierManager</div><div id='m_method'> M Method Name: load_state_dict(3)</div><div id='n_method'> N Method Name: load_state_dict(2)</div><div id='m_parent_class'> M Parent Class: BaseManager,Modifier</div><div id='n_parent_class'> N Parent Class: BaseManager,Modifier</div><div id='m_file'> M File Name: src/sparseml/pytorch/optim/manager.py</div><div id='n_file'> N File Name: src/sparseml/pytorch/optim/manager.py</div><div id='m_start'> M Start Line: 126</div><div id='m_end'> M End Line: 136</div><div id='n_start'> N Start Line: 271</div><div id='n_end'> N End Line: 300</div><BR>