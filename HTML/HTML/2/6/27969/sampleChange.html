<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                [prompt], generator=generator, guidance_scale=6.0, num_inference_steps=1, output_type="numpy"
            ).images

        generator = <a id="change">torch.manual_seed(0</a><a id="change">)</a>
        image = ldm(
            [prompt], generator=generator, guidance_scale=6.0, num_inference_steps=2, output_type="numpy"
        ).images
</code></pre><h3>After Change</h3><pre><code class='java'>
        return CLIPTextModel(config)

    def test_inference_text2img(self):
        <a id="change">if torch_device != "cpu"</a>:
            return

        unet = self.dummy_cond_unet
        scheduler = DDIMScheduler()
        vae = self.dummy_vae
        bert = self.dummy_text_encoder
        tokenizer = CLIPTokenizer.from_pretrained("hf-internal-testing/tiny-random-clip")

        ldm = LDMTextToImagePipeline(vqvae=vae, bert=bert, tokenizer=tokenizer, unet=unet, scheduler=scheduler)
        ldm.to(torch_device)
        ldm.set_progress_bar_config(disable=None)

        prompt = "A painting of a squirrel eating a burger"

        &#47&#47 Warmup pass when using mps (see &#47&#47372)
        if torch_device == "mps":
            generator = torch.manual_seed(0)
            _ = ldm(
                [prompt], generator=generator, guidance_scale=6.0, num_inference_steps=1, output_type="numpy"
            ).images

        device = torch_device if torch_device != "mps" else "cpu"
        generator = torch.Generator(device=device).manual_seed(0)

        image = ldm(
            [prompt], generator=generator, guidance_scale=6.0, num_inference_steps=2, output_type="numpy"
        ).images

        device = torch_device if torch_device != "mps" else "cpu"
        generator<a id="change"> = </a>torch.Generator(device=device).manual_seed(0)

        image_from_tuple = ldm(
            [prompt],</code></pre>