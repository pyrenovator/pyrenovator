<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.layers = nn.ModuleList([
            GCN(in_dim, hidden_dim, F.relu),
            GCN(hidden_dim, hidden_dim, F.relu)])
        self.classify = <a id="change">nn.Linear(</a>hidden_dim, n_classes<a id="change">)</a>

    def forward(self, g):
        &#47&#47 For undirected graphs, in_degree is the same as
        &#47&#47 out_degree.</code></pre><h3>After Change</h3><pre><code class='java'>

        for i, (name, param) in enumerate(self.config):
            if name is &quotlinear&quot:
                w<a id="change"> = nn</a><a id="change">.Parameter(</a>torch.ones(*param)<a id="change">)</a>
                &#47&#47 gain=1 according to cbfinn&quots implementation
                init.kaiming_normal_(w)
                self.vars.append(w)
                &#47&#47 [ch_out]
                self.vars.append(nn.Parameter(torch.zeros(param[0])))
            if name is &quotGraphConv&quot:
                &#47&#47 param: in_dim, hidden_dim
                w<a id="change"> = nn</a><a id="change">.Parameter(</a>torch.Tensor(param[0], param[1])<a id="change">)</a>
                init.xavier_uniform_(w)
                self.vars.append(w)
                self.vars.append(nn.Parameter(torch.zeros(param[1])))
                self.graph_conv.append(GraphConv(param[0], param[1], activation = F.relu))</code></pre>