<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            optimizer.zero_grad()
            &#47&#47 Split data into sub-batches of size batch_size
            for i in range(0, len(data), args.batch_size):
                data_batch<a id="change"> = </a>data[i:i<a id="change"> + </a>args.batch_size]
                target_batch = target[i:i + args.batch_size]
                output = model(data_batch)
                train_accuracy.update(accuracy(output, target_batch))
                loss = F.cross_entropy(output, target_batch)
                train_loss.update(loss)
                &#47&#47 Average gradients among sub-batches
                loss.div_(math.ceil(<a id="change">float(</a>len(data)<a id="change">)</a> / args.batch_size))
                loss.backward()
            &#47&#47 Gradient is applied across all ranks
            if args.kfac_update_freq &gt; 0:</code></pre><h3>After Change</h3><pre><code class='java'>
                preconditioner.step()
            optimizer.step()

            t.set_postfix_str(<a id="change">"loss: {:.4f}, acc: {:.2f}%".format(
                    </a>train_loss.avg.item(), 100*train_accuracy.avg.item()<a id="change">)</a>)
            t.update(1)

    if log_writer:</code></pre>