<html><h3>Pattern ID :33947
</h3><img src='97229512.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                if confidence_val &gt; self.threshold:
                    bio_label = self.id2label[confidence_index.item()]
                    if bio_label == ENTITY_OUTSIDE_SYMBOL:
                        yield bio_label<a id="change">, None, confidence_val</a>
                    else:
                        bio_label, class_label = bio_label.split("-")
                        yield bio_label<a id="change">, class_label, confidence_val</a>
                else:
                    break
            else:
                bio_label = self.id2label[confidence_index.item()]</code></pre><h3>After Change</h3><pre><code class='java'>
        self.id2label = id2label

    def get_labels(self, word: TokenizedWord) -&gt; Set[Tuple[str, Optional[str]]]:
        bio_and_class_labels<a id="change">: Set[Tuple[str, Optional[str]]] = set()</a>
        token_confidences_sorted = torch.argsort(word.token_confidences, dim=1, descending=True)

        for i, token_confidence_indices in enumerate(token_confidences_sorted):
            for confidence_index in token_confidence_indices:
                confidence_val: float = word.token_confidences[i][confidence_index].item()
                if confidence_val &gt; self.threshold:
                    bio_label = self.id2label[confidence_index.item()]
                    if bio_label == ENTITY_OUTSIDE_SYMBOL:
                        <a id="change">bio_and_class_labels.add(
                            </a>(
                                bio_label,
                                None,
                            )<a id="change">
                        )</a>
                    else:
                        bio_label, class_label = bio_label.split("-")
                        bio_and_class_labels.add(
                            (</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/astrazeneca/kazu/commit/422eb2bceb9b3988a63113d66d19b1298220cec1#diff-1ed01e16084b465052b00a2921f81152aeb1492e7e29cf6f1f8b12775c2e45cfL246' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 97229512</div><div id='project'> Project Name: astrazeneca/kazu</div><div id='commit'> Commit Name: 422eb2bceb9b3988a63113d66d19b1298220cec1</div><div id='time'> Time: 2022-11-30</div><div id='author'> Author: richard.jackson4@astrazeneca.com</div><div id='file'> File Name: kazu/steps/ner/tokenized_word_processor.py</div><div id='m_class'> M Class Name: SmartSpanFinder</div><div id='n_method'> N Class Name: SmartSpanFinder</div><div id='m_method'> M Method Name: get_labels(2)</div><div id='n_method'> N Method Name: get_labels(3)</div><div id='m_parent_class'> M Parent Class: SpanFinder</div><div id='n_parent_class'> N Parent Class: SpanFinder</div><div id='m_file'> M File Name: kazu/steps/ner/tokenized_word_processor.py</div><div id='n_file'> N File Name: kazu/steps/ner/tokenized_word_processor.py</div><div id='m_start'> M Start Line: 279</div><div id='m_end'> M End Line: 312</div><div id='n_start'> N Start Line: 246</div><div id='n_end'> N End Line: 272</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        confidences_indices_sorted = torch.argsort(
            word.token_confidences[label_index], dim=-1, descending=True
        )
        for <a id="change">confidence_index</a> in confidences_indices_sorted:
            confidence_val: float = word.token_confidences[label_index][confidence_index].item()
            bio_label = self.id2label[confidence_index.item()]
            if bio_label == ENTITY_OUTSIDE_SYMBOL:
                yield bio_label<a id="change">, None, confidence_val</a>
            else:
                bio_label, class_label = bio_label.split("-")
                yield bio_label<a id="change">, class_label, confidence_val</a>
            break

    def span_continue_condition(
        self, word: TokenizedWord, bio_and_class_labels: Set[Tuple[str, Optional[str]]]</code></pre><h3>After Change</h3><pre><code class='java'>
        super().__init__(text, id2label)

    def get_labels(self, word: TokenizedWord) -&gt; Set[Tuple[str, Optional[str]]]:
        bio_and_class_labels<a id="change">: Set[Tuple[str, Optional[str]]] = set()</a>
        token_confidences_sorted = torch.argsort(word.token_confidences, dim=1, descending=True)

        for token_confidence in token_confidences_sorted:
            confidence_index = token_confidence[0]
            bio_label = self.id2label[confidence_index.item()]
            if bio_label == ENTITY_OUTSIDE_SYMBOL:
                bio_and_class_labels.add(
                    (
                        bio_label,
                        None,
                    )
                )
            else:
                bio_label, class_label = bio_label.split("-")
                <a id="change">bio_and_class_labels.add(
                    </a>(
                        bio_label,
                        class_label,
                    )<a id="change">
                )</a>

        return bio_and_class_labels

    def span_continue_condition(</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/astrazeneca/kazu/commit/422eb2bceb9b3988a63113d66d19b1298220cec1#diff-1ed01e16084b465052b00a2921f81152aeb1492e7e29cf6f1f8b12775c2e45cfL190' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 97229518</div><div id='project'> Project Name: astrazeneca/kazu</div><div id='commit'> Commit Name: 422eb2bceb9b3988a63113d66d19b1298220cec1</div><div id='time'> Time: 2022-11-30</div><div id='author'> Author: richard.jackson4@astrazeneca.com</div><div id='file'> File Name: kazu/steps/ner/tokenized_word_processor.py</div><div id='m_class'> M Class Name: SimpleSpanFinder</div><div id='n_method'> N Class Name: SimpleSpanFinder</div><div id='m_method'> M Method Name: get_labels(2)</div><div id='n_method'> N Method Name: get_labels(3)</div><div id='m_parent_class'> M Parent Class: SpanFinder</div><div id='n_parent_class'> N Parent Class: SpanFinder</div><div id='m_file'> M File Name: kazu/steps/ner/tokenized_word_processor.py</div><div id='n_file'> N File Name: kazu/steps/ner/tokenized_word_processor.py</div><div id='m_start'> M Start Line: 191</div><div id='m_end'> M End Line: 210</div><div id='n_start'> N Start Line: 167</div><div id='n_end'> N End Line: 189</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 NOTE: setting add_comm_times_to_balance, is for only debug porpuses

    <a id="change">TOPO_AWARE</a> = False
    PRINT_THEORETICAL = False
    PRINT_MIN_MAX_BALANCE = False

    PRINT_VAR_STD = False

    UTILIZATION_SLOWDOWN_SPEEDUP = True
    PRINT_1F1B = True
    DO_THEORETICAL = False

    TRY_SSGD_ANALYSIS = False
    TRY_ASGD_ANALYSIS = True

    &#47&#47 given:
    &#47&#47 stages_on_same_gpu = [{0, 4}]
    &#47&#47 internal represntation:
    &#47&#47 stages_on_same_gpu[0] = {0, 4}
    &#47&#47 stages_on_same_gpu[4] = {0, 4}

    unique_stages_on_same_gpu = stages_on_same_gpu
    stages_on_same_gpu = defaultdict(set)

    for i in unique_stages_on_same_gpu:
        for j in i:
            stages_on_same_gpu[j] = i

    for i in unique_stages_on_same_gpu:
        assert len(i) &gt;= 1

    num_dummy_stages = sum((len(i) - 1) for i in unique_stages_on_same_gpu)

    if graph is not None and DO_THEORETICAL:
        &#47&#47 thoeretical analysis
        sequential_f, sequential_b, parallel_f, parallel_b = theoretical_analysis(
            graph, recomputation=recomputation, async_pipeline=async_pipeline)
        edges = edge_cut(graph)
        &#47&#47 theoretical analysis based on the graph assuming the computation is sequential
        theoretical_sequential_b_balance = worst_balance(sequential_b)
        theoretical_sequential_f_balance = worst_balance(sequential_f)
        if TOPO_AWARE:
            (topology_aware_sequential_f_balance<a id="change">,
             topology_aware_sequential_b_balance</a>) = topology_aware_balance(
                sequential_f, sequential_b, edges)
        &#47&#47 theoretical anaysis based on the graph assuming the computation is fully parallel
        theoretical_parallel_b_balance = worst_balance(parallel_b)
        theoretical_parallel_f_balance = worst_balance(parallel_f)
        if TOPO_AWARE:
            topology_aware_parallel_f_balance, topology_aware_parallel_b_balance = topology_aware_balance(
                parallel_f, parallel_b, edges)
    else:
        edges = None
        TOPO_AWARE = False
        PRINT_THEORETICAL = False

    &#47&#47 real statistics based on generated partitions
    if torch.cuda.is_available():
        torch.cuda.reset_max_memory_allocated()
    ((real_f_times, f_vars, f_deviance), (real_b_times, b_vars, b_deviance),
     comm_volume_stats, nocomm_real_f_times, nocomm_real_b_times,
     warnings_list) = profile_execution(
        sample,
        config,
        n_iter + 1,
        recomputation=recomputation,
        bw_GBps=bw_GBps,
        async_pipeline=async_pipeline,
        add_comm_times_to_balance=add_comm_times_to_balance,
        stages_on_same_gpu=stages_on_same_gpu)

    &#47&#47 max memory
    if torch.cuda.is_available():
        max_memory_allocated = torch.cuda.max_memory_allocated()

    def add_dicts(d1, d2):
        d = {}
        for (i1, v1), (i2, v2) in zip(d1.items(), d2.items()):
            assert i1 == i2
            d[i1] = v1 + v2
        return d

    def get_seq_no_recomp_no_comm_times():
        try:
            seq_times = profile_execution(
                sample,
                config,
                n_iter + 1,
                recomputation=False,
                bw_GBps=bw_GBps,  &#47&#47 don&quott care
                async_pipeline=False,  &#47&#47 don&quott care
                add_comm_times_to_balance=add_comm_times_to_balance,  &#47&#47 don&quott care
                stages_on_same_gpu=stages_on_same_gpu)  &#47&#47 don&quott care
        except Exception as e:
            print("-E- failed at get_seq_no_recomp_no_comm_times, known issue")
            raise e
        return seq_times

    def get_comm_vol_str(comm_volume_stats):
        communication_volume = dict()
        for idx, stats in comm_volume_stats.items():
            units = {
                "input size": "MB",
                "recieve_time": "ms",
                "out": "MB",
                "send time": "ms",
            }
            newd = {k: f"{stats[k]:.2f} {units[k]}" for k in stats}
            communication_volume[idx] = &quot, &quot.join("{!s}:{!r}".format(key, val)
                                                  for (key,
                                                       val) in newd.items())
        return communication_volume

    n_partitions = sum(1 for k in config if isinstance(k, int))
    num_real_stages = n_partitions - num_dummy_stages

    if n_partitions != num_real_stages:
        &#47&#47 TODO: shrink everything
        for i in unique_stages_on_same_gpu:
            j = min(i)
            for k in i:
                if k == j:
                    continue
                for means_list in [
                    real_f_times, real_b_times, nocomm_real_f_times,
                    nocomm_real_b_times, comm_volume_stats
                ]:
                    if isinstance(means_list[j], dict):

                        d1 = means_list[j]
                        d2 = means_list[k]

                        for key in d1:
                            d1[key] += d2[key]

                    else:
                        means_list[j] += means_list[k]

                    del means_list[k]

    comm_volume_str = get_comm_vol_str(comm_volume_stats)
    real_b_balance = worst_balance(real_b_times)
    real_f_balance = worst_balance(real_f_times)

    if TOPO_AWARE:
        (topology_aware_real_f_balance<a id="change">,
         topology_aware_real_b_balance</a>) = topology_aware_balance(
            real_f_times, real_b_times, edges)

    real_b_slowdown = slowdown(real_b_times, nocomm_real_b_times)</code></pre><h3>After Change</h3><pre><code class='java'>

    pipeline_representation_stage_to_device_map = list()
    for stage_id in range(n_partitions):
        seen_devices<a id="change"> = set()</a>
        if stage_id in stages_on_same_gpu:
            device_id = min(stages_on_same_gpu[stage_id])
        else:
            device_id = len(seen_devices)
        <a id="change">seen_devices.add(</a>device_id<a id="change">)</a>
        pipeline_representation_stage_to_device_map.append(device_id)

    if n_partitions != num_real_stages:
        &#47&#47 TODO: shrink everything</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/55ef4505394e58f0d443480215374451f331ea58#diff-ef7d2c9ed558aa89ae7ace7e088f551850b6412ae53ed8b8b2d91668b05204e6L29' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 97229507</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 55ef4505394e58f0d443480215374451f331ea58</div><div id='time'> Time: 2020-09-09</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: analysis/partition_analysis.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: run_analysis(11)</div><div id='n_method'> N Method Name: run_analysis(11)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: analysis/partition_analysis.py</div><div id='n_file'> N File Name: analysis/partition_analysis.py</div><div id='m_start'> M Start Line: 50</div><div id='m_end'> M End Line: 197</div><div id='n_start'> N Start Line: 74</div><div id='n_end'> N End Line: 292</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        confidences_indices_sorted = torch.argsort(
            word.token_confidences[label_index], dim=-1, descending=True
        )
        for <a id="change">confidence_index</a> in confidences_indices_sorted:
            confidence_val: float = word.token_confidences[label_index][confidence_index].item()
            if self.threshold is not None:
                if confidence_val &gt; self.threshold:
                    bio_label = self.id2label[confidence_index.item()]
                    if bio_label == ENTITY_OUTSIDE_SYMBOL:
                        yield bio_label, None, confidence_val
                    else:
                        bio_label, class_label = bio_label.split("-")
                        yield bio_label, class_label, confidence_val
                else:
                    break
            else:
                bio_label = self.id2label[confidence_index.item()]
                if bio_label == ENTITY_OUTSIDE_SYMBOL:
                    yield bio_label<a id="change">, None, confidence_val</a>
                else:
                    bio_label, class_label = bio_label.split("-")
                    yield bio_label<a id="change">, class_label, confidence_val</a>
                break

    def span_continue_condition(
        self, word: TokenizedWord, bio_and_class_labels: Set[Tuple[str, Optional[str]]]</code></pre><h3>After Change</h3><pre><code class='java'>
        self.id2label = id2label

    def get_labels(self, word: TokenizedWord) -&gt; Set[Tuple[str, Optional[str]]]:
        bio_and_class_labels<a id="change">: Set[Tuple[str, Optional[str]]] = set()</a>
        token_confidences_sorted = torch.argsort(word.token_confidences, dim=1, descending=True)

        for i, token_confidence_indices in enumerate(token_confidences_sorted):
            for confidence_index in token_confidence_indices:
                confidence_val: float = word.token_confidences[i][confidence_index].item()
                if confidence_val &gt; self.threshold:
                    bio_label = self.id2label[confidence_index.item()]
                    if bio_label == ENTITY_OUTSIDE_SYMBOL:
                        <a id="change">bio_and_class_labels.add(
                            </a>(
                                bio_label,
                                None,
                            )<a id="change">
                        )</a>
                    else:
                        bio_label, class_label = bio_label.split("-")
                        bio_and_class_labels.add(
                            (</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/astrazeneca/kazu/commit/422eb2bceb9b3988a63113d66d19b1298220cec1#diff-1ed01e16084b465052b00a2921f81152aeb1492e7e29cf6f1f8b12775c2e45cfL278' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 97229571</div><div id='project'> Project Name: astrazeneca/kazu</div><div id='commit'> Commit Name: 422eb2bceb9b3988a63113d66d19b1298220cec1</div><div id='time'> Time: 2022-11-30</div><div id='author'> Author: richard.jackson4@astrazeneca.com</div><div id='file'> File Name: kazu/steps/ner/tokenized_word_processor.py</div><div id='m_class'> M Class Name: SmartSpanFinder</div><div id='n_method'> N Class Name: SmartSpanFinder</div><div id='m_method'> M Method Name: get_labels(2)</div><div id='n_method'> N Method Name: get_labels(3)</div><div id='m_parent_class'> M Parent Class: SpanFinder</div><div id='n_parent_class'> N Parent Class: SpanFinder</div><div id='m_file'> M File Name: kazu/steps/ner/tokenized_word_processor.py</div><div id='n_file'> N File Name: kazu/steps/ner/tokenized_word_processor.py</div><div id='m_start'> M Start Line: 279</div><div id='m_end'> M End Line: 312</div><div id='n_start'> N Start Line: 246</div><div id='n_end'> N End Line: 272</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self._bins_per_octave = bins_per_octave
        self._resamplers = []
        self._bounds = bounds
        for <a id="change">i</a> in range(bounds[0], bounds[1] + 1):
            rate = 2.0 ** (-float(i) / bins_per_octave)
            s1<a id="change">, s2</a> = int(sample_rate / rate)<a id="change">, int(sample_rate)</a>
            self._resamplers.append(
                lilfilter.Resampler(
                    int(s1 / approximation_constant),
                    int(s2 / approximation_constant),</code></pre><h3>After Change</h3><pre><code class='java'>
        
        self._sample_rate = sample_rate
        self._resamplers = []
        self.fast_ratios<a id="change"> = set()</a>
        self._bins_per_octave = 12
        factors = primes.factors(sample_rate)
        products = []
        for i in range(1, len(factors) + 1):
            products.extend(
                [reduce(lambda x, y: x * y, x) for x in combinations(factors, i)]
            )
        for i in products:
            for j in products:
                f = Fraction(i, j)
                if condition(f):
                    <a id="change">self.fast_ratios.add(</a>f<a id="change">)</a>

    def __call__(self, input: torch.Tensor, shift: Fraction):
        
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/kentonishi/torch-pitch-shift/commit/104442cf773ee57bcb8c4a0c5556f60c35cb7f37#diff-c7cf0fbfc669511823403a429ad133c6c8a42e6336e1a501acd5da2857d273c7L12' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 97229520</div><div id='project'> Project Name: kentonishi/torch-pitch-shift</div><div id='commit'> Commit Name: 104442cf773ee57bcb8c4a0c5556f60c35cb7f37</div><div id='time'> Time: 2021-06-18</div><div id='author'> Author: kento24gs@outlook.com</div><div id='file'> File Name: torch_pitch_shift/main.py</div><div id='m_class'> M Class Name: PitchShifter</div><div id='n_method'> N Class Name: PitchShifter</div><div id='m_method'> M Method Name: __init__(3)</div><div id='n_method'> N Method Name: __init__(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: torch_pitch_shift/main.py</div><div id='n_file'> N File Name: torch_pitch_shift/main.py</div><div id='m_start'> M Start Line: 13</div><div id='m_end'> M End Line: 54</div><div id='n_start'> N Start Line: 16</div><div id='n_end'> N End Line: 45</div><BR>