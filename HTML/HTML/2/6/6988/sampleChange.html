<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            outputs = self.model(data)
            loss = self.loss(outputs, labels)

            shared_grads += <a id="change">[</a>torch.autograd.grad(loss, self.model.parameters())<a id="change"></a>]
            shared_buffers += [[b.clone().detach() for b in self.model.buffers()]]

        shared_data = dict(gradients=shared_grads, buffers=shared_buffers,</code></pre><h3>After Change</h3><pre><code class='java'>
                    buffer.copy_(server_state.to(**self.setup))

            optimizer = torch.optim.SGD(self.model.parameters(), lr=self.local_learning_rate)
            seen_data_idx<a id="change"> = </a>0

            for step in range(self.num_local_updates):

                data<a id="change"> = </a>user_data[seen_data_idx: seen_data_idx<a id="change"> + </a>self.num_data_per_local_update_step]
                labels = user_labels[seen_data_idx: seen_data_idx + self.num_data_per_local_update_step]
                seen_data_idx += self.num_data_per_local_update_step
                seen_data_idx<a id="change"> = </a>seen_data_idx % self.num_data_points

                optimizer.zero_grad()
                &#47&#47 Compute the forward pass
                outputs = self.model(data)
                loss = self.loss(outputs, labels)
                <a id="change">loss.backward()</a>
                optimizer.step()

            &#47&#47 Share differential to server version:
            &#47&#47 This is equivalent to sending the new stuff and letting the server do it, but in line</code></pre>