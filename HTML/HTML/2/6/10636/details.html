<html><h3>Pattern ID :10636
</h3><img src='36872626.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                state[&quotstep&quot] += 1

                if group[&quotweight_decay&quot] != 0:
                    grad<a id="change"> = </a>grad.add(p, alpha=group[&quotweight_decay&quot])

                &#47&#47 decay term
                <a id="change">p.mul_(</a>1 - group[&quotlambd&quot] * state[&quoteta&quot]<a id="change">)</a>

                &#47&#47 update parameter
                p.add_(grad, alpha=-state[&quoteta&quot])
</code></pre><h3>After Change</h3><pre><code class='java'>
            with torch.enable_grad():
                loss = closure()

        <a id="change">for </a>group in self.param_groups<a id="change">:
            </a>params_with_grad = []
            grads = []
            mus<a id="change"> = </a>[]
            axs = []
            etas = []
            state_steps = []</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/9a622f4cd9318d69896462823891588ef0bf719c#diff-b5441178f7d0764941fd9b5dc89b596d57da114d4665b7b31138beac59e93c7dL48' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 36872626</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 9a622f4cd9318d69896462823891588ef0bf719c</div><div id='time'> Time: 2021-05-19</div><div id='author'> Author: iramazanli@fb.com</div><div id='file'> File Name: torch/optim/asgd.py</div><div id='m_class'> M Class Name: ASGD</div><div id='n_method'> N Class Name: ASGD</div><div id='m_method'> M Method Name: step(2)</div><div id='n_method'> N Method Name: step(2)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: torch/optim/asgd.py</div><div id='n_file'> N File Name: torch/optim/asgd.py</div><div id='m_start'> M Start Line: 48</div><div id='m_end'> M End Line: 86</div><div id='n_start'> N Start Line: 49</div><div id='n_end'> N End Line: 94</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            dampening = group[&quotdampening&quot]
            nesterov = group[&quotnesterov&quot]

            for <a id="change">p</a> in group[&quotparams&quot]:
                if p.grad is None:
                    continue
                d_p<a id="change"> = </a>p.grad
                if weight_decay != 0:
                    d_p = d_p.add(p, alpha=weight_decay)
                if momentum != 0:
                    param_state = self.state[p]
                    if &quotmomentum_buffer&quot not in param_state:
                        buf = param_state[&quotmomentum_buffer&quot] = torch.clone(d_p).detach()
                    else:
                        buf = param_state[&quotmomentum_buffer&quot]
                        <a id="change">buf.mul_(</a>momentum<a id="change">)</a>.add_(d_p, alpha=1 - dampening)
                    if nesterov:
                        d_p = d_p.add(buf, alpha=momentum)
                    else:</code></pre><h3>After Change</h3><pre><code class='java'>
                  nesterov)

            &#47&#47 update momentum_buffers in state
            <a id="change">for </a>p, <a id="change">momentum_buffer</a> in zip(params_with_grad, momentum_buffer_list)<a id="change">:
                </a>state<a id="change"> = </a>self.state[p]
                state[&quotmomentum_buffer&quot] = momentum_buffer

        return loss</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/a0cf5566d88533c5caa7a490beb6eb0760eee9b4#diff-e5ea47a2193f1cfb039210c5c0ff83e8175739afc0551866052f6ad31bb91482L76' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 36872624</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: a0cf5566d88533c5caa7a490beb6eb0760eee9b4</div><div id='time'> Time: 2021-01-21</div><div id='author'> Author: wanchaol@users.noreply.github.com</div><div id='file'> File Name: torch/optim/sgd.py</div><div id='m_class'> M Class Name: SGD</div><div id='n_method'> N Class Name: SGD</div><div id='m_method'> M Method Name: step(2)</div><div id='n_method'> N Method Name: step(2)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: torch/optim/sgd.py</div><div id='n_file'> N File Name: torch/optim/sgd.py</div><div id='m_start'> M Start Line: 88</div><div id='m_end'> M End Line: 114</div><div id='n_start'> N Start Line: 89</div><div id='n_end'> N End Line: 124</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            with torch.enable_grad():
                loss = closure()

        for <a id="change">group</a> in self.param_groups:
            for p in group[&quotparams&quot]:
                if p.grad is None:
                    continue
                grad = p.grad
                if grad.is_sparse:
                    raise RuntimeError(&quotASGD does not support sparse gradients&quot)
                state = self.state[p]

                &#47&#47 State initialization
                if len(state) == 0:
                    state[&quotstep&quot] = 0
                    state[&quoteta&quot] = group[&quotlr&quot]
                    state[&quotmu&quot] = 1
                    state[&quotax&quot] = torch.zeros_like(p, memory_format=torch.preserve_format)

                state[&quotstep&quot] += 1

                if group[&quotweight_decay&quot] != 0:
                    grad<a id="change"> = </a>grad.add(p, alpha=group[&quotweight_decay&quot])

                &#47&#47 decay term
                <a id="change">p.mul_(</a>1 - group[&quotlambd&quot] * state[&quoteta&quot]<a id="change">)</a>

                &#47&#47 update parameter
                p.add_(grad, alpha=-state[&quoteta&quot])
</code></pre><h3>After Change</h3><pre><code class='java'>
            with torch.enable_grad():
                loss = closure()

        <a id="change">for group</a> in self.param_groups<a id="change">:
            </a>params_with_grad = []
            grads = []
            mus = []
            axs = []
            etas<a id="change"> = </a>[]
            state_steps = []

            for p in group[&quotparams&quot]:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/9a622f4cd9318d69896462823891588ef0bf719c#diff-b5441178f7d0764941fd9b5dc89b596d57da114d4665b7b31138beac59e93c7dL36' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 36872630</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 9a622f4cd9318d69896462823891588ef0bf719c</div><div id='time'> Time: 2021-05-19</div><div id='author'> Author: iramazanli@fb.com</div><div id='file'> File Name: torch/optim/asgd.py</div><div id='m_class'> M Class Name: ASGD</div><div id='n_method'> N Class Name: ASGD</div><div id='m_method'> M Method Name: step(2)</div><div id='n_method'> N Method Name: step(2)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: torch/optim/asgd.py</div><div id='n_file'> N File Name: torch/optim/asgd.py</div><div id='m_start'> M Start Line: 48</div><div id='m_end'> M End Line: 86</div><div id='n_start'> N Start Line: 49</div><div id='n_end'> N End Line: 94</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        
        for group in self.param_groups:
        
            for <a id="change">p</a> in group[&quotparams&quot]:
                if p.grad is None:
                    continue
                param_size += p.numel()
                grad = p.grad.data
                
                state = self.state[p]

                &#47&#47 State initialization
                if len(state) == 0:
                    state[&quotstep&quot] = 0
                    &#47&#47 Exponential moving average of gradient values
                    state[&quotexp_avg&quot] = torch.zeros_like(p.data, memory_format=torch.preserve_format)
                    &#47&#47 Exponential moving average of squared gradient values
                    state[&quotexp_avg_sq&quot] = torch.zeros_like(p.data, memory_format=torch.preserve_format)
                    &#47&#47 Exponential moving average of squared gradient values with the bias correction
                    state[&quotexp_avg_sq_hat&quot] = torch.zeros_like(p.data, memory_format=torch.preserve_format)
                    &#47&#47 Cumulative products of beta1
                    state[&quotbeta1_prod&quot] = torch.ones_like(p.data, memory_format=torch.preserve_format)
                    

                exp_avg_sq = state[&quotexp_avg_sq&quot]
                exp_avg_sq_hat = state[&quotexp_avg_sq_hat&quot]
                beta0, beta2 = group[&quotbetas&quot]

                state[&quotstep&quot] += 1
                bias_correction2 = 1 - beta2 ** state[&quotstep&quot]

                if group[&quotweight_decay&quot] != 0:
                    grad.add_(group[&quotweight_decay&quot], p.data)
                    
                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
                
                <a id="change">exp_avg_sq_hat.mul_(</a>0.<a id="change">)</a>.add_(exp_avg_sq / bias_correction2)
                
                exp_avg_sq_hat_sum<a id="change"> += </a>exp_avg_sq_hat.sum()
                
            &#47&#47 Calculate the mean of all elements in exp_avg_sq_hat
            exp_avg_sq_hat_mean = exp_avg_sq_hat_sum / param_size</code></pre><h3>After Change</h3><pre><code class='java'>
        param_size = 0
        exp_avg_sq_hat_sum = 0.
        
        <a id="change">for group</a> in self.param_groups<a id="change">:
            </a>for p in group[&quotparams&quot]:
                if p.grad is None:
                    continue
                param_size += p.numel()
                grad = p.grad.data
                
                state = self.state[p]

                &#47&#47 State initialization
                if len(state) == 0:
                    state[&quotstep&quot] = 0
                    &#47&#47 Exponential moving average of gradient values
                    state[&quotexp_avg&quot] = torch.zeros_like(p.data, memory_format=torch.preserve_format)
                    &#47&#47 Exponential moving average of squared gradient values
                    state[&quotexp_avg_sq&quot] = torch.zeros_like(p.data, memory_format=torch.preserve_format)
                    &#47&#47 Cumulative products of beta1
                    state[&quotbeta1_prod&quot] = torch.ones_like(p.data, memory_format=torch.preserve_format)
                    
                state[&quotstep&quot] += 1

                exp_avg_sq = state[&quotexp_avg_sq&quot]
                beta0, beta2 = group[&quotbetas&quot]

                bias_correction2 = 1 - beta2 ** state[&quotstep&quot]

                if group[&quotweight_decay&quot] != 0:
                    grad.add_(group[&quotweight_decay&quot], p.data)
                    
                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
                
                exp_avg_sq_hat_sum<a id="change"> += </a>exp_avg_sq.sum() / bias_correction2
                
        &#47&#47 Calculate the mean of all elements in exp_avg_sq_hat
        exp_avg_sq_hat_mean = exp_avg_sq_hat_sum / param_size</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/zeke-xie/adaptive-inertia-adai/commit/b9feb1e12a4a96244645ae9265f863c90499b380#diff-2a0bdaa11475a875c7f50fbcc6df752c68040c595f5c23971c452736bfe48f1eL39' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 36872634</div><div id='project'> Project Name: zeke-xie/adaptive-inertia-adai</div><div id='commit'> Commit Name: b9feb1e12a4a96244645ae9265f863c90499b380</div><div id='time'> Time: 2020-08-06</div><div id='author'> Author: 584649769@qq.com</div><div id='file'> File Name: adai_optim/adai.py</div><div id='m_class'> M Class Name: Adai</div><div id='n_method'> N Class Name: Adai</div><div id='m_method'> M Method Name: step(2)</div><div id='n_method'> N Method Name: step(2)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: adai_optim/adai.py</div><div id='n_file'> N File Name: adai_optim/adai.py</div><div id='m_start'> M Start Line: 51</div><div id='m_end'> M End Line: 103</div><div id='n_start'> N Start Line: 51</div><div id='n_end'> N End Line: 104</div><BR>