<html><h3>Pattern ID :11666
</h3><img src='39496154.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            if self.speaker_embedding:
                for datapoint in tqdm(self.datapoints):
                    tensored_datapoints.append([torch.Tensor(datapoint[0]),
                                                <a id="change">torch.LongTensor(</a>datapoint[1]<a id="change">)</a>,
                                                <a id="change">torch.Tensor(</a>datapoint[2]<a id="change">)</a>,
                                                torch.LongTensor(datapoint[3]),
                                                torch.LongTensor(datapoint[4]),
                                                torch.Tensor(datapoint[5]),
                                                torch.Tensor(datapoint[6]),
                                                torch.Tensor(datapoint[7])])
            else:
                for datapoint in tqdm(self.datapoints):
                    tensored_datapoints.append([torch.Tensor(datapoint[0]),
                                                torch.LongTensor(datapoint[1]),
                                                torch.Tensor(datapoint[2]),
                                                <a id="change">torch.LongTensor(</a>datapoint[3]<a id="change">)</a>,
                                                torch.LongTensor(datapoint[4]),
                                                torch.Tensor(datapoint[5]),
                                                torch.Tensor(datapoint[6])])</code></pre><h3>After Change</h3><pre><code class='java'>
                                rebuild_cache=rebuild_cache)
            datapoints = torch.load(os.path.join(cache_dir, "taco_train_cache.pt"), map_location=&quotcpu&quot)
            &#47&#47 we use the tacotron dataset as basis and augment it to contain the additional information we need for fastspeech.
            <a id="change">if </a>not isinstance(self.datapoints, tuple):  &#47&#47 check for backwards compatibility
                TacotronDataset(path_to_transcript_dict=path_to_transcript_dict,
                                cache_dir=cache_dir,
                                lang=lang,
                                speaker_embedding=speaker_embedding,
                                loading_processes=loading_processes,
                                device=device,
                                min_len_in_seconds=min_len_in_seconds,
                                max_len_in_seconds=max_len_in_seconds,
                                cut_silences=cut_silence,
                                rebuild_cache=True)
                datapoints<a id="change"> = </a>torch.load(os.path.join(cache_dir, "taco_train_cache.pt"), map_location=&quotcpu&quot)
            dataset = datapoints[0]
            norm_waves = datapoints[1]
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/d75af2f091227a4695d4887a9a8f23dc5d22b12b#diff-528f1d1f9b65f826c6eec25f3934452fe1a81965d76a0070a71db9ef9ef36aa0L36' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 39496154</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: d75af2f091227a4695d4887a9a8f23dc5d22b12b</div><div id='time'> Time: 2021-09-15</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: TrainingInterfaces/Text_to_Spectrogram/FastSpeech2/FastSpeechDataset.py</div><div id='m_class'> M Class Name: FastSpeechDataset</div><div id='n_method'> N Class Name: FastSpeechDataset</div><div id='m_method'> M Method Name: __init__(13)</div><div id='n_method'> N Method Name: __init__(13)</div><div id='m_parent_class'> M Parent Class: Dataset</div><div id='n_parent_class'> N Parent Class: Dataset</div><div id='m_file'> M File Name: TrainingInterfaces/Text_to_Spectrogram/FastSpeech2/FastSpeechDataset.py</div><div id='n_file'> N File Name: TrainingInterfaces/Text_to_Spectrogram/FastSpeech2/FastSpeechDataset.py</div><div id='m_start'> M Start Line: 36</div><div id='m_end'> M End Line: 96</div><div id='n_start'> N Start Line: 36</div><div id='n_end'> N End Line: 109</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            if self.speaker_embedding:
                for datapoint in tqdm(self.datapoints):
                    tensored_datapoints.append([torch.Tensor(datapoint[0]),
                                                <a id="change">torch.LongTensor(</a>datapoint[1]<a id="change">)</a>,
                                                torch.Tensor(datapoint[2]),
                                                torch.LongTensor(datapoint[3]),
                                                torch.LongTensor(datapoint[4]),
                                                torch.Tensor(datapoint[5]),
                                                torch.Tensor(datapoint[6]),
                                                <a id="change">torch.Tensor(</a>datapoint[7]<a id="change">)</a>])
            else:
                for datapoint in tqdm(self.datapoints):
                    tensored_datapoints.append([torch.Tensor(datapoint[0]),
                                                <a id="change">torch.LongTensor(</a>datapoint[1]<a id="change">)</a>,
                                                torch.Tensor(datapoint[2]),
                                                torch.LongTensor(datapoint[3]),
                                                torch.LongTensor(datapoint[4]),</code></pre><h3>After Change</h3><pre><code class='java'>
                                rebuild_cache=rebuild_cache)
            datapoints = torch.load(os.path.join(cache_dir, "taco_train_cache.pt"), map_location=&quotcpu&quot)
            &#47&#47 we use the tacotron dataset as basis and augment it to contain the additional information we need for fastspeech.
            <a id="change">if </a>not isinstance(self.datapoints, tuple):  &#47&#47 check for backwards compatibility
                TacotronDataset(path_to_transcript_dict=path_to_transcript_dict,
                                cache_dir=cache_dir,
                                lang=lang,
                                speaker_embedding=speaker_embedding,
                                loading_processes=loading_processes,
                                device=device,
                                min_len_in_seconds=min_len_in_seconds,
                                max_len_in_seconds=max_len_in_seconds,
                                cut_silences=cut_silence,
                                rebuild_cache=True)
                datapoints<a id="change"> = </a>torch.load(os.path.join(cache_dir, "taco_train_cache.pt"), map_location=&quotcpu&quot)
            dataset = datapoints[0]
            norm_waves = datapoints[1]
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/d75af2f091227a4695d4887a9a8f23dc5d22b12b#diff-528f1d1f9b65f826c6eec25f3934452fe1a81965d76a0070a71db9ef9ef36aa0L22' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 39496155</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: d75af2f091227a4695d4887a9a8f23dc5d22b12b</div><div id='time'> Time: 2021-09-15</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: TrainingInterfaces/Text_to_Spectrogram/FastSpeech2/FastSpeechDataset.py</div><div id='m_class'> M Class Name: FastSpeechDataset</div><div id='n_method'> N Class Name: FastSpeechDataset</div><div id='m_method'> M Method Name: __init__(13)</div><div id='n_method'> N Method Name: __init__(13)</div><div id='m_parent_class'> M Parent Class: Dataset</div><div id='n_parent_class'> N Parent Class: Dataset</div><div id='m_file'> M File Name: TrainingInterfaces/Text_to_Spectrogram/FastSpeech2/FastSpeechDataset.py</div><div id='n_file'> N File Name: TrainingInterfaces/Text_to_Spectrogram/FastSpeech2/FastSpeechDataset.py</div><div id='m_start'> M Start Line: 36</div><div id='m_end'> M End Line: 96</div><div id='n_start'> N Start Line: 36</div><div id='n_end'> N End Line: 109</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def __getitem__(self, index):
        &#47&#47 create tensors on correct device
        text = <a id="change">torch.LongTensor(</a>self.feature_list[index][0]<a id="change">)</a>.to(self.device)
        text_len = <a id="change">torch.LongTensor(</a>[self.feature_list[index][1]]<a id="change">)</a>.to(self.device)
        speech = torch.transpose(torch.Tensor(self.feature_list[index][2]), 0, 1).to(self.device)
        speech_len = torch.LongTensor([self.feature_list[index][3]]).to(self.device)
        if self.spemb:
            spemb = <a id="change">torch.Tensor(</a>self.feature_list[index][4]<a id="change">)</a>.to(self.device)
            return text, text_len, speech, speech_len, spemb
        return text, text_len, speech, speech_len
</code></pre><h3>After Change</h3><pre><code class='java'>
        transcript = self.path_to_transcript_dict[self.key_list[index]]
        path = self.key_list[index]
        wave, sr = sf.read(os.path.join("Corpora/CSS10/", path))
        <a id="change">if </a>self.ap is None:
            self.ap<a id="change"> = </a>AudioPreprocessor(input_sr=sr, output_sr=16000, melspec_buckets=80, hop_length=256, n_fft=1024)
        text = self.tf.string_to_tensor(transcript).long()
        text_len = torch.LongTensor(text.shape(0))
        speech = torch.transpose(self.ap.audio_to_mel_spec_tensor(wave), 0, 1)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/1e192df888be8f1dc1c20971132b31fe73153b7d#diff-0965ad6e3a8f04afe3a358a7558e203eaa1f244ad6e8e6fbfe73f8a2052b8cb1L12' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 39496168</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 1e192df888be8f1dc1c20971132b31fe73153b7d</div><div id='time'> Time: 2021-02-22</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: TransformerTTS/TransformerTTSDataset.py</div><div id='m_class'> M Class Name: TransformerTTSDataset</div><div id='n_method'> N Class Name: TransformerTTSDataset</div><div id='m_method'> M Method Name: __getitem__(2)</div><div id='n_method'> N Method Name: __getitem__(2)</div><div id='m_parent_class'> M Parent Class: Dataset</div><div id='n_parent_class'> N Parent Class: Dataset</div><div id='m_file'> M File Name: TransformerTTS/TransformerTTSDataset.py</div><div id='n_file'> N File Name: TransformerTTS/TransformerTTSDataset.py</div><div id='m_start'> M Start Line: 14</div><div id='m_end'> M End Line: 21</div><div id='n_start'> N Start Line: 32</div><div id='n_end'> N End Line: 44</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def __getitem__(self, index):
        &#47&#47 load the audio from the path, clean it, process it into a spectrogram
        &#47&#47 return a pair of cleaned audio and spectrogram
        text = <a id="change">torch.LongTensor(</a>self.feature_list[index][0]<a id="change">)</a>.to(self.device)
        text_len = <a id="change">torch.LongTensor(</a>[self.feature_list[index][1]]<a id="change">)</a>.to(self.device)
        speech = torch.transpose(torch.Tensor(self.feature_list[index][2]), 0, 1).to(self.device)
        speech_len = torch.LongTensor([self.feature_list[index][3]]).to(self.device)
        if self.spemb:
            spemb = <a id="change">torch.Tensor(</a>self.feature_list[index][4]<a id="change">)</a>.to(self.device)
            return text, text_len, speech, speech_len, spemb
        return text, text_len, speech, speech_len
</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 return a pair of cleaned audio and spectrogram
        file_path = self.list_of_paths[index]
        wave, sr = sf.read(file_path)
        <a id="change">if </a>self.ap is None:
            self.ap<a id="change"> = </a>AudioPreprocessor(input_sr=sr, output_sr=16000, melspec_buckets=80)
        normalized_wave = self.ap.audio_to_wave_tensor(wave, normalize=True, mulaw=False).to(self.device)
        melspec = self.ap.audio_to_mel_spec_tensor(normalized_wave, normalize=False).to(self.device)
        return normalized_wave, melspec</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/b43437ffd52b1d82638a75e3648c752f2492652c#diff-6d37714a960ee957f0bd8c25ac6b1f0f61e2e6e8f14e269209172239a689a904L16' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 39496178</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: b43437ffd52b1d82638a75e3648c752f2492652c</div><div id='time'> Time: 2021-02-19</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: MelGAN/MelGANDataset.py</div><div id='m_class'> M Class Name: MelGANDataset</div><div id='n_method'> N Class Name: MelGANDataset</div><div id='m_method'> M Method Name: __getitem__(2)</div><div id='n_method'> N Method Name: __getitem__(2)</div><div id='m_parent_class'> M Parent Class: Dataset</div><div id='n_parent_class'> N Parent Class: Dataset</div><div id='m_file'> M File Name: MelGAN/MelGANDataset.py</div><div id='n_file'> N File Name: MelGAN/MelGANDataset.py</div><div id='m_start'> M Start Line: 19</div><div id='m_end'> M End Line: 26</div><div id='n_start'> N Start Line: 23</div><div id='n_end'> N End Line: 29</div><BR>