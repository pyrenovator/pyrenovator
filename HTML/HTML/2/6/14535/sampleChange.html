<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
def test_conv3d_transpose(
    x_n_filters_n_pad_n_outshp_n_res, dtype, tensor_fn, device, call
):
    if call in <a id="change">[</a>helpers.tf_call, helpers.tf_graph_call<a id="change"></a>] and "cpu" in device:
        &#47&#47 tf conv3d transpose does not work when CUDA is installed, but array is on CPU
        pytest.skip()
    &#47&#47 smoke test</code></pre><h3>After Change</h3><pre><code class='java'>
    if fw == &quotmxnet&quot and "cpu" in device:
        &#47&#47 mxnet only supports 3d transpose convolutions with CUDNN
        return
    <a id="change">if fw == &quottorch&quot</a> and (&quot16&quot in dtype[0] or &quot16&quot in dtype[1]):
        &#47&#47 not implemented for half
        <a id="change">return</a>
    x = np.random.uniform(size=array_shape).astype(dtype[0])
    x = np.expand_dims(x, (-1))
    filters = np.random.uniform(size=(filter_shape,
                                      filter_shape,
                                      filter_shape,
                                      1,
                                      1)).astype(dtype[1])
    <a id="change">helpers.test_array_function(
        </a>dtype,
        as_variable,
        False,
        num_positional_args,
        native_array,
        container,
        instance_method,
        fw,
        <a id="change">"conv3d_transpose"</a><a id="change">,
        x=x,
        filters=filters,
        strides=stride,
        padding=pad,
        output_shape=output_shape,
        data_format=data_format,
        dilations=dilations
    )</a>


&#47&#47 LSTM &#47&#47
&#47&#47 -----&#47&#47</code></pre>