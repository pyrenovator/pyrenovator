<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        dnn_neurons=512,
    ):

        blocks<a id="change"> = </a><a id="change">[]</a>

        for block_index in range(cnn_blocks):
            if not using_2d_pooling:
                pooling = sb.nnet.Pooling1d(
                    pool_type="max",
                    kernel_size=inter_layer_pooling_size,
                    pool_axis=2,
                )
            else:
                pooling = sb.nnet.Pooling2d(
                    pool_type="max",
                    kernel_size=(
                        inter_layer_pooling_size,
                        inter_layer_pooling_size,
                    ),
                    pool_axis=(1, 2),
                )

            blocks.extend(
                [
                    sb.nnet.Conv2d(
                        out_channels=cnn_channels[block_index],
                        kernel_size=cnn_kernelsize,
                    ),
                    sb.nnet.LayerNorm(),
                    activation(),
                    sb.nnet.Conv2d(
                        out_channels=cnn_channels[block_index],
                        kernel_size=cnn_kernelsize,
                    ),
                    sb.nnet.LayerNorm(),
                    activation(),
                    &#47&#47 Inter-layer Pooling
                    pooling,
                    sb.nnet.Dropout2d(drop_rate=dropout),
                ]
            )

        if time_pooling:
            blocks.extend(
                [
                    sb.nnet.Pooling1d(
                        pool_type="max",
                        kernel_size=time_pooling_size,
                        pool_axis=1,
                    ),
                ]
            )

        if self_attention:
            blocks.append(
                TransformerEncoder(
                    num_layers=self_attention_layers,
                    nhead=self_attention_num_heads,
                    d_ffn=self_attention_hidden_dim,
                ),
            )

        if rnn_layers &gt; 0:
            <a id="change">blocks.extend(
                </a><a id="change">[
                    </a>rnn_class(
                        hidden_size=rnn_neurons,
                        num_layers=rnn_layers,
                        dropout=dropout,
                        bidirectional=rnn_bidirectional,
                        re_init=rnn_re_init,
                    )<a id="change"></a>,
                ]<a id="change">
            )</a>

        for block_index in range(dnn_blocks):
            if dnn_postionalwise:
                blocks.extend(</code></pre><h3>After Change</h3><pre><code class='java'>
            else:
                self.append(sb.nnet.Linear, n_neurons=dnn_neurons, bias=True)
                self.append(sb.nnet.BatchNorm1d),
                <a id="change">self.append(</a><a id="change">activation())</a>,
                self.append(torch.nn.Dropout(p=dropout))
</code></pre>