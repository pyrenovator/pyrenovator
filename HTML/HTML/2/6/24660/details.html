<html><h3>Pattern ID :24660
</h3><img src='76478142.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                self._env,
                self.device,
            )
            if <a id="change">path</a>:
                leaf_module = self._dmp_wrapped_module
                split_path = path.split(".")
                <a id="change">for </a>name in split_path[:-1]<a id="change">:
                    </a>leaf_module<a id="change"> = </a>getattr(leaf_module, name)
                setattr(leaf_module, split_path[-1], sharded_module)
            else:
                self._dmp_wrapped_module = sharded_module</code></pre><h3>After Change</h3><pre><code class='java'>
    ) -&gt; nn.Module:

        &#47&#47 pre-sharded module
        <a id="change">if </a>isinstance(module, ShardedModule):
            return module

        &#47&#47 shardable module
        module_sharding_plan = self._plan.get_plan_for_module(path)
        if module_sharding_plan:
            sharder_key = sharder_name(type(module))
            module = self._sharder_map[sharder_key].shard(
                module,
                module_sharding_plan,
                self._env,
                self.device,
            )
            return module

        for name, child in module.named_children():
            child = self._shard_modules_impl(
                child,
                path + "." + name if path else name,
            )
            <a id="change">setattr(</a>module, name, child<a id="change">)</a>

        return module

    def _init_parameters(self, module: nn.Module) -&gt; None:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/f13d0c06f6492b058beeb9fe7e2b9e2bbaa5f0f2#diff-088378575010e7b9e27ea06507ba52fcc8604bcf9cb107f4939a9a49b0d52c89L342' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 76478142</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: f13d0c06f6492b058beeb9fe7e2b9e2bbaa5f0f2</div><div id='time'> Time: 2022-11-07</div><div id='author'> Author: dstaay@meta.com</div><div id='file'> File Name: torchrec/distributed/model_parallel.py</div><div id='m_class'> M Class Name: DistributedModelParallel</div><div id='n_method'> N Class Name: DistributedModelParallel</div><div id='m_method'> M Method Name: _shard_modules_impl(3)</div><div id='n_method'> N Method Name: _shard_modules_impl(4)</div><div id='m_parent_class'> M Parent Class: FusedOptimizerModule,nn.Module</div><div id='n_parent_class'> N Parent Class: FusedOptimizerModule,nn.Module</div><div id='m_file'> M File Name: torchrec/distributed/model_parallel.py</div><div id='n_file'> N File Name: torchrec/distributed/model_parallel.py</div><div id='m_start'> M Start Line: 342</div><div id='m_end'> M End Line: 371</div><div id='n_start'> N Start Line: 352</div><div id='n_end'> N End Line: 374</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            if path:
                leaf_module = self._dmp_wrapped_module
                split_path = path.split(".")
                <a id="change">for name</a> in split_path[:-1]<a id="change">:
                    </a>leaf_module<a id="change"> = </a>getattr(leaf_module, name)
                setattr(leaf_module, split_path[-1], sharded_module)
            else:
                self._dmp_wrapped_module = sharded_module</code></pre><h3>After Change</h3><pre><code class='java'>
    ) -&gt; nn.Module:

        &#47&#47 pre-sharded module
        <a id="change">if </a>isinstance(module, ShardedModule):
            return module

        &#47&#47 shardable module
        module_sharding_plan = self._plan.get_plan_for_module(path)
        if module_sharding_plan:
            sharder_key = sharder_name(type(module))
            module = self._sharder_map[sharder_key].shard(
                module,
                module_sharding_plan,
                self._env,
                self.device,
            )
            return module

        for name, child in module.named_children():
            child = self._shard_modules_impl(
                child,
                path + "." + name if path else name,
            )
            <a id="change">setattr(</a>module, name, child<a id="change">)</a>

        return module

    def _init_parameters(self, module: nn.Module) -&gt; None:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/f13d0c06f6492b058beeb9fe7e2b9e2bbaa5f0f2#diff-088378575010e7b9e27ea06507ba52fcc8604bcf9cb107f4939a9a49b0d52c89L338' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 76478140</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: f13d0c06f6492b058beeb9fe7e2b9e2bbaa5f0f2</div><div id='time'> Time: 2022-11-07</div><div id='author'> Author: dstaay@meta.com</div><div id='file'> File Name: torchrec/distributed/model_parallel.py</div><div id='m_class'> M Class Name: DistributedModelParallel</div><div id='n_method'> N Class Name: DistributedModelParallel</div><div id='m_method'> M Method Name: _shard_modules_impl(3)</div><div id='n_method'> N Method Name: _shard_modules_impl(4)</div><div id='m_parent_class'> M Parent Class: FusedOptimizerModule,nn.Module</div><div id='n_parent_class'> N Parent Class: FusedOptimizerModule,nn.Module</div><div id='m_file'> M File Name: torchrec/distributed/model_parallel.py</div><div id='n_file'> N File Name: torchrec/distributed/model_parallel.py</div><div id='m_start'> M Start Line: 342</div><div id='m_end'> M End Line: 371</div><div id='n_start'> N Start Line: 352</div><div id='n_end'> N End Line: 374</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        device,
    )

    for <a id="change">_</a> in range(args.epochs):
        <a id="change">for _</a> in tqdm(range(args.limit_train_batches))<a id="change">:
            </a>loss<a id="change">, logits, labels = </a>train_pipeline.progress(iterator)


if __name__ == "__main__":</code></pre><h3>After Change</h3><pre><code class='java'>
    if args.in_memory_binary_criteo_path is None:
        for stage in STAGES:
            attr = f"limit_{stage}_batches"
            <a id="change">if </a>getattr(args, attr) is None:
                <a id="change">setattr(</a>args, attr, 10<a id="change">)</a>

    eb_configs = [
        EmbeddingBagConfig(
            name=f"t_{feature_name}",</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/46f6aa6a827299037c1f1827f54e929344022d87#diff-a48799eee3d167033c2a435d6559e00b0511896eaf454ab7f78cc261acfe9acfL116' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 76478138</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: 46f6aa6a827299037c1f1827f54e929344022d87</div><div id='time'> Time: 2021-11-30</div><div id='author'> Author: rahulkindi@fb.com</div><div id='file'> File Name: torchrec/examples/dlrm/dlrm_main.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: torchrec/examples/dlrm/dlrm_main.py</div><div id='n_file'> N File Name: torchrec/examples/dlrm/dlrm_main.py</div><div id='m_start'> M Start Line: 117</div><div id='m_end'> M End Line: 189</div><div id='n_start'> N Start Line: 298</div><div id='n_end'> N End Line: 374</div><BR>