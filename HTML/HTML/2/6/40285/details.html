<html><h3>Pattern ID :40285
</h3><img src='114229931.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.iteration_num = 0
        self.median = None

        <a id="change">if </a><a id="change">hasattr(config, "pretraining_round_num") and isinstance(
            config.pretraining_round_num, int
        )</a>:
            self.pretraining_round_number = config.pretraining_round_num
        else:
            self.pretraining_round_number = 2</code></pre><h3>After Change</h3><pre><code class='java'>
            self.bound_param = config.bound_param
        else:
            self.bound_param = 1
        if <a id="change">hasattr(config, "score_function") and config.score_function in ["l2", "krum", "cosine", "krum_cosine", "l2_cosine", "l1", "l1_cosine"]</a>:
            self.score_function = config.score_function
            print(f"score function = {config.score_function}")
        else:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/fedml-ai/fedml/commit/9eea2148a0bf84dde93cdea7db3d90921f4800f9#diff-70cf382fe695f6a8a80eeceb699773854944cd2ed6df4f50fde94cb85acba11fL26' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 114229931</div><div id='project'> Project Name: fedml-ai/fedml</div><div id='commit'> Commit Name: 9eea2148a0bf84dde93cdea7db3d90921f4800f9</div><div id='time'> Time: 2022-10-31</div><div id='author'> Author: sshan0731@hotmail.com</div><div id='file'> File Name: python/fedml/core/security/defense/three_sigma_krum_defense.py</div><div id='m_class'> M Class Name: ThreeSigmaKrumDefense</div><div id='n_method'> N Class Name: ThreeSigmaKrumDefense</div><div id='m_method'> M Method Name: __init__(2)</div><div id='n_method'> N Method Name: __init__(2)</div><div id='m_parent_class'> M Parent Class: BaseDefenseMethod</div><div id='n_parent_class'> N Parent Class: BaseDefenseMethod</div><div id='m_file'> M File Name: python/fedml/core/security/defense/three_sigma_krum_defense.py</div><div id='n_file'> N File Name: python/fedml/core/security/defense/three_sigma_krum_defense.py</div><div id='m_start'> M Start Line: 26</div><div id='m_end'> M End Line: 34</div><div id='n_start'> N Start Line: 27</div><div id='n_end'> N End Line: 53</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    g = [], [], []  &#47&#47 optimizer parameter groups
    bn = tuple(v for k, v in nn.__dict__.items() if &quotNorm&quot in k)  &#47&#47 normalization layers, i.e. BatchNorm2d()
    for v in model.modules():
        <a id="change">if </a><a id="change">hasattr(v, &quotbias&quot) and isinstance(v.bias, nn.Parameter)</a>:  &#47&#47 bias (no decay)
            g[2].append(v.bias)
        if isinstance(v, bn):  &#47&#47 weight (no decay)
            g[1].append(v.weight)</code></pre><h3>After Change</h3><pre><code class='java'>
        for p_name, p in v.named_parameters(recurse=0):
            if p_name == &quotbias&quot:  &#47&#47 bias (no decay)
                g[2].append(p)
            elif <a id="change">p_name == &quotweight&quot and isinstance(v, bn)</a>:  &#47&#47 weight (no decay)
                g[1].append(p)
            else:
                g[0].append(p)  &#47&#47 weight (with decay)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/ultralytics/yolov5/commit/e42c89d4efc99bfbd8c5c208ffe67c11632da84a#diff-a4cd44fa2e00ba300e04c8799657a2329509edef8e79a488ccfb3464e7dc4394L317' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 114229926</div><div id='project'> Project Name: ultralytics/yolov5</div><div id='commit'> Commit Name: e42c89d4efc99bfbd8c5c208ffe67c11632da84a</div><div id='time'> Time: 2022-10-16</div><div id='author'> Author: glenn.jocher@ultralytics.com</div><div id='file'> File Name: utils/torch_utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: smart_optimizer(5)</div><div id='n_method'> N Method Name: smart_optimizer(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: utils/torch_utils.py</div><div id='n_file'> N File Name: utils/torch_utils.py</div><div id='m_start'> M Start Line: 320</div><div id='m_end'> M End Line: 329</div><div id='n_start'> N Start Line: 320</div><div id='n_end'> N End Line: 330</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                else:
                    if self.pl_module.trainer.lr_scheduler_configs:
                        for config in self.pl_module.trainer.lr_scheduler_configs:
                            show_warn_lambdas = <a id="change">(
                                hasattr(config.scheduler, "lr_lambdas")
                                and config.scheduler.lr_lambdas[-1] is not None  &#47&#47 type: ignore[union-attr]
                                and not self.apply_lambdas_new_pgs
                            )</a>
                            <a id="change">if </a>show_warn_lambdas:
                                rank_zero_warn(
                                    "The lr scheduler used in this phase has lr_lambdas but will use a "
                                    "configured lr for new parameter groups because `apply_lambdas_new_pgs` is "</code></pre><h3>After Change</h3><pre><code class='java'>
                _, self._fts_state._curr_thawed_params = self.strategy_adapter.exec_ft_phase(
                    self.pl_module, thaw_pl=next_tl["params"]
                )
                if <a id="change">new_optimizer_cfg and i == 0</a>:
                    &#47&#47 If reinitializing the optimizer, we need to re-add the initial parameter groups (phase 0)
                    optimizer = self.reinit_optimizer(
                        new_optimizer=new_optimizer_cfg, trainer=self.pl_module.trainer, init_params=next_tl["params"]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/speediedan/finetuning-scheduler/commit/3822650061e2c56b5d05f7b277a6e824920a358e#diff-1e40f5edc419001c8841d4ee49ba3fe40f5c1a9f4122eff482a4037740bd7683L323' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 114229924</div><div id='project'> Project Name: speediedan/finetuning-scheduler</div><div id='commit'> Commit Name: 3822650061e2c56b5d05f7b277a6e824920a358e</div><div id='time'> Time: 2023-03-30</div><div id='author'> Author: danny.dale@gmail.com</div><div id='file'> File Name: src/finetuning_scheduler/fts.py</div><div id='m_class'> M Class Name: FinetuningScheduler</div><div id='n_method'> N Class Name: FinetuningScheduler</div><div id='m_method'> M Method Name: step_pg(5)</div><div id='n_method'> N Method Name: step_pg(4)</div><div id='m_parent_class'> M Parent Class: ScheduleImplMixin,ScheduleParsingMixin,CallbackDepMixin,BaseFinetuning</div><div id='n_parent_class'> N Parent Class: ScheduleImplMixin,ScheduleParsingMixin,CallbackDepMixin,BaseFinetuning</div><div id='m_file'> M File Name: src/finetuning_scheduler/fts.py</div><div id='n_file'> N File Name: src/finetuning_scheduler/fts.py</div><div id='m_start'> M Start Line: 341</div><div id='m_end'> M End Line: 381</div><div id='n_start'> N Start Line: 342</div><div id='n_end'> N End Line: 385</div><BR>