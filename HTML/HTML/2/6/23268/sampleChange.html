<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            _fold_to_weight(conv, bn, fold_backward=fold_backward)


    with <a id="change">utils</a>.on_cpu(model), utils.in_eval_mode(model), torch.no_grad():
        for conv, bn in conv_bn_pairs:
            _fold(conv, bn, fold_backward=True)
</code></pre><h3>After Change</h3><pre><code class='java'>
    
    &#47&#47 pylint: disable=protected-access
    device = utils.get_device(model)
    <a id="change">if device != torch.device("cpu")</a>:
        <a id="change">raise </a><a id="change">RuntimeError(f"Expected model to be on cpu, not {device}."</a><a id="change">)</a>

    for bn, conv in bn_conv_pairs:
        if isinstance(conv, QcQuantizeWrapper):
            raise RuntimeError(f"Forward folding to scale is not possible. Got {conv}")</code></pre>