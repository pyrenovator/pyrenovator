<html><h3>Pattern ID :26840
</h3><img src='80206206.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                        "set the dataloader with .set_loaders(...)"
                    )

            dataset_size<a id="change"> = </a><a id="change">getattr(</a>self, "dali_epoch_size", None<a id="change">)</a> or len(dataloader.dataset)

            dataset_size = self.trainer.limit_train_batches * dataset_size
</code></pre><h3>After Change</h3><pre><code class='java'>

        if self._num_training_steps is None:
            try:
                dataset<a id="change"> = </a>self.extra_args.get("dataset", None)
                <a id="change">if </a>dataset not in ["cifar10", "cifar100", "stl10"]:
                    folder<a id="change"> = </a><a id="change">os.path.join(</a>self.extra_args["data_dir"], self.extra_args["train_dir"]<a id="change">)</a>
                else:
                    folder = None
                no_labels = self.extra_args.get("no_labels", False)
                data_fraction = self.extra_args.get("data_fraction", -1.0)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/vturrisi/contrastive-learning/commit/eb07a9c7c2872efb1ae83767f59a67fa616a7652#diff-306471ca72f10d091f7adabfddc03e702f662348314d589d172c5c700eae27abL184' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 80206206</div><div id='project'> Project Name: vturrisi/contrastive-learning</div><div id='commit'> Commit Name: eb07a9c7c2872efb1ae83767f59a67fa616a7652</div><div id='time'> Time: 2022-05-02</div><div id='author'> Author: vt.turrisi@gmail.com</div><div id='file'> File Name: solo/methods/linear.py</div><div id='m_class'> M Class Name: LinearModel</div><div id='n_method'> N Class Name: LinearModel</div><div id='m_method'> M Method Name: num_training_steps(1)</div><div id='n_method'> N Method Name: num_training_steps(1)</div><div id='m_parent_class'> M Parent Class: pl.LightningModule</div><div id='n_parent_class'> N Parent Class: pl.LightningModule</div><div id='m_file'> M File Name: solo/methods/linear.py</div><div id='n_file'> N File Name: solo/methods/linear.py</div><div id='m_start'> M Start Line: 199</div><div id='m_end'> M End Line: 208</div><div id='n_start'> N Start Line: 184</div><div id='n_end'> N End Line: 206</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            self.head = torch.nn.Linear(self.hidden_dims, self.num_classes, bias=False)
        else: &#47&#47 The head contains multiple layer
            self.head_last_layer_name = [n for n,c in self.head.named_children()][-1]
            head_last_layer<a id="change"> = </a><a id="change">getattr(</a>self.head, self.head_last_layer_name<a id="change">)</a>
            assert isinstance(head_last_layer, torch.nn.Linear), "The retrieved last layer is not a linear layer."

            self.hidden_dims = head_last_layer.weight.shape[-1]
            setattr(self.head, self.head_last_layer_name, torch.nn.Linear(self.hidden_dims, self.num_classes, bias=False)) &#47&#47 automatically requires_grad</code></pre><h3>After Change</h3><pre><code class='java'>
        self.head = copy.deepcopy(getattr(plm, head_name))
        max_loop = 5
        if not isinstance(self.head, torch.nn.Linear):
            module<a id="change"> = </a>self.head
            found = False
            last_layer_full_name = []
            for i in range(max_loop):
                last_layer_name = [n for n,c in module.named_children()][-1]
                last_layer_full_name.append(last_layer_name)
                parent_module = module
                module = getattr(module, last_layer_name)
                <a id="change">if </a>isinstance(module, torch.nn.Linear):
                    found = True
                    break
            if not found:
                raise RuntimeError(f"Can&quott not retrieve a linear layer in {max_loop} loop from the plm.")
            self.original_head_last_layer = module.weight.data
            self.hidden_dims = self.original_head_last_layer.shape[-1]
            self.head_last_layer_full_name<a id="change"> = </a><a id="change">".".join(</a>last_layer_full_name<a id="change">)</a>
            setattr(parent_module, last_layer_name, torch.nn.Linear(self.hidden_dims, self.num_classes, bias=False))
        else:
            self.hidden_dims = self.head.weight.shape[-1]
            self.original_head_last_layer = getattr(plm, head_name).weight.data</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thunlp/openprompt/commit/e13980ab1a1b37f25c2ebbc58a3a5fee76e87ed9#diff-7a92838d06840854c72f2e47bd966019f58eab60dcb83efeda1a76cb7b357a30L32' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 80206189</div><div id='project'> Project Name: thunlp/openprompt</div><div id='commit'> Commit Name: e13980ab1a1b37f25c2ebbc58a3a5fee76e87ed9</div><div id='time'> Time: 2021-11-05</div><div id='author'> Author: shengdinghu@gmail.com</div><div id='file'> File Name: openprompt/prompts/soft_verbalizer.py</div><div id='m_class'> M Class Name: SoftVerbalizer</div><div id='n_method'> N Class Name: SoftVerbalizer</div><div id='m_method'> M Method Name: __init__(8)</div><div id='n_method'> N Method Name: __init__(8)</div><div id='m_parent_class'> M Parent Class: Verbalizer</div><div id='n_parent_class'> N Parent Class: Verbalizer</div><div id='m_file'> M File Name: openprompt/prompts/soft_verbalizer.py</div><div id='n_file'> N File Name: openprompt/prompts/soft_verbalizer.py</div><div id='m_start'> M Start Line: 45</div><div id='m_end'> M End Line: 59</div><div id='n_start'> N Start Line: 46</div><div id='n_end'> N End Line: 70</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                        "set the dataloader with .set_loaders(...)"
                    )

            dataset_size<a id="change"> = </a><a id="change">getattr(</a>self, "dali_epoch_size", None<a id="change">)</a> or len(dataloader.dataset)

            dataset_size = self.trainer.limit_train_batches * dataset_size
</code></pre><h3>After Change</h3><pre><code class='java'>

        if self._num_training_steps is None:
            try:
                dataset<a id="change"> = </a>self.extra_args.get("dataset", None)
                <a id="change">if </a>dataset not in ["cifar10", "cifar100", "stl10"]:
                    folder<a id="change"> = </a><a id="change">os.path.join(</a>self.extra_args["data_dir"], self.extra_args["train_dir"]<a id="change">)</a>
                else:
                    folder = None
                no_labels = self.extra_args.get("no_labels", False)
                data_fraction = self.extra_args.get("data_fraction", -1.0)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/vturrisi/contrastive-learning/commit/eb07a9c7c2872efb1ae83767f59a67fa616a7652#diff-bf0f79abb0e767723150e5fe64a4a5935492d2fcf2eb222cfe5f8c901bcd11b0L379' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 80206203</div><div id='project'> Project Name: vturrisi/contrastive-learning</div><div id='commit'> Commit Name: eb07a9c7c2872efb1ae83767f59a67fa616a7652</div><div id='time'> Time: 2022-05-02</div><div id='author'> Author: vt.turrisi@gmail.com</div><div id='file'> File Name: solo/methods/base.py</div><div id='m_class'> M Class Name: BaseMethod</div><div id='n_method'> N Class Name: BaseMethod</div><div id='m_method'> M Method Name: num_training_steps(1)</div><div id='n_method'> N Method Name: num_training_steps(1)</div><div id='m_parent_class'> M Parent Class: pl.LightningModule</div><div id='n_parent_class'> N Parent Class: pl.LightningModule</div><div id='m_file'> M File Name: solo/methods/base.py</div><div id='n_file'> N File Name: solo/methods/base.py</div><div id='m_start'> M Start Line: 383</div><div id='m_end'> M End Line: 392</div><div id='n_start'> N Start Line: 362</div><div id='n_end'> N End Line: 384</div><BR>