<html><h3>Pattern ID :18525
</h3><img src='60485178.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 in case of conditional generation, if enc_mask is not provided use the correct context_mask
        context = kwargs.pop(&quotcontext&quot, None)
        context_mask = kwargs.pop(&quotcontext_mask&quot, None)
        if <a id="change">context is not None and context_mask is None</a>:
            context_mask = torch.full(context.shape[:2], True, dtype=torch.bool, device=out.device)

        for _ in range(seq_len):</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 in case of conditional generation, if enc_mask is not provided use the correct context_mask
        context_mask = kwargs.pop(&quotcontext_mask&quot, None)

        if &quotcontext&quot in kwargs and not <a id="change">exists(</a>context_mask<a id="change">)</a>:
            context<a id="change"> = </a>kwargs[&quotcontext&quot]
            context_mask = torch.full(context.shape[:2], True, dtype=torch.bool, device=out.device)
            kwargs.update(context_mask = context_mask)
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/performer-pytorch/commit/e10ec182c2aba2eba66b59a175ed767251aa3889#diff-192164ea998deaff038129dced1fc9dbccf034f70eaf2ffa88b31af860963378L52' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60485178</div><div id='project'> Project Name: lucidrains/performer-pytorch</div><div id='commit'> Commit Name: e10ec182c2aba2eba66b59a175ed767251aa3889</div><div id='time'> Time: 2020-12-08</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: performer_pytorch/autoregressive_wrapper.py</div><div id='m_class'> M Class Name: AutoregressiveWrapper</div><div id='n_method'> N Class Name: AutoregressiveWrapper</div><div id='m_method'> M Method Name: generate(7)</div><div id='n_method'> N Method Name: generate(7)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: performer_pytorch/autoregressive_wrapper.py</div><div id='n_file'> N File Name: performer_pytorch/autoregressive_wrapper.py</div><div id='m_start'> M Start Line: 52</div><div id='m_end'> M End Line: 57</div><div id='n_start'> N Start Line: 55</div><div id='n_end'> N End Line: 62</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 calculate nearest neighbors

        nbhd_indices = None
        if <a id="change">num_nn &gt; 0 and num_nn &lt; n</a>:
            rel_dist = rel_coors.norm(dim = -1, p = 2)
            nbhd_indices = rel_dist.topk(num_nn, dim = -1, largest = False).indices
</code></pre><h3>After Change</h3><pre><code class='java'>
    ):
        n, num_nn = feats.shape[1], self.num_nearest_neighbors

        if <a id="change">exists(</a>self.token_emb<a id="change">)</a>:
            feats<a id="change"> = </a>self.token_emb(feats)

        &#47&#47 main network
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/en-transformer/commit/2a279568aee65464c3b29fc032811c0073d069db#diff-f288a1692c1c58a186cb9ac763046f632757db1647271b97852e59e3fc5696b9L266' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60485177</div><div id='project'> Project Name: lucidrains/en-transformer</div><div id='commit'> Commit Name: 2a279568aee65464c3b29fc032811c0073d069db</div><div id='time'> Time: 2021-03-19</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: en_transformer/en_transformer.py</div><div id='m_class'> M Class Name: EnTransformer</div><div id='n_method'> N Class Name: EnTransformer</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: en_transformer/en_transformer.py</div><div id='n_file'> N File Name: en_transformer/en_transformer.py</div><div id='m_start'> M Start Line: 275</div><div id='m_end'> M End Line: 286</div><div id='n_start'> N Start Line: 285</div><div id='n_end'> N End Line: 290</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            out = torch.cat((out, sample), dim=-1)
            mask = F.pad(mask, (0, 1), value=True)

            if <a id="change">eos_token is not None and (sample == eos_token).all()</a>:
                break

        out = out[:, t:]</code></pre><h3>After Change</h3><pre><code class='java'>
            out = torch.cat((out, sample), dim=-1)
            mask = F.pad(mask, (0, 1), value=True)

            if <a id="change">exists(</a>eos_token<a id="change">)</a>:
                is_eos_token = (out == eos_token)

                if is_eos_token.any(dim = -1).all():
                    &#47&#47 mask out everything after the eos tokens
                    shifted_is_eos_tokens = F.pad(is_eos_tokens, (1, -1))
                    mask = shifted_is_eos_tokens.float().cumsum(dim = -1) &gt;= 1
                    out<a id="change"> = </a>out.masked_fill(mask, self.pad_value)
                    break

        out = out[:, t:]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/x-transformers/commit/b32913eeb4fe332ee366e455f7214fc08c9a2aa6#diff-b70a3173d353a448f8f499d8b685672d9bccac7b78bc1d36b77f84afe33be694L45' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60485182</div><div id='project'> Project Name: lucidrains/x-transformers</div><div id='commit'> Commit Name: b32913eeb4fe332ee366e455f7214fc08c9a2aa6</div><div id='time'> Time: 2021-08-10</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: x_transformers/autoregressive_wrapper.py</div><div id='m_class'> M Class Name: AutoregressiveWrapper</div><div id='n_method'> N Class Name: AutoregressiveWrapper</div><div id='m_method'> M Method Name: generate(7)</div><div id='n_method'> N Method Name: generate(7)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: x_transformers/autoregressive_wrapper.py</div><div id='n_file'> N File Name: x_transformers/autoregressive_wrapper.py</div><div id='m_start'> M Start Line: 75</div><div id='m_end'> M End Line: 81</div><div id='n_start'> N Start Line: 80</div><div id='n_end'> N End Line: 93</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

            out = torch.cat((out, sample), dim=-1)

            if <a id="change">eos_token is not None and (sample == eos_token).all()</a>:
                break

        out = out[:, t:]</code></pre><h3>After Change</h3><pre><code class='java'>

            out = torch.cat((out, sample), dim=-1)

            if <a id="change">exists(</a>eos_token<a id="change">)</a>:
                is_eos_token<a id="change"> = </a>(out == eos_token)

                if is_eos_token.any(dim = -1).all():
                    &#47&#47 mask out everything after the eos tokens</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/h-transformer-1d/commit/e3cb1afee7a1c8895180d89aa3a9c51ef42c8d3a#diff-0f303296ed562e372011921aeac491b11b749bd83b2b0709c95e628de17507a6L36' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60485170</div><div id='project'> Project Name: lucidrains/h-transformer-1d</div><div id='commit'> Commit Name: e3cb1afee7a1c8895180d89aa3a9c51ef42c8d3a</div><div id='time'> Time: 2021-08-08</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: h_transformer_1d/autoregressive_wrapper.py</div><div id='m_class'> M Class Name: AutoregressiveWrapper</div><div id='n_method'> N Class Name: AutoregressiveWrapper</div><div id='m_method'> M Method Name: generate(7)</div><div id='n_method'> N Method Name: generate(7)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: h_transformer_1d/autoregressive_wrapper.py</div><div id='n_file'> N File Name: h_transformer_1d/autoregressive_wrapper.py</div><div id='m_start'> M Start Line: 55</div><div id='m_end'> M End Line: 60</div><div id='n_start'> N Start Line: 60</div><div id='n_end'> N End Line: 72</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 in case of conditional generation, if enc_mask is not provided use the correct context_mask
        context = kwargs.pop(&quotcontext&quot, None)
        context_mask = kwargs.pop(&quotcontext_mask&quot, None)
        if <a id="change">context is not None and context_mask is None</a>:
            context_mask = torch.full(context.shape[:2], True, dtype=torch.bool, device=out.device)

        for _ in range(seq_len):</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 in case of conditional generation, if enc_mask is not provided use the correct context_mask
        context_mask = kwargs.pop(&quotcontext_mask&quot, None)

        if &quotcontext&quot in kwargs and not <a id="change">exists(</a>context_mask<a id="change">)</a>:
            context<a id="change"> = </a>kwargs[&quotcontext&quot]
            context_mask = torch.full(context.shape[:2], True, dtype=torch.bool, device=out.device)
            kwargs.update(context_mask = context_mask)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/performer-pytorch/commit/e10ec182c2aba2eba66b59a175ed767251aa3889#diff-192164ea998deaff038129dced1fc9dbccf034f70eaf2ffa88b31af860963378L35' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60485173</div><div id='project'> Project Name: lucidrains/performer-pytorch</div><div id='commit'> Commit Name: e10ec182c2aba2eba66b59a175ed767251aa3889</div><div id='time'> Time: 2020-12-08</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: performer_pytorch/autoregressive_wrapper.py</div><div id='m_class'> M Class Name: AutoregressiveWrapper</div><div id='n_method'> N Class Name: AutoregressiveWrapper</div><div id='m_method'> M Method Name: generate(7)</div><div id='n_method'> N Method Name: generate(7)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: performer_pytorch/autoregressive_wrapper.py</div><div id='n_file'> N File Name: performer_pytorch/autoregressive_wrapper.py</div><div id='m_start'> M Start Line: 52</div><div id='m_end'> M End Line: 57</div><div id='n_start'> N Start Line: 55</div><div id='n_end'> N End Line: 62</div><BR>