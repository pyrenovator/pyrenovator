<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def compute_alphas(self):
        &#47&#47 Construct matrix for alpha calculation
        objPts_w = np.array(self.objPts).transpose()[0]
        mat_objPts_w<a id="change"> = </a><a id="change">np.concatenate(</a>(objPts_w, np.array(<a id="change">[</a>np.ones((self.n))<a id="change"></a>]))<a id="change">, axis=0)</a>
        contPts_w = self.contPts_w.transpose()
        mat_contPts_w = np.concatenate((contPts_w, np.array([np.ones((4))])), axis=0)
        
        &#47&#47 Calculate Alpha</code></pre><h3>After Change</h3><pre><code class='java'>
        batched_ones = torch.ones((batch_size, 4, 1), dtype=contPts_w.dtype, device=contPts_w.device)
        contPts_w = torch.cat((contPts_w, batched_ones), dim=-1)

        <a id="change">if </a>linear_least_square:
            NotImplementedError("Linear least square method is not implemented yet.")
            &#47&#47 Calculate Alpha TODO: CHECK if logic is correct, or change to general method
            alpha = torch.bmm(torch.linalg.inv(contPts_w), objPts) &#47&#47 simple method
            alpha<a id="change"> = </a>alpha.transpose()
        else:
            alpha<a id="change"> = </a>torch.linalg.solve(contPts_w, objPts, left=False) &#47&#47 General method
            
        return alpha
    </code></pre>