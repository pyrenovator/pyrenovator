<html><h3>Pattern ID :18041
</h3><img src='59166278.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                break &#47&#47 don&quott start from i=0; there is a large difference between the length of the first and last element

        self.last_batch_start_index += len(batch_of_indices)
        <a id="change">assert </a>len(batch_of_indices) &gt; 0, &quotFound an example larger than batch size.&quot
        return batch_of_indices

    def _get_next_batch_start_index(self):</code></pre><h3>After Change</h3><pre><code class='java'>
            else:
                longest_example = new_example &#47&#47 this is the first element in batch, and therefore the longest
            examples_size = self.batch_size_fn(longest_example)
            <a id="change">if examples_size &gt; self.batch_size</a>:
                <a id="change">logger.warning(&quotSkipping an example larger than batch size. Consider increasing the batch size to avoid this warning&quot</a><a id="change">)</a>
                self.last_batch_start_index += 1
                i<a id="change"> = </a>self._get_next_batch_start_index()
                continue
                
            new_batch_size = current_batch_size + self.batch_size_fn(longest_example)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/stanford-oval/genienlp/commit/e8111865f01fdac8e25bc9ac379eb4d84f8964ff#diff-c117e0d022bc8fb1b3aed917f667c3d74d7f7471f186ba90a4f66a25224fe146L106' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 59166278</div><div id='project'> Project Name: stanford-oval/genienlp</div><div id='commit'> Commit Name: e8111865f01fdac8e25bc9ac379eb4d84f8964ff</div><div id='time'> Time: 2021-01-04</div><div id='author'> Author: mehrad@stanford.edu</div><div id='file'> File Name: genienlp/data_utils/iterator.py</div><div id='m_class'> M Class Name: LengthSortedIterator</div><div id='n_method'> N Class Name: LengthSortedIterator</div><div id='m_method'> M Method Name: __next__(1)</div><div id='n_method'> N Method Name: __next__(1)</div><div id='m_parent_class'> M Parent Class: torch.utils.data.Sampler</div><div id='n_parent_class'> N Parent Class: torch.utils.data.Sampler</div><div id='m_file'> M File Name: genienlp/data_utils/iterator.py</div><div id='n_file'> N File Name: genienlp/data_utils/iterator.py</div><div id='m_start'> M Start Line: 116</div><div id='m_end'> M End Line: 116</div><div id='n_start'> N Start Line: 106</div><div id='n_end'> N End Line: 113</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                assert weights[1].size() == (self.embedding_dim, self.block_dim), \
                    &quotpassed linear layer dimensions are wrong: {x1} vs {x2} \
                    &quot.format(x1=weights[1].size(), x2=(self.embedding_dim, self.block_dim))
            <a id="change">assert </a>blk_embed.base_embedding_dim == self.embedding_dim
            self.embed = BlockEmbeddingBag.from_pretrained(
                                                weights=weights,
                                                base_embedding_dim=blk_embed.base_embedding_dim,</code></pre><h3>After Change</h3><pre><code class='java'>
                assert weights[1].size() == (self.embedding_dim, self.block_dim), \
                    &quotpassed linear layer dimensions are wrong: {x1} vs {x2} \
                    &quot.format(x1=weights[1].size(), x2=(self.embedding_dim, self.block_dim))
            <a id="change">if base_embedding_dim != self.embedding_dim</a>:
                <a id="change">DISTLogger.warning(&quotBase embedding dimension provided by blk_embed is different from \
                    default or manually passed. Will overwrite by blk_embed.base_embedding_dim&quot</a><a id="change">)</a>
                self.embedding_dim<a id="change"> = </a>base_embedding_dim
            self.embed = BlockEmbeddingBag.from_pretrained(
                                                weights=weights,
                                                base_embedding_dim=base_embedding_dim,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/hpcaitech/freqcacheembedding/commit/fb122de439977692f191fa6f3533a471d5b98f4d#diff-5eb5715d975b7975822b38af2e5bd3c8cd410d833addf110e32fc3ca3e425e81L192' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 59166274</div><div id='project'> Project Name: hpcaitech/freqcacheembedding</div><div id='commit'> Commit Name: fb122de439977692f191fa6f3533a471d5b98f4d</div><div id='time'> Time: 2022-07-04</div><div id='author'> Author: jiatong.han@u.nus.edu</div><div id='file'> File Name: recsys/modules/embeddings/parallel_mix_vocab_embedding.py</div><div id='m_class'> M Class Name: ParallelMixVocabEmbeddingBag</div><div id='n_method'> N Class Name: ParallelMixVocabEmbeddingBag</div><div id='m_method'> M Method Name: __init__(8)</div><div id='n_method'> N Method Name: __init__(9)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: recsys/modules/embeddings/parallel_mix_vocab_embedding.py</div><div id='n_file'> N File Name: recsys/modules/embeddings/parallel_mix_vocab_embedding.py</div><div id='m_start'> M Start Line: 200</div><div id='m_end'> M End Line: 238</div><div id='n_start'> N Start Line: 202</div><div id='n_end'> N End Line: 236</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    LOGGER.info(f&quot{PREFIX}Running kmeans for {n} anchors on {len(wh)} points...&quot)
    s = wh.std(0)  &#47&#47 sigmas for whitening
    k, dist = kmeans(wh / s, n, iter=30)  &#47&#47 points, mean distance
    <a id="change">assert </a>len(k) == n, f&quot{PREFIX}ERROR: scipy.cluster.vq.kmeans requested {n} points but returned only {len(k)}&quot
    k *= s
    wh = torch.tensor(wh, dtype=torch.float32)  &#47&#47 filtered
    wh0 = torch.tensor(wh0, dtype=torch.float32)  &#47&#47 unfiltered</code></pre><h3>After Change</h3><pre><code class='java'>
    LOGGER.info(f&quot{PREFIX}Running kmeans for {n} anchors on {len(wh)} points...&quot)
    s = wh.std(0)  &#47&#47 sigmas for whitening
    k = kmeans(wh / s, n, iter=30)[0] * s  &#47&#47 points
    <a id="change">if len(k) != n</a>:  &#47&#47 kmeans may return fewer points than requested if wh is insufficient or too similar
        <a id="change">LOGGER.warning(f&quot{PREFIX}WARNING: scipy.cluster.vq.kmeans returned only {len(k)} of {n} requested points&quot</a><a id="change">)</a>
        k<a id="change"> = </a>np.sort(npr.rand(n * 2)).reshape(n, 2) * img_size  &#47&#47 random init
    wh = torch.tensor(wh, dtype=torch.float32)  &#47&#47 filtered
    wh0 = torch.tensor(wh0, dtype=torch.float32)  &#47&#47 unfiltered
    k = print_results(k, verbose=False)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/ultralytics/yolov5/commit/2e5c67e537860e060852b53544b182e6bca21dcd#diff-ee8b9dce3a676a623ab0f7b33308bc2caafbc6095a3d5ac0156659daec326663L65' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 59166269</div><div id='project'> Project Name: ultralytics/yolov5</div><div id='commit'> Commit Name: 2e5c67e537860e060852b53544b182e6bca21dcd</div><div id='time'> Time: 2022-02-17</div><div id='author'> Author: glenn.jocher@ultralytics.com</div><div id='file'> File Name: utils/autoanchor.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: kmean_anchors(6)</div><div id='n_method'> N Method Name: kmean_anchors(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: utils/autoanchor.py</div><div id='n_file'> N File Name: utils/autoanchor.py</div><div id='m_start'> M Start Line: 128</div><div id='m_end'> M End Line: 149</div><div id='n_start'> N Start Line: 84</div><div id='n_end'> N End Line: 134</div><BR>