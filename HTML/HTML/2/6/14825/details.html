<html><h3>Pattern ID :14825
</h3><img src='49345687.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            )
            rank_zero_warn(error_msg)
            raise MisconfigurationException(error_msg)
        <a id="change">assert </a>isinstance(testlr, torch.optim.lr_scheduler._LRScheduler)


class ScheduleImplMixin(ABC):</code></pre><h3>After Change</h3><pre><code class='java'>
                " reinitialized at every fine-tuning phase with implicit mode fine-tuning."
            )
        test_lr_init = copy(lr_scheduler_init.get("init_args", {}))
        <a id="change">if </a>min_lr_param:
            <a id="change">del test_lr_init["min_lr"]</a>  &#47&#47 our mock optimizer will not have any param groups
        try:
            testlr = lrs_class(optimizer=_MockOptimizer(), **test_lr_init)
        except Exception as err:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/speediedan/finetuning-scheduler/commit/0a3b2b88155d1af569652f3ddb0c64078ae30a93#diff-2d4f76c6f0c8223fab0ac9c5108a651abdb2c3dd1e3bf3f7db29dbe500990ab3L844' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 49345687</div><div id='project'> Project Name: speediedan/finetuning-scheduler</div><div id='commit'> Commit Name: 0a3b2b88155d1af569652f3ddb0c64078ae30a93</div><div id='time'> Time: 2022-08-10</div><div id='author'> Author: danny.dale@gmail.com</div><div id='file'> File Name: src/finetuning_scheduler/fts_supporters.py</div><div id='m_class'> M Class Name: ScheduleParsingMixin</div><div id='n_method'> N Class Name: ScheduleParsingMixin</div><div id='m_method'> M Method Name: _lr_scheduler_sanity_chk(2)</div><div id='n_method'> N Method Name: _lr_scheduler_sanity_chk(1)</div><div id='m_parent_class'> M Parent Class: ABC</div><div id='n_parent_class'> N Parent Class: ABC</div><div id='m_file'> M File Name: src/finetuning_scheduler/fts_supporters.py</div><div id='n_file'> N File Name: src/finetuning_scheduler/fts_supporters.py</div><div id='m_start'> M Start Line: 844</div><div id='m_end'> M End Line: 877</div><div id='n_start'> N Start Line: 859</div><div id='n_end'> N End Line: 906</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            )
            rank_zero_warn(error_msg)
            raise MisconfigurationException(error_msg)
        <a id="change">assert </a>isinstance(testlr, torch.optim.lr_scheduler._LRScheduler)


class ScheduleImplMixin(ABC):</code></pre><h3>After Change</h3><pre><code class='java'>
            True if min_lr_param and (isinstance(min_lr_param, list) or isinstance(min_lr_param, tuple)) else False
        )
        reinit_rlrop = is_implicit_mode and issubclass(lrs_class, torch.optim.lr_scheduler.ReduceLROnPlateau)
        <a id="change">if </a>reinit_rlrop and invalid_min_lr:
            raise MisconfigurationException(
                "In the lr scheduler configuration passed via `reinit_lr_cfg` (i.e. implicit mode training)"
                " `min_lr` cannot be a list or tuple since the same lr scheduler configuration is intended to be"
                " reinitialized at every fine-tuning phase with implicit mode fine-tuning."
            )
        <a id="change">test_lr_init</a> = copy(lr_scheduler_init.get("init_args", {}))
        if min_lr_param:
            <a id="change">del test_lr_init["min_lr"]</a>  &#47&#47 our mock optimizer will not have any param groups
        try:
            testlr = lrs_class(optimizer=_MockOptimizer(), **test_lr_init)
        except Exception as err:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/speediedan/finetuning-scheduler/commit/0a3b2b88155d1af569652f3ddb0c64078ae30a93#diff-2d4f76c6f0c8223fab0ac9c5108a651abdb2c3dd1e3bf3f7db29dbe500990ab3L844' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 49345688</div><div id='project'> Project Name: speediedan/finetuning-scheduler</div><div id='commit'> Commit Name: 0a3b2b88155d1af569652f3ddb0c64078ae30a93</div><div id='time'> Time: 2022-08-10</div><div id='author'> Author: danny.dale@gmail.com</div><div id='file'> File Name: src/finetuning_scheduler/fts_supporters.py</div><div id='m_class'> M Class Name: ScheduleParsingMixin</div><div id='n_method'> N Class Name: ScheduleParsingMixin</div><div id='m_method'> M Method Name: _lr_scheduler_sanity_chk(2)</div><div id='n_method'> N Method Name: _lr_scheduler_sanity_chk(1)</div><div id='m_parent_class'> M Parent Class: ABC</div><div id='n_parent_class'> N Parent Class: ABC</div><div id='m_file'> M File Name: src/finetuning_scheduler/fts_supporters.py</div><div id='n_file'> N File Name: src/finetuning_scheduler/fts_supporters.py</div><div id='m_start'> M Start Line: 844</div><div id='m_end'> M End Line: 877</div><div id='n_start'> N Start Line: 859</div><div id='n_end'> N End Line: 906</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    if out_file.endswith(&quot.pth&quot):
        out_file = out_file[:-4]
    final_file = out_file + f&quot-{sha[:8]}.pth&quot
    <a id="change">assert </a>final_file != in_file, \
        "The output filename is the same as the input file."
    print("Output file: {}".format(final_file))
    subprocess.Popen([&quotmv&quot, tmp_file, final_file])</code></pre><h3>After Change</h3><pre><code class='java'>


def process_checkpoint(in_file, out_file, decode=False):
    <a id="change">checkpoint</a> = torch.load(in_file, map_location=&quotcpu&quot)
    &#47&#47 remove optimizer for smaller file size
    <a id="change">if </a>&quotoptimizer&quot in checkpoint:
        <a id="change">del checkpoint[&quotoptimizer&quot]</a>
    &#47&#47 if it is necessary to remove some sensitive data in checkpoint[&quotmeta&quot],
    &#47&#47 add the code here.
    if torch.__version__ &gt;= &quot1.6&quot:
        torch.save(checkpoint, out_file, _use_new_zipfile_serialization=False)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/westlake-ai/openmixup/commit/cb7ae3bae6ffe76f12f0c307e16325e7952f4274#diff-df2bdd976500f5594a90d51275819e5dbbc8baf1a1834b8e6327bf5707d879d8L13' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 49345689</div><div id='project'> Project Name: westlake-ai/openmixup</div><div id='commit'> Commit Name: cb7ae3bae6ffe76f12f0c307e16325e7952f4274</div><div id='time'> Time: 2023-02-26</div><div id='author'> Author: 1070535169@qq.com</div><div id='file'> File Name: tools/model_converters/publish_model.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: process_checkpoint(3)</div><div id='n_method'> N Method Name: process_checkpoint(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: tools/model_converters/publish_model.py</div><div id='n_file'> N File Name: tools/model_converters/publish_model.py</div><div id='m_start'> M Start Line: 13</div><div id='m_end'> M End Line: 24</div><div id='n_start'> N Start Line: 24</div><div id='n_end'> N End Line: 45</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    if out_file.endswith(&quot.pth&quot):
        out_file = out_file[:-4]
    final_file = out_file + f&quot-{sha[:8]}.pth&quot
    <a id="change">assert </a>final_file != in_file, \
        "The output filename is the same as the input file."
    print("Output file: {}".format(final_file))
    subprocess.Popen([&quotmv&quot, tmp_file, final_file])</code></pre><h3>After Change</h3><pre><code class='java'>


def process_checkpoint(in_file, out_file, decode=False):
    <a id="change">checkpoint</a> = torch.load(in_file, map_location=&quotcpu&quot)
    &#47&#47 remove optimizer for smaller file size
    <a id="change">if </a>&quotoptimizer&quot in checkpoint:
        <a id="change">del checkpoint[&quotoptimizer&quot]</a>
    &#47&#47 if it is necessary to remove some sensitive data in checkpoint[&quotmeta&quot],
    &#47&#47 add the code here.
    if torch.__version__ &gt;= &quot1.6&quot:
        torch.save(checkpoint, out_file, _use_new_zipfile_serialization=False)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/westlake-ai/openbioseq/commit/f678533fb9c8c5320abdd86ba57bd50d6357835f#diff-df2bdd976500f5594a90d51275819e5dbbc8baf1a1834b8e6327bf5707d879d8L13' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 49345690</div><div id='project'> Project Name: westlake-ai/openbioseq</div><div id='commit'> Commit Name: f678533fb9c8c5320abdd86ba57bd50d6357835f</div><div id='time'> Time: 2023-03-05</div><div id='author'> Author: 1070535169@qq.com</div><div id='file'> File Name: tools/model_converters/publish_model.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: process_checkpoint(3)</div><div id='n_method'> N Method Name: process_checkpoint(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: tools/model_converters/publish_model.py</div><div id='n_file'> N File Name: tools/model_converters/publish_model.py</div><div id='m_start'> M Start Line: 13</div><div id='m_end'> M End Line: 24</div><div id='n_start'> N Start Line: 24</div><div id='n_end'> N End Line: 45</div><BR>