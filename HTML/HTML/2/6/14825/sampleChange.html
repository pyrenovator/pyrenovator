<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            )
            rank_zero_warn(error_msg)
            raise MisconfigurationException(error_msg)
        <a id="change">assert </a>isinstance(testlr, torch.optim.lr_scheduler._LRScheduler)


class ScheduleImplMixin(ABC):</code></pre><h3>After Change</h3><pre><code class='java'>
                " reinitialized at every fine-tuning phase with implicit mode fine-tuning."
            )
        test_lr_init = copy(lr_scheduler_init.get("init_args", {}))
        <a id="change">if </a>min_lr_param:
            <a id="change">del test_lr_init["min_lr"]</a>  &#47&#47 our mock optimizer will not have any param groups
        try:
            testlr = lrs_class(optimizer=_MockOptimizer(), **test_lr_init)
        except Exception as err:</code></pre>