<html><h3>Pattern ID :14376
</h3><img src='47442771.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        key = reshape(self.key(key_flat)).transpose(2, 3)
        value = reshape(self.value(key_flat))

        attn<a id="change"> = </a>torch.matmul(query, key) / sqrt(self.dim_head)
        mask, start_mask = causal_mask(height * width)
        mask = mask.type_as(query)
        start_mask = start_mask.type_as(query)
        attn = attn.masked_fill(mask == 0, -1e4)
        attn = torch.softmax(attn, 3) * start_mask
        attn = <a id="change">self.dropout(</a>attn<a id="change">)</a>

        out = attn @ value
        out<a id="change"> = </a>out.transpose(1, 2).reshape(
            batch, height, width, self.dim_head * self.n_head
        )
        out<a id="change"> = </a>out.permute(0, 3, 1, 2)

        <a id="change">return </a>out

class PixelBlock(HelperModule):
    def build(</code></pre><h3>After Change</h3><pre><code class='java'>

        attn = (q @ k) / sqrt(self.head_dim)
        mask, start_mask = causal_mask(height*width)
        mask<a id="change">, start_mask</a> = mask.type_as(q), start_mask.type_as(q)

        attn = attn.masked_fill(mask == 0, -1e4)
        attn = torch.softmax(attn, dim=3) * start_mask</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/vvvm23/vqvae-2/commit/e15b9e3d7c13f912682873648e5aaaf81b1dfec5#diff-a112c46d500b0c8291c2990801b6878d4693377171a2ede123da1a50dcc72c61L110' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 47442771</div><div id='project'> Project Name: vvvm23/vqvae-2</div><div id='commit'> Commit Name: e15b9e3d7c13f912682873648e5aaaf81b1dfec5</div><div id='time'> Time: 2021-06-02</div><div id='author'> Author: alexander.f.mckinney@durham.ac.uk</div><div id='file'> File Name: pixelsnail.py</div><div id='m_class'> M Class Name: CausalAttention</div><div id='n_method'> N Class Name: CausalAttention</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: HelperModule</div><div id='n_parent_class'> N Parent Class: HelperModule</div><div id='m_file'> M File Name: pixelsnail.py</div><div id='n_file'> N File Name: pixelsnail.py</div><div id='m_start'> M Start Line: 184</div><div id='m_end'> M End Line: 204</div><div id='n_start'> N Start Line: 110</div><div id='n_end'> N End Line: 130</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        first_h_a = self.initHidden_alpha(x.shape[1])
        first_h_b = self.initHidden_beta(x.shape[1])

        self.emb<a id="change"> = </a>self.embedding(x)
        if self.drop &lt; 1:
            self.emb<a id="change"> = </a><a id="change">self.dropout(</a>self.emb<a id="change">)</a>

        count = np.arange(x.shape[0]) + 1
        self.c_t<a id="change"> = </a>torch.zeros_like(self.emb)  &#47&#47 shape=(seq_len, batch_size, day_dim)
        for i, att_timesteps in enumerate(count):
            &#47&#47 按时间步迭代，计算每个时间步的经attention的gru输出
            self.c_t[i] = self.attentionStep(first_h_a, first_h_b, att_timesteps)

        if self.drop &lt; 1.0:
            self.c_t = self.dropout(self.c_t)

        &#47&#47 &#47&#47 output层
        &#47&#47 y_hat = self.out(self.c_t)
        &#47&#47 y_hat = torch.sigmoid(y_hat)
        <a id="change">return </a>self.c_t
</code></pre><h3>After Change</h3><pre><code class='java'>
        return out

    def forward(self, x):
        batch_size<a id="change">, time_steps, _</a> = x.size()
        x = self.proj(x)
        x = self.dropout(x)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/yhzhu99/covid-ehr-benchmarks/commit/b3d4ba85ad8e8cfeb3e45e07e5fadfa3fd4a25fa#diff-77d2a7ae5e3183c97c4a93ec2d675da0a9f18941a477d8d2a5d9e1d496f4b387L63' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 47442766</div><div id='project'> Project Name: yhzhu99/covid-ehr-benchmarks</div><div id='commit'> Commit Name: b3d4ba85ad8e8cfeb3e45e07e5fadfa3fd4a25fa</div><div id='time'> Time: 2022-06-25</div><div id='author'> Author: yhzhu99@gmail.com</div><div id='file'> File Name: app/models/backbones/retain.py</div><div id='m_class'> M Class Name: RETAIN</div><div id='n_method'> N Class Name: RETAIN</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: app/models/backbones/retain.py</div><div id='n_file'> N File Name: app/models/backbones/retain.py</div><div id='m_start'> M Start Line: 64</div><div id='m_end'> M End Line: 83</div><div id='n_start'> N Start Line: 44</div><div id='n_end'> N End Line: 53</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 (add 1 to account for the embedding layer - the decoder will drop it later)
        self_attended_context = context_embedded.all_layers[-(self.args.transformer_layers + 1):]
        final_context = context_embedded.last_layer
        final_question<a id="change"> = </a>question_embedded.last_layer

        if self.projection is not None:
            final_context = self.dropout(final_context)
            final_context = self.projection(final_context)

            final_question<a id="change"> = </a><a id="change">self.dropout(</a>final_question<a id="change">)</a>
            final_question<a id="change"> = </a>self.projection(final_question)

        context_rnn_state = None
        question_rnn_state = None
        if self.args.rnn_layers &gt; 0:
            batch_size = context.size(0)
            if self.args.rnn_zero_state == &quotzero&quot:

                zero = torch.zeros(self.args.rnn_layers, batch_size, self.args.rnn_dimension,
                                   dtype=torch.float, requires_grad=False, device=context.device)
                context_rnn_state = (zero, zero)
                question_rnn_state = (zero, zero)
            else:
                if self.args.rnn_zero_state == &quotcls&quot:
                    packed_rnn_state = self.norm(self.pool(context_embedded.last_layer[:, 0, :]))

                elif self.args.rnn_zero_state == &quotaverage&quot:
                    masked_final_context = context_embedded.last_layer.masked_fill(context_padding.unsqueeze(2), 0)
                    summed_context = torch.sum(masked_final_context, dim=1)
                    average_context = summed_context / context_lengths.unsqueeze(1)

                    packed_rnn_state = self.norm(self.pool(average_context))

                &#47&#47 packed_rnn_state is (batch, 2 * rnn_layers * rnn_dim)
                packed_rnn_state = packed_rnn_state.reshape(batch_size, 2, self.args.rnn_layers,
                                                            self.args.rnn_dimension)
                &#47&#47 transpose to (2, batch, rnn_layers, rnn_dimension)
                packed_rnn_state = packed_rnn_state.transpose(0, 1)
                &#47&#47 transpose to (2, rnn_layers, batch, rnn_dimension)
                packed_rnn_state = packed_rnn_state.transpose(1, 2)
                &#47&#47 convert to a tuple of two (rnn_layers, batch, rnn_dimension) tensors
                packed_rnn_state = packed_rnn_state.chunk(2, dim=0)
                context_rnn_state = (packed_rnn_state[0].squeeze(0), packed_rnn_state[1].squeeze(0))

        <a id="change">return </a>self_attended_context, final_context, context_rnn_state, final_question, question_rnn_state
</code></pre><h3>After Change</h3><pre><code class='java'>
                packed_rnn_state = packed_rnn_state.chunk(2, dim=0)
                context_rnn_state = (packed_rnn_state[0].squeeze(0), packed_rnn_state[1].squeeze(0))

        return self_attended_context<a id="change">, final_context, context_rnn_state</a>
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/stanford-oval/genienlp/commit/25cc6202b03c0b3866c249cc83412618eb894797#diff-d64eeb20506890f64fbc35dbbe9eb96bfc16a0c55673ab8dba41f3ed32d89537L61' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 47442780</div><div id='project'> Project Name: stanford-oval/genienlp</div><div id='commit'> Commit Name: 25cc6202b03c0b3866c249cc83412618eb894797</div><div id='time'> Time: 2020-12-05</div><div id='author'> Author: gcampagn@cs.stanford.edu</div><div id='file'> File Name: genienlp/models/identity_encoder.py</div><div id='m_class'> M Class Name: IdentityEncoder</div><div id='n_method'> N Class Name: IdentityEncoder</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: genienlp/models/identity_encoder.py</div><div id='n_file'> N File Name: genienlp/models/identity_encoder.py</div><div id='m_start'> M Start Line: 63</div><div id='m_end'> M End Line: 116</div><div id='n_start'> N Start Line: 70</div><div id='n_end'> N End Line: 107</div><BR>