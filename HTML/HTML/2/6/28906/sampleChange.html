<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        
        &#47&#47compute policy and ent loss
        policy_loss = ((self.alpha * new_curr_state_log_pi) - new_min_curr_state_q_value).mean()
        policy_loss_value<a id="change"> = </a><a id="change">policy_loss.detach().cpu()</a>.numpy()

        if self.automatic_entropy_tuning:
            alpha_loss = -(self.log_alpha * (new_curr_state_log_pi + self.target_entropy).detach()).mean()</code></pre><h3>After Change</h3><pre><code class='java'>
        reward_batch = reward_batch * self.reward_scale
        curr_state_q1_value = self.q1_network(torch.cat([obs_batch, action_batch],dim=1))
        curr_state_q2_value = self.q2_network(torch.cat([obs_batch, action_batch],dim=1))
        <a id="change">with torch.no_grad()</a><a id="change">:
            </a>next_state_action, next_state_log_pi = \
                itemgetter("action_scaled", "log_prob")(self.policy_network.sample(next_obs_batch))

            next_state_q1_value = self.target_q1_network(torch.cat([next_obs_batch, next_state_action], dim=1))</code></pre>