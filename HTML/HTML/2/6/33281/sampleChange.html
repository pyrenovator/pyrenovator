<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        network_prediction = self.q_network.forward(batch_inputs).view(self.batch_size, self.agents, self.number_actions)
        &#47&#47 Bellman equation
        batch_labels_tensor = batch_labels + (discount_factor * max_target_net)
        td_errors<a id="change"> = </a>(network_prediction<a id="change"> - </a><a id="change">batch_labels_tensor.unsqueeze(-1</a><a id="change">)</a>).detach()

        index = torch.tensor(transitions[1], dtype=torch.long).unsqueeze(-1)
        y_pred = (torch.gather(network_prediction, -1, index)).squeeze()</code></pre><h3>After Change</h3><pre><code class='java'>
        next_state = torch.tensor(transitions[3])

        &#47&#47 Labels are the rewards
        rewards = <a id="change">torch.clamp(</a>torch.tensor(transitions[2], dtype=torch.float32), <a id="change">-1</a>, 1<a id="change">)</a>
        y = self.target_network.forward(next_state).detach().squeeze() &#47&#47 TODO: should it be next state or current state that we forward?
        y = y.view(self.batch_size, self.agents, self.number_actions)
        &#47&#47 Get the maximum prediction for the next state from the target network
        max_target_net = y.max(-1)[0]</code></pre>