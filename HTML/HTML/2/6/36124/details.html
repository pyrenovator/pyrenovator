<html><h3>Pattern ID :36124
</h3><img src='102477531.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        num_heads = self.relative_position_bias_table.shape[0]
        &#47&#47 pos_emb = tf.gather(self.relative_position_bias_table, self.relative_position_index, axis=1).numpy()
        hh = ww = int(<a id="change">tf.math.sqrt(</a>float(self.relative_position_bias_table.shape[1] - self.cls_token_pos_len)<a id="change">)</a>)
        pos_emb = tf.reshape(self.relative_position_bias_table[:, : hh * ww], (num_heads, hh, ww)).numpy()
        cols = int(tf.math.ceil(num_heads / rows))
        fig, axes = plt.subplots(rows, cols, figsize=(base_size * cols, base_size * rows))</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 pos_emb = tf.gather(self.relative_position_bias_table, self.relative_position_index, axis=1).numpy()
        hh = ww = int(float(self.relative_position_bias_table.shape[1] - self.cls_token_pos_len) ** 0.5)
        pos_emb = self.relative_position_bias_table[:, : hh * ww]
        pos_emb = <a id="change">pos_emb.detach()</a>.numpy()<a id="change"> if </a><a id="change">hasattr(pos_emb</a>, <a id="change">"detach"</a><a id="change">) else </a>pos_emb.numpy()
        pos_emb = pos_emb.reshape((num_heads, hh, ww))
        cols = int(math.ceil(num_heads / rows))
        fig, axes = plt.subplots(rows, cols, figsize=(base_size * cols, base_size * rows))</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/1aa29dc686f862bc1ff66a11700fa8ef16bd2b8a#diff-02508ae7c53082aa4dbb63a5c5fd259b84bc5bdbf0b1971ed6cb2060ef9b9779L128' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 102477531</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 1aa29dc686f862bc1ff66a11700fa8ef16bd2b8a</div><div id='time'> Time: 2023-02-03</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/beit/beit.py</div><div id='m_class'> M Class Name: MultiHeadRelativePositionalEmbedding</div><div id='n_method'> N Class Name: MultiHeadRelativePositionalEmbedding</div><div id='m_method'> M Method Name: show_pos_emb(3)</div><div id='n_method'> N Method Name: show_pos_emb(3)</div><div id='m_parent_class'> M Parent Class: layers.Layer</div><div id='n_parent_class'> N Parent Class: keras.layers.Layer</div><div id='m_file'> M File Name: keras_cv_attention_models/beit/beit.py</div><div id='n_file'> N File Name: keras_cv_attention_models/beit/beit.py</div><div id='m_start'> M Start Line: 128</div><div id='m_end'> M End Line: 130</div><div id='n_start'> N Start Line: 132</div><div id='n_end'> N End Line: 136</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        else:
            source_tt = source_layer.bb  &#47&#47 layer
        num_heads = source_tt.shape[0]
        source_query_hh = source_query_ww = int(<a id="change">tf.math.sqrt(</a>float(source_tt.shape[1])<a id="change">)</a>)  &#47&#47 assume source weights are all square shape
        source_kv_hh = source_kv_ww = int(tf.math.sqrt(float(source_tt.shape[2])))  &#47&#47 assume source weights are all square shape

        tt = tf.reshape(source_tt, [num_heads, source_query_hh, source_query_ww, source_kv_hh * source_kv_ww])  &#47&#47 resize on query dimension first</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 source_tt = source_layer["pos_emb:0"]  &#47&#47 weights
        else:
            source_tt = source_layer.bb  &#47&#47 layer
        source_tt = np.array(<a id="change">source_tt.detach()</a><a id="change"> if </a><a id="change">hasattr(</a>source_tt, <a id="change">"detach"</a><a id="change">) else </a>source_tt)

        num_heads = source_tt.shape[0]
        source_query_hh = source_query_ww = int(float(source_tt.shape[1]) ** 0.5)  &#47&#47 assume source weights are all square shape</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/2ba27b0132168f3590dd4b3bead9edc15a70ba7d#diff-f52477ea03b2a8f18695211ce6217a2ea1e5dd5afaa7adfd75a231c5a04692bcL57' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 102477534</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 2ba27b0132168f3590dd4b3bead9edc15a70ba7d</div><div id='time'> Time: 2023-02-11</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/cmt/cmt.py</div><div id='m_class'> M Class Name: BiasPositionalEmbedding</div><div id='n_method'> N Class Name: BiasPositionalEmbedding</div><div id='m_method'> M Method Name: load_resized_weights(3)</div><div id='n_method'> N Method Name: load_resized_weights(3)</div><div id='m_parent_class'> M Parent Class: layers.Layer</div><div id='n_parent_class'> N Parent Class: keras.layers.Layer</div><div id='m_file'> M File Name: keras_cv_attention_models/cmt/cmt.py</div><div id='n_file'> N File Name: keras_cv_attention_models/cmt/cmt.py</div><div id='m_start'> M Start Line: 59</div><div id='m_end'> M End Line: 74</div><div id='n_start'> N Start Line: 61</div><div id='n_end'> N End Line: 79</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            source_bb = source_layer["positional_embedding:0"]  &#47&#47 weights
        else:
            source_bb = source_layer.bb  &#47&#47 layer
        hh = ww = int(<a id="change">tf.math.sqrt(</a>float(source_bb.shape[0])<a id="change">)</a>)
        ss = tf.reshape(source_bb, (hh, ww, source_bb.shape[-1]))  &#47&#47 [hh, ww, num_heads]
        &#47&#47 target_hh = target_ww = int(tf.math.sqrt(float(self.bb.shape[0])))
        tt = tf.image.resize(ss, [self.k_blocks_h, self.k_blocks_w], method=method)  &#47&#47 [target_hh, target_ww, num_heads]</code></pre><h3>After Change</h3><pre><code class='java'>
            source_bb = list(source_layer.values())[0]  &#47&#47 weights
        else:
            source_bb = source_layer.bb  &#47&#47 layer
        source_tt = np.array(<a id="change">source_tt.detach()</a><a id="change"> if </a><a id="change">hasattr(</a>source_tt, <a id="change">"detach"</a><a id="change">) else </a>source_tt)
        hh = ww = int(float(source_bb.shape[0]) ** 0.5)
        ss = source_bb.reshape((hh, ww, source_bb.shape[-1]))  &#47&#47 [hh, ww, num_heads]
        &#47&#47 target_hh = target_ww = int(float(self.bb.shape[0]) ** 0.5)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/c870bf2e8d3e6b8b0e969d5468d550085414c0cd#diff-419ea771811e38fb2e106cf19724ad992d4d6f2fbae3c511c734548a78b6d63bL64' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 102477532</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: c870bf2e8d3e6b8b0e969d5468d550085414c0cd</div><div id='time'> Time: 2023-02-05</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/levit/levit.py</div><div id='m_class'> M Class Name: MultiHeadPositionalEmbedding</div><div id='n_method'> N Class Name: MultiHeadPositionalEmbedding</div><div id='m_method'> M Method Name: load_resized_weights(3)</div><div id='n_method'> N Method Name: load_resized_weights(3)</div><div id='m_parent_class'> M Parent Class: layers.Layer</div><div id='n_parent_class'> N Parent Class: keras.layers.Layer</div><div id='m_file'> M File Name: keras_cv_attention_models/levit/levit.py</div><div id='n_file'> N File Name: keras_cv_attention_models/levit/levit.py</div><div id='m_start'> M Start Line: 66</div><div id='m_end'> M End Line: 74</div><div id='n_start'> N Start Line: 73</div><div id='n_end'> N End Line: 82</div><BR>