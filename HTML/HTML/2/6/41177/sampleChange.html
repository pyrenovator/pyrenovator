<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                data, target = data.cuda(), target.cuda()
            optimizer.zero_grad()
            &#47&#47 Split data into sub-batches of size batch_size
            <a id="change">for </a>i in <a id="change">range(0</a>, len(data), args.batch_size<a id="change">):
                </a>data_batch = data[i:i + args.batch_size]
                target_batch = target[i:i + args.batch_size]
                output = model(data_batch)
                train_accuracy.update(accuracy(output, target_batch))
                loss = F.cross_entropy(output, target_batch)
                train_loss.update(loss)
                &#47&#47 Average gradients among sub-batches
                loss.div_(math.ceil(float(len(data)) / args.batch_size))
                loss.backward()
            &#47&#47 Gradient is applied across all ranks
            if args.kfac_update_freq &gt; 0:
                preconditioner.step()
            optimizer.step()
            t.set_postfix(<a id="change">{</a>&quotloss&quot: train_loss.avg.item(),
                           &quotaccuracy&quot: 100. * train_accuracy.avg.item()<a id="change">}</a>)
            t.update(1)

    if log_writer:</code></pre><h3>After Change</h3><pre><code class='java'>

            loss = F.cross_entropy(output, target)
            train_loss.update(loss)
            <a id="change">train_accuracy.update(</a>accuracy(output, target)<a id="change">)</a>
            loss.backward()

            &#47&#47 Gradient is applied across all ranks
            if use_kfac:</code></pre>