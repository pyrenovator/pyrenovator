<html><h3>Pattern ID :21154
</h3><img src='67811663.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                _check_cuobjdump_output(expected[1], is_ptx=True)
        finally:
            if IS_WINDOWS:
                <a id="change">print("Not wiping extensions build folder because Windows"</a><a id="change">)</a>
            else:
                shutil.rmtree(temp_dir)

            if old_envvar is None:</code></pre><h3>After Change</h3><pre><code class='java'>
                "build_directory": temp_dir,
            }

            if <a id="change">IS_WINDOWS</a>:
                p<a id="change"> = </a><a id="change">mp.Process(target=torch.utils.cpp_extension.load, kwargs=params)</a>

                &#47&#47 Compile and load the test CUDA arch in a different Python process to avoid
                &#47&#47 polluting the current one and causes test_jit_cuda_extension to fail on
                &#47&#47 Windows. There is no clear way to unload a module after it has been imported
                &#47&#47 and torch.utils.cpp_extension.load builds and loads the module in one go.
                &#47&#47 See https://github.com/pytorch/pytorch/issues/61655 for more details
                <a id="change">p.start()</a>
                p.join()
            else:
                torch.utils.cpp_extension.load(**params)
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/54eedf6fa6a5104dbdf92f868b0236c43c90dd21#diff-5ee802fb38a81bf55153346d7086ace60471c1876c5183cc81ea05e74e9b1438L148' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 67811663</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 54eedf6fa6a5104dbdf92f868b0236c43c90dd21</div><div id='time'> Time: 2023-02-01</div><div id='author'> Author: huydhn@gmail.com</div><div id='file'> File Name: test/test_cpp_extensions_jit.py</div><div id='m_class'> M Class Name: TestCppExtensionJIT</div><div id='n_method'> N Class Name: TestCppExtensionJIT</div><div id='m_method'> M Method Name: _run_jit_cuda_archflags(3)</div><div id='n_method'> N Method Name: _run_jit_cuda_archflags(3)</div><div id='m_parent_class'> M Parent Class: common.TestCase</div><div id='n_parent_class'> N Parent Class: common.TestCase</div><div id='m_file'> M File Name: test/test_cpp_extensions_jit.py</div><div id='n_file'> N File Name: test/test_cpp_extensions_jit.py</div><div id='m_start'> M Start Line: 148</div><div id='m_end'> M End Line: 169</div><div id='n_start'> N Start Line: 148</div><div id='n_end'> N End Line: 189</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
class TestAsyncModelAverage(unittest.TestCase):
    def test_algorithm(self):
        if not torch.cuda.is_available():
            <a id="change">print("skip tests since cuda is not available"</a><a id="change">)</a>
            return

        nprocs = torch.cuda.device_count()
        os.environ["WORLD_SIZE"] = str(nprocs)</code></pre><h3>After Change</h3><pre><code class='java'>

        mp = multiprocessing.get_context("spawn")
        processes = []
        for <a id="change">i</a> in range(nprocs):
            p<a id="change"> = </a><a id="change">mp.Process(target=run_model, args=(i, env))</a>
            <a id="change">p.start()</a>
            processes.append(p)

        for p in processes:
            p.join(timeout=60)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/baguasys/bagua/commit/a9529bef66e367884316a2b2ebc917ff35bf6334#diff-c46b7f4c320ba27d2783f4d47a513380419a948da235a60e6b47ccf8be3f194bL61' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 67811666</div><div id='project'> Project Name: baguasys/bagua</div><div id='commit'> Commit Name: a9529bef66e367884316a2b2ebc917ff35bf6334</div><div id='time'> Time: 2021-08-26</div><div id='author'> Author: 45031995+wangraying@users.noreply.github.com</div><div id='file'> File Name: tests/torch_api/test_async_model_average.py</div><div id='m_class'> M Class Name: TestAsyncModelAverage</div><div id='n_method'> N Class Name: TestAsyncModelAverage</div><div id='m_method'> M Method Name: test_algorithm(1)</div><div id='n_method'> N Method Name: test_algorithm(1)</div><div id='m_parent_class'> M Parent Class: unittest.TestCase</div><div id='n_parent_class'> N Parent Class: unittest.TestCase</div><div id='m_file'> M File Name: tests/torch_api/test_async_model_average.py</div><div id='n_file'> N File Name: tests/torch_api/test_async_model_average.py</div><div id='m_start'> M Start Line: 62</div><div id='m_end'> M End Line: 76</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 87</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

def run_test_locally(fn):
    if not torch.cuda.is_available():
        <a id="change">print("skip tests since cuda is not available"</a><a id="change">)</a>
        return []

    nprocs = torch.cuda.device_count()
    os.environ["WORLD_SIZE"] = str(nprocs)</code></pre><h3>After Change</h3><pre><code class='java'>
    mp = multiprocessing.get_context("spawn")
    results = [Result() for _ in range(nprocs)]
    processes = []
    for <a id="change">i</a> in range(nprocs):
        p<a id="change"> = </a><a id="change">mp.Process(
            target=fn,
            args=(i, nprocs, results, env),
        )</a>
        <a id="change">p.start()</a>
        processes.append(p)

    for p in processes:
        p.join(timeout=60)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/baguasys/bagua/commit/a9529bef66e367884316a2b2ebc917ff35bf6334#diff-7a16c2809e9e060c1dc2b8f83387654f4a97459627405dbf5948e60b05bcea7dL105' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 67811665</div><div id='project'> Project Name: baguasys/bagua</div><div id='commit'> Commit Name: a9529bef66e367884316a2b2ebc917ff35bf6334</div><div id='time'> Time: 2021-08-26</div><div id='author'> Author: 45031995+wangraying@users.noreply.github.com</div><div id='file'> File Name: tests/comm/test_communicator.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: run_test_locally(1)</div><div id='n_method'> N Method Name: run_test_locally(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: tests/comm/test_communicator.py</div><div id='n_file'> N File Name: tests/comm/test_communicator.py</div><div id='m_start'> M Start Line: 106</div><div id='m_end'> M End Line: 122</div><div id='n_start'> N Start Line: 115</div><div id='n_end'> N End Line: 138</div><BR>