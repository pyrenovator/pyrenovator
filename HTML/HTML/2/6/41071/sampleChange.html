<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            self.input_channel = layer_spec[&quotout_dim&quot]
        &#47&#47 Needs to be a ModuleList rather than just a list for the parameters of the listed layers
        &#47&#47 to be visible as part of the module .parameters() return
        self.conv_layers<a id="change"> = nn</a><a id="change">.ModuleList(</a>self.conv_layers<a id="change">)</a>

        for ind, layer_spec in enumerate(architecture[&quotDENSE&quot][:-1]):
            in_dim, out_dim = layer_spec.get(&quotin_dim&quot), layer_spec.get(&quotout_dim&quot)
            self.dense_layers.append(nn.Linear(in_dim, out_dim))
        self.mean_layer = nn.Linear(architecture[&quotDENSE&quot][-1][&quotin_dim&quot], self.representation_dim)
        if learn_scale:
            self.scale_layer = nn.Linear(architecture[&quotDENSE&quot][-1][&quotin_dim&quot], self.representation_dim)
        else:
            self.scale_layer = lambda x: torch.ones(self.representation_dim)

        self.dense_layers<a id="change"> = nn</a><a id="change">.ModuleList(</a>self.dense_layers<a id="change">)</a>
        self.relu = nn.ReLU()

    def forward(self, x, traj_info=None):
        x = x.permute(0, 3, 1, 2)</code></pre><h3>After Change</h3><pre><code class='java'>
            shared_network_layers.append(nn.ReLU())
            self.input_channel = layer_spec[&quotout_dim&quot]

        <a id="change">shared_network_layers.append(</a>nn.Flatten()<a id="change">)</a>
        for ind, layer_spec in enumerate(architecture[&quotDENSE&quot][:-1]):
            in_dim, out_dim = layer_spec.get(&quotin_dim&quot), layer_spec.get(&quotout_dim&quot)
            shared_network_layers.append(nn.Linear(in_dim, out_dim))
            shared_network_layers.append(nn.ReLU())</code></pre>