<html><h3>Pattern ID :7170
</h3><img src='24073839.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        bias_keys = []
        for bidirectional in [True, False]:
            num_directions = 2 if bidirectional else 1
            <a id="change">for layer</a> in range(num_layers)<a id="change">:
                </a>for direction in range(num_directions):
                    suffix = &quot_reverse&quot if <a id="change">direction == 1</a> else &quot&quot
                    key_name1 = &quotweight_ih_l{layer_idx}{suffix}&quot.format(layer_idx=layer, suffix=suffix)
                    key_name2<a id="change"> = </a>&quotweight_hh_l{layer_idx}{suffix}&quot.format(layer_idx=layer, suffix=suffix)
                    weight_keys.append(key_name1)
                    weight_keys.append(key_name2)
                    key_name1 = &quotbias_ih_l{layer_idx}{suffix}&quot.format(layer_idx=layer, suffix=suffix)
                    key_name2<a id="change"> = </a>&quotbias_hh_l{layer_idx}{suffix}&quot.format(layer_idx=layer, suffix=suffix)
                    bias_keys.append(key_name1)
                    bias_keys.append(key_name2)
</code></pre><h3>After Change</h3><pre><code class='java'>
            for wn in fp32_rnn._flat_weights_names:
                setattr(ref_rnn, wn, copy.deepcopy(getattr(fp32_rnn, wn)))

            ref_rnn._flat_weights = <a id="change">copy.deepcopy(</a>fp32_rnn._flat_weights<a id="change">)</a>

            &#47&#47 quantize and dequantize the weights for fp32_rnn module
            flat_weights = []
            for wn in fp32_rnn._flat_weights_names:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/7ddf212f3390f14abdbd5373f5fbdb04ffde5fd8#diff-daa03e81d706f5eb9d02e3c68221dc7735672898f9859b83de883d0a5706391fL1546' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 24073839</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 7ddf212f3390f14abdbd5373f5fbdb04ffde5fd8</div><div id='time'> Time: 2022-03-11</div><div id='author'> Author: jerryzh@fb.com</div><div id='file'> File Name: test/quantization/core/test_quantized_module.py</div><div id='m_class'> M Class Name: TestReferenceQuantizedModule</div><div id='n_method'> N Class Name: TestReferenceQuantizedModule</div><div id='m_method'> M Method Name: test_rnn(1)</div><div id='n_method'> N Method Name: test_rnn(1)</div><div id='m_parent_class'> M Parent Class: QuantizationTestCase</div><div id='n_parent_class'> N Parent Class: QuantizationTestCase</div><div id='m_file'> M File Name: test/quantization/core/test_quantized_module.py</div><div id='n_file'> N File Name: test/quantization/core/test_quantized_module.py</div><div id='m_start'> M Start Line: 1546</div><div id='m_end'> M End Line: 1595</div><div id='n_start'> N Start Line: 1552</div><div id='n_end'> N End Line: 1590</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    
    if args.use_lora:                       &#47&#47 merge lora params with origin model
        key_list = [key for key, _ in model.base_model.model.named_modules() if "lora" not in key]
        <a id="change">for key</a> in key_list<a id="change">:
            </a>parent<a id="change">, target, target_name = </a>model.base_model._get_submodules(key)
            if isinstance(target, peft.tuners.lora.Linear):
                bias = <a id="change">target.bias is not None</a>
                new_module<a id="change"> = </a>nn.Linear(target.in_features, target.out_features, bias=bias)
                model.base_model._replace_module(parent, target_name, new_module, target)
        model = model.base_model.model
        model.save_pretrained(cur_save_dir)</code></pre><h3>After Change</h3><pre><code class='java'>
        cur_save_path (str): 存储路径。
    
    if args.use_lora:                       &#47&#47 merge lora params with origin model
        merged_model = <a id="change">copy.deepcopy(</a>model<a id="change">)</a>
        merged_model = merged_model.merge_and_unload()
        merged_model.save_pretrained(cur_save_dir)
    else:
        model.save_pretrained(cur_save_dir)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/harderthenharder/transformers_tasks/commit/67a2d33f4bb0adaa6f89e8680ec912f9ceeda877#diff-24116351c07b35bfd9cf5eef75d871611e35abb3854f19f0f6d7a5b5373343e3L145' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 24073807</div><div id='project'> Project Name: harderthenharder/transformers_tasks</div><div id='commit'> Commit Name: 67a2d33f4bb0adaa6f89e8680ec912f9ceeda877</div><div id='time'> Time: 2023-04-09</div><div id='author'> Author: 1414463123@qq.com</div><div id='file'> File Name: LLM/finetune/train_multi_gpu.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: save_model(2)</div><div id='n_method'> N Method Name: save_model(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: LLM/finetune/train_multi_gpu.py</div><div id='n_file'> N File Name: LLM/finetune/train_multi_gpu.py</div><div id='m_start'> M Start Line: 156</div><div id='m_end'> M End Line: 164</div><div id='n_start'> N Start Line: 157</div><div id='n_end'> N End Line: 159</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    
    if args.use_lora:                       &#47&#47 merge lora params with origin model
        key_list = [key for key, _ in model.base_model.model.named_modules() if "lora" not in key]
        <a id="change">for key</a> in key_list<a id="change">:
            </a>parent<a id="change">, target, target_name = </a>model.base_model._get_submodules(key)
            if isinstance(target, peft.tuners.lora.Linear):
                bias = <a id="change">target.bias is not None</a>
                new_module<a id="change"> = </a>nn.Linear(target.in_features, target.out_features, bias=bias)
                model.base_model._replace_module(parent, target_name, new_module, target)
        model = model.base_model.model
        model.save_pretrained(cur_save_dir)</code></pre><h3>After Change</h3><pre><code class='java'>
        cur_save_path (str): 存储路径。
    
    if args.use_lora:                       &#47&#47 merge lora params with origin model
        merged_model = <a id="change">copy.deepcopy(</a>model<a id="change">)</a>
        merged_model = merged_model.merge_and_unload()
        merged_model.save_pretrained(cur_save_dir)
    else:
        model.save_pretrained(cur_save_dir)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/harderthenharder/transformers_tasks/commit/67a2d33f4bb0adaa6f89e8680ec912f9ceeda877#diff-e789cd65d45cbd033d0aaccef8148de824b87d66f7ac54d0b874bcd3c8a0bd42L147' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 24073832</div><div id='project'> Project Name: harderthenharder/transformers_tasks</div><div id='commit'> Commit Name: 67a2d33f4bb0adaa6f89e8680ec912f9ceeda877</div><div id='time'> Time: 2023-04-09</div><div id='author'> Author: 1414463123@qq.com</div><div id='file'> File Name: LLM/finetune/train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: save_model(2)</div><div id='n_method'> N Method Name: save_model(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: LLM/finetune/train.py</div><div id='n_file'> N File Name: LLM/finetune/train.py</div><div id='m_start'> M Start Line: 158</div><div id='m_end'> M End Line: 166</div><div id='n_start'> N Start Line: 159</div><div id='n_end'> N End Line: 161</div><BR>