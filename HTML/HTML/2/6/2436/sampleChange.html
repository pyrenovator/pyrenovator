<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    for embt, embi in zip(text_reader(batch_size=NUM_TEST_EMBEDDINGS, start=tstart, end=tend), 
            image_reader(batch_size=NUM_TEST_EMBEDDINGS, start=tstart, end=tend)):
       &#47&#47 make a copy of the text embeddings for shuffling
       text_embed<a id="change"> = </a><a id="change">torch.tensor(</a>embt[0]<a id="change">)</a>.to(device)
       text_embed_shuffled = text_embed.clone()
        &#47&#47 roll the text embeddings to simulate "unrelated" captions
       rolled_idx = torch.roll(torch.arange(NUM_TEST_EMBEDDINGS), 1)
       text_embed_shuffled = text_embed_shuffled[rolled_idx]
       text_embed_shuffled = text_embed_shuffled / \
           text_embed_shuffled.norm(dim=1, keepdim=True)
       test_text_shuffled_cond = dict(text_embed=text_embed_shuffled)
        &#47&#47 prepare the text embedding
       text_embed = text_embed / text_embed.norm(dim=1, keepdim=True)
       test_text_cond<a id="change"> = </a>dict(text_embed=text_embed)
        &#47&#47 prepare image embeddings
       test_image_embeddings = torch.tensor(embi[0]).to(device)
       test_image_embeddings = test_image_embeddings / \
           test_image_embeddings.norm(dim=1, keepdim=True)
        &#47&#47 predict on the unshuffled text embeddings
       predicted_image_embeddings = diffusion_prior.p_sample_loop(
           (NUM_TEST_EMBEDDINGS<a id="change">, 768</a>), text_cond=test_text_cond)
       predicted_image_embeddings = predicted_image_embeddings / \
           predicted_image_embeddings.norm(dim=1, keepdim=True)
        &#47&#47 predict on the shuffled embeddings</code></pre><h3>After Change</h3><pre><code class='java'>

    cos = nn.CosineSimilarity(dim=1, eps=1e-6)

    for test_image_embeddings, text_data in <a id="change">tqdm(</a>dataloader<a id="change">)</a>:

        &#47&#47 we are text conditioned, we produce an embedding from the tokenized text
        if text_conditioned:</code></pre>