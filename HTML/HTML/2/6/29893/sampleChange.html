<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        P_bar = self.dropout(P_bar)

        VW = self.w_v(V)
        VW = <a id="change">torch.einsum("nk,bnc-&gt;bkc"</a>, F, VW<a id="change">)</a>
        out_tensor = torch.einsum("bnk,bkc-&gt;bnc", P_bar, VW)

        return out_tensor
</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward(self, Q, K, V, E, F):
        KW = self.w_k(K)
        KW = torch.matmul(E, KW)
        KW = <a id="change">torch.transpose(</a>KW, <a id="change">1</a>, <a id="change">2</a><a id="change">)</a>
        QW = self.w_q(Q)
        QW<a id="change"> = </a>torch.matmul(QW, KW)

        &#47&#47 TODO: Possibly change the dtype?
        P_bar = QW/torch.sqrt(torch.tensor(self.dim, dtype=torch.float32))</code></pre>