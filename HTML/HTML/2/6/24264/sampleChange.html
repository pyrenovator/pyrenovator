<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 x: (batch_size, num_route_nodes, in_channels)
        &#47&#47 route_weights: (num_route_nodes, num_capsules, in_channels, out_channels)
        &#47&#47 u_hat: (batch_size, num_capsules, num_route_nodes, out_channels)
        u_hat = <a id="change">torch.einsum(&quotijk, jlkm -&gt; iljm&quot</a>, x, self.route_weights<a id="change">)</a>
        &#47&#47 Detatch u_hat during routing iterations
        u_hat_temp = u_hat.detach()

        &#47&#47 Dynamic route</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward(self, x):
        batch_size = x.size(0)
        &#47&#47 (batch_size, in_caps, in_dim) -&gt; (batch_size, 1, in_caps, in_dim, 1)
        x = <a id="change">x.unsqueeze(1).unsqueeze(4</a><a id="change">)</a>
        &#47&#47
        &#47&#47 W @ x =
        &#47&#47 (1, num_caps, in_caps, dim_caps, in_dim) @ (batch_size, 1, in_caps, in_dim, 1) =
        &#47&#47 (batch_size, num_caps, in_caps, dim_caps, 1)
        u_hat = <a id="change">torch.matmul(</a>self.W, x<a id="change">)</a>
        &#47&#47 (batch_size, num_caps, in_caps, dim_caps)
        u_hat<a id="change"> = </a>u_hat.squeeze(-1)
        &#47&#47 detach u_hat during routing iterations to prevent gradients from flowing
        temp_u_hat = u_hat.detach()
</code></pre>