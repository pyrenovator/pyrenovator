<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    train_target_iter = ForeverDataIterator(train_target_loader)

    &#47&#47 create model
    num_classes<a id="change"> = </a>train_source_dataset.num_classes
    backbone = models.__dict__[args.arch](pretrained=True)
    classifier = ImageClassifier(backbone, num_classes, args.bottleneck_dim).to(device)

    &#47&#47 define optimizer and lr scheduler
    optimizer = Adam(classifier.get_parameters(), args.lr)
    lr_scheduler = LambdaLR(optimizer, lambda x: args.lr * (1. + args.lr_gamma * float(x)) ** (-args.lr_decay))

    &#47&#47 resume from the best checkpoint
    if args.phase != &quottrain&quot:
        checkpoint = torch.load(logger.get_checkpoint_path(&quotbest&quot), map_location=&quotcpu&quot)
        classifier.load_state_dict(checkpoint)

    &#47&#47 analysis the model
    if args.phase == &quotanalysis&quot:
        &#47&#47 extract features from both domains
        feature_extractor = nn.Sequential(classifier.backbone, classifier.bottleneck).to(device)
        source_feature = collect_feature(train_source_loader, feature_extractor, device)
        target_feature = collect_feature(train_target_loader, feature_extractor, device)
        &#47&#47 plot t-SNE
        tSNE_filename = osp.join(logger.visualize_directory, &quotTSNE.png&quot)
        tsne.visualize(source_feature, target_feature, tSNE_filename)
        print("Saving t-SNE to", tSNE_filename)
        &#47&#47 calculate A-distance, which is a measure for distribution discrepancy
        A_distance = a_distance.calculate(source_feature, target_feature, device)
        print("A-distance =", A_distance)
        return

    if args.phase == &quottest&quot:
        acc1 = validate(test_loader, classifier, args)
        print(acc1)
        return

    if args.pretrain is None:
        &#47&#47 first pretrain the classifier wish source data
        print("Pretraining the model on source domain.")
        args.pretrain = logger.get_checkpoint_path(&quotpretrain&quot)
        pretrained_model = ImageClassifier(backbone, num_classes, args.bottleneck_dim).to(device)
        pretrain_optimizer = Adam(pretrained_model.get_parameters(), args.pretrain_lr)
        pretrain_lr_scheduler = LambdaLR(pretrain_optimizer,
                                         lambda x: args.pretrain_lr * (1. + args.lr_gamma * float(x)) ** (
                                             -args.lr_decay))

        &#47&#47 start pretraining
        for epoch in range(args.pretrain_epochs):
            &#47&#47 pretrain for one epoch
            pretrain(train_source_iter, pretrained_model, pretrain_optimizer, pretrain_lr_scheduler, epoch, args)
            &#47&#47 validate to show pretrain process
            validate(val_loader, pretrained_model, args)

        torch.save(pretrained_model.state_dict(), args.pretrain)

        &#47&#47 show pretrain result
        pretrain_acc = validate(val_loader, pretrained_model, args)
        print("pretrain_acc1 = {:3.1f}".format(pretrain_acc))

    checkpoint = torch.load(args.pretrain, map_location=&quotcpu&quot)
    classifier.load_state_dict(checkpoint)
    teacher = EmaTeacher(classifier, alpha=args.alpha)
    consistent_loss = L2ConsistencyLoss().to(device)
    class_balance_loss = ClassBalanceLoss(num_classes).to(device)

    &#47&#47 start training
    best_acc1 = 0.
    for epoch in range(args.epochs):
        print(lr_scheduler.get_lr())
        &#47&#47 train for one epoch
        train(train_source_iter, train_target_iter, classifier, teacher, consistent_loss, class_balance_loss, optimizer,
              lr_scheduler, epoch, args)

        &#47&#47 evaluate on validation set
        acc1 = validate(val_loader, classifier, args)

        &#47&#47 remember best acc@1 and save checkpoint
        torch.save(classifier.state_dict(), logger.get_checkpoint_path(&quotlatest&quot))
        if acc1 &gt; best_acc1:
            shutil.copy(logger.get_checkpoint_path(&quotlatest&quot), logger.get_checkpoint_path(&quotbest&quot))
        best_acc1 = max(acc1, best_acc1)

    print("best_acc1 = {:3.1f}".format(best_acc1))

    &#47&#47 evaluate on test set
    classifier.load_state_dict(torch.load(logger.get_checkpoint_path(&quotbest&quot)))
    acc1<a id="change"> = </a><a id="change">validate(</a>test_loader, classifier, args<a id="change">)</a>
    print("test_acc1 = {:3.1f}".format(acc1))

    logger.close()
</code></pre><h3>After Change</h3><pre><code class='java'>

    &#47&#47 create model
    backbone = utils.get_model(args.arch)
    pool_layer = <a id="change">nn.Identity() if args.no_pool</a><a id="change"> else </a>None
    classifier = ImageClassifier(backbone, num_classes, bottleneck_dim=args.bottleneck_dim, pool_layer=pool_layer).to(device)

    &#47&#47 define optimizer and lr scheduler</code></pre>