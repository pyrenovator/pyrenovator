<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                                           )
            input_ids.append(encoding_dict.input_ids)
        input_ids = torch.cat(input_ids, dim=0).to(self.device)
        decoder_input_ids = <a id="change">input_ids[:, :-1].contiguous()</a>
        labels = input_ids[:, 1:].contiguous()
        perm_mask = torch.ones((input_ids.shape[0],
                                decoder_input_ids.shape[1],
                                decoder_input_ids.shape[1]),
                               dtype=torch.float).to(self.device)
        for t_index in range(self.max_seq_length-1):
            perm_mask[:, -t_index, -(self.max_seq_length-1):-t_index] = 0.0
        perm_mask = perm_mask.contiguous()
        target_mapping = torch.zeros((input_ids.shape[0], decoder_input_ids.shape[1], decoder_input_ids.shape[1]),
                                     dtype=torch.float).to(self.device)
        for index in range(self.max_seq_length-1):
            target_mapping[:, index, index] = 1.0
        target_mapping<a id="change"> = </a>target_mapping.contiguous()
        outputs = self.decoder(decoder_input_ids,
                               labels=labels, perm_mask=perm_mask, target_mapping=target_mapping)
</code></pre><h3>After Change</h3><pre><code class='java'>
    def calculate_loss(self, corpus, epoch_idx=-1, nll_test=False):
        text_sequence = corpus[&quottarget_text&quot]
        input_ids = []
        <a id="change">attn_masks</a> = <a id="change">[]</a>
        for text in text_sequence:
            sentence = &quot &quot.join([self.sos_token] + text + [self.eos_token])
            encoding_dict = self.tokenizer(
                sentence,
                max_length=self.max_seq_length,
                padding="max_length",
                truncation=True,
                return_tensors="pt",
                add_special_tokens=False
            )
            input_ids.append(encoding_dict.input_ids)
            <a id="change">attn_masks.append(</a>encoding_dict[&quotattention_mask&quot]<a id="change">)</a>
        input_ids = torch.cat(input_ids, dim=0).to(self.device)
        attn_masks = <a id="change">torch.cat(attn_masks</a><a id="change">, dim=0)</a>.to(self.device)

        decoder_target_ids = input_ids[:, 1:].contiguous()
</code></pre>