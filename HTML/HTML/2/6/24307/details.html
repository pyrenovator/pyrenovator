<html><h3>Pattern ID :24307
</h3><img src='75521395.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        elif isinstance(optimizer, torch.optim.Optimizer):
            self.optimizer = optimizer
        else:
            <a id="change">raise </a><a id="change">TypeError(</a><a id="change">"invalid optimizer: &quot{}&quot".format(</a>optimizer<a id="change">))</a>

    def load_checkpoint(self, checkpoint, strict=False):
        
        Load checkpoint from a file or an URL.</code></pre><h3>After Change</h3><pre><code class='java'>
            self.optimizer = nncore.build_object(
                optimizer, torch.optim, params=self.model.parameters())
        else:
            <a id="change">raise </a><a id="change">TypeError(
                </a>"optimizer should be an optim.Optimizer object or a dict, but "
                "got: &quot{}&quot".format(optimizer)<a id="change">)</a>

    def load_checkpoint(self, checkpoint, strict=False):
        
        Load checkpoint from a file or an URL.</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/yeliudev/nncore/commit/731c9b0185f4da75d92df5dd7e283ec794163a60#diff-f301e71f1b206fb014cc7b05cf40d35bbf2feac02ca56ff77c6fbc81583978acL262' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 75521395</div><div id='project'> Project Name: yeliudev/nncore</div><div id='commit'> Commit Name: 731c9b0185f4da75d92df5dd7e283ec794163a60</div><div id='time'> Time: 2021-04-03</div><div id='author'> Author: yeliudev@outlook.com</div><div id='file'> File Name: nncore/engine/engine.py</div><div id='m_class'> M Class Name: Engine</div><div id='n_method'> N Class Name: Engine</div><div id='m_method'> M Method Name: build_optimizer(2)</div><div id='n_method'> N Method Name: build_optimizer(2)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: nncore/engine/engine.py</div><div id='n_file'> N File Name: nncore/engine/engine.py</div><div id='m_start'> M Start Line: 269</div><div id='m_end'> M End Line: 277</div><div id='n_start'> N Start Line: 262</div><div id='n_end'> N End Line: 272</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            &#47&#47 no return value; assumes in-place transform of the graph object
            return_graphs = (graph_obj,)
        else:
            <a id="change">raise </a><a id="change">TypeError(</a><a id="change">&quotTransform function returns a value of unknown type ({})&quot.format(
                </a>return_graphs[0].__class__<a id="change">))</a>
        if update_graphs:
            for return_graph in return_graphs:
                return_graph._update_graphs()
        if update_tensors:</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 no return value; assumes in-place transform of the graph object
            return_graphs = (graph_obj,)
        else:
            <a id="change">raise </a><a id="change">TypeError(</a>&quotTransform function returns a value of unknown type ({return_graphs[0].__class__)})&quot<a id="change">)</a>
        if update_graphs:
            for return_graph in return_graphs:
                return_graph._update_graphs()
        if update_tensors:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/snap-stanford/deepsnap/commit/1c4209c866b1963581884a1716bf4e3b76669fcf#diff-6e00332a84b17561d6f88c10cc82271c84ecfd1b67966a9866553cf6ae1d24aaL636' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 75521396</div><div id='project'> Project Name: snap-stanford/deepsnap</div><div id='commit'> Commit Name: 1c4209c866b1963581884a1716bf4e3b76669fcf</div><div id='time'> Time: 2020-06-23</div><div id='author'> Author: xhe17@stanford.edu</div><div id='file'> File Name: deepsnap/graph.py</div><div id='m_class'> M Class Name: Graph</div><div id='n_method'> N Class Name: Graph</div><div id='m_method'> M Method Name: apply_transform_multi(5)</div><div id='n_method'> N Method Name: apply_transform_multi(5)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: deepsnap/graph.py</div><div id='n_file'> N File Name: deepsnap/graph.py</div><div id='m_start'> M Start Line: 656</div><div id='m_end'> M End Line: 667</div><div id='n_start'> N Start Line: 665</div><div id='n_end'> N End Line: 665</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                            print(&quotdevice is &quot, device)
                            exit()
            else:
                <a id="change">raise </a><a id="change">TypeError(
                    </a><a id="change">&quotBatch to_tensor, only support int or float, and you give {}&quot.format(</a>self.feature_name[key]<a id="change">))</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
                for i in range(len(self.data[key])):
                    self.data[key][i] = torch.FloatTensor(np.array(self.data[key][i])).to(device)
            else:
                <a id="change">raise </a><a id="change">TypeError(
                    </a>&quotBatch to_tensor, only support int, float, array of int, no_pad_float.\
                    and you give {}&quot.format(self.feature_name[key])<a id="change">)</a>
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/libcity/bigscity-libcity/commit/228d630c9f19e8fe277e06f6c912ef5800410ce6#diff-15681246b654a21e3502dc009660602280ec4f11bf6415fd0cdfa93e96918a55L89' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 75521397</div><div id='project'> Project Name: libcity/bigscity-libcity</div><div id='commit'> Commit Name: 228d630c9f19e8fe277e06f6c912ef5800410ce6</div><div id='time'> Time: 2021-04-22</div><div id='author'> Author: 33283819+WenMellors@users.noreply.github.com</div><div id='file'> File Name: trafficdl/data/batch.py</div><div id='m_class'> M Class Name: Batch</div><div id='n_method'> N Class Name: Batch</div><div id='m_method'> M Method Name: to_tensor(2)</div><div id='n_method'> N Method Name: to_tensor(2)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: trafficdl/data/batch.py</div><div id='n_file'> N File Name: trafficdl/data/batch.py</div><div id='m_start'> M Start Line: 97</div><div id='m_end'> M End Line: 111</div><div id='n_start'> N Start Line: 96</div><div id='n_end'> N End Line: 118</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                        param_group["params"][j] = param

                    else:
                        <a id="change">raise </a><a id="change">TypeError(
                            </a><a id="change">"Wrapped parameters must be one of "
                            "torch.cuda.FloatTensor,  "
                            "torch.cuda.HalfTensor, or "
                            "torch.cuda.BFloat16Tensor. "
                            "Received {}".format(</a>param.type()<a id="change">)
                        )</a>

                &#47&#47 Add gradient accumulation hook for fp32 grad accumulation
                if self._fp32_grad_accum:
                    &#47&#47 Expand so we get access to grad_fn.</code></pre><h3>After Change</h3><pre><code class='java'>
                        param_group["params"][j] = param

                    else:
                        <a id="change">raise </a><a id="change">TypeError(
                            </a>f"Wrapped parameters must be one of torch.cuda.FloatTensor,  torch.cuda.HalfTensor, or torch.cuda.BFloat16Tensor. Received {param.type()}"<a id="change">
                        )</a>

                &#47&#47 Add gradient accumulation hook for fp32 grad accumulation
                if self._fp32_grad_accum:
                    &#47&#47 Expand so we get access to grad_fn.</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/wdika/mridc/commit/70158a80210f0729fbd07a1f56670be2a481f70c#diff-cda7de318bdb22b08e6977c50725b8a9b6b0a7c3a6d52bbdc0c29b340fa5703fL97' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 75521398</div><div id='project'> Project Name: wdika/mridc</div><div id='commit'> Commit Name: 70158a80210f0729fbd07a1f56670be2a481f70c</div><div id='time'> Time: 2022-03-14</div><div id='author'> Author: 58596630+sourcery-ai[bot]@users.noreply.github.com</div><div id='file'> File Name: mridc/core/optim/optimizer_with_master_params.py</div><div id='m_class'> M Class Name: MasterOptimizerWrapper</div><div id='n_method'> N Class Name: MasterOptimizerWrapper</div><div id='m_method'> M Method Name: __init__(5)</div><div id='n_method'> N Method Name: __init__(5)</div><div id='m_parent_class'> M Parent Class: torch.optim.Optimizer</div><div id='n_parent_class'> N Parent Class: torch.optim.Optimizer</div><div id='m_file'> M File Name: mridc/core/optim/optimizer_with_master_params.py</div><div id='n_file'> N File Name: mridc/core/optim/optimizer_with_master_params.py</div><div id='m_start'> M Start Line: 161</div><div id='m_end'> M End Line: 198</div><div id='n_start'> N Start Line: 191</div><div id='n_end'> N End Line: 193</div><BR>