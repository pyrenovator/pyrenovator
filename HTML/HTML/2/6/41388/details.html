<html><h3>Pattern ID :41388
</h3><img src='116451089.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    if output_device is None:
        output_device = device_ids[0]

    device_ids = <a id="change">list(</a><a id="change">map(</a>lambda x: _get_device_index(x, True), device_ids<a id="change">))</a>
    output_device = _get_device_index(output_device, True)
    src_device_obj = torch.device(device_type, device_ids[0])

    for t in chain(module.parameters(), module.buffers()):</code></pre><h3>After Change</h3><pre><code class='java'>
    if output_device is None:
        output_device = device_ids[0]

    device_ids = <a id="change">[_get_device_index(x, True) for x in device_ids]</a>
    output_device = _get_device_index(output_device, True)
    src_device_obj = torch.device(device_type, device_ids[0])

    for t in chain(module.parameters(), module.buffers()):</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 7</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/5b0f40048899e398d286fe7b55f297991f93ba2c#diff-51f2a6c1f7a1ab8141442f1ae24b1d6e5b190b92a79ab24edb2a9692575cf9f1L198' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116451089</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 5b0f40048899e398d286fe7b55f297991f93ba2c</div><div id='time'> Time: 2020-10-19</div><div id='author'> Author: alexander.grund@tu-dresden.de</div><div id='file'> File Name: torch/nn/parallel/data_parallel.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: data_parallel(6)</div><div id='n_method'> N Method Name: data_parallel(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: torch/nn/parallel/data_parallel.py</div><div id='n_file'> N File Name: torch/nn/parallel/data_parallel.py</div><div id='m_start'> M Start Line: 198</div><div id='m_end'> M End Line: 203</div><div id='n_start'> N Start Line: 198</div><div id='n_end'> N End Line: 203</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    has less than 75% of the memory or cores of GPU {}. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
    device_ids = <a id="change">list(</a><a id="change">map(</a>lambda x: _get_device_index(x, True), device_ids<a id="change">))</a>
    dev_props = _get_devices_properties(device_ids)

    def warn_imbalance(get_prop):
        values = [get_prop(props) for props in dev_props]</code></pre><h3>After Change</h3><pre><code class='java'>
    has less than 75% of the memory or cores of GPU {}. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
    device_ids = <a id="change">[_get_device_index(x, True) for x in device_ids]</a>
    dev_props = _get_devices_properties(device_ids)

    def warn_imbalance(get_prop):
        values = [get_prop(props) for props in dev_props]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/5b0f40048899e398d286fe7b55f297991f93ba2c#diff-51f2a6c1f7a1ab8141442f1ae24b1d6e5b190b92a79ab24edb2a9692575cf9f1L16' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116451090</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 5b0f40048899e398d286fe7b55f297991f93ba2c</div><div id='time'> Time: 2020-10-19</div><div id='author'> Author: alexander.grund@tu-dresden.de</div><div id='file'> File Name: torch/nn/parallel/data_parallel.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _check_balance(1)</div><div id='n_method'> N Method Name: _check_balance(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: torch/nn/parallel/data_parallel.py</div><div id='n_file'> N File Name: torch/nn/parallel/data_parallel.py</div><div id='m_start'> M Start Line: 22</div><div id='m_end'> M End Line: 22</div><div id='n_start'> N Start Line: 22</div><div id='n_end'> N End Line: 22</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    @staticmethod
    def forward(ctx, target_gpus, chunk_sizes, dim, input):
        target_gpus = <a id="change">list(</a><a id="change">map(</a>lambda x: _get_device_index(x, True), target_gpus<a id="change">))</a>
        ctx.dim = dim
        ctx.input_device = input.get_device() if input.device.type != "cpu" else -1
        streams = None
        if torch.cuda.is_available() and ctx.input_device == -1:</code></pre><h3>After Change</h3><pre><code class='java'>

    @staticmethod
    def forward(ctx, target_gpus, chunk_sizes, dim, input):
        target_gpus = <a id="change">[_get_device_index(x, True) for x in target_gpus]</a>
        ctx.dim = dim
        ctx.input_device = input.get_device() if input.device.type != "cpu" else -1
        streams = None
        if torch.cuda.is_available() and ctx.input_device == -1:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/5b0f40048899e398d286fe7b55f297991f93ba2c#diff-869639a3228aac812ac636b669fb78d14088862ac4605e95d8cc003e826d708eL84' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116451091</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 5b0f40048899e398d286fe7b55f297991f93ba2c</div><div id='time'> Time: 2020-10-19</div><div id='author'> Author: alexander.grund@tu-dresden.de</div><div id='file'> File Name: torch/nn/parallel/_functions.py</div><div id='m_class'> M Class Name: Scatter</div><div id='n_method'> N Class Name: Scatter</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: torch/nn/parallel/_functions.py</div><div id='n_file'> N File Name: torch/nn/parallel/_functions.py</div><div id='m_start'> M Start Line: 85</div><div id='m_end'> M End Line: 85</div><div id='n_start'> N Start Line: 85</div><div id='n_end'> N End Line: 85</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        assert len(modules) == len(devices)
    else:
        devices = [None] * len(modules)
    devices = <a id="change">list(</a><a id="change">map(</a>lambda x: _get_device_index(x, True), devices<a id="change">))</a>
    lock = threading.Lock()
    results = {}
    grad_enabled, autocast_enabled = torch.is_grad_enabled(), torch.is_autocast_enabled()
</code></pre><h3>After Change</h3><pre><code class='java'>
        assert len(modules) == len(devices)
    else:
        devices = [None] * len(modules)
    devices = <a id="change">[_get_device_index(x, True) for x in devices]</a>
    lock = threading.Lock()
    results = {}
    grad_enabled, autocast_enabled = torch.is_grad_enabled(), torch.is_autocast_enabled()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/5b0f40048899e398d286fe7b55f297991f93ba2c#diff-a4bc0ed12a10f5341ec6fef794685f765aa545c127c14358a4f001e092073d50L23' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116451092</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 5b0f40048899e398d286fe7b55f297991f93ba2c</div><div id='time'> Time: 2020-10-19</div><div id='author'> Author: alexander.grund@tu-dresden.de</div><div id='file'> File Name: torch/nn/parallel/parallel_apply.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: parallel_apply(4)</div><div id='n_method'> N Method Name: parallel_apply(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: torch/nn/parallel/parallel_apply.py</div><div id='n_file'> N File Name: torch/nn/parallel/parallel_apply.py</div><div id='m_start'> M Start Line: 46</div><div id='m_end'> M End Line: 47</div><div id='n_start'> N Start Line: 46</div><div id='n_end'> N End Line: 47</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        assert all(map(lambda i: i.device.type != &quotcpu&quot, inputs)), (
            &quotBroadcast function not implemented for CPU tensors&quot
        )
        target_gpus = <a id="change">list(</a><a id="change">map(</a>lambda x: _get_device_index(x, True), target_gpus<a id="change">))</a>
        ctx.target_gpus = target_gpus
        if len(inputs) == 0:
            return tuple()
        ctx.num_inputs = len(inputs)</code></pre><h3>After Change</h3><pre><code class='java'>
        assert all(map(lambda i: i.device.type != &quotcpu&quot, inputs)), (
            &quotBroadcast function not implemented for CPU tensors&quot
        )
        target_gpus = <a id="change">[_get_device_index(x, True) for x in target_gpus]</a>
        ctx.target_gpus = target_gpus
        if len(inputs) == 0:
            return tuple()
        ctx.num_inputs = len(inputs)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/5b0f40048899e398d286fe7b55f297991f93ba2c#diff-869639a3228aac812ac636b669fb78d14088862ac4605e95d8cc003e826d708eL12' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116451093</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 5b0f40048899e398d286fe7b55f297991f93ba2c</div><div id='time'> Time: 2020-10-19</div><div id='author'> Author: alexander.grund@tu-dresden.de</div><div id='file'> File Name: torch/nn/parallel/_functions.py</div><div id='m_class'> M Class Name: Broadcast</div><div id='n_method'> N Class Name: Broadcast</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: torch/nn/parallel/_functions.py</div><div id='n_file'> N File Name: torch/nn/parallel/_functions.py</div><div id='m_start'> M Start Line: 16</div><div id='m_end'> M End Line: 16</div><div id='n_start'> N Start Line: 16</div><div id='n_end'> N End Line: 16</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        raise RuntimeError("Cannot replicate network where python modules are "
                           "childrens of ScriptModule")

    devices = <a id="change">list(</a><a id="change">map(</a>lambda x: _get_device_index(x, True), devices<a id="change">))</a>
    num_replicas = len(devices)

    params = list(network.parameters())
    param_indices = {param: idx for idx, param in enumerate(params)}</code></pre><h3>After Change</h3><pre><code class='java'>
        raise RuntimeError("Cannot replicate network where python modules are "
                           "childrens of ScriptModule")

    devices = <a id="change">[_get_device_index(x, True) for x in devices]</a>
    num_replicas = len(devices)

    params = list(network.parameters())
    param_indices = {param: idx for idx, param in enumerate(params)}</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/5b0f40048899e398d286fe7b55f297991f93ba2c#diff-ca8403b96ba19acf4bc5c11e3255c519b56334cf94891bcb115a3fc43b146c6cL78' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116451094</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 5b0f40048899e398d286fe7b55f297991f93ba2c</div><div id='time'> Time: 2020-10-19</div><div id='author'> Author: alexander.grund@tu-dresden.de</div><div id='file'> File Name: torch/nn/parallel/replicate.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: replicate(3)</div><div id='n_method'> N Method Name: replicate(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: torch/nn/parallel/replicate.py</div><div id='n_file'> N File Name: torch/nn/parallel/replicate.py</div><div id='m_start'> M Start Line: 83</div><div id='m_end'> M End Line: 83</div><div id='n_start'> N Start Line: 83</div><div id='n_end'> N End Line: 83</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        self.dim = dim
        self.module = module
        self.device_ids = <a id="change">list(</a><a id="change">map(</a>lambda x: _get_device_index(x, True), device_ids<a id="change">))</a>
        self.output_device = _get_device_index(output_device, True)
        self.src_device_obj = torch.device(device_type, self.device_ids[0])

        _check_balance(self.device_ids)</code></pre><h3>After Change</h3><pre><code class='java'>

        self.dim = dim
        self.module = module
        self.device_ids = <a id="change">[_get_device_index(x, True) for x in device_ids]</a>
        self.output_device = _get_device_index(output_device, True)
        self.src_device_obj = torch.device(device_type, self.device_ids[0])

        _check_balance(self.device_ids)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/5b0f40048899e398d286fe7b55f297991f93ba2c#diff-51f2a6c1f7a1ab8141442f1ae24b1d6e5b190b92a79ab24edb2a9692575cf9f1L121' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116451095</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 5b0f40048899e398d286fe7b55f297991f93ba2c</div><div id='time'> Time: 2020-10-19</div><div id='author'> Author: alexander.grund@tu-dresden.de</div><div id='file'> File Name: torch/nn/parallel/data_parallel.py</div><div id='m_class'> M Class Name: DataParallel</div><div id='n_method'> N Class Name: DataParallel</div><div id='m_method'> M Method Name: __init__(5)</div><div id='n_method'> N Method Name: __init__(5)</div><div id='m_parent_class'> M Parent Class: Module</div><div id='n_parent_class'> N Parent Class: Module</div><div id='m_file'> M File Name: torch/nn/parallel/data_parallel.py</div><div id='n_file'> N File Name: torch/nn/parallel/data_parallel.py</div><div id='m_start'> M Start Line: 131</div><div id='m_end'> M End Line: 138</div><div id='n_start'> N Start Line: 131</div><div id='n_end'> N End Line: 138</div><BR>