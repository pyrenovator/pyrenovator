<html><h3>Pattern ID :1057
</h3><img src='5270480.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        

        lambda_ = self.lambda_ if lambda_ is None else lambda_
        neural_model_prob<a id="change"> = </a><a id="change">F.softmax(</a>neural_model_logit<a id="change">, dim=-1)</a>
        combined_probs = knn_prob<a id="change"> * lambda_ + </a>neural_model_prob * (1 - lambda_)

        &#47&#47 some extra infomation
        extra = {}
        extra["neural_probs"] = neural_model_prob
        extra["unlog_combined_probs"]<a id="change"> = </a>combined_probs

        if log_probs:
            combined_probs =  torch.log(combined_probs)
        <a id="change">return </a>combined_probs, extra</code></pre><h3>After Change</h3><pre><code class='java'>
        If parameter `lambda_` is given, it will suppress the self.lambda_ 
        
        lambda_ = lambda_ if lambda_ is not None else self.lambda_
        <a id="change">return </a>calculate_combined_prob(knn_prob, neural_model_logit, lambda_, log_probs)
        

        </code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/njunlp/knn-box/commit/edfb991d35e4428b1ab1ef2a4dd189f805efcad4#diff-328da295aec9c52fedfd89c515776c7c4673395104ee9c5c71439cbdfaf2a002L26' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5270480</div><div id='project'> Project Name: njunlp/knn-box</div><div id='commit'> Commit Name: edfb991d35e4428b1ab1ef2a4dd189f805efcad4</div><div id='time'> Time: 2022-11-16</div><div id='author'> Author: qianfeng1024@gmail.com</div><div id='file'> File Name: knnbox/combiner/combiner.py</div><div id='m_class'> M Class Name: Combiner</div><div id='n_method'> N Class Name: Combiner</div><div id='m_method'> M Method Name: get_combined_prob(5)</div><div id='n_method'> N Method Name: get_combined_prob(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: knnbox/combiner/combiner.py</div><div id='n_file'> N File Name: knnbox/combiner/combiner.py</div><div id='m_start'> M Start Line: 32</div><div id='m_end'> M End Line: 50</div><div id='n_start'> N Start Line: 26</div><div id='n_end'> N End Line: 32</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            mask = rearrange(mask, &quotb n -&gt; b () n ()&quot)
            k.masked_fill_(~mask, -torch.finfo(k.dtype).max)

        q<a id="change"> = </a><a id="change">q.softmax(dim = -1)</a>
        k = k.softmax(dim = -2)

        q<a id="change"> = </a>q<a id="change"> * </a>self.scale

        if exists(mask):
            v.masked_fill_(~mask, 0.)

        context = einsum(&quotb h n d, b h n e -&gt; b h d e&quot, k, v)
        out = einsum(&quotb h d e, b h n d -&gt; b h n e&quot, context, q)
        out = rearrange(out, &quotb h n d -&gt; b n (h d)&quot)
        <a id="change">return </a>self.to_out(out), 0

class EquivariantAttention(nn.Module):
    def __init__(</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward(self, x, queries, mask = None):
        induced = self.attn1(queries, x, mask = mask)
        out     = self.attn2(x, induced)
        <a id="change">return </a>out, 0

class EquivariantAttention(nn.Module):
    def __init__(</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/en-transformer/commit/6bd1817d780502d24a2515e850c9cd1600f24642#diff-f288a1692c1c58a186cb9ac763046f632757db1647271b97852e59e3fc5696b9L131' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5270418</div><div id='project'> Project Name: lucidrains/en-transformer</div><div id='commit'> Commit Name: 6bd1817d780502d24a2515e850c9cd1600f24642</div><div id='time'> Time: 2021-06-04</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: en_transformer/en_transformer.py</div><div id='m_class'> M Class Name: GlobalLinearAttention</div><div id='n_method'> N Class Name: GlobalLinearAttention</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: en_transformer/en_transformer.py</div><div id='n_file'> N File Name: en_transformer/en_transformer.py</div><div id='m_start'> M Start Line: 132</div><div id='m_end'> M End Line: 151</div><div id='n_start'> N Start Line: 160</div><div id='n_end'> N End Line: 162</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 attendee: (batch, seq_len, hid_dim)
        attender = attender.unsqueeze(-1)
        attn_weights = torch.bmm(attendee, attender) &#47&#47 (batch, seq_len, 1)
        attn_weights<a id="change"> = </a><a id="change">F.softmax(</a>attn_weights<a id="change">, dim=1)</a>
        res<a id="change"> = </a>torch.sum(attendee<a id="change"> * </a>attn_weights, dim=1) &#47&#47 (batch, hid_dim)
        <a id="change">return </a>res

    def forward(self, X_text, X_audio, X_visual):
        &#47&#47 (batch, seq_len, num_directions * hidden_size)</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 attn_weights = F.softmax(attn_weights, dim=1)
        &#47&#47 res = torch.sum(attendee * attn_weights, dim=1) &#47&#47 (batch, hid_dim)
        &#47&#47 return res
        <a id="change">return </a>attn_weights.squeeze(-1)

    def forward(self, X_text, X_audio, X_visual):
        &#47&#47 TODO: try residual connection</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/wenliangdai/modality-transferable-mer/commit/b0e565d11d6b3bf9f65fb1dcbdc8c641a2bc8054#diff-9dca09e4df72ee7b751c74b7ea8c66a06c2d78f76fef6a3bdb604dc6680fd2efL42' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 5270451</div><div id='project'> Project Name: wenliangdai/modality-transferable-mer</div><div id='commit'> Commit Name: b0e565d11d6b3bf9f65fb1dcbdc8c641a2bc8054</div><div id='time'> Time: 2020-06-10</div><div id='author'> Author: wenliang.dai.1995@gmail.com</div><div id='file'> File Name: src/models/temp.py</div><div id='m_class'> M Class Name: EmotionEmbAttnModel</div><div id='n_method'> N Class Name: EmotionEmbAttnModel</div><div id='m_method'> M Method Name: attention(3)</div><div id='n_method'> N Method Name: attention(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/models/temp.py</div><div id='n_file'> N File Name: src/models/temp.py</div><div id='m_start'> M Start Line: 46</div><div id='m_end'> M End Line: 49</div><div id='n_start'> N Start Line: 61</div><div id='n_end'> N End Line: 65</div><BR>