<html><h3>Pattern ID :29367
</h3><img src='86247332.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    output_folder = os.path.join(dataset_cfg[&quotOUT_PATH&quot], &quotmodels&quot)
    os.makedirs(output_folder, exist_ok=True)

    train_dataloader, val_dataloader, node_feat, labels = <a id="change">get_dataloader(</a>dataset_cfg, graph_data<a id="change">)</a>
    num_train_optimization_steps = (int(len(train_dataloader) + dataset_cfg[&quotGRADIENT_ACCUMULATION_STEPS&quot] - 1)
                                    / dataset_cfg[&quotGRADIENT_ACCUMULATION_STEPS&quot]) * dataset_cfg[&quotEPOCHS&quot]

    model = init_model(model_cfg, device)</code></pre><h3>After Change</h3><pre><code class='java'>
    graph, labels, train_nid, val_nid, test_nid, node_feats = graph_data
    graph = dgl.remove_self_loop(graph)

    val_dataloader<a id="change"> = </a><a id="change">get_dataloader(dataset_cfg</a>, graph, val_nid<a id="change">, drop=False)</a>

    train_num = math.ceil(len(train_nid) / dataset_cfg[&quotBATCH_SIZE&quot])
    num_train_optimization_steps = (int(train_num + dataset_cfg[&quotGRADIENT_ACCUMULATION_STEPS&quot] - 1)
                                    / dataset_cfg[&quotGRADIENT_ACCUMULATION_STEPS&quot]) * dataset_cfg[&quotEPOCHS&quot]

    model = init_model(model_cfg, device)
    logging.info(&quotModel = %s&quot, str(model))
    logging.info(&quotModel config = %s&quot, str(dict(model_cfg)))
    logging.info(&quotDataset config = %s&quot, str(dict(dataset_cfg)))
    logging.info(&quotOptimizer config = %s&quot, str(dict(optimizer_cfg)))
    logging.info(&quotCriterion config = %s&quot, str(dict(criterion_cfg)))
    logging.info(&quotTraining parameters = %d&quot, sum(p.numel() for p in model.parameters() if p.requires_grad))
    logging.info(&quotModel parameters = %d&quot, sum(p.numel() for p in model.parameters()))
    logging.info(&quotNum steps = %d&quot, num_train_optimization_steps * dataset_cfg[&quotGRADIENT_ACCUMULATION_STEPS&quot])

    optimizer = prep_optimizer(optimizer_cfg, model, num_train_optimization_steps)
    criterion = prep_criterion(criterion_cfg, device)

    global_step = 0
    best_record = {&quotepoch&quot: -1, &quottrain loss&quot: -1, &quottrain acc&quot: 0.0, &quotval loss&quot: -1, &quotval acc&quot: 0.0}
    
    for epoch in range(dataset_cfg[&quotEPOCHS&quot]):
        train_dataloader<a id="change"> = </a><a id="change">get_dataloader(dataset_cfg</a>, graph, train_nid<a id="change">, drop=True)</a>

        tr_loss, tr_acc, global_step = train_epoch(epoch, model, train_dataloader, dataset_cfg, node_feats, labels,
                                                   optimizer, criterion, device, global_step)
        logging.info(&quotTrain Epoch %d/%s Finished | Train Loss: %f | Train Acc: %f&quot, epoch + 1,</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/langgege-cqu/maxp_dgl/commit/f1f10fccf1844722311242e00bfefcc9f4d1caf6#diff-20e2889ed6a8069c6a5c3713e45974c8bfa5b3aad38b424deb261b5a87f395eaL212' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 86247332</div><div id='project'> Project Name: langgege-cqu/maxp_dgl</div><div id='commit'> Commit Name: f1f10fccf1844722311242e00bfefcc9f4d1caf6</div><div id='time'> Time: 2021-11-21</div><div id='author'> Author: 8747734+bugczw@user.noreply.gitee.com</div><div id='file'> File Name: maxp_model_czw/train_yaml.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train(6)</div><div id='n_method'> N Method Name: train(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: maxp_model_czw/train_yaml.py</div><div id='n_file'> N File Name: maxp_model_czw/train_yaml.py</div><div id='m_start'> M Start Line: 212</div><div id='m_end'> M End Line: 256</div><div id='n_start'> N Start Line: 216</div><div id='n_end'> N End Line: 265</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        num_embeddings = args.num_embeddings

    &#47&#47 TODO add CriteoIterDataPipe support and add random_dataloader arg
    iterator = iter(<a id="change">get_dataloader(</a>args, backend<a id="change">)</a>)

    eb_configs = [
        EmbeddingBagConfig(</code></pre><h3>After Change</h3><pre><code class='java'>
        args.num_embeddings = None

    &#47&#47 TODO add CriteoIterDataPipe support and add random_dataloader arg
    train_dataloader<a id="change"> = </a><a id="change">get_dataloader(</a>args, backend, "train"<a id="change">)</a>
    val_dataloader<a id="change"> = </a><a id="change">get_dataloader(</a>args, backend, "val"<a id="change">)</a>
    test_dataloader = get_dataloader(args, backend, "test")

    &#47&#47 Sets default limits for random dataloader iterations when left unspecified.
    if args.in_memory_binary_criteo_path is None:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/46f6aa6a827299037c1f1827f54e929344022d87#diff-a48799eee3d167033c2a435d6559e00b0511896eaf454ab7f78cc261acfe9acfL116' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 86247396</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: 46f6aa6a827299037c1f1827f54e929344022d87</div><div id='time'> Time: 2021-11-30</div><div id='author'> Author: rahulkindi@fb.com</div><div id='file'> File Name: torchrec/examples/dlrm/dlrm_main.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: torchrec/examples/dlrm/dlrm_main.py</div><div id='n_file'> N File Name: torchrec/examples/dlrm/dlrm_main.py</div><div id='m_start'> M Start Line: 117</div><div id='m_end'> M End Line: 189</div><div id='n_start'> N Start Line: 298</div><div id='n_end'> N End Line: 374</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    output_folder = os.path.join(dataset_cfg[&quotOUT_PATH&quot], &quotmodels&quot)
    os.makedirs(output_folder, exist_ok=True)

    train_dataloader, val_dataloader, node_feat, labels = <a id="change">get_dataloader(</a>dataset_cfg, graph_data<a id="change">)</a>
    num_train_optimization_steps = (int(len(train_dataloader) + dataset_cfg[&quotGRADIENT_ACCUMULATION_STEPS&quot] - 1)
                                    / dataset_cfg[&quotGRADIENT_ACCUMULATION_STEPS&quot]) * dataset_cfg[&quotEPOCHS&quot]

    model = init_model(model_cfg, device)</code></pre><h3>After Change</h3><pre><code class='java'>
    os.makedirs(output_folder, exist_ok=True)

    graph, labels, train_nid, val_nid, test_nid, node_feats = graph_data
    <a id="change">graph</a> = dgl.remove_self_loop(graph)

    val_dataloader<a id="change"> = </a><a id="change">get_dataloader(</a>dataset_cfg, graph, val_nid<a id="change">, drop=False)</a>

    train_num = math.ceil(len(train_nid) / dataset_cfg[&quotBATCH_SIZE&quot])
    num_train_optimization_steps = (int(train_num + dataset_cfg[&quotGRADIENT_ACCUMULATION_STEPS&quot] - 1)
                                    / dataset_cfg[&quotGRADIENT_ACCUMULATION_STEPS&quot]) * dataset_cfg[&quotEPOCHS&quot]

    model = init_model(model_cfg, device)
    logging.info(&quotModel = %s&quot, str(model))
    logging.info(&quotModel config = %s&quot, str(dict(model_cfg)))
    logging.info(&quotDataset config = %s&quot, str(dict(dataset_cfg)))
    logging.info(&quotOptimizer config = %s&quot, str(dict(optimizer_cfg)))
    logging.info(&quotCriterion config = %s&quot, str(dict(criterion_cfg)))
    logging.info(&quotTraining parameters = %d&quot, sum(p.numel() for p in model.parameters() if p.requires_grad))
    logging.info(&quotModel parameters = %d&quot, sum(p.numel() for p in model.parameters()))
    logging.info(&quotNum steps = %d&quot, num_train_optimization_steps * dataset_cfg[&quotGRADIENT_ACCUMULATION_STEPS&quot])

    optimizer = prep_optimizer(optimizer_cfg, model, num_train_optimization_steps)
    criterion = prep_criterion(criterion_cfg, device)

    global_step = 0
    best_record = {&quotepoch&quot: -1, &quottrain loss&quot: -1, &quottrain acc&quot: 0.0, &quotval loss&quot: -1, &quotval acc&quot: 0.0}
    
    for epoch in range(dataset_cfg[&quotEPOCHS&quot]):
        train_dataloader<a id="change"> = </a><a id="change">get_dataloader(</a>dataset_cfg, graph, train_nid<a id="change">, drop=True)</a>

        tr_loss, tr_acc, global_step = train_epoch(epoch, model, train_dataloader, dataset_cfg, node_feats, labels,
                                                   optimizer, criterion, device, global_step)
        logging.info(&quotTrain Epoch %d/%s Finished | Train Loss: %f | Train Acc: %f&quot, epoch + 1,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/langgege-cqu/maxp_dgl/commit/f1f10fccf1844722311242e00bfefcc9f4d1caf6#diff-20e2889ed6a8069c6a5c3713e45974c8bfa5b3aad38b424deb261b5a87f395eaL207' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 86247333</div><div id='project'> Project Name: langgege-cqu/maxp_dgl</div><div id='commit'> Commit Name: f1f10fccf1844722311242e00bfefcc9f4d1caf6</div><div id='time'> Time: 2021-11-21</div><div id='author'> Author: 8747734+bugczw@user.noreply.gitee.com</div><div id='file'> File Name: maxp_model_czw/train_yaml.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train(6)</div><div id='n_method'> N Method Name: train(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: maxp_model_czw/train_yaml.py</div><div id='n_file'> N File Name: maxp_model_czw/train_yaml.py</div><div id='m_start'> M Start Line: 212</div><div id='m_end'> M End Line: 256</div><div id='n_start'> N Start Line: 216</div><div id='n_end'> N End Line: 265</div><BR>