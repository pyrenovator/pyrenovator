<html><h3>Pattern ID :12896
</h3><img src='43575128.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    model = VisionTransformer(
        patch_size=16, num_classes=num_classes, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,
        representation_size=1024, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    model.default_cfg<a id="change"> = </a><a id="change">default_cfgs[&quotvit_large_patch16_224_in21k&quot]</a>
    if pretrained:
        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get(&quotin_chans&quot, 3))
    return model
</code></pre><h3>After Change</h3><pre><code class='java'>
def vit_large_patch16_224_in21k(pretrained=False, **kwargs):
    model_kwargs = dict(
        patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, representation_size=1024, **kwargs)
    model<a id="change"> = </a><a id="change">_create_vision_transformer(&quotvit_large_patch16_224_in21k&quot</a><a id="change">, pretrained=pretrained, **model_kwargs)</a>
    return model


&#47&#47 @register_model</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 6</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/feng-lab/pytorch-image-models/commit/55f7dfa9ea8bab0296c774dd7234d602b8396ce5#diff-a457ded0c066f18843cce88854130edb587dffef98fef0189c9442093ac110abL469' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43575128</div><div id='project'> Project Name: feng-lab/pytorch-image-models</div><div id='commit'> Commit Name: 55f7dfa9ea8bab0296c774dd7234d602b8396ce5</div><div id='time'> Time: 2021-01-18</div><div id='author'> Author: rwightman@gmail.com</div><div id='file'> File Name: timm/models/vision_transformer.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: vit_large_patch16_224_in21k(1)</div><div id='n_method'> N Method Name: vit_large_patch16_224_in21k(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: timm/models/vision_transformer.py</div><div id='n_file'> N File Name: timm/models/vision_transformer.py</div><div id='m_start'> M Start Line: 469</div><div id='m_end'> M End Line: 476</div><div id='n_start'> N Start Line: 499</div><div id='n_end'> N End Line: 501</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    model = VisionTransformer(
        patch_size=16, num_classes=num_classes, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,
        representation_size=768, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    model.default_cfg<a id="change"> = </a><a id="change">default_cfgs[&quotvit_base_patch16_224_in21k&quot]</a>
    if pretrained:
        load_pretrained(
            model, num_classes=model.num_classes, in_chans=kwargs.get(&quotin_chans&quot, 3), filter_fn=_conv_filter)
    return model</code></pre><h3>After Change</h3><pre><code class='java'>
def vit_base_patch16_224_in21k(pretrained=False, **kwargs):
    model_kwargs = dict(
        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, representation_size=768, **kwargs)
    model<a id="change"> = </a><a id="change">_create_vision_transformer(&quotvit_base_patch16_224_in21k&quot</a><a id="change">, pretrained=pretrained, **model_kwargs)</a>
    return model


@register_model</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/feng-lab/pytorch-image-models/commit/55f7dfa9ea8bab0296c774dd7234d602b8396ce5#diff-a457ded0c066f18843cce88854130edb587dffef98fef0189c9442093ac110abL443' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43575129</div><div id='project'> Project Name: feng-lab/pytorch-image-models</div><div id='commit'> Commit Name: 55f7dfa9ea8bab0296c774dd7234d602b8396ce5</div><div id='time'> Time: 2021-01-18</div><div id='author'> Author: rwightman@gmail.com</div><div id='file'> File Name: timm/models/vision_transformer.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: vit_base_patch16_224_in21k(1)</div><div id='n_method'> N Method Name: vit_base_patch16_224_in21k(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: timm/models/vision_transformer.py</div><div id='n_file'> N File Name: timm/models/vision_transformer.py</div><div id='m_start'> M Start Line: 444</div><div id='m_end'> M End Line: 452</div><div id='n_start'> N Start Line: 475</div><div id='n_end'> N End Line: 477</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    model = VisionTransformer(
        img_size=224, patch_size=14, num_classes=num_classes, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4,
        qkv_bias=True, representation_size=1280, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    model.default_cfg<a id="change"> = </a><a id="change">default_cfgs[&quotvit_huge_patch14_224_in21k&quot]</a>
    if pretrained:
        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get(&quotin_chans&quot, 3))
    return model
</code></pre><h3>After Change</h3><pre><code class='java'>
def vit_huge_patch14_224_in21k(pretrained=False, **kwargs):
    model_kwargs = dict(
        patch_size=14, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4, representation_size=1280, **kwargs)
    model<a id="change"> = </a><a id="change">_create_vision_transformer(&quotvit_huge_patch14_224_in21k&quot</a><a id="change">, pretrained=pretrained, **model_kwargs)</a>
    return model


@register_model</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/feng-lab/pytorch-image-models/commit/55f7dfa9ea8bab0296c774dd7234d602b8396ce5#diff-a457ded0c066f18843cce88854130edb587dffef98fef0189c9442093ac110abL492' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43575130</div><div id='project'> Project Name: feng-lab/pytorch-image-models</div><div id='commit'> Commit Name: 55f7dfa9ea8bab0296c774dd7234d602b8396ce5</div><div id='time'> Time: 2021-01-18</div><div id='author'> Author: rwightman@gmail.com</div><div id='file'> File Name: timm/models/vision_transformer.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: vit_huge_patch14_224_in21k(1)</div><div id='n_method'> N Method Name: vit_huge_patch14_224_in21k(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: timm/models/vision_transformer.py</div><div id='n_file'> N File Name: timm/models/vision_transformer.py</div><div id='m_start'> M Start Line: 493</div><div id='m_end'> M End Line: 500</div><div id='n_start'> N Start Line: 523</div><div id='n_end'> N End Line: 525</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    model = VisionTransformer(
        img_size=224, num_classes=num_classes, patch_size=32, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,
        qkv_bias=True, representation_size=768, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    model.default_cfg<a id="change"> = </a><a id="change">default_cfgs[&quotvit_base_patch32_224_in21k&quot]</a>
    if pretrained:
        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get(&quotin_chans&quot, 3))
    return model
</code></pre><h3>After Change</h3><pre><code class='java'>
def vit_base_patch32_224_in21k(pretrained=False, **kwargs):
    model_kwargs = dict(
        patch_size=32, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, representation_size=768, **kwargs)
    model<a id="change"> = </a><a id="change">_create_vision_transformer(&quotvit_base_patch32_224_in21k&quot</a><a id="change">, pretrained=pretrained, **model_kwargs)</a>
    return model


@register_model</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/feng-lab/pytorch-image-models/commit/55f7dfa9ea8bab0296c774dd7234d602b8396ce5#diff-a457ded0c066f18843cce88854130edb587dffef98fef0189c9442093ac110abL456' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43575131</div><div id='project'> Project Name: feng-lab/pytorch-image-models</div><div id='commit'> Commit Name: 55f7dfa9ea8bab0296c774dd7234d602b8396ce5</div><div id='time'> Time: 2021-01-18</div><div id='author'> Author: rwightman@gmail.com</div><div id='file'> File Name: timm/models/vision_transformer.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: vit_base_patch32_224_in21k(1)</div><div id='n_method'> N Method Name: vit_base_patch32_224_in21k(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: timm/models/vision_transformer.py</div><div id='n_file'> N File Name: timm/models/vision_transformer.py</div><div id='m_start'> M Start Line: 457</div><div id='m_end'> M End Line: 464</div><div id='n_start'> N Start Line: 491</div><div id='n_end'> N End Line: 493</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    model = VisionTransformer(
        img_size=224, num_classes=num_classes, patch_size=32, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,
        qkv_bias=True, representation_size=1024, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    model.default_cfg<a id="change"> = </a><a id="change">default_cfgs[&quotvit_large_patch32_224_in21k&quot]</a>
    if pretrained:
        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get(&quotin_chans&quot, 3))
    return model
</code></pre><h3>After Change</h3><pre><code class='java'>
def vit_large_patch32_224_in21k(pretrained=False, **kwargs):
    model_kwargs = dict(
        patch_size=32, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, representation_size=1024, **kwargs)
    model<a id="change"> = </a><a id="change">_create_vision_transformer(&quotvit_large_patch32_224_in21k&quot</a><a id="change">, pretrained=pretrained, **model_kwargs)</a>
    return model


@register_model</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/feng-lab/pytorch-image-models/commit/55f7dfa9ea8bab0296c774dd7234d602b8396ce5#diff-a457ded0c066f18843cce88854130edb587dffef98fef0189c9442093ac110abL480' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43575132</div><div id='project'> Project Name: feng-lab/pytorch-image-models</div><div id='commit'> Commit Name: 55f7dfa9ea8bab0296c774dd7234d602b8396ce5</div><div id='time'> Time: 2021-01-18</div><div id='author'> Author: rwightman@gmail.com</div><div id='file'> File Name: timm/models/vision_transformer.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: vit_large_patch32_224_in21k(1)</div><div id='n_method'> N Method Name: vit_large_patch32_224_in21k(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: timm/models/vision_transformer.py</div><div id='n_file'> N File Name: timm/models/vision_transformer.py</div><div id='m_start'> M Start Line: 481</div><div id='m_end'> M End Line: 488</div><div id='n_start'> N Start Line: 515</div><div id='n_end'> N End Line: 517</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    model = VisionTransformer(
        img_size=224, num_classes=num_classes, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,
        hybrid_backbone=backbone, representation_size=768, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    model.default_cfg<a id="change"> = </a><a id="change">default_cfgs[&quotvit_base_resnet50_224_in21k&quot]</a>
    if pretrained:
        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get(&quotin_chans&quot, 3))
    return model
</code></pre><h3>After Change</h3><pre><code class='java'>
    model_kwargs = dict(
        embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, hybrid_backbone=backbone,
        representation_size=768, **kwargs)
    model<a id="change"> = </a><a id="change">_create_vision_transformer(&quotvit_base_resnet50_224_in21k&quot</a><a id="change">, pretrained=pretrained, **model_kwargs)</a>
    return model


@register_model</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/feng-lab/pytorch-image-models/commit/55f7dfa9ea8bab0296c774dd7234d602b8396ce5#diff-a457ded0c066f18843cce88854130edb587dffef98fef0189c9442093ac110abL504' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43575127</div><div id='project'> Project Name: feng-lab/pytorch-image-models</div><div id='commit'> Commit Name: 55f7dfa9ea8bab0296c774dd7234d602b8396ce5</div><div id='time'> Time: 2021-01-18</div><div id='author'> Author: rwightman@gmail.com</div><div id='file'> File Name: timm/models/vision_transformer.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: vit_base_resnet50_224_in21k(1)</div><div id='n_method'> N Method Name: vit_base_resnet50_224_in21k(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: timm/models/vision_transformer.py</div><div id='n_file'> N File Name: timm/models/vision_transformer.py</div><div id='m_start'> M Start Line: 506</div><div id='m_end'> M End Line: 515</div><div id='n_start'> N Start Line: 534</div><div id='n_end'> N End Line: 537</div><BR>