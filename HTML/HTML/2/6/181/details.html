<html><h3>Pattern ID :181
</h3><img src='1642936.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 Default return value. Erring on the side of caution here by
        &#47&#47 being super verbose.
        <a id="change">return </a>[([]<a id="change">, []</a>)]
</code></pre><h3>After Change</h3><pre><code class='java'>
        except IndexError:
            infinite_pairs = None

        <a id="change">if </a>infinite_pairs is not None:
            print(&quotinfinite_pairs =&quot, infinite_pairs)

            &#47&#47 &quotPair off&quot all the indices
            max_index = torch.argmax(x)
            fake_destroyers<a id="change"> = </a><a id="change">torch.empty_like(</a>infinite_pairs<a id="change">)</a>.fill_(max_index)

            infinite_pairs = torch.stack(
                (infinite_pairs<a id="change">, fake_destroyers</a>), 1
            )

            print(&quotinfinite pairs, fixed =&quot, infinite_pairs)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/aidos-lab/pytorch-topological/commit/0dd25924d9fd7691de14281351dcee1edd8cfc26#diff-55e452f337a40ea32b5684b3ef902cea8be96c3dfbf8d1bf1649e99874c84074L74' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 1642936</div><div id='project'> Project Name: aidos-lab/pytorch-topological</div><div id='commit'> Commit Name: 0dd25924d9fd7691de14281351dcee1edd8cfc26</div><div id='time'> Time: 2021-12-21</div><div id='author'> Author: bastian@rieck.me</div><div id='file'> File Name: torch_topological/nn/cubical.py</div><div id='m_class'> M Class Name: Cubical</div><div id='n_method'> N Class Name: Cubical</div><div id='m_method'> M Method Name: _extract_generators_and_diagrams(4)</div><div id='n_method'> N Method Name: _extract_generators_and_diagrams(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: torch_topological/nn/cubical.py</div><div id='n_file'> N File Name: torch_topological/nn/cubical.py</div><div id='m_start'> M Start Line: 88</div><div id='m_end'> M End Line: 119</div><div id='n_start'> N Start Line: 74</div><div id='n_end'> N End Line: 141</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        hidden_states, present = self.gpt_neox(
            input_ids, position_ids, cu_seqlens, max_s, past_key_values
        )
        <a id="change">return </a>self.embed_out(hidden_states)<a id="change">, present</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
        )
        logits = self.embed_out(hidden_states)

        <a id="change">if </a>self.gpt_neox.tp_embeddings:
            &#47&#47 Logits are sharded, so we need to gather them
            world_logits<a id="change"> = </a>[<a id="change">torch.empty_like(</a>logits<a id="change">)</a> for _ in range(self.world_size)]
            torch.distributed.all_gather(world_logits, logits, group=self.process_group)
            world_logits = torch.cat(world_logits, dim=1)

            return world_logits<a id="change">, present</a>
        return logits, present
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/huggingface/text-generation-inference/commit/9987960062e40de2deae030ab7e4ad6f57de0b20#diff-e24db7a565766a50b02649a0a1ac904f9bacff6ae58a2901fefc092befcbd980L660' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 1642939</div><div id='project'> Project Name: huggingface/text-generation-inference</div><div id='commit'> Commit Name: 9987960062e40de2deae030ab7e4ad6f57de0b20</div><div id='time'> Time: 2023-04-09</div><div id='author'> Author: olivier@huggingface.co</div><div id='file'> File Name: server/text_generation_server/models/custom_modeling/flash_neox_modeling.py</div><div id='m_class'> M Class Name: FlashGPTNeoXForCausalLM</div><div id='n_method'> N Class Name: FlashGPTNeoXForCausalLM</div><div id='m_method'> M Method Name: forward(6)</div><div id='n_method'> N Method Name: forward(6)</div><div id='m_parent_class'> M Parent Class: FlashGPTNeoXPreTrainedModel</div><div id='n_parent_class'> N Parent Class: FlashGPTNeoXPreTrainedModel</div><div id='m_file'> M File Name: server/text_generation_server/models/custom_modeling/flash_neox_modeling.py</div><div id='n_file'> N File Name: server/text_generation_server/models/custom_modeling/flash_neox_modeling.py</div><div id='m_start'> M Start Line: 671</div><div id='m_end'> M End Line: 671</div><div id='n_start'> N Start Line: 674</div><div id='n_end'> N End Line: 683</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        hidden_states, present = self.transformer(
            input_ids, position_ids, cu_seqlens, max_s, past_key_values
        )
        <a id="change">return </a>self.lm_head(hidden_states)<a id="change">, present</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
        )
        logits = self.lm_head(hidden_states)

        <a id="change">if </a>self.transformer.tp_embeddings:
            &#47&#47 Logits are sharded, so we need to gather them
            world_logits<a id="change"> = </a>[
                <a id="change">torch.empty_like(</a>logits<a id="change">)</a> for _ in range(self.transformer.tp_world_size)
            ]
            torch.distributed.all_gather(
                world_logits, logits, group=self.transformer.process_group
            )
            world_logits = torch.cat(world_logits, dim=1)

            return world_logits<a id="change">, present</a>

        return logits, present
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/huggingface/text-generation-inference/commit/880a76eed5f058043367d9643be8a498b286bde2#diff-4294535c61ba845a58c3811602b0f1895f3ce53ab074172e37be99a81e32129fL344' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 1642931</div><div id='project'> Project Name: huggingface/text-generation-inference</div><div id='commit'> Commit Name: 880a76eed5f058043367d9643be8a498b286bde2</div><div id='time'> Time: 2023-04-12</div><div id='author'> Author: olivier@huggingface.co</div><div id='file'> File Name: server/text_generation_server/models/custom_modeling/flash_santacoder_modeling.py</div><div id='m_class'> M Class Name: FlashSantacoderForCausalLM</div><div id='n_method'> N Class Name: FlashSantacoderForCausalLM</div><div id='m_method'> M Method Name: forward(6)</div><div id='n_method'> N Method Name: forward(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: server/text_generation_server/models/custom_modeling/flash_santacoder_modeling.py</div><div id='n_file'> N File Name: server/text_generation_server/models/custom_modeling/flash_santacoder_modeling.py</div><div id='m_start'> M Start Line: 355</div><div id='m_end'> M End Line: 355</div><div id='n_start'> N Start Line: 526</div><div id='n_end'> N End Line: 540</div><BR>