<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 implicit grad
    implicit_grad = torch.autograd.grad(loss, path[1].trainable_parameters())
    <a id="change">for </a>i in range(1, len(path)-1)<a id="change">:
        </a>implicit_grad<a id="change"> = </a>darts_helper(implicit_grad, path[i], path[i+1], config)

    return [add_with_none(dg, ig) for dg, ig in zip(direct_grad, implicit_grad)]
</code></pre><h3>After Change</h3><pre><code class='java'>
    achieves better memory efficiency, training wall time, and test accuracy that other methods.
    
    config = curr.config
    R<a id="change"> = </a>config.darts_alpha
    eps = R / <a id="change">to_vec(vector).norm()</a>

    &#47&#47 positive
    for p, v in zip(curr.trainable_parameters(), vector):
        p.data.add_(v.data, alpha=eps)
    loss_p<a id="change"> = </a>curr.training_step(curr.cur_batch)
    grad_p = torch.autograd.grad(loss_p, prev.trainable_parameters())

    &#47&#47 negative
    for p, v in zip(curr.trainable_parameters(), vector):
        p.data.add_(v.data, alpha=-2*eps)
    loss_n = curr.training_step(curr.cur_batch)
    grad_n = torch.autograd.grad(loss_n, prev.trainable_parameters())

    &#47&#47 reverse weight change
    for p, v in zip(curr.trainable_parameters(), vector):
        p.data.add(v.data, alpha=eps)

    implicit_grad<a id="change"> = </a>[(x - y).div_(2 * eps) for x, y in zip(grad_n, grad_p)]

    return implicit_grad
</code></pre>