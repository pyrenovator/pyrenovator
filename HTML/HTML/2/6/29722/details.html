<html><h3>Pattern ID :29722
</h3><img src='88089057.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 implicit grad
    implicit_grad = torch.autograd.grad(loss, path[1].trainable_parameters())
    <a id="change">for </a>i in range(1, len(path)-1)<a id="change">:
        </a>implicit_grad<a id="change"> = </a>darts_helper(implicit_grad, path[i], path[i+1], config)

    return [add_with_none(dg, ig) for dg, ig in zip(direct_grad, implicit_grad)]
</code></pre><h3>After Change</h3><pre><code class='java'>
    achieves better memory efficiency, training wall time, and test accuracy that other methods.
    
    config = curr.config
    R<a id="change"> = </a>config.darts_alpha
    eps = R / <a id="change">to_vec(vector).norm()</a>

    &#47&#47 positive
    for p, v in zip(curr.trainable_parameters(), vector):
        p.data.add_(v.data, alpha=eps)
    loss_p<a id="change"> = </a>curr.training_step(curr.cur_batch)
    grad_p = torch.autograd.grad(loss_p, prev.trainable_parameters())

    &#47&#47 negative
    for p, v in zip(curr.trainable_parameters(), vector):
        p.data.add_(v.data, alpha=-2*eps)
    loss_n = curr.training_step(curr.cur_batch)
    grad_n = torch.autograd.grad(loss_n, prev.trainable_parameters())

    &#47&#47 reverse weight change
    for p, v in zip(curr.trainable_parameters(), vector):
        p.data.add(v.data, alpha=eps)

    implicit_grad<a id="change"> = </a>[(x - y).div_(2 * eps) for x, y in zip(grad_n, grad_p)]

    return implicit_grad
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/leopard-ai/betty/commit/5542eb6dd30fc3b008a0626760a339f3fe31088e#diff-fee71c6e11eca0a047fb000e6ce9c0986274faf27ef8f81ac06ee49870b9ef48L6' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 88089057</div><div id='project'> Project Name: leopard-ai/betty</div><div id='commit'> Commit Name: 5542eb6dd30fc3b008a0626760a339f3fe31088e</div><div id='time'> Time: 2022-06-03</div><div id='author'> Author: sangkeuc@andrew.cmu.edu</div><div id='file'> File Name: betty/hypergradient/darts.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: darts(3)</div><div id='n_method'> N Method Name: darts(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: betty/hypergradient/darts.py</div><div id='n_file'> N File Name: betty/hypergradient/darts.py</div><div id='m_start'> M Start Line: 6</div><div id='m_end'> M End Line: 49</div><div id='n_start'> N Start Line: 6</div><div id='n_end'> N End Line: 36</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def cal_explanation_feature(self, saliency_maps: torch.Tensor) -&gt; int:
        exp_features = []
        <a id="change">for </a>smap in saliency_maps<a id="change">:
            </a>sparse_feat = torch.sum(torch.abs(smap))

            n_channels<a id="change"> = </a>smap.shape[0]
            kernel = torch.tensor([[0., 1., 0.],
                                    [1., -4., 1.],
                                    [0., 1., 0.]])</code></pre><h3>After Change</h3><pre><code class='java'>
        return torch.cat(saliency_maps)

    def cal_explanation_feature(self, saliency_maps: torch.Tensor) -&gt; float:
        sparse_feats = <a id="change">saliency_maps.flatten(start_dim=1).norm(p=1)</a>    &#47&#47 (N)
        smooth_feats<a id="change"> = </a>self.conv2d(saliency_maps).flatten(start_dim=1).norm(p=1)    &#47&#47 (N)
        persist_feats<a id="change"> = </a>0.0  &#47&#47 todo (N)

        exp_feats<a id="change"> = </a>self.lambd_sp * sparse_feats + self.lambd_sm * smooth_feats + self.lambd_pe * persist_feats
        return exp_feats.median()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/ain-soph/trojanzoo/commit/afe7bbd2d2e9f901ee8cf56c3b9320b9272a81af#diff-5528b4636bddd4d9ccc9486227970324e4344e01059ddca8b9863336ce85d70cL67' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 88089092</div><div id='project'> Project Name: ain-soph/trojanzoo</div><div id='commit'> Commit Name: afe7bbd2d2e9f901ee8cf56c3b9320b9272a81af</div><div id='time'> Time: 2020-07-22</div><div id='author'> Author: ain-soph@live.com</div><div id='file'> File Name: trojanzoo/defense/backdoor/neuron_inspect.py</div><div id='m_class'> M Class Name: Neuron_Inspect</div><div id='n_method'> N Class Name: Neuron_Inspect</div><div id='m_method'> M Method Name: cal_explanation_feature(2)</div><div id='n_method'> N Method Name: cal_explanation_feature(2)</div><div id='m_parent_class'> M Parent Class: Defense_Backdoor</div><div id='n_parent_class'> N Parent Class: Defense_Backdoor</div><div id='m_file'> M File Name: trojanzoo/defense/backdoor/neuron_inspect.py</div><div id='n_file'> N File Name: trojanzoo/defense/backdoor/neuron_inspect.py</div><div id='m_start'> M Start Line: 68</div><div id='m_end'> M End Line: 84</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 73</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        parameters = [parameters]
    parameters = list(filter(lambda p: p.grad is not None, parameters))
    norm_type = float(norm_type)
    total_norm<a id="change"> = </a>0
    <a id="change">for </a>p in parameters<a id="change">:
        </a>param_norm = p.grad.data.norm(norm_type)
        total_norm<a id="change"> += </a>param_norm.item() ** norm_type
    total_norm<a id="change"> = </a>total_norm ** (1. / norm_type)
    return total_norm
</code></pre><h3>After Change</h3><pre><code class='java'>
    if norm_type == inf:
        total_norm = max(p.grad.detach().abs().max() for p in parameters)
    else:
        total_norm<a id="change"> = </a><a id="change">torch.norm(</a>torch.stack([torch.norm(p.grad.detach(), norm_type) for p in parameters]), norm_type<a id="change">)</a>
    &#47&#47 clip_coef = max_norm / (total_norm + 1e-6)
    &#47&#47 if clip_coef &lt; 1:
    &#47&#47     for p in parameters:
    &#47&#47         p.grad.detach().mul_(clip_coef)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/393221700c680cde75b94b03b33ff60cde5d2918#diff-ed437419f1c6426c0e789a7364bc4b03da30df687083a76efe1ee9448c852547L4' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 88089079</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 393221700c680cde75b94b03b33ff60cde5d2918</div><div id='time'> Time: 2020-05-06</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: pipeline/training/utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: calc_norm(2)</div><div id='n_method'> N Method Name: calc_norm(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: pipeline/training/utils.py</div><div id='n_file'> N File Name: pipeline/training/utils.py</div><div id='m_start'> M Start Line: 8</div><div id='m_end'> M End Line: 14</div><div id='n_start'> N Start Line: 7</div><div id='n_end'> N End Line: 18</div><BR>