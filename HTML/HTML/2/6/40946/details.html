<html><h3>Pattern ID :40946
</h3><img src='115464059.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        note = torch.LongTensor([note]) &#47&#47 1
        x = self.emb(note) &#47&#47 1, emb_size
        
        <a id="change">if </a>self.cell_state is None:
            self.cell_state<a id="change"> = </a>[[<a id="change">self.h0.detach()</a>.clone()]]
        
        self.cell_state = self.rnn.step(x, self.cell_state)
        h = self.cell_state[0]</code></pre><h3>After Change</h3><pre><code class='java'>
        x = self.emb(note) &#47&#47 1, 1, emb_size
        
        h, new_state = self.rnn(x, self.cell_state)
        <a id="change">for </a>t,new_t in <a id="change">zip(</a>self.cell_state, new_state<a id="change">):
            </a>t[:]<a id="change"> = </a>new_t
        
        logits = self.proj(h) &#47&#47 1, 1, hidden_size
        ret = logits.squeeze().softmax(0)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/intelligent-instruments-lab/iil-python-tools/commit/dcfbe6d6d3dc8806da6f71fe5d25663a248cd880#diff-40cd3638789a4762e91c747bb354a0f97a8c9b7cfa59a76105a95a39e5005cecL143' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 115464059</div><div id='project'> Project Name: intelligent-instruments-lab/iil-python-tools</div><div id='commit'> Commit Name: dcfbe6d6d3dc8806da6f71fe5d25663a248cd880</div><div id='time'> Time: 2022-02-25</div><div id='author'> Author: victor.shepardson@gmail.com</div><div id='file'> File Name: notepredictor/notepredictor/model.py</div><div id='m_class'> M Class Name: PitchPredictor</div><div id='n_method'> N Class Name: PitchPredictor</div><div id='m_method'> M Method Name: predict(3)</div><div id='n_method'> N Method Name: predict(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: notepredictor/notepredictor/model.py</div><div id='n_file'> N File Name: notepredictor/notepredictor/model.py</div><div id='m_start'> M Start Line: 143</div><div id='m_end'> M End Line: 154</div><div id='n_start'> N Start Line: 147</div><div id='n_end'> N End Line: 155</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            &#47&#47 group.setdefault(&quotparams_old&quot, group[&quotparams&quot])

            for p in group[&quotparams&quot]:
                <a id="change">if </a>p.grad is None:
                    continue

                p_t = p
                param_state = self.state[p]
                if &quotparam_old&quot not in param_state:
                    param_state[&quotparam_old&quot]<a id="change"> = </a><a id="change">torch.clone(p).detach()</a>
                else:
                    p_t = param_state[&quotparam_old&quot]

                d_p = p.grad.data + lamda*(p-p_t)</code></pre><h3>After Change</h3><pre><code class='java'>
    @torch.no_grad()
    def step(self, global_params, device):
        for group in self.param_groups:
            <a id="change">for </a>p, g in <a id="change">zip(</a>group[&quotparams&quot], global_params<a id="change">):
                </a>g<a id="change"> = </a>g.to(device)
                d_p = p.grad.data + group[&quotmu&quot] * (p.data - g.data)
                p.data.add_(d_p, alpha=-group[&quotlr&quot])
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tsingz0/pfl-non-iid/commit/330ce2d6169fd4a54cec28895418bd38c206b6a4#diff-a99c19163c19c03eede69bdda22f1a2d923b8eb7d0f4b0463d797d66d5fc8e87L89' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 115464061</div><div id='project'> Project Name: tsingz0/pfl-non-iid</div><div id='commit'> Commit Name: 330ce2d6169fd4a54cec28895418bd38c206b6a4</div><div id='time'> Time: 2021-03-24</div><div id='author'> Author: 2719584131@qq.com</div><div id='file'> File Name: system/flcore/optimizers/fedoptimizer.py</div><div id='m_class'> M Class Name: PerturbedGradientDescent</div><div id='n_method'> N Class Name: PerturbedGradientDescent</div><div id='m_method'> M Method Name: step(3)</div><div id='n_method'> N Method Name: step(1)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: system/flcore/optimizers/fedoptimizer.py</div><div id='n_file'> N File Name: system/flcore/optimizers/fedoptimizer.py</div><div id='m_start'> M Start Line: 89</div><div id='m_end'> M End Line: 108</div><div id='n_start'> N Start Line: 116</div><div id='n_end'> N End Line: 121</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            nesterov = group[&quotnesterov&quot]

            for p in group[&quotparams&quot]:
                <a id="change">if </a>p.grad is None:
                    continue
                d_p = p.grad
                if weight_decay != 0:
                    d_p = d_p.add(p, alpha=weight_decay)
                if momentum != 0:
                    param_state = self.state[p]
                    if &quotmomentum_buffer&quot not in param_state:
                        buf = param_state[&quotmomentum_buffer&quot]<a id="change"> = </a><a id="change">torch.clone(d_p).detach()</a>
                    else:
                        buf = param_state[&quotmomentum_buffer&quot]
                        buf.mul_(momentum).add_(d_p, alpha=1 - dampening)
                    if nesterov:</code></pre><h3>After Change</h3><pre><code class='java'>
                  nesterov)

            &#47&#47 update momentum_buffers in state
            <a id="change">for </a>p, momentum_buffer in <a id="change">zip(</a>params_with_grad, momentum_buffer_list<a id="change">):
                </a>state<a id="change"> = </a>self.state[p]
                state[&quotmomentum_buffer&quot] = momentum_buffer

        return loss</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/a0cf5566d88533c5caa7a490beb6eb0760eee9b4#diff-e5ea47a2193f1cfb039210c5c0ff83e8175739afc0551866052f6ad31bb91482L76' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 115464060</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: a0cf5566d88533c5caa7a490beb6eb0760eee9b4</div><div id='time'> Time: 2021-01-21</div><div id='author'> Author: wanchaol@users.noreply.github.com</div><div id='file'> File Name: torch/optim/sgd.py</div><div id='m_class'> M Class Name: SGD</div><div id='n_method'> N Class Name: SGD</div><div id='m_method'> M Method Name: step(2)</div><div id='n_method'> N Method Name: step(2)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: torch/optim/sgd.py</div><div id='n_file'> N File Name: torch/optim/sgd.py</div><div id='m_start'> M Start Line: 88</div><div id='m_end'> M End Line: 114</div><div id='n_start'> N Start Line: 89</div><div id='n_end'> N End Line: 124</div><BR>