<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.activation = activation
        self.use_bn = bn
        self.init_method = init_method
        self.logger<a id="change"> = </a><a id="change">getLogger()</a>

        mlp_modules = []
        for idx, (input_size, output_size) in enumerate(zip(self.layers[:-1], self.layers[1:])):
            mlp_modules.append(nn.Dropout(p=self.dropout))
            mlp_modules.append(nn.Linear(input_size, output_size))
            if self.use_bn:
                mlp_modules.append(nn.BatchNorm1d(num_features=output_size))
            if self.activation.lower() == &quotsigmoid&quot:
                mlp_modules.append(nn.Sigmoid())
            elif self.activation.lower() == &quottanh&quot:
                mlp_modules.append(nn.Tanh())
            elif self.activation.lower() == &quotrelu&quot:
                mlp_modules.append(nn.ReLU())
            elif self.activation.lower() == &quotleakyrelu&quot:
                mlp_modules.append(nn.LeakyReLU())
            elif self.activation.lower() == &quotdice&quot:
                mlp_modules.append(Dice(output_size))
            elif self.activation.lower() == &quotnone&quot:
                pass
            else:
                <a id="change">self.logger.warning(&quotReceived unrecognized activation function, set default activation function&quot</a><a id="change">)</a>
        self.mlp_layers = nn.Sequential(*mlp_modules)
        if self.init_method is not None:
            self.apply(self.init_weights)</code></pre><h3>After Change</h3><pre><code class='java'>
            if self.use_bn:
                mlp_modules.append(nn.BatchNorm1d(num_features=output_size))
            activation_fun = activation_layer(self.activation, output_size)
            <a id="change">if activation_fun is not None</a>:
                mlp_modules.append(activation_fun)

        self.mlp_layers = nn.Sequential(*mlp_modules)</code></pre>