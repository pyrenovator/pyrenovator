<html><h3>Pattern ID :3270
</h3><img src='12659489.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

def latent_optimise(zs, fake_labels, gen_model, dis_model, conditional_strategy, latent_op_step, latent_op_rate,
                    latent_op_alpha, latent_op_beta, trans_cost, default_device):
    batch_size = <a id="change">zs.shape[0]</a>
    for step in range(latent_op_step):
        drop_mask = (torch.FloatTensor(batch_size, 1).uniform_() &gt; 1 - latent_op_rate).to(default_device)
        z_gradients, z_gradients_norm = calc_derv(zs, fake_labels, dis_model, conditional_strategy, default_device, gen_model)
        delta_z = latent_op_alpha*z_gradients/(latent_op_beta + z_gradients_norm)</code></pre><h3>After Change</h3><pre><code class='java'>
        fake_images = generator(zs, fake_labels)
        output_dict = discriminator(fake_images, fake_labels, eval=False)
        z_grads = cal_deriv(inputs=zs, outputs=output_dict["adv_output"], device=device)
        z_grads_norm<a id="change"> = </a>torch.unsqueeze((<a id="change">z_grads.norm(</a>2<a id="change">, dim=1)</a>**2), dim=1)
        delta_z<a id="change"> = </a>lo_alpha*z_grads/(lo_beta + z_grads_norm)
        zs = torch.clamp(zs + drop_mask*delta_z, -1.0, 1.0)

        if cal_trsf_cost:
            if step == 0:
                trsf_cost = (delta_z.norm(2, dim=1)**2).mean()
            else:
                trsf_cost += (delta_z.norm(2, dim=1)**2).mean()
        else:
            trsf_cost = None
        <a id="change">return </a>zs, trsf_cost

def cal_deriv4gp(real_images, real_labels, fake_images, discriminator, device):
    batch_size, c, h, w = real_images.shape</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 5</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/postech-cvlab/pytorch-studiogan/commit/8820045ea9457f0849d37143aab4d7ac330b8a11#diff-30ced123c233cf81fc549762f2d9ad602330f78bbf84c78ee852c0e6173d1ea2L49' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 12659489</div><div id='project'> Project Name: postech-cvlab/pytorch-studiogan</div><div id='commit'> Commit Name: 8820045ea9457f0849d37143aab4d7ac330b8a11</div><div id='time'> Time: 2021-07-29</div><div id='author'> Author: first287@naver.com</div><div id='file'> File Name: src/utils/losses.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: latent_optimise(11)</div><div id='n_method'> N Method Name: latent_optimise(11)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/utils/losses.py</div><div id='n_file'> N File Name: src/utils/losses.py</div><div id='m_start'> M Start Line: 49</div><div id='m_end'> M End Line: 67</div><div id='n_start'> N Start Line: 96</div><div id='n_end'> N End Line: 111</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                                      allow_unused=allow_unused)

    &#47&#47 implicit grad
    implicit_grad = torch.autograd.grad(loss, <a id="change">path[1]</a>.trainable_parameters())
    for i in range(1, len(path)-1):
        implicit_grad = darts_helper(implicit_grad, path[i], path[i+1], config)
</code></pre><h3>After Change</h3><pre><code class='java'>
    
    config = curr.config
    R = config.darts_alpha
    eps<a id="change"> = </a>R / <a id="change">to_vec(vector).norm()</a>

    &#47&#47 positive
    for p, v in zip(curr.trainable_parameters(), vector):
        p.data.add_(v.data, alpha=eps)
    loss_p = curr.training_step(curr.cur_batch)
    grad_p = torch.autograd.grad(loss_p, prev.trainable_parameters())

    &#47&#47 negative
    for p, v in zip(curr.trainable_parameters(), vector):
        p.data.add_(v.data, alpha=-2*eps)
    loss_n = curr.training_step(curr.cur_batch)
    grad_n = torch.autograd.grad(loss_n, prev.trainable_parameters())

    &#47&#47 reverse weight change
    for p, v in zip(curr.trainable_parameters(), vector):
        p.data.add(v.data, alpha=eps)

    implicit_grad<a id="change"> = </a>[(x - y).div_(2 * eps) for x, y in zip(grad_n, grad_p)]

    <a id="change">return </a>implicit_grad
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leopard-ai/betty/commit/5542eb6dd30fc3b008a0626760a339f3fe31088e#diff-fee71c6e11eca0a047fb000e6ce9c0986274faf27ef8f81ac06ee49870b9ef48L6' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 12659481</div><div id='project'> Project Name: leopard-ai/betty</div><div id='commit'> Commit Name: 5542eb6dd30fc3b008a0626760a339f3fe31088e</div><div id='time'> Time: 2022-06-03</div><div id='author'> Author: sangkeuc@andrew.cmu.edu</div><div id='file'> File Name: betty/hypergradient/darts.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: darts(3)</div><div id='n_method'> N Method Name: darts(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: betty/hypergradient/darts.py</div><div id='n_file'> N File Name: betty/hypergradient/darts.py</div><div id='m_start'> M Start Line: 6</div><div id='m_end'> M End Line: 49</div><div id='n_start'> N Start Line: 6</div><div id='n_end'> N End Line: 36</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        for smap in saliency_maps:
            sparse_feat = torch.sum(torch.abs(smap))

            n_channels = <a id="change">smap.shape[0]</a>
            kernel = torch.tensor([[0., 1., 0.],
                                    [1., -4., 1.],
                                    [0., 1., 0.]])
            kernel = kernel.view(1, 1, 3, 3).repeat(1, n_channels, 1, 1)</code></pre><h3>After Change</h3><pre><code class='java'>

    def cal_explanation_feature(self, saliency_maps: torch.Tensor) -&gt; float:
        sparse_feats = saliency_maps.flatten(start_dim=1).norm(p=1)    &#47&#47 (N)
        smooth_feats<a id="change"> = </a><a id="change">self.conv2d(saliency_maps).flatten(start_dim=1).norm(p=1)</a>    &#47&#47 (N)
        persist_feats = 0.0  &#47&#47 todo (N)

        exp_feats<a id="change"> = </a>self.lambd_sp * sparse_feats + self.lambd_sm * smooth_feats + self.lambd_pe * persist_feats
        <a id="change">return </a>exp_feats.median()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/ain-soph/trojanzoo/commit/afe7bbd2d2e9f901ee8cf56c3b9320b9272a81af#diff-5528b4636bddd4d9ccc9486227970324e4344e01059ddca8b9863336ce85d70cL67' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 12659497</div><div id='project'> Project Name: ain-soph/trojanzoo</div><div id='commit'> Commit Name: afe7bbd2d2e9f901ee8cf56c3b9320b9272a81af</div><div id='time'> Time: 2020-07-22</div><div id='author'> Author: ain-soph@live.com</div><div id='file'> File Name: trojanzoo/defense/backdoor/neuron_inspect.py</div><div id='m_class'> M Class Name: Neuron_Inspect</div><div id='n_method'> N Class Name: Neuron_Inspect</div><div id='m_method'> M Method Name: cal_explanation_feature(2)</div><div id='n_method'> N Method Name: cal_explanation_feature(2)</div><div id='m_parent_class'> M Parent Class: Defense_Backdoor</div><div id='n_parent_class'> N Parent Class: Defense_Backdoor</div><div id='m_file'> M File Name: trojanzoo/defense/backdoor/neuron_inspect.py</div><div id='n_file'> N File Name: trojanzoo/defense/backdoor/neuron_inspect.py</div><div id='m_start'> M Start Line: 68</div><div id='m_end'> M End Line: 84</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 73</div><BR>