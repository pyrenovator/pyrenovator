<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

def latent_optimise(zs, fake_labels, gen_model, dis_model, conditional_strategy, latent_op_step, latent_op_rate,
                    latent_op_alpha, latent_op_beta, trans_cost, default_device):
    batch_size = <a id="change">zs.shape[0]</a>
    for step in range(latent_op_step):
        drop_mask = (torch.FloatTensor(batch_size, 1).uniform_() &gt; 1 - latent_op_rate).to(default_device)
        z_gradients, z_gradients_norm = calc_derv(zs, fake_labels, dis_model, conditional_strategy, default_device, gen_model)
        delta_z = latent_op_alpha*z_gradients/(latent_op_beta + z_gradients_norm)</code></pre><h3>After Change</h3><pre><code class='java'>
        fake_images = generator(zs, fake_labels)
        output_dict = discriminator(fake_images, fake_labels, eval=False)
        z_grads = cal_deriv(inputs=zs, outputs=output_dict["adv_output"], device=device)
        z_grads_norm<a id="change"> = </a>torch.unsqueeze((<a id="change">z_grads.norm(</a>2<a id="change">, dim=1)</a>**2), dim=1)
        delta_z<a id="change"> = </a>lo_alpha*z_grads/(lo_beta + z_grads_norm)
        zs = torch.clamp(zs + drop_mask*delta_z, -1.0, 1.0)

        if cal_trsf_cost:
            if step == 0:
                trsf_cost = (delta_z.norm(2, dim=1)**2).mean()
            else:
                trsf_cost += (delta_z.norm(2, dim=1)**2).mean()
        else:
            trsf_cost = None
        <a id="change">return </a>zs, trsf_cost

def cal_deriv4gp(real_images, real_labels, fake_images, discriminator, device):
    batch_size, c, h, w = real_images.shape</code></pre>