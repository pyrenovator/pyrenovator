<html><h3>Pattern ID :41204
</h3><img src='116223249.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def greedy(self, self_attended_context, context, context_indices, oov_to_limited_idx, rnn_state=None):
        B, TC, C = context.size()
        T = self.args.max_output_length
        outs = Variable(<a id="change">context.data.new(B, T).long().fill_(
            </a>self.field.decoder_stoi[&quot&lt;pad&gt;&quot]<a id="change">)</a>, volatile=True)
        hiddens = [<a id="change">Variable(self_attended_context[0].data.new(B, T, C).zero_()</a><a id="change">, volatile=True)</a>
                   for l in range(len(self.self_attentive_decoder.layers) + 1)]
        <a id="change">hiddens[0]</a> = hiddens[0] + positional_encodings_like(hiddens[0])
        eos_yet = context.data.new(B).byte().zero_()

        rnn_output, context_alignment = None, None
        for t in range(T):
            if t == 0:
                embedding = self.decoder_embeddings(<a id="change">Variable(
                    self_attended_context[-1].data.new(B).long().fill_(
                        self.field.vocab.stoi[&quot&lt;init&gt;&quot]), volatile=True).unsqueeze(1</a><a id="change">)</a>, [1]*B)
            else:
                embedding = self.decoder_embeddings(outs[:, t - 1].unsqueeze(1), [1]*B)
            hiddens[0][:, t] = hiddens[0][:, t] + (math.sqrt(self.self_attentive_decoder.d_model) * embedding).squeeze(1)</code></pre><h3>After Change</h3><pre><code class='java'>

    def greedy(self, self_attended_context, context, context_indices, oov_to_limited_idx, rnn_state=None):
        B, TC, C = context.size()
        <a id="change">T</a> = self.args.max_output_length
        outs = context.new_full((<a id="change">B</a><a id="change">, T</a>), self.field.decoder_stoi[&quot&lt;pad&gt;&quot], dtype=torch.long)
        hiddens = [<a id="change">self_attended_context[0].new_zeros(</a>(<a id="change">B</a><a id="change">, T, C</a>)<a id="change">)</a>
                   for l in range(len(self.self_attentive_decoder.layers) + 1)]
        <a id="change">hiddens[0]</a> = hiddens[0] + positional_encodings_like(hiddens[0])
        eos_yet = context.data.new(B).byte().zero_()

        rnn_output, context_alignment = None, None
        for t in range(T):
            if t == 0:
                embedding = self.decoder_embeddings(
                    <a id="change">self_attended_context[-1].new_full(</a>(<a id="change">B</a><a id="change">, 1</a>), <a id="change">self.field.vocab.stoi[&quot&lt;init&gt;&quot]</a><a id="change">, dtype=torch.long)</a>, [1]*B)
            else:
                embedding = self.decoder_embeddings(outs[:, t - 1].unsqueeze(1), [1]*B)
            hiddens[0][:, t] = hiddens[0][:, t] + (math.sqrt(self.self_attentive_decoder.d_model) * embedding).squeeze(1)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 24</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/stanford-oval/genienlp/commit/4a962fa2bc3a99d5b5b177453210699f75472cbc#diff-ed5b3c3850653ac07b17b1db126163f7a6d160ffc06c6186a8b293fc0297d985L136' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116223249</div><div id='project'> Project Name: stanford-oval/genienlp</div><div id='commit'> Commit Name: 4a962fa2bc3a99d5b5b177453210699f75472cbc</div><div id='time'> Time: 2018-10-25</div><div id='author'> Author: bryan.mccann.is@gmail.com</div><div id='file'> File Name: models/self_attentive_pointer_generator.py</div><div id='m_class'> M Class Name: SelfAttentivePointerGenerator</div><div id='n_method'> N Class Name: SelfAttentivePointerGenerator</div><div id='m_method'> M Method Name: greedy(6)</div><div id='n_method'> N Method Name: greedy(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/self_attentive_pointer_generator.py</div><div id='n_file'> N File Name: models/self_attentive_pointer_generator.py</div><div id='m_start'> M Start Line: 137</div><div id='m_end'> M End Line: 171</div><div id='n_start'> N Start Line: 136</div><div id='n_end'> N End Line: 169</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def greedy(self, self_attended_context, context, question, context_indices, question_indices, oov_to_limited_idx, rnn_state=None):
        B, TC, C = context.size()
        <a id="change">T</a> = self.args.max_output_length
        outs = Variable(<a id="change">context.data.new(B, T).long().fill_(
            </a>self.field.decoder_stoi[&quot&lt;pad&gt;&quot]<a id="change">)</a>, volatile=True)
        hiddens = [<a id="change">Variable(self_attended_context[0].data.new(B, T, C).zero_()</a><a id="change">, volatile=True)</a>
                   for l in range(len(self.self_attentive_decoder.layers) + 1)]
        <a id="change">hiddens[0]</a> = hiddens[0] + positional_encodings_like(hiddens[0])
        eos_yet = context.data.new(B).byte().zero_()

        rnn_output, context_alignment, question_alignment = None, None, None
        for t in range(T):
            if t == 0:
                embedding = self.decoder_embeddings(<a id="change">Variable(
                    self_attended_context[-1].data.new(B).long().fill_(
                        self.field.vocab.stoi[&quot&lt;init&gt;&quot]), volatile=True).unsqueeze(1</a><a id="change">)</a>, [1]*B)
            else:
                embedding = self.decoder_embeddings(outs[:, t - 1].unsqueeze(1), [1]*B)
            hiddens[0][:, t] = hiddens[0][:, t] + (math.sqrt(self.self_attentive_decoder.d_model) * embedding).squeeze(1)</code></pre><h3>After Change</h3><pre><code class='java'>
    def greedy(self, self_attended_context, context, question, context_indices, question_indices, oov_to_limited_idx, rnn_state=None):
        with torch.no_grad():
            B, TC, C = context.size()
            <a id="change">T</a> = self.args.max_output_length
            outs = context.new_full((B<a id="change">, T</a>), self.field.decoder_stoi[&quot&lt;pad&gt;&quot], dtype=torch.long)
            hiddens = [<a id="change">self_attended_context[0].new_zeros(</a>(B<a id="change">, T, C</a>)<a id="change">)</a>
                       for l in range(len(self.self_attentive_decoder.layers) + 1)]
            <a id="change">hiddens[0]</a> = hiddens[0] + positional_encodings_like(hiddens[0])
            eos_yet = context.new_zeros((B, )).byte()
    
            rnn_output, context_alignment, question_alignment = None, None, None
            for t in range(T):
                if t == 0:
                    embedding = self.decoder_embeddings(
                        <a id="change">self_attended_context[-1].new_full(</a>(B<a id="change">, 1</a>), <a id="change">self.field.vocab.stoi[&quot&lt;init&gt;&quot]</a><a id="change">, dtype=torch.long)</a>, [1]*B)
                else:
                    embedding = self.decoder_embeddings(outs[:, t - 1].unsqueeze(1), [1]*B)
                hiddens[0][:, t] = hiddens[0][:, t] + (math.sqrt(self.self_attentive_decoder.d_model) * embedding).squeeze(1)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/stanford-oval/genienlp/commit/76304734b69186932db4b58d16dd077b1426c074#diff-d068f20368b9e24ab81f7be561e5b946ca2a0fcb2267e6f1a46ec25745b1389cL177' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116223248</div><div id='project'> Project Name: stanford-oval/genienlp</div><div id='commit'> Commit Name: 76304734b69186932db4b58d16dd077b1426c074</div><div id='time'> Time: 2018-10-25</div><div id='author'> Author: bryan.mccann.is@gmail.com</div><div id='file'> File Name: models/multitask_question_answering_network.py</div><div id='m_class'> M Class Name: MultitaskQuestionAnsweringNetwork</div><div id='n_method'> N Class Name: MultitaskQuestionAnsweringNetwork</div><div id='m_method'> M Method Name: greedy(8)</div><div id='n_method'> N Method Name: greedy(8)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/multitask_question_answering_network.py</div><div id='n_file'> N File Name: models/multitask_question_answering_network.py</div><div id='m_start'> M Start Line: 178</div><div id='m_end'> M End Line: 215</div><div id='n_start'> N Start Line: 175</div><div id='n_end'> N End Line: 213</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def greedy(self, self_attended_context, context, context_indices, oov_to_limited_idx, rnn_state=None):
        B, TC, C = context.size()
        <a id="change">T</a> = self.args.max_output_length
        outs = Variable(<a id="change">context.data.new(B, T).long().fill_(
            </a>self.field.decoder_stoi[&quot&lt;pad&gt;&quot]<a id="change">)</a>, volatile=True)
        hiddens = [<a id="change">Variable(self_attended_context[0].data.new(B, T, C).zero_()</a><a id="change">, volatile=True)</a>
                   for l in range(len(self.self_attentive_decoder.layers) + 1)]
        <a id="change">hiddens[0]</a> = hiddens[0] + positional_encodings_like(hiddens[0])
        eos_yet = context.data.new(B).byte().zero_()

        rnn_output, context_alignment = None, None
        for t in range(T):
            if t == 0:
                embedding = self.decoder_embeddings(<a id="change">Variable(
                    self_attended_context[-1].data.new(B).long().fill_(
                        self.field.vocab.stoi[&quot&lt;init&gt;&quot]), volatile=True).unsqueeze(1</a><a id="change">)</a>, [1]*B)
            else:
                embedding = self.decoder_embeddings(outs[:, t - 1].unsqueeze(1), [1]*B)
            hiddens[0][:, t] = hiddens[0][:, t] + (math.sqrt(self.self_attentive_decoder.d_model) * embedding).squeeze(1)</code></pre><h3>After Change</h3><pre><code class='java'>

    def greedy(self, self_attended_context, context, context_indices, oov_to_limited_idx, rnn_state=None):
        B, TC, C = context.size()
        <a id="change">T</a> = self.args.max_output_length
        outs = context.new_full((B<a id="change">, T</a>), self.field.decoder_stoi[&quot&lt;pad&gt;&quot], dtype=torch.long)
        hiddens = [<a id="change">self_attended_context[0].new_zeros(</a>(B<a id="change">, T, C</a>)<a id="change">)</a>
                   for l in range(len(self.self_attentive_decoder.layers) + 1)]

        <a id="change">hiddens[0]</a> = hiddens[0] + positional_encodings_like(hiddens[0])
        eos_yet = context.data.new(B).byte().zero_()

        rnn_output, context_alignment = None, None
        for t in range(T):
            if t == 0:
                embedding = self.decoder_embeddings(
                    <a id="change">self_attended_context[-1].new_full(</a>(B<a id="change">, 1</a>), <a id="change">self.field.vocab.stoi[&quot&lt;init&gt;&quot]</a><a id="change">, dtype=torch.long)</a>, [1]*B)
            else:
                embedding = self.decoder_embeddings(outs[:, t - 1].unsqueeze(1), [1]*B)
            hiddens[0][:, t] = hiddens[0][:, t] + (math.sqrt(self.self_attentive_decoder.d_model) * embedding).squeeze(1)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/stanford-oval/genienlp/commit/59a1dcd47dda8c0cd19248b3a119ee6125795fdc#diff-132fc63ce5affe92920aa4dc16792a1932aa59397084f8e47fcb4e2b203475a9L142' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116223250</div><div id='project'> Project Name: stanford-oval/genienlp</div><div id='commit'> Commit Name: 59a1dcd47dda8c0cd19248b3a119ee6125795fdc</div><div id='time'> Time: 2018-10-25</div><div id='author'> Author: bryan.mccann.is@gmail.com</div><div id='file'> File Name: models/coattentive_pointer_generator.py</div><div id='m_class'> M Class Name: CoattentivePointerGenerator</div><div id='n_method'> N Class Name: CoattentivePointerGenerator</div><div id='m_method'> M Method Name: greedy(6)</div><div id='n_method'> N Method Name: greedy(6)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: models/coattentive_pointer_generator.py</div><div id='n_file'> N File Name: models/coattentive_pointer_generator.py</div><div id='m_start'> M Start Line: 144</div><div id='m_end'> M End Line: 178</div><div id='n_start'> N Start Line: 143</div><div id='n_end'> N End Line: 177</div><BR>