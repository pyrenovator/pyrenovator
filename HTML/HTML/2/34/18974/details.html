<html><h3>Pattern ID :18974
</h3><img src='61709805.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 calculate p2 reweighting

        <a id="change">register_buffer(&quotp2_loss_weight&quot</a>, (p2_loss_weight_k<a id="change"> + alphas_cumprod</a><a id="change"> / </a>(1<a id="change"> - </a>alphas_cumprod))<a id="change"> ** </a><a id="change">-</a>p2_loss_weight_gamma<a id="change">)</a>

    def predict_start_from_noise(self, x_t, t, noise):
        return (
            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 loss weight

        <a id="change">snr</a><a id="change"> = alphas_cumprod</a><a id="change"> / </a>(1<a id="change"> - </a>alphas_cumprod)

        <a id="change">maybe_clipped_snr = snr.clone()</a>
        <a id="change">if min_snr_loss_weight</a>:
            <a id="change">maybe_clipped_snr.clamp_(min = min_snr_gamma)</a>

        <a id="change">if objective == &quotpred_noise&quot</a>:
            loss_weight = <a id="change">maybe_clipped_snr</a><a id="change"> / snr</a>
        elif <a id="change">objective == &quotpred_x0&quot</a>:
            loss_weight = maybe_clipped_snr
        elif <a id="change">objective == &quotpred_v&quot</a>:
            loss_weight = <a id="change">maybe_clipped_snr</a><a id="change"> / </a>(<a id="change">snr</a><a id="change"> + 1</a>)

        register_buffer(&quotloss_weight&quot, loss_weight)
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 22</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/denoising-diffusion-pytorch/commit/58717e5faaafcc73a7f8569d5c40dc7d6ab3e804#diff-4701f6335fabcf514faf3fc9949d5a43d041f7e4e8c137cd5fdd22d74fde7eb2L514' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 61709805</div><div id='project'> Project Name: lucidrains/denoising-diffusion-pytorch</div><div id='commit'> Commit Name: 58717e5faaafcc73a7f8569d5c40dc7d6ab3e804</div><div id='time'> Time: 2023-03-17</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: denoising_diffusion_pytorch/classifier_free_guidance.py</div><div id='m_class'> M Class Name: GaussianDiffusion</div><div id='n_method'> N Class Name: GaussianDiffusion</div><div id='m_method'> M Method Name: __init__(2)</div><div id='n_method'> N Method Name: __init__(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: denoising_diffusion_pytorch/classifier_free_guidance.py</div><div id='n_file'> N File Name: denoising_diffusion_pytorch/classifier_free_guidance.py</div><div id='m_start'> M Start Line: 514</div><div id='m_end'> M End Line: 561</div><div id='n_start'> N Start Line: 514</div><div id='n_end'> N End Line: 574</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        betas = beta_schedule_fn(timesteps, **schedule_fn_kwargs)

        alphas = 1. - betas
        <a id="change">alphas_cumprod</a> = torch.cumprod(alphas, dim=0)
        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)

        timesteps, = betas.shape
        self.num_timesteps = int(timesteps)
        self.loss_type = loss_type

        &#47&#47 sampling related parameters

        self.sampling_timesteps = default(sampling_timesteps, timesteps) &#47&#47 default num sampling timesteps to number of timesteps at training

        assert self.sampling_timesteps &lt;= timesteps
        self.is_ddim_sampling = self.sampling_timesteps &lt; timesteps
        self.ddim_sampling_eta = ddim_sampling_eta

        &#47&#47 helper function to register buffer from float64 to float32

        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))

        register_buffer(&quotbetas&quot, betas)
        register_buffer(&quotalphas_cumprod&quot, alphas_cumprod)
        register_buffer(&quotalphas_cumprod_prev&quot, alphas_cumprod_prev)

        &#47&#47 calculations for diffusion q(x_t | x_{t-1}) and others

        register_buffer(&quotsqrt_alphas_cumprod&quot, torch.sqrt(alphas_cumprod))
        register_buffer(&quotsqrt_one_minus_alphas_cumprod&quot, torch.sqrt(1. - alphas_cumprod))
        register_buffer(&quotlog_one_minus_alphas_cumprod&quot, torch.log(1. - alphas_cumprod))
        register_buffer(&quotsqrt_recip_alphas_cumprod&quot, torch.sqrt(1. / alphas_cumprod))
        register_buffer(&quotsqrt_recipm1_alphas_cumprod&quot, torch.sqrt(1. / alphas_cumprod - 1))

        &#47&#47 calculations for posterior q(x_{t-1} | x_t, x_0)

        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)

        &#47&#47 above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)

        register_buffer(&quotposterior_variance&quot, posterior_variance)

        &#47&#47 below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain

        register_buffer(&quotposterior_log_variance_clipped&quot, torch.log(posterior_variance.clamp(min =1e-20)))
        register_buffer(&quotposterior_mean_coef1&quot, betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))
        register_buffer(&quotposterior_mean_coef2&quot, (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))

        &#47&#47 calculate p2 reweighting

        <a id="change">register_buffer(&quotp2_loss_weight&quot</a>, (p2_loss_weight_k<a id="change"> + </a>alphas_cumprod<a id="change"> / </a>(1<a id="change"> - </a>alphas_cumprod))<a id="change"> ** </a><a id="change">-</a>p2_loss_weight_gamma<a id="change">)</a>

        &#47&#47 auto-normalization of data [0, 1] -&gt; [-1, 1] - can turn off by setting it to be False

        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity</code></pre><h3>After Change</h3><pre><code class='java'>
        betas = beta_schedule_fn(timesteps, **schedule_fn_kwargs)

        alphas = 1. - betas
        <a id="change">alphas_cumprod</a> = torch.cumprod(alphas, dim=0)
        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)

        timesteps, = betas.shape
        self.num_timesteps = int(timesteps)
        self.loss_type = loss_type

        &#47&#47 sampling related parameters

        self.sampling_timesteps = default(sampling_timesteps, timesteps) &#47&#47 default num sampling timesteps to number of timesteps at training

        assert self.sampling_timesteps &lt;= timesteps
        self.is_ddim_sampling = self.sampling_timesteps &lt; timesteps
        self.ddim_sampling_eta = ddim_sampling_eta

        &#47&#47 helper function to register buffer from float64 to float32

        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))

        register_buffer(&quotbetas&quot, betas)
        register_buffer(&quotalphas_cumprod&quot, alphas_cumprod)
        register_buffer(&quotalphas_cumprod_prev&quot, alphas_cumprod_prev)

        &#47&#47 calculations for diffusion q(x_t | x_{t-1}) and others

        register_buffer(&quotsqrt_alphas_cumprod&quot, torch.sqrt(alphas_cumprod))
        register_buffer(&quotsqrt_one_minus_alphas_cumprod&quot, torch.sqrt(1. - alphas_cumprod))
        register_buffer(&quotlog_one_minus_alphas_cumprod&quot, torch.log(1. - alphas_cumprod))
        register_buffer(&quotsqrt_recip_alphas_cumprod&quot, torch.sqrt(1. / alphas_cumprod))
        register_buffer(&quotsqrt_recipm1_alphas_cumprod&quot, torch.sqrt(1. / alphas_cumprod - 1))

        &#47&#47 calculations for posterior q(x_{t-1} | x_t, x_0)

        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)

        &#47&#47 above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)

        register_buffer(&quotposterior_variance&quot, posterior_variance)

        &#47&#47 below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain

        register_buffer(&quotposterior_log_variance_clipped&quot, torch.log(posterior_variance.clamp(min =1e-20)))
        register_buffer(&quotposterior_mean_coef1&quot, betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))
        register_buffer(&quotposterior_mean_coef2&quot, (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))

        &#47&#47 loss weight

        <a id="change">snr</a><a id="change"> = </a>alphas_cumprod<a id="change"> / </a>(1<a id="change"> - </a>alphas_cumprod)

        <a id="change">maybe_clipped_snr = snr.clone()</a>
        <a id="change">if min_snr_loss_weight</a>:
            <a id="change">maybe_clipped_snr.clamp_(min = min_snr_gamma)</a>

        <a id="change">if objective == &quotpred_noise&quot</a>:
            loss_weight = maybe_clipped_snr<a id="change"> / </a>snr
        elif <a id="change">objective == &quotpred_x0&quot</a>:
            loss_weight = maybe_clipped_snr
        elif <a id="change">objective == &quotpred_v&quot</a>:
            loss_weight = maybe_clipped_snr<a id="change"> / </a>(snr<a id="change"> + 1</a>)

        register_buffer(&quotloss_weight&quot, loss_weight)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/denoising-diffusion-pytorch/commit/58717e5faaafcc73a7f8569d5c40dc7d6ab3e804#diff-e1013dc666ef2b866c9c7ee43c2132394faaca402ed0e6e3a7c2d142c8cb9bbfL442' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 61709804</div><div id='project'> Project Name: lucidrains/denoising-diffusion-pytorch</div><div id='commit'> Commit Name: 58717e5faaafcc73a7f8569d5c40dc7d6ab3e804</div><div id='time'> Time: 2023-03-17</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: denoising_diffusion_pytorch/guided_diffusion.py</div><div id='m_class'> M Class Name: GaussianDiffusion</div><div id='n_method'> N Class Name: GaussianDiffusion</div><div id='m_method'> M Method Name: __init__(2)</div><div id='n_method'> N Method Name: __init__(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: denoising_diffusion_pytorch/guided_diffusion.py</div><div id='n_file'> N File Name: denoising_diffusion_pytorch/guided_diffusion.py</div><div id='m_start'> M Start Line: 484</div><div id='m_end'> M End Line: 531</div><div id='n_start'> N Start Line: 484</div><div id='n_end'> N End Line: 544</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        betas = beta_schedule_fn(timesteps, **schedule_fn_kwargs)

        alphas = 1. - betas
        <a id="change">alphas_cumprod</a> = torch.cumprod(alphas, dim=0)
        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)

        timesteps, = betas.shape
        self.num_timesteps = int(timesteps)
        self.loss_type = loss_type

        &#47&#47 sampling related parameters

        self.sampling_timesteps = default(sampling_timesteps, timesteps) &#47&#47 default num sampling timesteps to number of timesteps at training

        assert self.sampling_timesteps &lt;= timesteps
        self.is_ddim_sampling = self.sampling_timesteps &lt; timesteps
        self.ddim_sampling_eta = ddim_sampling_eta

        &#47&#47 helper function to register buffer from float64 to float32

        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))

        register_buffer(&quotbetas&quot, betas)
        register_buffer(&quotalphas_cumprod&quot, alphas_cumprod)
        register_buffer(&quotalphas_cumprod_prev&quot, alphas_cumprod_prev)

        &#47&#47 calculations for diffusion q(x_t | x_{t-1}) and others

        register_buffer(&quotsqrt_alphas_cumprod&quot, torch.sqrt(alphas_cumprod))
        register_buffer(&quotsqrt_one_minus_alphas_cumprod&quot, torch.sqrt(1. - alphas_cumprod))
        register_buffer(&quotlog_one_minus_alphas_cumprod&quot, torch.log(1. - alphas_cumprod))
        register_buffer(&quotsqrt_recip_alphas_cumprod&quot, torch.sqrt(1. / alphas_cumprod))
        register_buffer(&quotsqrt_recipm1_alphas_cumprod&quot, torch.sqrt(1. / alphas_cumprod - 1))

        &#47&#47 calculations for posterior q(x_{t-1} | x_t, x_0)

        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)

        &#47&#47 above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)

        register_buffer(&quotposterior_variance&quot, posterior_variance)

        &#47&#47 below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain

        register_buffer(&quotposterior_log_variance_clipped&quot, torch.log(posterior_variance.clamp(min =1e-20)))
        register_buffer(&quotposterior_mean_coef1&quot, betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))
        register_buffer(&quotposterior_mean_coef2&quot, (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))

        &#47&#47 calculate p2 reweighting

        <a id="change">register_buffer(&quotp2_loss_weight&quot</a>, (p2_loss_weight_k<a id="change"> + </a>alphas_cumprod<a id="change"> / </a>(1<a id="change"> - </a>alphas_cumprod))<a id="change"> ** </a><a id="change">-</a>p2_loss_weight_gamma<a id="change">)</a>

        &#47&#47 auto-normalization of data [0, 1] -&gt; [-1, 1] - can turn off by setting it to be False

        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity</code></pre><h3>After Change</h3><pre><code class='java'>
        betas = beta_schedule_fn(timesteps, **schedule_fn_kwargs)

        alphas = 1. - betas
        <a id="change">alphas_cumprod</a> = torch.cumprod(alphas, dim=0)
        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)

        timesteps, = betas.shape
        self.num_timesteps = int(timesteps)
        self.loss_type = loss_type

        &#47&#47 sampling related parameters

        self.sampling_timesteps = default(sampling_timesteps, timesteps) &#47&#47 default num sampling timesteps to number of timesteps at training

        assert self.sampling_timesteps &lt;= timesteps
        self.is_ddim_sampling = self.sampling_timesteps &lt; timesteps
        self.ddim_sampling_eta = ddim_sampling_eta

        &#47&#47 helper function to register buffer from float64 to float32

        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))

        register_buffer(&quotbetas&quot, betas)
        register_buffer(&quotalphas_cumprod&quot, alphas_cumprod)
        register_buffer(&quotalphas_cumprod_prev&quot, alphas_cumprod_prev)

        &#47&#47 calculations for diffusion q(x_t | x_{t-1}) and others

        register_buffer(&quotsqrt_alphas_cumprod&quot, torch.sqrt(alphas_cumprod))
        register_buffer(&quotsqrt_one_minus_alphas_cumprod&quot, torch.sqrt(1. - alphas_cumprod))
        register_buffer(&quotlog_one_minus_alphas_cumprod&quot, torch.log(1. - alphas_cumprod))
        register_buffer(&quotsqrt_recip_alphas_cumprod&quot, torch.sqrt(1. / alphas_cumprod))
        register_buffer(&quotsqrt_recipm1_alphas_cumprod&quot, torch.sqrt(1. / alphas_cumprod - 1))

        &#47&#47 calculations for posterior q(x_{t-1} | x_t, x_0)

        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)

        &#47&#47 above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)

        register_buffer(&quotposterior_variance&quot, posterior_variance)

        &#47&#47 below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain

        register_buffer(&quotposterior_log_variance_clipped&quot, torch.log(posterior_variance.clamp(min =1e-20)))
        register_buffer(&quotposterior_mean_coef1&quot, betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))
        register_buffer(&quotposterior_mean_coef2&quot, (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))

        &#47&#47 derive loss weight
        &#47&#47 snr - signal noise ratio

        <a id="change">snr</a><a id="change"> = </a>alphas_cumprod<a id="change"> / </a>(1<a id="change"> - </a>alphas_cumprod)

        &#47&#47 https://arxiv.org/abs/2303.09556

        <a id="change">maybe_clipped_snr = snr.clone()</a>
        <a id="change">if min_snr_loss_weight</a>:
            <a id="change">maybe_clipped_snr.clamp_(min = min_snr_gamma)</a>

        <a id="change">if objective == &quotpred_noise&quot</a>:
            register_buffer(&quotloss_weight&quot, maybe_clipped_snr<a id="change"> / </a>snr)
        elif <a id="change">objective == &quotpred_x0&quot</a>:
            register_buffer(&quotloss_weight&quot, maybe_clipped_snr)
        elif <a id="change">objective == &quotpred_v&quot</a>:
            register_buffer(&quotloss_weight&quot, maybe_clipped_snr<a id="change"> / </a>(snr<a id="change"> + 1</a>))

        &#47&#47 auto-normalization of data [0, 1] -&gt; [-1, 1] - can turn off by setting it to be False
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/denoising-diffusion-pytorch/commit/81ef4e4955f0a87a374aaea09a9b47bd3f7a507b#diff-1ef0846017d5c27bd277b66e5afcab8798667afd39675e6f9382fc7e87a1fe28L444' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 61709806</div><div id='project'> Project Name: lucidrains/denoising-diffusion-pytorch</div><div id='commit'> Commit Name: 81ef4e4955f0a87a374aaea09a9b47bd3f7a507b</div><div id='time'> Time: 2023-03-17</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: denoising_diffusion_pytorch/denoising_diffusion_pytorch.py</div><div id='m_class'> M Class Name: GaussianDiffusion</div><div id='n_method'> N Class Name: GaussianDiffusion</div><div id='m_method'> M Method Name: __init__(2)</div><div id='n_method'> N Method Name: __init__(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: denoising_diffusion_pytorch/denoising_diffusion_pytorch.py</div><div id='n_file'> N File Name: denoising_diffusion_pytorch/denoising_diffusion_pytorch.py</div><div id='m_start'> M Start Line: 487</div><div id='m_end'> M End Line: 534</div><div id='n_start'> N Start Line: 487</div><div id='n_end'> N End Line: 552</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            raise ValueError(f&quotunknown beta schedule {beta_schedule}&quot)

        alphas = 1. - betas
        <a id="change">alphas_cumprod</a> = torch.cumprod(alphas, dim=0)
        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)

        timesteps, = betas.shape
        self.num_timesteps = int(timesteps)
        self.loss_type = loss_type

        &#47&#47 sampling related parameters

        self.sampling_timesteps = default(sampling_timesteps, timesteps) &#47&#47 default num sampling timesteps to number of timesteps at training

        assert self.sampling_timesteps &lt;= timesteps
        self.is_ddim_sampling = self.sampling_timesteps &lt; timesteps
        self.ddim_sampling_eta = ddim_sampling_eta

        &#47&#47 helper function to register buffer from float64 to float32

        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))

        register_buffer(&quotbetas&quot, betas)
        register_buffer(&quotalphas_cumprod&quot, alphas_cumprod)
        register_buffer(&quotalphas_cumprod_prev&quot, alphas_cumprod_prev)

        &#47&#47 calculations for diffusion q(x_t | x_{t-1}) and others

        register_buffer(&quotsqrt_alphas_cumprod&quot, torch.sqrt(alphas_cumprod))
        register_buffer(&quotsqrt_one_minus_alphas_cumprod&quot, torch.sqrt(1. - alphas_cumprod))
        register_buffer(&quotlog_one_minus_alphas_cumprod&quot, torch.log(1. - alphas_cumprod))
        register_buffer(&quotsqrt_recip_alphas_cumprod&quot, torch.sqrt(1. / alphas_cumprod))
        register_buffer(&quotsqrt_recipm1_alphas_cumprod&quot, torch.sqrt(1. / alphas_cumprod - 1))

        &#47&#47 calculations for posterior q(x_{t-1} | x_t, x_0)

        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)

        &#47&#47 above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)

        register_buffer(&quotposterior_variance&quot, posterior_variance)

        &#47&#47 below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain

        register_buffer(&quotposterior_log_variance_clipped&quot, torch.log(posterior_variance.clamp(min =1e-20)))
        register_buffer(&quotposterior_mean_coef1&quot, betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))
        register_buffer(&quotposterior_mean_coef2&quot, (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))

        &#47&#47 calculate p2 reweighting

        <a id="change">register_buffer(&quotp2_loss_weight&quot</a>, (p2_loss_weight_k<a id="change"> + </a>alphas_cumprod<a id="change"> / </a>(1<a id="change"> - </a>alphas_cumprod))<a id="change"> ** </a><a id="change">-</a>p2_loss_weight_gamma<a id="change">)</a>

    def predict_start_from_noise(self, x_t, t, noise):
        return (
            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -</code></pre><h3>After Change</h3><pre><code class='java'>
            raise ValueError(f&quotunknown beta schedule {beta_schedule}&quot)

        alphas = 1. - betas
        <a id="change">alphas_cumprod</a> = torch.cumprod(alphas, dim=0)
        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)

        timesteps, = betas.shape
        self.num_timesteps = int(timesteps)
        self.loss_type = loss_type

        &#47&#47 sampling related parameters

        self.sampling_timesteps = default(sampling_timesteps, timesteps) &#47&#47 default num sampling timesteps to number of timesteps at training

        assert self.sampling_timesteps &lt;= timesteps
        self.is_ddim_sampling = self.sampling_timesteps &lt; timesteps
        self.ddim_sampling_eta = ddim_sampling_eta

        &#47&#47 helper function to register buffer from float64 to float32

        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))

        register_buffer(&quotbetas&quot, betas)
        register_buffer(&quotalphas_cumprod&quot, alphas_cumprod)
        register_buffer(&quotalphas_cumprod_prev&quot, alphas_cumprod_prev)

        &#47&#47 calculations for diffusion q(x_t | x_{t-1}) and others

        register_buffer(&quotsqrt_alphas_cumprod&quot, torch.sqrt(alphas_cumprod))
        register_buffer(&quotsqrt_one_minus_alphas_cumprod&quot, torch.sqrt(1. - alphas_cumprod))
        register_buffer(&quotlog_one_minus_alphas_cumprod&quot, torch.log(1. - alphas_cumprod))
        register_buffer(&quotsqrt_recip_alphas_cumprod&quot, torch.sqrt(1. / alphas_cumprod))
        register_buffer(&quotsqrt_recipm1_alphas_cumprod&quot, torch.sqrt(1. / alphas_cumprod - 1))

        &#47&#47 calculations for posterior q(x_{t-1} | x_t, x_0)

        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)

        &#47&#47 above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)

        register_buffer(&quotposterior_variance&quot, posterior_variance)

        &#47&#47 below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain

        register_buffer(&quotposterior_log_variance_clipped&quot, torch.log(posterior_variance.clamp(min =1e-20)))
        register_buffer(&quotposterior_mean_coef1&quot, betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))
        register_buffer(&quotposterior_mean_coef2&quot, (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))

        &#47&#47 loss weight

        <a id="change">snr</a><a id="change"> = </a>alphas_cumprod<a id="change"> / </a>(1<a id="change"> - </a>alphas_cumprod)

        <a id="change">maybe_clipped_snr = snr.clone()</a>
        <a id="change">if min_snr_loss_weight</a>:
            <a id="change">maybe_clipped_snr.clamp_(min = min_snr_gamma)</a>

        <a id="change">if objective == &quotpred_noise&quot</a>:
            loss_weight = maybe_clipped_snr<a id="change"> / </a>snr
        elif <a id="change">objective == &quotpred_x0&quot</a>:
            loss_weight = maybe_clipped_snr
        elif <a id="change">objective == &quotpred_v&quot</a>:
            loss_weight = maybe_clipped_snr<a id="change"> / </a>(snr<a id="change"> + 1</a>)

        register_buffer(&quotloss_weight&quot, loss_weight)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/denoising-diffusion-pytorch/commit/58717e5faaafcc73a7f8569d5c40dc7d6ab3e804#diff-4701f6335fabcf514faf3fc9949d5a43d041f7e4e8c137cd5fdd22d74fde7eb2L479' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 61709801</div><div id='project'> Project Name: lucidrains/denoising-diffusion-pytorch</div><div id='commit'> Commit Name: 58717e5faaafcc73a7f8569d5c40dc7d6ab3e804</div><div id='time'> Time: 2023-03-17</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: denoising_diffusion_pytorch/classifier_free_guidance.py</div><div id='m_class'> M Class Name: GaussianDiffusion</div><div id='n_method'> N Class Name: GaussianDiffusion</div><div id='m_method'> M Method Name: __init__(2)</div><div id='n_method'> N Method Name: __init__(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: denoising_diffusion_pytorch/classifier_free_guidance.py</div><div id='n_file'> N File Name: denoising_diffusion_pytorch/classifier_free_guidance.py</div><div id='m_start'> M Start Line: 514</div><div id='m_end'> M End Line: 561</div><div id='n_start'> N Start Line: 514</div><div id='n_end'> N End Line: 574</div><BR>