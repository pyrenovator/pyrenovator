<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        exp_avg_diff = exp_avg_diffs[i]
        pre_grad = pre_grads[i]

        <a id="change">grad = </a><a id="change">grad.mul_(</a>clip_global_grad_norm<a id="change">)</a>
        copy_grads.append(<a id="change">grad.clone()</a>)
        
        diff<a id="change"> = grad</a><a id="change"> - pre_grad</a>
        update<a id="change"> = grad</a><a id="change"> + beta2</a><a id="change"> * </a>diff
        
        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  &#47&#47 m_t
        exp_avg_diff.mul_(beta2).add_(diff, alpha=1 - beta2)  &#47&#47 diff_t
        exp_avg_sq.mul_(beta3).addcmul_(update, update, value=1 - beta3)  &#47&#47 n_t

        denom = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)
        update = <a id="change">((exp_avg / bias_correction1 + beta2 * exp_avg_diff / bias_correction2)).div_(denom</a><a id="change">)</a>

        if no_prox:
            param.mul_(1 - lr * weight_decay)
            param.add_(update, alpha=-lr)</code></pre><h3>After Change</h3><pre><code class='java'>
        exp_avg_diff = exp_avg_diffs[i]
        neg_grad_or_diff = neg_pre_grads[i]

        <a id="change">grad.mul_(</a>clip_global_grad_norm<a id="change">)</a>

        &#47&#47 for memory saving, we use `neg_grad_or_diff`
        &#47&#47 to get some temp variable in a inplace way
        neg_grad_or_diff.add_(grad)

        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  &#47&#47 m_t
        exp_avg_diff.mul_(beta2).add_(neg_grad_or_diff,
                                      alpha=1 - beta2)  &#47&#47 diff_t

        <a id="change">neg_grad_or_diff.mul_(beta2).add_(grad</a><a id="change">)</a>
        exp_avg_sq.mul_(beta3).addcmul_(neg_grad_or_diff,
                                        neg_grad_or_diff,
                                        value=1 - beta3)  &#47&#47 n_t

        <a id="change">denom</a> = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)
        step_size_diff<a id="change"> = lr</a><a id="change"> * beta2 / </a>bias_correction2
        step_size = <a id="change">lr</a><a id="change"> / </a>bias_correction1

        if no_prox:
            param.mul_(1 - lr * weight_decay)
            param.addcdiv_(exp_avg, denom, value=-step_size)
            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)
        else:
            <a id="change">param.addcdiv_(</a>exp_avg, <a id="change">denom</a><a id="change">, value=-step_size)</a>
            <a id="change">param.addcdiv_(</a>exp_avg_diff, <a id="change">denom</a><a id="change">, value=-step_size_diff)</a>
            param.div_(1 + lr * weight_decay)

        <a id="change">neg_grad_or_diff.zero_().add_(grad</a><a id="change">, alpha=-1.0)</a>


def _multi_tensor_adan(
    params: List[Tensor],</code></pre>