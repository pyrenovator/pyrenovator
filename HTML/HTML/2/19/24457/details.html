<html><h3>Pattern ID :24457
</h3><img src='76010693.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            output_ = output_.cuda()
            output_.spec.dist_spec.process_group = self.process_group
        output = output_.convert_to_dist_spec(distspec.shard(self.process_group, [0], [self.world_size]))
        <a id="change">return </a>output


class FusedDenseModules(nn.Module):</code></pre><h3>After Change</h3><pre><code class='java'>
        self.world_size = gpc.get_world_size(parallel_mode)

    def forward(self, sparse_features):
        keys = <a id="change">sparse_features.keys()</a>
        assert len(keys) == len(self.offsets), f"keys len: {len(keys)}, offsets len: {len(self.offsets)}"

        <a id="change">sparse_dict</a><a id="change"> = sparse_features</a><a id="change">.to_dict()</a>
        flattened_sparse_features<a id="change"> = torch.cat(
            </a><a id="change">[sparse_dict[key].values() + offset for key, offset in zip(keys, self.offsets)])</a>
        batch_offsets<a id="change"> = sparse_features</a><a id="change">.offsets()</a>

        batch_size = len(sparse_features.lengths()) // len(keys)
        flattened_sparse_features<a id="change"> = </a>self.embed(flattened_sparse_features, batch_offsets)

        if self.output_device == &quotcuda&quot and self.offsets.device.type == &quotcpu&quot:
            flattened_sparse_features = flattened_sparse_features.cuda()
            flattened_sparse_features.spec.dist_spec.process_group = self.group
        &#47&#47 [batch size * feature size, hidden size / world size]
        &#47&#47 it must convert to [batch size, feature size, hidden size / world size] before all to all
        flattened_sparse_features = flattened_sparse_features.view(batch_size, len(keys), -1)
        &#47&#47 print(f"before shape: {flattened_sparse_features.shape}, spec: {flattened_sparse_features.spec.dist_spec}")
        flattened_sparse_features = flattened_sparse_features.convert_to_dist_spec(
            distspec.shard(self.group, [0], [self.world_size]))
        &#47&#47 print(f"after shape: {flattened_sparse_features.shape}, spec: {flattened_sparse_features.spec.dist_spec}")
        <a id="change">return </a>flattened_sparse_features


class FusedDenseModules(nn.Module):</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 17</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/hpcaitech/cachedembedding/commit/54937021916b3d17347b193c2dddbace875ec95f#diff-51b25fbeb15d0629aa2192982cd705f7df76644d237674b1143984f99f7a1d4fL100' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 76010693</div><div id='project'> Project Name: hpcaitech/cachedembedding</div><div id='commit'> Commit Name: 54937021916b3d17347b193c2dddbace875ec95f</div><div id='time'> Time: 2022-06-23</div><div id='author'> Author: zhangg1998@outlook.com</div><div id='file'> File Name: colo_recsys/models/colossal_dlrm.py</div><div id='m_class'> M Class Name: FusedSparseModules</div><div id='n_method'> N Class Name: FusedSparseModules</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: SparseArch</div><div id='m_file'> M File Name: colo_recsys/models/colossal_dlrm.py</div><div id='n_file'> N File Name: colo_recsys/models/colossal_dlrm.py</div><div id='m_start'> M Start Line: 100</div><div id='m_end'> M End Line: 106</div><div id='n_start'> N Start Line: 135</div><div id='n_end'> N End Line: 156</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 print(&quotSaved params (M)&quot,emb_dim*(sum(field_dims) - math.ceil(math.sqrt(sum(field_dims))))//1_000_000)

    def forward(self,x):
        <a id="change">return </a>self.embedding(x)
    

class FeatureLinear(nn.Module):</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 print(&quotSaved params (M)&quot,emb_dim*(sum(field_dims) - math.ceil(math.sqrt(sum(field_dims))))//1_000_000)

    def forward(self,sparse_features):
        keys = <a id="change">sparse_features.keys()</a>
        assert len(keys) == len(self.offsets), f"keys len: {len(keys)}, offsets len: {len(self.offsets)}"

        <a id="change">sparse_dict</a><a id="change"> = </a><a id="change">sparse_features.to_dict()</a>
        flattened_sparse_features<a id="change"> = torch.cat(
            </a><a id="change">[sparse_dict[key].values() + offset for key, offset in zip(keys, self.offsets)])</a>
        batch_offsets<a id="change"> = </a><a id="change">sparse_features.offsets()</a>

        batch_size = len(sparse_features.lengths()) // len(keys)
        feature_size = len(keys)
        flattened_sparse_embeddings<a id="change"> = </a>self.embedding(flattened_sparse_features,
                                                 batch_offsets,
                                                 send_shape=(batch_size, feature_size, -1))
        <a id="change">return </a>flattened_sparse_embeddings
    

class FeatureLinear(nn.Module):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/hpcaitech/freqcacheembedding/commit/c4a062cbe73906f8701b27e27d66140903fd8da9#diff-36c7253ab28f5345c9fc415cb857f171b7764514fd0c3a81ac3447db983def4aL22' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 76010709</div><div id='project'> Project Name: hpcaitech/freqcacheembedding</div><div id='commit'> Commit Name: c4a062cbe73906f8701b27e27d66140903fd8da9</div><div id='time'> Time: 2022-07-14</div><div id='author'> Author: jiatong.han@u.nus.edu</div><div id='file'> File Name: recsys/models/deepfm.py</div><div id='m_class'> M Class Name: FeatureEmbedding</div><div id='n_method'> N Class Name: FeatureEmbedding</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: recsys/models/deepfm.py</div><div id='n_file'> N File Name: recsys/models/deepfm.py</div><div id='m_start'> M Start Line: 23</div><div id='m_end'> M End Line: 23</div><div id='n_start'> N Start Line: 23</div><div id='n_end'> N End Line: 36</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            output_ = output_.cuda()
            output_.spec.dist_spec.process_group = self.process_group
        output = output_.convert_to_dist_spec(distspec.shard(self.process_group, [0], [self.world_size]))
        <a id="change">return </a>output


class FusedDenseModules(nn.Module):</code></pre><h3>After Change</h3><pre><code class='java'>
        self.world_size = gpc.get_world_size(parallel_mode)

    def forward(self, sparse_features):
        keys = <a id="change">sparse_features.keys()</a>
        assert len(keys) == len(self.offsets), f"keys len: {len(keys)}, offsets len: {len(self.offsets)}"

        <a id="change">sparse_dict</a><a id="change"> = </a><a id="change">sparse_features.to_dict()</a>
        flattened_sparse_features<a id="change"> = torch.cat(
            </a><a id="change">[sparse_dict[key].values() + offset for key, offset in zip(keys, self.offsets)])</a>
        batch_offsets<a id="change"> = </a><a id="change">sparse_features.offsets()</a>

        batch_size = len(sparse_features.lengths()) // len(keys)
        flattened_sparse_features = self.embed(flattened_sparse_features, batch_offsets)

        if self.output_device == &quotcuda&quot and self.offsets.device.type == &quotcpu&quot:
            flattened_sparse_features = flattened_sparse_features.cuda()
            flattened_sparse_features.spec.dist_spec.process_group = self.group
        &#47&#47 [batch size * feature size, hidden size / world size]
        &#47&#47 it must convert to [batch size, feature size, hidden size / world size] before all to all
        flattened_sparse_features<a id="change"> = </a>flattened_sparse_features.view(batch_size, len(keys), -1)
        &#47&#47 print(f"before shape: {flattened_sparse_features.shape}, spec: {flattened_sparse_features.spec.dist_spec}")
        flattened_sparse_features = flattened_sparse_features.convert_to_dist_spec(
            distspec.shard(self.group, [0], [self.world_size]))
        &#47&#47 print(f"after shape: {flattened_sparse_features.shape}, spec: {flattened_sparse_features.spec.dist_spec}")
        <a id="change">return </a>flattened_sparse_features


class FusedDenseModules(nn.Module):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/hpcaitech/cachedembedding/commit/54937021916b3d17347b193c2dddbace875ec95f#diff-51b25fbeb15d0629aa2192982cd705f7df76644d237674b1143984f99f7a1d4fL99' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 76010692</div><div id='project'> Project Name: hpcaitech/cachedembedding</div><div id='commit'> Commit Name: 54937021916b3d17347b193c2dddbace875ec95f</div><div id='time'> Time: 2022-06-23</div><div id='author'> Author: zhangg1998@outlook.com</div><div id='file'> File Name: colo_recsys/models/colossal_dlrm.py</div><div id='m_class'> M Class Name: FusedSparseModules</div><div id='n_method'> N Class Name: FusedSparseModules</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: SparseArch</div><div id='m_file'> M File Name: colo_recsys/models/colossal_dlrm.py</div><div id='n_file'> N File Name: colo_recsys/models/colossal_dlrm.py</div><div id='m_start'> M Start Line: 100</div><div id='m_end'> M End Line: 106</div><div id='n_start'> N Start Line: 135</div><div id='n_end'> N End Line: 156</div><BR>