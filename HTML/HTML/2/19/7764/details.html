<html><h3>Pattern ID :7764
</h3><img src='27773255.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

class ResNet(nn.Module):
    def __init__(self) -&gt; None:
        <a id="change">super(</a>ResNet, self<a id="change">)</a>.__init__()
        self.backbone = resnet.resnet18()

    def forward(self, x):</code></pre><h3>After Change</h3><pre><code class='java'>

        self.in_channels = 64

        self.conv1<a id="change"> = nn</a><a id="change">.Sequential(
            nn.Conv2d(3</a>, <a id="change">64</a><a id="change">, kernel_size=3, padding=1, bias=False)</a>,
            <a id="change">nn.BatchNorm2d(64</a><a id="change">)</a>,
            <a id="change">nn.ReLU(inplace=True)</a><a id="change">)</a>
        &#47&#47we use a different inputsize than the original paper
        &#47&#47so conv2_x&quots stride is 1
        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)
        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)
        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)
        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)
        self.avg_pool<a id="change"> = nn</a><a id="change">.AdaptiveAvgPool2d(</a>(<a id="change">1</a><a id="change">, 1</a>)<a id="change">)</a>
        self.fc<a id="change"> = nn</a><a id="change">.Linear(512</a><a id="change"> * </a>block.expansion, num_classes<a id="change">)</a>

    def _make_layer(self, block, out_channels, num_blocks, stride):
        make resnet layers(by layer i didnt mean this &quotlayer&quot was the
        same as a neuron netowork layer, ex. conv layer), one layer may</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 12</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/shaoeric/torch-atom/commit/87af6f2da4a9d8d8cd3710b0c643085ab8abbec6#diff-32f9903dd9a14d37c5300ee56d88ce85da3b030c80d54d5b026b73976800555cL11' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 27773255</div><div id='project'> Project Name: shaoeric/torch-atom</div><div id='commit'> Commit Name: 87af6f2da4a9d8d8cd3710b0c643085ab8abbec6</div><div id='time'> Time: 2022-03-06</div><div id='author'> Author: shaoeric@foxmail.com</div><div id='file'> File Name: src/models/resnet.py</div><div id='m_class'> M Class Name: ResNet</div><div id='n_method'> N Class Name: ResNet</div><div id='m_method'> M Method Name: __init__(4)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/models/resnet.py</div><div id='n_file'> N File Name: src/models/resnet.py</div><div id='m_start'> M Start Line: 11</div><div id='m_end'> M End Line: 13</div><div id='n_start'> N Start Line: 82</div><div id='n_end'> N End Line: 98</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

class Model(ResNet):
    def __init__(self):
        <a id="change">super(</a>Model, self<a id="change">)</a>.__init__(BasicBlock,[2,2,2,2])

class Loss(nn.Module):
    def __init__(self):</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self, block=BasicBlock, num_block=[2,2,2,2], num_classes=10):
        super().__init__()
        self.in_channels = 64
        self.conv1<a id="change"> = </a><a id="change">nn.Sequential(
            nn.Conv2d(3</a>, <a id="change">64</a><a id="change">, kernel_size=3, padding=1, bias=False)</a>,
            <a id="change">nn.BatchNorm2d(64</a><a id="change">)</a>,
            <a id="change">nn.ReLU(inplace=True)</a><a id="change">)</a>
        &#47&#47we use a different inputsize than the original paper
        &#47&#47so conv2_x&quots stride is 1
        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)
        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)
        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)
        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)
        self.avg_pool<a id="change"> = </a><a id="change">nn.AdaptiveAvgPool2d(</a>(<a id="change">1</a><a id="change">, 1</a>)<a id="change">)</a>
        self.fc<a id="change"> = </a><a id="change">nn.Linear(512</a><a id="change"> * </a>block.expansion, num_classes<a id="change">)</a>

    def _make_layer(self, block, out_channels, num_blocks, stride):
        make resnet layers(by layer i didnt mean this &quotlayer&quot was the
        same as a neuron netowork layer, ex. conv layer), one layer may</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/wwzzz/easyfl/commit/ddf7d4db52a22b726ed3c58c0d4f2638f9d22ed8#diff-52eea8bf07e92c026d50c9ab8c8273f354b717a159053d14df333981119ec139L135' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 27773254</div><div id='project'> Project Name: wwzzz/easyfl</div><div id='commit'> Commit Name: ddf7d4db52a22b726ed3c58c0d4f2638f9d22ed8</div><div id='time'> Time: 2021-08-25</div><div id='author'> Author: zwang@stu.xmu.edu.cn</div><div id='file'> File Name: benchmark/cifar10/model/resnet18.py</div><div id='m_class'> M Class Name: Model</div><div id='n_method'> N Class Name: Model</div><div id='m_method'> M Method Name: __init__(4)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: FModule</div><div id='n_parent_class'> N Parent Class: ResNet</div><div id='m_file'> M File Name: benchmark/cifar10/model/resnet18.py</div><div id='n_file'> N File Name: benchmark/cifar10/model/resnet18.py</div><div id='m_start'> M Start Line: 135</div><div id='m_end'> M End Line: 136</div><div id='n_start'> N Start Line: 80</div><div id='n_end'> N End Line: 94</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

class Model(ResNet):
    def __init__(self):
        <a id="change">super(</a>Model, self<a id="change">)</a>.__init__(BasicBlock,[2,2,2,2])

class Loss(nn.Module):
    def __init__(self):</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self, block=BasicBlock, num_block=[2,2,2,2], num_classes=100):
        super().__init__()
        self.in_channels = 64
        self.conv1<a id="change"> = </a><a id="change">nn.Sequential(
            nn.Conv2d(3</a>, <a id="change">64</a><a id="change">, kernel_size=3, padding=1, bias=False)</a>,
            <a id="change">nn.BatchNorm2d(64</a><a id="change">)</a>,
            <a id="change">nn.ReLU(inplace=True)</a><a id="change">)</a>
        &#47&#47we use a different inputsize than the original paper
        &#47&#47so conv2_x&quots stride is 1
        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)
        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)
        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)
        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)
        self.avg_pool<a id="change"> = </a><a id="change">nn.AdaptiveAvgPool2d(</a>(<a id="change">1</a><a id="change">, 1</a>)<a id="change">)</a>
        self.fc<a id="change"> = </a><a id="change">nn.Linear(512</a><a id="change"> * </a>block.expansion, num_classes<a id="change">)</a>

    def _make_layer(self, block, out_channels, num_blocks, stride):
        make resnet layers(by layer i didnt mean this &quotlayer&quot was the
        same as a neuron netowork layer, ex. conv layer), one layer may</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/wwzzz/easyfl/commit/ddf7d4db52a22b726ed3c58c0d4f2638f9d22ed8#diff-0ccfc1fe340c1c377b71ad61cb577828def8c8dd26f612507360d5c44170168cL137' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 27773252</div><div id='project'> Project Name: wwzzz/easyfl</div><div id='commit'> Commit Name: ddf7d4db52a22b726ed3c58c0d4f2638f9d22ed8</div><div id='time'> Time: 2021-08-25</div><div id='author'> Author: zwang@stu.xmu.edu.cn</div><div id='file'> File Name: benchmark/cifar100/model/resnet18.py</div><div id='m_class'> M Class Name: Model</div><div id='n_method'> N Class Name: Model</div><div id='m_method'> M Method Name: __init__(4)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: FModule</div><div id='n_parent_class'> N Parent Class: ResNet</div><div id='m_file'> M File Name: benchmark/cifar100/model/resnet18.py</div><div id='n_file'> N File Name: benchmark/cifar100/model/resnet18.py</div><div id='m_start'> M Start Line: 137</div><div id='m_end'> M End Line: 138</div><div id='n_start'> N Start Line: 80</div><div id='n_end'> N End Line: 94</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

class ResNet(nn.Module):
    def __init__(self) -&gt; None:
        <a id="change">super(</a>ResNet, self<a id="change">)</a>.__init__()
        self.backbone = resnet.resnet18()

    def forward(self, x):</code></pre><h3>After Change</h3><pre><code class='java'>

        self.in_channels = 64

        self.conv1<a id="change"> = </a><a id="change">nn.Sequential(
            nn.Conv2d(3</a>, <a id="change">64</a><a id="change">, kernel_size=3, padding=1, bias=False)</a>,
            <a id="change">nn.BatchNorm2d(64</a><a id="change">)</a>,
            <a id="change">nn.ReLU(inplace=True)</a><a id="change">)</a>
        &#47&#47we use a different inputsize than the original paper
        &#47&#47so conv2_x&quots stride is 1
        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)
        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)
        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)
        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)
        self.avg_pool<a id="change"> = </a><a id="change">nn.AdaptiveAvgPool2d(</a>(<a id="change">1</a><a id="change">, 1</a>)<a id="change">)</a>
        self.fc<a id="change"> = </a><a id="change">nn.Linear(512</a><a id="change"> * </a>block.expansion, num_classes<a id="change">)</a>

    def _make_layer(self, block, out_channels, num_blocks, stride):
        make resnet layers(by layer i didnt mean this &quotlayer&quot was the
        same as a neuron netowork layer, ex. conv layer), one layer may</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/shaoeric/torch-atom/commit/87af6f2da4a9d8d8cd3710b0c643085ab8abbec6#diff-32f9903dd9a14d37c5300ee56d88ce85da3b030c80d54d5b026b73976800555cL11' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 27773247</div><div id='project'> Project Name: shaoeric/torch-atom</div><div id='commit'> Commit Name: 87af6f2da4a9d8d8cd3710b0c643085ab8abbec6</div><div id='time'> Time: 2022-03-06</div><div id='author'> Author: shaoeric@foxmail.com</div><div id='file'> File Name: src/models/resnet.py</div><div id='m_class'> M Class Name: ResNet</div><div id='n_method'> N Class Name: ResNet</div><div id='m_method'> M Method Name: __init__(4)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/models/resnet.py</div><div id='n_file'> N File Name: src/models/resnet.py</div><div id='m_start'> M Start Line: 11</div><div id='m_end'> M End Line: 13</div><div id='n_start'> N Start Line: 82</div><div id='n_end'> N End Line: 98</div><BR>