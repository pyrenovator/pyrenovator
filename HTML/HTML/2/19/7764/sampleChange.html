<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

class ResNet(nn.Module):
    def __init__(self) -&gt; None:
        <a id="change">super(</a>ResNet, self<a id="change">)</a>.__init__()
        self.backbone = resnet.resnet18()

    def forward(self, x):</code></pre><h3>After Change</h3><pre><code class='java'>

        self.in_channels = 64

        self.conv1<a id="change"> = nn</a><a id="change">.Sequential(
            nn.Conv2d(3</a>, <a id="change">64</a><a id="change">, kernel_size=3, padding=1, bias=False)</a>,
            <a id="change">nn.BatchNorm2d(64</a><a id="change">)</a>,
            <a id="change">nn.ReLU(inplace=True)</a><a id="change">)</a>
        &#47&#47we use a different inputsize than the original paper
        &#47&#47so conv2_x&quots stride is 1
        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)
        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)
        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)
        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)
        self.avg_pool<a id="change"> = nn</a><a id="change">.AdaptiveAvgPool2d(</a>(<a id="change">1</a><a id="change">, 1</a>)<a id="change">)</a>
        self.fc<a id="change"> = nn</a><a id="change">.Linear(512</a><a id="change"> * </a>block.expansion, num_classes<a id="change">)</a>

    def _make_layer(self, block, out_channels, num_blocks, stride):
        make resnet layers(by layer i didnt mean this &quotlayer&quot was the
        same as a neuron netowork layer, ex. conv layer), one layer may</code></pre>