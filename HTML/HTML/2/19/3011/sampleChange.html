<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
):
    if call in [helpers.tf_call, helpers.tf_graph_call] and "cpu" in device:
        &#47&#47 tf conv1d does not work when CUDA is installed, but array is on CPU
        <a id="change">pytest.skip()</a>
    if call is helpers.mx_call:
        &#47&#47 to_scalar syncrhonization issues
        <a id="change">pytest.skip()</a>
    if call in [helpers.np_call, helpers.jnp_call]:
        &#47&#47 numpy and jax do not yet support conv1d
        pytest.skip()
    &#47&#47 smoke test</code></pre><h3>After Change</h3><pre><code class='java'>
    if call in [helpers.np_call, helpers.jnp_call]:
        &#47&#47 numpy and jax do not yet support conv1d
        return
    <a id="change">if call in [helpers.torch_call] and (dtype == "float16")</a>:
        &#47&#47 torch.nn.functional.conv2d doesn&quott handle float16
        <a id="change">return</a>
    &#47&#47 smoke test
    output_shape = (batch_size<a id="change">, height, width, input_channels</a>)
    filter_shape = <a id="change">[</a>filter_height, filter_width<a id="change"></a>]
    x = ivy.asarray(
        <a id="change">ivy.random_normal(shape=output_shape)</a>,
        dtype=dtype,
        device=device,
    )
    <a id="change">if as_variable</a>:
        x<a id="change"> = ivy.variable(</a>x<a id="change">)</a>
    if with_v:
        np.random.seed(0)
        wlim = (6 / (output_channels + input_channels)) ** 0.5
        w = ivy.variable(</code></pre>