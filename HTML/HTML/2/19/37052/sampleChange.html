<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

            if is_mixed_precision:
                &#47&#47 2/3 & 3/3 of mixed precision training with amp
                <a id="change">with amp.scale_loss(</a><a id="change">loss, optimizer) as scaled_loss:
                    </a>scaled_loss.backward()
            else:
                loss.backward()
            <a id="change">optimizer.step()</a>
            if lr_scheduler is not None:
                lr_scheduler.step()

            &#47&#47 EMA update</code></pre><h3>After Change</h3><pre><code class='java'>
    if val_num_steps is None:
        val_num_steps = len(train_loader)

    <a id="change">if is_mixed_precision</a>:
        scaler<a id="change"> = GradScaler()</a>

    net.train()

    &#47&#47 Use EMA to report final performance instead of select best checkpoint with valtiny
    ema = EMA(net=net, decay=decay)

    epoch = 0

    &#47&#47 Training
    running_loss = 0.0
    while epoch &lt; num_epochs:
        train_correct = 0
        train_all = 0
        time_now = time.time()
        for i, data in enumerate(train_loader, 0):
            inputs, labels = data
            inputs = inputs.to(device)
            labels = labels.to(device)
            optimizer.zero_grad()
            train_all += labels.shape[0]

            &#47&#47 mixup data within the batch
            if alpha is not None:
                inputs, labels_a, labels_b, lam = mixup_data(x=inputs, y=labels, alpha=alpha)

            <a id="change">with autocast</a><a id="change">(is_mixed_precision):
                </a>outputs = net(inputs)

            if alpha is not None:
                &#47&#47 Pseudo training accuracy & interesting loss
                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)
                predicted = outputs.argmax(1)
                train_correct += (lam * (predicted == labels_a).sum().float().item()
                                  + (1 - lam) * (predicted == labels_b).sum().float().item())
            else:
                train_correct += (labels == outputs.argmax(1)).sum().item()
                loss = criterion(outputs, labels)

            if is_mixed_precision:
                <a id="change">accelerator.backward(</a><a id="change">scaler.scale(loss</a><a id="change">))</a>
                scaler.step(optimizer)
                scaler.update()
            else:
                <a id="change">accelerator.backward(loss</a><a id="change">)</a>
                <a id="change">optimizer.step()</a>
            if lr_scheduler is not None:
                lr_scheduler.step()

            &#47&#47 EMA update</code></pre>