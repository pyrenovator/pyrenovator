<html><h3>Pattern ID :4182
</h3><img src='15396026.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            )  &#47&#47 TODO generalize

            &#47&#47 Serialize the overall grad_inputs
            serialized_grad_inputs<a id="change"> = </a><a id="change">[
                serialize_torch_tensor(result.to(proto.dtype), proto.compression, allow_inplace=True)
                for result, proto in zip(grads, nested_flatten(grad_inputs_schema_with_prompts))
            ]</a>
            &#47&#47 Split the serialized_grad_inputs for streaming and respond
            output_split<a id="change"> = </a><a id="change">[
                part for tensor in serialized_grad_inputs for part in split_for_streaming(tensor, DEFAULT_MAX_MSG_SIZE)
            ]</a>

            <a id="change">async for </a><a id="change">part</a> in <a id="change">as_aiter(</a>*<a id="change">output_split)</a><a id="change">:
                </a><a id="change">yield </a>runtime_pb2.ExpertResponse(tensors=[part])

    def _check_uids(self, uids: str) -&gt; Sequence[ModuleUID]:
        Check that the first request to rpc_inference is valid</code></pre><h3>After Change</h3><pre><code class='java'>
                *flat_tensors, requested_backends=requested_backends, prioritizer=self._prioritizer, points=points
            )
            &#47&#47 Split the serialized_grad_inputs for streaming and respond
            <a id="change">for </a>tensor in self._serialize_grads(grads, requested_backends, metadata)<a id="change">:
                </a><a id="change">for part</a> in <a id="change">split_for_streaming(tensor</a>, DEFAULT_MAX_MSG_SIZE<a id="change">)</a><a id="change">:
                    yield </a>runtime_pb2.ExpertResponse(tensors=[part])

    def _serialize_grads(
        self,</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 19</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/bigscience-workshop/petals/commit/43ac6016accd2c397465880fbeab5f5d83ae4429#diff-1ec81fca927b01043d0984c86a71115967ba3530e5d168e50ebd69d48a3c0728L284' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15396026</div><div id='project'> Project Name: bigscience-workshop/petals</div><div id='commit'> Commit Name: 43ac6016accd2c397465880fbeab5f5d83ae4429</div><div id='time'> Time: 2022-11-30</div><div id='author'> Author: borzunov.alexander@gmail.com</div><div id='file'> File Name: src/petals/server/handler.py</div><div id='m_class'> M Class Name: TransformerConnectionHandler</div><div id='n_method'> N Class Name: TransformerConnectionHandler</div><div id='m_method'> M Method Name: rpc_backward_stream(3)</div><div id='n_method'> N Method Name: rpc_backward_stream(3)</div><div id='m_parent_class'> M Parent Class: ConnectionHandler</div><div id='n_parent_class'> N Parent Class: ConnectionHandler</div><div id='m_file'> M File Name: src/petals/server/handler.py</div><div id='n_file'> N File Name: src/petals/server/handler.py</div><div id='m_start'> M Start Line: 292</div><div id='m_end'> M End Line: 322</div><div id='n_start'> N Start Line: 284</div><div id='n_end'> N End Line: 298</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            )  &#47&#47 TODO generalize

            &#47&#47 Serialize the overall grad_inputs
            serialized_grad_inputs<a id="change"> = </a><a id="change">[
                serialize_torch_tensor(result.to(proto.dtype), proto.compression, allow_inplace=True)
                for result, proto in zip(grads, nested_flatten(grad_inputs_schema_with_prompts))
            ]</a>
            &#47&#47 Split the serialized_grad_inputs for streaming and respond
            output_split<a id="change"> = </a><a id="change">[
                part for tensor in serialized_grad_inputs for part in split_for_streaming(tensor, DEFAULT_MAX_MSG_SIZE)
            ]</a>

            <a id="change">async for </a><a id="change">part</a> in <a id="change">as_aiter(</a>*<a id="change">output_split)</a><a id="change">:
                </a><a id="change">yield </a>runtime_pb2.ExpertResponse(tensors=[part])

    def _check_uids(self, uids: str) -&gt; Sequence[ModuleUID]:
        Check that the first request to rpc_inference is valid</code></pre><h3>After Change</h3><pre><code class='java'>
                *flat_tensors, requested_backends=requested_backends, prioritizer=self._prioritizer, points=points
            )
            &#47&#47 Split the serialized_grad_inputs for streaming and respond
            <a id="change">for tensor</a> in self._serialize_grads(grads, requested_backends, metadata)<a id="change">:
                </a><a id="change">for part</a> in <a id="change">split_for_streaming(</a>tensor, DEFAULT_MAX_MSG_SIZE<a id="change">)</a><a id="change">:
                    yield </a>runtime_pb2.ExpertResponse(tensors=[part])

    def _serialize_grads(
        self,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/bigscience-workshop/distributed-bloom/commit/43ac6016accd2c397465880fbeab5f5d83ae4429#diff-1ec81fca927b01043d0984c86a71115967ba3530e5d168e50ebd69d48a3c0728L284' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15396024</div><div id='project'> Project Name: bigscience-workshop/distributed-bloom</div><div id='commit'> Commit Name: 43ac6016accd2c397465880fbeab5f5d83ae4429</div><div id='time'> Time: 2022-11-30</div><div id='author'> Author: borzunov.alexander@gmail.com</div><div id='file'> File Name: src/petals/server/handler.py</div><div id='m_class'> M Class Name: TransformerConnectionHandler</div><div id='n_method'> N Class Name: TransformerConnectionHandler</div><div id='m_method'> M Method Name: rpc_backward_stream(3)</div><div id='n_method'> N Method Name: rpc_backward_stream(3)</div><div id='m_parent_class'> M Parent Class: ConnectionHandler</div><div id='n_parent_class'> N Parent Class: ConnectionHandler</div><div id='m_file'> M File Name: src/petals/server/handler.py</div><div id='n_file'> N File Name: src/petals/server/handler.py</div><div id='m_start'> M Start Line: 292</div><div id='m_end'> M End Line: 322</div><div id='n_start'> N Start Line: 284</div><div id='n_end'> N End Line: 298</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            ), "hidden_states must be a 3d tensor"

            &#47&#47 Serialize the overall output
            serialized_output<a id="change"> = </a><a id="change">[
                serialize_torch_tensor(result.to(proto.dtype), proto.compression, allow_inplace=True)
                for result, proto in zip((hidden_states,), nested_flatten(requested_backends[-1].outputs_schema))
            ]</a>

            &#47&#47 Split the serialized_output for streaming and respond to client
            output_split<a id="change"> = </a><a id="change">[
                part for tensor in serialized_output for part in split_for_streaming(tensor, DEFAULT_MAX_MSG_SIZE)
            ]</a>
            <a id="change">async for </a><a id="change">part</a> in <a id="change">as_aiter(</a>*<a id="change">output_split)</a><a id="change">:
                </a><a id="change">yield </a>runtime_pb2.ExpertResponse(tensors=[part])

    async def rpc_backward(self, request: runtime_pb2.ExpertRequest, context: P2PContext) -&gt; runtime_pb2.ExpertResponse:
        async with timeout(self.request_timeout):</code></pre><h3>After Change</h3><pre><code class='java'>
            )

            &#47&#47 Split the serialized_output for streaming and respond to client
            <a id="change">for tensor</a> in self._serialize_outputs(hidden_states, requested_backends, metadata)<a id="change">:
                </a><a id="change">for part</a> in <a id="change">split_for_streaming(</a>tensor, DEFAULT_MAX_MSG_SIZE<a id="change">)</a><a id="change">:
                    yield </a>runtime_pb2.ExpertResponse(tensors=[part])

    def _serialize_outputs(
        self,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/bigscience-workshop/petals/commit/43ac6016accd2c397465880fbeab5f5d83ae4429#diff-1ec81fca927b01043d0984c86a71115967ba3530e5d168e50ebd69d48a3c0728L215' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15396025</div><div id='project'> Project Name: bigscience-workshop/petals</div><div id='commit'> Commit Name: 43ac6016accd2c397465880fbeab5f5d83ae4429</div><div id='time'> Time: 2022-11-30</div><div id='author'> Author: borzunov.alexander@gmail.com</div><div id='file'> File Name: src/petals/server/handler.py</div><div id='m_class'> M Class Name: TransformerConnectionHandler</div><div id='n_method'> N Class Name: TransformerConnectionHandler</div><div id='m_method'> M Method Name: rpc_forward_stream(3)</div><div id='n_method'> N Method Name: rpc_forward_stream(3)</div><div id='m_parent_class'> M Parent Class: ConnectionHandler</div><div id='n_parent_class'> N Parent Class: ConnectionHandler</div><div id='m_file'> M File Name: src/petals/server/handler.py</div><div id='n_file'> N File Name: src/petals/server/handler.py</div><div id='m_start'> M Start Line: 224</div><div id='m_end'> M End Line: 250</div><div id='n_start'> N Start Line: 218</div><div id='n_end'> N End Line: 233</div><BR>