<html><h3>Pattern ID :5556
</h3><img src='19578218.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
      _, labels = next(test_iterator)  &#47&#47 pytype: disable=attribute-error
      logits = logits_dataset[:, (step*batch_size):((step+1)*batch_size)]
      labels = tf.cast(tf.reshape(labels, [-1]), tf.int32)
      negative_log_likelihood = <a id="change">um.ensemble_cross_entropy(labels</a>, <a id="change">logits</a><a id="change">)</a>
      per_probs = tf.nn.softmax(logits)
      probs = tf.reduce_mean(per_probs, axis=0)
      if name == &quotclean&quot:
        gibbs_ce = <a id="change">um.gibbs_cross_entropy(labels</a>, <a id="change">logits</a><a id="change">)</a>
        metrics[&quottest/negative_log_likelihood&quot].update_state(
            negative_log_likelihood)
        metrics[&quottest/gibbs_cross_entropy&quot].update_state(gibbs_ce)
        metrics[&quottest/accuracy&quot].update_state(labels, probs)</code></pre><h3>After Change</h3><pre><code class='java'>
      _, labels = next(test_iterator)  &#47&#47 pytype: disable=attribute-error
      logits = logits_dataset[:, (step*batch_size):((step+1)*batch_size)]
      labels = tf.cast(tf.reshape(labels, [-1]), tf.int32)
      <a id="change">negative_log_likelihood_metric</a> = <a id="change">rm.metrics.EnsembleCrossEntropy()</a>
      <a id="change">negative_log_likelihood_metric.add_batch(logits</a><a id="change">, labels=labels)</a>
      negative_log_likelihood<a id="change"> = </a><a id="change">list(
          negative_log_likelihood_metric.result().values())[0]</a>
      per_probs = tf.nn.softmax(logits)
      probs = tf.reduce_mean(per_probs, axis=0)
      if name == &quotclean&quot:
        <a id="change">gibbs_ce_metric</a> = <a id="change">rm.metrics.GibbsCrossEntropy()</a>
        <a id="change">gibbs_ce_metric.add_batch(logits</a><a id="change">, labels=labels)</a>
        gibbs_ce<a id="change"> = </a><a id="change">list(gibbs_ce_metric.result().values())[0]</a>
        metrics[&quottest/negative_log_likelihood&quot].update_state(
            negative_log_likelihood)
        metrics[&quottest/gibbs_cross_entropy&quot].update_state(gibbs_ce)
        metrics[&quottest/accuracy&quot].update_state(labels, probs)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 5</div><BR><div id='size'>Non-data size: 16</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/google/uncertainty-baselines/commit/0dbf6f6829654c06ada1666b9780f72357eef22a#diff-e9488386f6fb43d624e6eaf2b9a5bfe9a0e0d1b498ce561d2eadb50978e7e38bL231' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 19578218</div><div id='project'> Project Name: google/uncertainty-baselines</div><div id='commit'> Commit Name: 0dbf6f6829654c06ada1666b9780f72357eef22a</div><div id='time'> Time: 2021-04-26</div><div id='author'> Author: no-reply@google.com</div><div id='file'> File Name: baselines/imagenet/sngp_ensemble.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: baselines/imagenet/sngp_ensemble.py</div><div id='n_file'> N File Name: baselines/imagenet/sngp_ensemble.py</div><div id='m_start'> M Start Line: 232</div><div id='m_end'> M End Line: 238</div><div id='n_start'> N Start Line: 231</div><div id='n_end'> N End Line: 242</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    for step in range(steps_per_eval[name]):
      inputs = next(test_iterator)
      _, labels = bert_utils.create_feature_and_label(inputs, feature_size)
      <a id="change">logits</a> = logits_dataset[:, (step * batch_size):((step + 1) * batch_size)]
      <a id="change">labels</a> = tf.cast(labels, tf.int32)
      negative_log_likelihood = <a id="change">um.ensemble_cross_entropy(</a>labels, logits<a id="change">)</a>
      per_probs = tf.nn.softmax(logits)
      probs = tf.reduce_mean(per_probs, axis=0)
      if name == &quotclean&quot:
        gibbs_ce = <a id="change">um.gibbs_cross_entropy(</a>labels, logits<a id="change">)</a>
        metrics[&quottest/negative_log_likelihood&quot].update_state(
            negative_log_likelihood)
        metrics[&quottest/gibbs_cross_entropy&quot].update_state(gibbs_ce)
        metrics[&quottest/accuracy&quot].update_state(labels, probs)</code></pre><h3>After Change</h3><pre><code class='java'>
    for step in range(steps_per_eval[name]):
      inputs = next(test_iterator)
      _, labels = bert_utils.create_feature_and_label(inputs, feature_size)
      <a id="change">logits</a> = logits_dataset[:, (step * batch_size):((step + 1) * batch_size)]
      labels = tf.cast(labels, tf.int32)
      <a id="change">negative_log_likelihood_metric</a> = <a id="change">rm.metrics.EnsembleCrossEntropy()</a>
      <a id="change">negative_log_likelihood_metric.add_batch(</a>logits<a id="change">, labels=labels)</a>
      negative_log_likelihood<a id="change"> = </a><a id="change">list(
          negative_log_likelihood_metric.result().values())[0]</a>
      per_probs = tf.nn.softmax(logits)
      probs = tf.reduce_mean(per_probs, axis=0)
      if name == &quotclean&quot:
        <a id="change">gibbs_ce_metric</a> = <a id="change">rm.metrics.GibbsCrossEntropy()</a>
        <a id="change">gibbs_ce_metric.add_batch(</a>logits<a id="change">, labels=labels)</a>
        gibbs_ce<a id="change"> = </a><a id="change">list(gibbs_ce_metric.result().values())[0]</a>
        metrics[&quottest/negative_log_likelihood&quot].update_state(
            negative_log_likelihood)
        metrics[&quottest/gibbs_cross_entropy&quot].update_state(gibbs_ce)
        metrics[&quottest/accuracy&quot].update_state(labels, probs)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/google/uncertainty-baselines/commit/0dbf6f6829654c06ada1666b9780f72357eef22a#diff-cd030c61f5468d27fe41274aa314d5ffbbb82bd291ed6c83d75344df37d8bb55L51' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 19578219</div><div id='project'> Project Name: google/uncertainty-baselines</div><div id='commit'> Commit Name: 0dbf6f6829654c06ada1666b9780f72357eef22a</div><div id='time'> Time: 2021-04-26</div><div id='author'> Author: no-reply@google.com</div><div id='file'> File Name: baselines/clinc_intent/sngp_ensemble.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: baselines/clinc_intent/sngp_ensemble.py</div><div id='n_file'> N File Name: baselines/clinc_intent/sngp_ensemble.py</div><div id='m_start'> M Start Line: 207</div><div id='m_end'> M End Line: 213</div><div id='n_start'> N Start Line: 206</div><div id='n_end'> N End Line: 217</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    test_iterator = iter(test_dataset)
    for step in range(steps_per_eval):
      _, labels = next(test_iterator)  &#47&#47 pytype: disable=attribute-error
      <a id="change">logits</a> = logits_dataset[:, (step*batch_size):((step+1)*batch_size)]
      <a id="change">labels</a> = tf.cast(tf.reshape(labels, [-1]), tf.int32)
      negative_log_likelihood = <a id="change">um.ensemble_cross_entropy(</a>labels, logits<a id="change">)</a>
      per_probs = tf.nn.softmax(logits)
      probs = tf.reduce_mean(per_probs, axis=0)
      if name == &quotclean&quot:
        gibbs_ce = <a id="change">um.gibbs_cross_entropy(</a>labels, logits<a id="change">)</a>
        metrics[&quottest/negative_log_likelihood&quot].update_state(
            negative_log_likelihood)
        metrics[&quottest/gibbs_cross_entropy&quot].update_state(gibbs_ce)
        metrics[&quottest/accuracy&quot].update_state(labels, probs)</code></pre><h3>After Change</h3><pre><code class='java'>
    test_iterator = iter(test_dataset)
    for step in range(steps_per_eval):
      _, labels = next(test_iterator)  &#47&#47 pytype: disable=attribute-error
      <a id="change">logits</a> = logits_dataset[:, (step*batch_size):((step+1)*batch_size)]
      labels = tf.cast(tf.reshape(labels, [-1]), tf.int32)
      <a id="change">negative_log_likelihood_metric</a> = <a id="change">rm.metrics.EnsembleCrossEntropy()</a>
      <a id="change">negative_log_likelihood_metric.add_batch(</a>logits<a id="change">, labels=labels)</a>
      negative_log_likelihood<a id="change"> = </a><a id="change">list(
          negative_log_likelihood_metric.result().values())[0]</a>
      per_probs = tf.nn.softmax(logits)
      probs = tf.reduce_mean(per_probs, axis=0)
      if name == &quotclean&quot:
        <a id="change">gibbs_ce_metric</a> = <a id="change">rm.metrics.GibbsCrossEntropy()</a>
        <a id="change">gibbs_ce_metric.add_batch(</a>logits<a id="change">, labels=labels)</a>
        gibbs_ce<a id="change"> = </a><a id="change">list(gibbs_ce_metric.result().values())[0]</a>
        metrics[&quottest/negative_log_likelihood&quot].update_state(
            negative_log_likelihood)
        metrics[&quottest/gibbs_cross_entropy&quot].update_state(gibbs_ce)
        metrics[&quottest/accuracy&quot].update_state(labels, probs)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/google/uncertainty-baselines/commit/0dbf6f6829654c06ada1666b9780f72357eef22a#diff-ece71a2b7a1637ad21fb53f5e684601212a3c79e013f6e411c9e580c09702771L71' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 19578216</div><div id='project'> Project Name: google/uncertainty-baselines</div><div id='commit'> Commit Name: 0dbf6f6829654c06ada1666b9780f72357eef22a</div><div id='time'> Time: 2021-04-26</div><div id='author'> Author: no-reply@google.com</div><div id='file'> File Name: baselines/imagenet/het_ensemble.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: baselines/imagenet/het_ensemble.py</div><div id='n_file'> N File Name: baselines/imagenet/het_ensemble.py</div><div id='m_start'> M Start Line: 172</div><div id='m_end'> M End Line: 178</div><div id='n_start'> N Start Line: 171</div><div id='n_end'> N End Line: 182</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
      inputs = next(test_iterator)
      _, labels = deterministic.create_feature_and_label(
          inputs, feature_size, model_family=FLAGS.model_family)
      <a id="change">logits</a> = logits_dataset[:, (step * batch_size):((step + 1) * batch_size)]
      <a id="change">labels</a> = tf.cast(labels, tf.int32)
      negative_log_likelihood = <a id="change">um.ensemble_cross_entropy(</a>labels, logits<a id="change">)</a>
      per_probs = tf.nn.softmax(logits)
      probs = tf.reduce_mean(per_probs, axis=0)
      if name == &quotclean&quot:
        gibbs_ce = <a id="change">um.gibbs_cross_entropy(</a>labels, logits<a id="change">)</a>
        metrics[&quottest/negative_log_likelihood&quot].update_state(
            negative_log_likelihood)
        metrics[&quottest/gibbs_cross_entropy&quot].update_state(gibbs_ce)
        metrics[&quottest/accuracy&quot].update_state(labels, probs)</code></pre><h3>After Change</h3><pre><code class='java'>
      inputs = next(test_iterator)
      _, labels = deterministic.create_feature_and_label(
          inputs, feature_size, model_family=FLAGS.model_family)
      <a id="change">logits</a> = logits_dataset[:, (step * batch_size):((step + 1) * batch_size)]
      labels = tf.cast(labels, tf.int32)
      <a id="change">negative_log_likelihood_metric</a> = <a id="change">rm.metrics.EnsembleCrossEntropy()</a>
      <a id="change">negative_log_likelihood_metric.add_batch(</a>logits<a id="change">, labels=labels)</a>
      negative_log_likelihood<a id="change"> = </a><a id="change">list(
          negative_log_likelihood_metric.result().values())[0]</a>
      per_probs = tf.nn.softmax(logits)
      probs = tf.reduce_mean(per_probs, axis=0)
      if name == &quotclean&quot:
        <a id="change">gibbs_ce_metric</a> = <a id="change">rm.metrics.GibbsCrossEntropy()</a>
        <a id="change">gibbs_ce_metric.add_batch(</a>logits<a id="change">, labels=labels)</a>
        gibbs_ce<a id="change"> = </a><a id="change">list(gibbs_ce_metric.result().values())[0]</a>
        metrics[&quottest/negative_log_likelihood&quot].update_state(
            negative_log_likelihood)
        metrics[&quottest/gibbs_cross_entropy&quot].update_state(gibbs_ce)
        metrics[&quottest/accuracy&quot].update_state(labels, probs)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/google/uncertainty-baselines/commit/0dbf6f6829654c06ada1666b9780f72357eef22a#diff-fde21841ca7101dd56146b4bafba33af539b87999225371bc629962b4ca43d77L51' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 19578217</div><div id='project'> Project Name: google/uncertainty-baselines</div><div id='commit'> Commit Name: 0dbf6f6829654c06ada1666b9780f72357eef22a</div><div id='time'> Time: 2021-04-26</div><div id='author'> Author: no-reply@google.com</div><div id='file'> File Name: baselines/clinc_intent/ensemble.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: baselines/clinc_intent/ensemble.py</div><div id='n_file'> N File Name: baselines/clinc_intent/ensemble.py</div><div id='m_start'> M Start Line: 196</div><div id='m_end'> M End Line: 202</div><div id='n_start'> N Start Line: 195</div><div id='n_end'> N End Line: 206</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    test_iterator = iter(test_dataset)
    for step in range(steps_per_eval):
      _, labels = next(test_iterator)  &#47&#47 pytype: disable=attribute-error
      <a id="change">logits</a> = logits_dataset[:, (step*batch_size):((step+1)*batch_size)]
      <a id="change">labels</a> = tf.cast(tf.reshape(labels, [-1]), tf.int32)
      negative_log_likelihood = <a id="change">um.ensemble_cross_entropy(</a>labels, logits<a id="change">)</a>
      per_probs = tf.nn.softmax(logits)
      probs = tf.reduce_mean(per_probs, axis=0)
      if name == &quotclean&quot:
        gibbs_ce = <a id="change">um.gibbs_cross_entropy(</a>labels, logits<a id="change">)</a>
        metrics[&quottest/negative_log_likelihood&quot].update_state(
            negative_log_likelihood)
        metrics[&quottest/gibbs_cross_entropy&quot].update_state(gibbs_ce)
        metrics[&quottest/accuracy&quot].update_state(labels, probs)</code></pre><h3>After Change</h3><pre><code class='java'>
    test_iterator = iter(test_dataset)
    for step in range(steps_per_eval):
      _, labels = next(test_iterator)  &#47&#47 pytype: disable=attribute-error
      <a id="change">logits</a> = logits_dataset[:, (step*batch_size):((step+1)*batch_size)]
      labels = tf.cast(tf.reshape(labels, [-1]), tf.int32)
      <a id="change">negative_log_likelihood_metric</a> = <a id="change">rm.metrics.EnsembleCrossEntropy()</a>
      <a id="change">negative_log_likelihood_metric.add_batch(</a>logits<a id="change">, labels=labels)</a>
      negative_log_likelihood<a id="change"> = </a><a id="change">list(
          negative_log_likelihood_metric.result().values())[0]</a>
      per_probs = tf.nn.softmax(logits)
      probs = tf.reduce_mean(per_probs, axis=0)
      if name == &quotclean&quot:
        <a id="change">gibbs_ce_metric</a> = <a id="change">rm.metrics.GibbsCrossEntropy()</a>
        <a id="change">gibbs_ce_metric.add_batch(</a>logits<a id="change">, labels=labels)</a>
        gibbs_ce<a id="change"> = </a><a id="change">list(gibbs_ce_metric.result().values())[0]</a>
        metrics[&quottest/negative_log_likelihood&quot].update_state(
            negative_log_likelihood)
        metrics[&quottest/gibbs_cross_entropy&quot].update_state(gibbs_ce)
        metrics[&quottest/accuracy&quot].update_state(labels, probs)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/google/uncertainty-baselines/commit/0dbf6f6829654c06ada1666b9780f72357eef22a#diff-b992a8701202971f8187f23704750fdca13cb72e9c5436f0a85b83f6b54032f4L61' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 19578210</div><div id='project'> Project Name: google/uncertainty-baselines</div><div id='commit'> Commit Name: 0dbf6f6829654c06ada1666b9780f72357eef22a</div><div id='time'> Time: 2021-04-26</div><div id='author'> Author: no-reply@google.com</div><div id='file'> File Name: baselines/imagenet/ensemble.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: baselines/imagenet/ensemble.py</div><div id='n_file'> N File Name: baselines/imagenet/ensemble.py</div><div id='m_start'> M Start Line: 160</div><div id='m_end'> M End Line: 166</div><div id='n_start'> N Start Line: 159</div><div id='n_end'> N End Line: 170</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    test_iterator = iter(test_dataset)
    for step in range(steps_per_eval):
      _, labels = next(test_iterator)  &#47&#47 pytype: disable=attribute-error
      <a id="change">logits</a> = logits_dataset[:, (step*batch_size):((step+1)*batch_size)]
      <a id="change">labels</a> = tf.cast(tf.reshape(labels, [-1]), tf.int32)
      negative_log_likelihood = <a id="change">um.ensemble_cross_entropy(</a>labels, logits<a id="change">)</a>
      per_probs = tf.nn.softmax(logits)
      probs = tf.reduce_mean(per_probs, axis=0)
      if name == &quotclean&quot:
        gibbs_ce = <a id="change">um.gibbs_cross_entropy(</a>labels, logits<a id="change">)</a>
        metrics[&quottest/negative_log_likelihood&quot].update_state(
            negative_log_likelihood)
        metrics[&quottest/gibbs_cross_entropy&quot].update_state(gibbs_ce)
        metrics[&quottest/accuracy&quot].update_state(labels, probs)</code></pre><h3>After Change</h3><pre><code class='java'>
    test_iterator = iter(test_dataset)
    for step in range(steps_per_eval):
      _, labels = next(test_iterator)  &#47&#47 pytype: disable=attribute-error
      <a id="change">logits</a> = logits_dataset[:, (step*batch_size):((step+1)*batch_size)]
      labels = tf.cast(tf.reshape(labels, [-1]), tf.int32)
      <a id="change">negative_log_likelihood_metric</a> = <a id="change">rm.metrics.EnsembleCrossEntropy()</a>
      <a id="change">negative_log_likelihood_metric.add_batch(</a>logits<a id="change">, labels=labels)</a>
      negative_log_likelihood<a id="change"> = </a><a id="change">list(
          negative_log_likelihood_metric.result().values())[0]</a>
      per_probs = tf.nn.softmax(logits)
      probs = tf.reduce_mean(per_probs, axis=0)
      if name == &quotclean&quot:
        <a id="change">gibbs_ce_metric</a> = <a id="change">rm.metrics.GibbsCrossEntropy()</a>
        <a id="change">gibbs_ce_metric.add_batch(</a>logits<a id="change">, labels=labels)</a>
        gibbs_ce<a id="change"> = </a><a id="change">list(gibbs_ce_metric.result().values())[0]</a>
        metrics[&quottest/negative_log_likelihood&quot].update_state(
            negative_log_likelihood)
        metrics[&quottest/gibbs_cross_entropy&quot].update_state(gibbs_ce)
        metrics[&quottest/accuracy&quot].update_state(labels, probs)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/google/uncertainty-baselines/commit/0dbf6f6829654c06ada1666b9780f72357eef22a#diff-e9488386f6fb43d624e6eaf2b9a5bfe9a0e0d1b498ce561d2eadb50978e7e38bL113' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 19578211</div><div id='project'> Project Name: google/uncertainty-baselines</div><div id='commit'> Commit Name: 0dbf6f6829654c06ada1666b9780f72357eef22a</div><div id='time'> Time: 2021-04-26</div><div id='author'> Author: no-reply@google.com</div><div id='file'> File Name: baselines/imagenet/sngp_ensemble.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: baselines/imagenet/sngp_ensemble.py</div><div id='n_file'> N File Name: baselines/imagenet/sngp_ensemble.py</div><div id='m_start'> M Start Line: 232</div><div id='m_end'> M End Line: 238</div><div id='n_start'> N Start Line: 231</div><div id='n_end'> N End Line: 242</div><BR>