<html><h3>Pattern ID :3213
</h3><img src='12245816.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        grads, vars = zip(*grads_and_vars)
        <a id="change">if </a>self.weight_decay != 0.0:
            self.weight_decay<a id="change"> = </a><a id="change">tf.convert_to_tensor(</a>self.weight_decay<a id="change">)</a>
            <a id="change">for </a>weight in vars<a id="change">:
                weight</a><a id="change">.assign_sub(</a>self.weight_decay<a id="change"> * weight</a><a id="change">)</a>
        if self.grad_clip is not None:
            if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                clip_grad, _ = self.grad_clip(grads)
            else:</code></pre><h3>After Change</h3><pre><code class='java'>
    def apply_gradients(self, grads_and_vars):
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        <a id="change">if self.weight_decay != 0.0 or self.grad_clip is not None</a>:
            grads, vars = zip(*grads_and_vars)
            <a id="change">if </a>self.weight_decay != 0.0:
                <a id="change">new_grads = </a><a id="change">[]</a>
                <a id="change">for </a>grad, <a id="change">var</a> in <a id="change">zip(</a>grads, vars<a id="change">):
                    </a>grad<a id="change"> = </a>grad<a id="change"> + </a>self.weight_decay<a id="change"> * var</a>
                    <a id="change">new_grads.append(</a>grad<a id="change">)</a>
                grads<a id="change"> = new_grads</a>
            if self.grad_clip is not None:
                if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                    new_grads, _ = self.grad_clip(grads)
                else:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 9</div><BR><div id='size'>Non-data size: 20</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/e1d0a8eac81bfce74977e2670201b85ff4f3f91b#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL263' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 12245816</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: e1d0a8eac81bfce74977e2670201b85ff4f3f91b</div><div id='time'> Time: 2022-05-17</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: Adamax</div><div id='n_method'> N Class Name: Adamax</div><div id='m_method'> M Method Name: apply_gradients(2)</div><div id='n_method'> N Method Name: apply_gradients(2)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 263</div><div id='m_end'> M End Line: 276</div><div id='n_start'> N Start Line: 272</div><div id='n_end'> N End Line: 289</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        grads, vars = zip(*grads_and_vars)
        <a id="change">if </a>self.weight_decay != 0.0:
            self.weight_decay<a id="change"> = </a><a id="change">tf.convert_to_tensor(</a>self.weight_decay<a id="change">)</a>
            <a id="change">for </a><a id="change">weight</a> in vars<a id="change">:
                </a><a id="change">weight.assign_sub(</a>self.weight_decay<a id="change"> * </a>weight<a id="change">)</a>
        if self.grad_clip is not None:
            if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                clip_grad, _ = self.grad_clip(grads)
            else:</code></pre><h3>After Change</h3><pre><code class='java'>
    def apply_gradients(self, grads_and_vars):
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        <a id="change">if self.weight_decay != 0.0 or self.grad_clip is not None</a>:
            grads, vars = zip(*grads_and_vars)
            <a id="change">if </a>self.weight_decay != 0.0:
                <a id="change">new_grads = </a><a id="change">[]</a>
                <a id="change">for </a>grad, <a id="change">var</a> in <a id="change">zip(</a>grads, vars<a id="change">):
                    </a>grad<a id="change"> = </a>grad<a id="change"> + </a>self.weight_decay<a id="change"> * </a>var
                    <a id="change">new_grads.append(</a>grad<a id="change">)</a>
                grads<a id="change"> = </a>new_grads
            if self.grad_clip is not None:
                if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                    new_grads, _ = self.grad_clip(grads)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/e1d0a8eac81bfce74977e2670201b85ff4f3f91b#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL55' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 12245803</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: e1d0a8eac81bfce74977e2670201b85ff4f3f91b</div><div id='time'> Time: 2022-05-17</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: Adadelta</div><div id='n_method'> N Class Name: Adadelta</div><div id='m_method'> M Method Name: apply_gradients(2)</div><div id='n_method'> N Method Name: apply_gradients(2)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 58</div><div id='m_end'> M End Line: 71</div><div id='n_start'> N Start Line: 58</div><div id='n_end'> N End Line: 75</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        grads, vars = zip(*grads_and_vars)
        <a id="change">if </a>self.weight_decay != 0.0:
            self.weight_decay<a id="change"> = </a><a id="change">tf.convert_to_tensor(</a>self.weight_decay<a id="change">)</a>
            <a id="change">for </a><a id="change">weight</a> in vars<a id="change">:
                </a><a id="change">weight.assign_sub(</a>self.weight_decay<a id="change"> * </a>weight<a id="change">)</a>
        if self.grad_clip is not None:
            if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                clip_grad, _ = self.grad_clip(grads)
            else:</code></pre><h3>After Change</h3><pre><code class='java'>
    def apply_gradients(self, grads_and_vars):
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        <a id="change">if self.weight_decay != 0.0 or self.grad_clip is not None</a>:
            grads, vars = zip(*grads_and_vars)
            <a id="change">if </a>self.weight_decay != 0.0:
                <a id="change">new_grads = </a><a id="change">[]</a>
                <a id="change">for </a>grad, <a id="change">var</a> in <a id="change">zip(</a>grads, vars<a id="change">):
                    </a>grad<a id="change"> = </a>grad<a id="change"> + </a>self.weight_decay<a id="change"> * </a>var
                    <a id="change">new_grads.append(</a>grad<a id="change">)</a>
                grads<a id="change"> = </a>new_grads
            if self.grad_clip is not None:
                if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                    new_grads, _ = self.grad_clip(grads)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/e1d0a8eac81bfce74977e2670201b85ff4f3f91b#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL416' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 12245810</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: e1d0a8eac81bfce74977e2670201b85ff4f3f91b</div><div id='time'> Time: 2022-05-17</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: Nadam</div><div id='n_method'> N Class Name: Nadam</div><div id='m_method'> M Method Name: apply_gradients(2)</div><div id='n_method'> N Method Name: apply_gradients(2)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 419</div><div id='m_end'> M End Line: 432</div><div id='n_start'> N Start Line: 434</div><div id='n_end'> N End Line: 451</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        grads, vars = zip(*grads_and_vars)
        <a id="change">if </a>self.weight_decay != 0.0:
            self.weight_decay<a id="change"> = </a><a id="change">tf.convert_to_tensor(</a>self.weight_decay<a id="change">)</a>
            <a id="change">for </a><a id="change">weight</a> in vars<a id="change">:
                </a><a id="change">weight.assign_sub(</a>self.weight_decay<a id="change"> * </a>weight<a id="change">)</a>
        if self.grad_clip is not None:
            if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                clip_grad, _ = self.grad_clip(grads)
            else:</code></pre><h3>After Change</h3><pre><code class='java'>
    def apply_gradients(self, grads_and_vars):
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        <a id="change">if self.weight_decay != 0.0 or self.grad_clip is not None</a>:
            grads, vars = zip(*grads_and_vars)
            <a id="change">if </a>self.weight_decay != 0.0:
                <a id="change">new_grads = </a><a id="change">[]</a>
                <a id="change">for </a>grad, <a id="change">var</a> in <a id="change">zip(</a>grads, vars<a id="change">):
                    </a>grad<a id="change"> = </a>grad<a id="change"> + </a>self.weight_decay<a id="change"> * </a>var
                    <a id="change">new_grads.append(</a>grad<a id="change">)</a>
                grads<a id="change"> = </a>new_grads
            if self.grad_clip is not None:
                if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                    new_grads, _ = self.grad_clip(grads)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/e1d0a8eac81bfce74977e2670201b85ff4f3f91b#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL260' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 12245811</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: e1d0a8eac81bfce74977e2670201b85ff4f3f91b</div><div id='time'> Time: 2022-05-17</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: Adamax</div><div id='n_method'> N Class Name: Adamax</div><div id='m_method'> M Method Name: apply_gradients(2)</div><div id='n_method'> N Method Name: apply_gradients(2)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 263</div><div id='m_end'> M End Line: 276</div><div id='n_start'> N Start Line: 272</div><div id='n_end'> N End Line: 289</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        grads, vars = zip(*grads_and_vars)
        <a id="change">if </a>self.weight_decay != 0.0:
            self.weight_decay<a id="change"> = </a><a id="change">tf.convert_to_tensor(</a>self.weight_decay<a id="change">)</a>
            <a id="change">for </a><a id="change">weight</a> in vars<a id="change">:
                </a><a id="change">weight.assign_sub(</a>self.weight_decay<a id="change"> * </a>weight<a id="change">)</a>
        if self.grad_clip is not None:
            if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                clip_grad, _ = self.grad_clip(grads)
            else:</code></pre><h3>After Change</h3><pre><code class='java'>
    def apply_gradients(self, grads_and_vars):
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        <a id="change">if self.weight_decay != 0.0 or self.grad_clip is not None</a>:
            grads, vars = zip(*grads_and_vars)
            <a id="change">if </a>self.weight_decay != 0.0:
                <a id="change">new_grads = </a><a id="change">[]</a>
                <a id="change">for </a>grad, <a id="change">var</a> in <a id="change">zip(</a>grads, vars<a id="change">):
                    </a>grad<a id="change"> = </a>grad<a id="change"> + </a>self.weight_decay<a id="change"> * </a>var
                    <a id="change">new_grads.append(</a>grad<a id="change">)</a>
                grads<a id="change"> = </a>new_grads
            if self.grad_clip is not None:
                if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                    new_grads, _ = self.grad_clip(grads)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/e1d0a8eac81bfce74977e2670201b85ff4f3f91b#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL347' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 12245808</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: e1d0a8eac81bfce74977e2670201b85ff4f3f91b</div><div id='time'> Time: 2022-05-17</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: Ftrl</div><div id='n_method'> N Class Name: Ftrl</div><div id='m_method'> M Method Name: apply_gradients(2)</div><div id='n_method'> N Method Name: apply_gradients(2)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 350</div><div id='m_end'> M End Line: 363</div><div id='n_start'> N Start Line: 362</div><div id='n_end'> N End Line: 379</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        grads, vars = zip(*grads_and_vars)
        <a id="change">if </a>self.weight_decay != 0.0:
            self.weight_decay<a id="change"> = </a><a id="change">tf.convert_to_tensor(</a>self.weight_decay<a id="change">)</a>
            <a id="change">for </a><a id="change">weight</a> in vars<a id="change">:
                </a><a id="change">weight.assign_sub(</a>self.weight_decay<a id="change"> * </a>weight<a id="change">)</a>
        if self.grad_clip is not None:
            if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                clip_grad, _ = self.grad_clip(grads)
            else:</code></pre><h3>After Change</h3><pre><code class='java'>
    def apply_gradients(self, grads_and_vars):
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        <a id="change">if self.weight_decay != 0.0 or self.grad_clip is not None</a>:
            grads, vars = zip(*grads_and_vars)
            <a id="change">if </a>self.weight_decay != 0.0:
                <a id="change">new_grads = </a><a id="change">[]</a>
                <a id="change">for </a>grad, <a id="change">var</a> in <a id="change">zip(</a>grads, vars<a id="change">):
                    </a>grad<a id="change"> = </a>grad<a id="change"> + </a>self.weight_decay<a id="change"> * </a>var
                    <a id="change">new_grads.append(</a>grad<a id="change">)</a>
                grads<a id="change"> = </a>new_grads
            if self.grad_clip is not None:
                if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                    new_grads, _ = self.grad_clip(grads)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/e1d0a8eac81bfce74977e2670201b85ff4f3f91b#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL494' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 12245809</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: e1d0a8eac81bfce74977e2670201b85ff4f3f91b</div><div id='time'> Time: 2022-05-17</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: RMSprop</div><div id='n_method'> N Class Name: RMSprop</div><div id='m_method'> M Method Name: apply_gradients(2)</div><div id='n_method'> N Method Name: apply_gradients(2)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 497</div><div id='m_end'> M End Line: 510</div><div id='n_start'> N Start Line: 515</div><div id='n_end'> N End Line: 532</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        grads, vars = zip(*grads_and_vars)
        <a id="change">if </a>self.weight_decay != 0.0:
            self.weight_decay<a id="change"> = </a><a id="change">tf.convert_to_tensor(</a>self.weight_decay<a id="change">)</a>
            <a id="change">for </a><a id="change">weight</a> in vars<a id="change">:
                </a><a id="change">weight.assign_sub(</a>self.weight_decay<a id="change"> * </a>weight<a id="change">)</a>
        if self.grad_clip is not None:
            if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                clip_grad, _ = self.grad_clip(grads)
            else:</code></pre><h3>After Change</h3><pre><code class='java'>
    def apply_gradients(self, grads_and_vars):
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        <a id="change">if self.weight_decay != 0.0 or self.grad_clip is not None</a>:
            grads, vars = zip(*grads_and_vars)
            <a id="change">if </a>self.weight_decay != 0.0:
                <a id="change">new_grads = </a><a id="change">[]</a>
                <a id="change">for </a>grad, <a id="change">var</a> in <a id="change">zip(</a>grads, vars<a id="change">):
                    </a>grad<a id="change"> = </a>grad<a id="change"> + </a>self.weight_decay<a id="change"> * </a>var
                    <a id="change">new_grads.append(</a>grad<a id="change">)</a>
                grads<a id="change"> = </a>new_grads
            if self.grad_clip is not None:
                if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                    new_grads, _ = self.grad_clip(grads)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/e1d0a8eac81bfce74977e2670201b85ff4f3f91b#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL627' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 12245814</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: e1d0a8eac81bfce74977e2670201b85ff4f3f91b</div><div id='time'> Time: 2022-05-17</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: Momentum</div><div id='n_method'> N Class Name: Momentum</div><div id='m_method'> M Method Name: apply_gradients(2)</div><div id='n_method'> N Method Name: apply_gradients(2)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 630</div><div id='m_end'> M End Line: 643</div><div id='n_start'> N Start Line: 654</div><div id='n_end'> N End Line: 671</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        grads, vars = zip(*grads_and_vars)
        <a id="change">if </a>self.weight_decay != 0.0:
            self.weight_decay<a id="change"> = </a><a id="change">tf.convert_to_tensor(</a>self.weight_decay<a id="change">)</a>
            <a id="change">for </a><a id="change">weight</a> in vars<a id="change">:
                </a><a id="change">weight.assign_sub(</a>self.weight_decay<a id="change"> * </a>weight<a id="change">)</a>
        if self.grad_clip is not None:
            if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                clip_grad, _ = self.grad_clip(grads)
            else:</code></pre><h3>After Change</h3><pre><code class='java'>
    def apply_gradients(self, grads_and_vars):
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        <a id="change">if self.weight_decay != 0.0 or self.grad_clip is not None</a>:
            grads, vars = zip(*grads_and_vars)
            <a id="change">if </a>self.weight_decay != 0.0:
                <a id="change">new_grads = </a><a id="change">[]</a>
                <a id="change">for </a>grad, <a id="change">var</a> in <a id="change">zip(</a>grads, vars<a id="change">):
                    </a>grad<a id="change"> = </a>grad<a id="change"> + </a>self.weight_decay<a id="change"> * </a>var
                    <a id="change">new_grads.append(</a>grad<a id="change">)</a>
                grads<a id="change"> = </a>new_grads
            if self.grad_clip is not None:
                if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                    new_grads, _ = self.grad_clip(grads)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/e1d0a8eac81bfce74977e2670201b85ff4f3f91b#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL556' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 12245815</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: e1d0a8eac81bfce74977e2670201b85ff4f3f91b</div><div id='time'> Time: 2022-05-17</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: SGD</div><div id='n_method'> N Class Name: SGD</div><div id='m_method'> M Method Name: apply_gradients(2)</div><div id='n_method'> N Method Name: apply_gradients(2)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 559</div><div id='m_end'> M End Line: 572</div><div id='n_start'> N Start Line: 580</div><div id='n_end'> N End Line: 597</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        grads, vars = zip(*grads_and_vars)
        <a id="change">if </a>self.weight_decay != 0.0:
            self.weight_decay<a id="change"> = </a><a id="change">tf.convert_to_tensor(</a>self.weight_decay<a id="change">)</a>
            <a id="change">for </a><a id="change">weight</a> in vars<a id="change">:
                </a><a id="change">weight.assign_sub(</a>self.weight_decay<a id="change"> * </a>weight<a id="change">)</a>
        if self.grad_clip is not None:
            if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                clip_grad, _ = self.grad_clip(grads)
            else:</code></pre><h3>After Change</h3><pre><code class='java'>
    def apply_gradients(self, grads_and_vars):
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        <a id="change">if self.weight_decay != 0.0 or self.grad_clip is not None</a>:
            grads, vars = zip(*grads_and_vars)
            <a id="change">if </a>self.weight_decay != 0.0:
                <a id="change">new_grads = </a><a id="change">[]</a>
                <a id="change">for </a>grad, <a id="change">var</a> in <a id="change">zip(</a>grads, vars<a id="change">):
                    </a>grad<a id="change"> = </a>grad<a id="change"> + </a>self.weight_decay<a id="change"> * </a>var
                    <a id="change">new_grads.append(</a>grad<a id="change">)</a>
                grads<a id="change"> = </a>new_grads
            if self.grad_clip is not None:
                if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                    new_grads, _ = self.grad_clip(grads)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/e1d0a8eac81bfce74977e2670201b85ff4f3f91b#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL191' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 12245812</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: e1d0a8eac81bfce74977e2670201b85ff4f3f91b</div><div id='time'> Time: 2022-05-17</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: Adam</div><div id='n_method'> N Class Name: Adam</div><div id='m_method'> M Method Name: apply_gradients(2)</div><div id='n_method'> N Method Name: apply_gradients(2)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 194</div><div id='m_end'> M End Line: 207</div><div id='n_start'> N Start Line: 200</div><div id='n_end'> N End Line: 217</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        grads, vars = zip(*grads_and_vars)
        <a id="change">if </a>self.weight_decay != 0.0:
            self.weight_decay<a id="change"> = </a><a id="change">tf.convert_to_tensor(</a>self.weight_decay<a id="change">)</a>
            <a id="change">for </a><a id="change">weight</a> in vars<a id="change">:
                </a><a id="change">weight.assign_sub(</a>self.weight_decay<a id="change"> * </a>weight<a id="change">)</a>
        if self.grad_clip is not None:
            if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                clip_grad, _ = self.grad_clip(grads)
            else:</code></pre><h3>After Change</h3><pre><code class='java'>
    def apply_gradients(self, grads_and_vars):
        if grads_and_vars is None:
            raise ValueError(&quotgrads_and_vars is not set.&quot)
        <a id="change">if self.weight_decay != 0.0 or self.grad_clip is not None</a>:
            grads, vars = zip(*grads_and_vars)
            <a id="change">if </a>self.weight_decay != 0.0:
                <a id="change">new_grads = </a><a id="change">[]</a>
                <a id="change">for </a>grad, <a id="change">var</a> in <a id="change">zip(</a>grads, vars<a id="change">):
                    </a>grad<a id="change"> = </a>grad<a id="change"> + </a>self.weight_decay<a id="change"> * </a>var
                    <a id="change">new_grads.append(</a>grad<a id="change">)</a>
                grads<a id="change"> = </a>new_grads
            if self.grad_clip is not None:
                if isinstance(self.grad_clip, tlx.ops.ClipByGlobalNorm):
                    new_grads, _ = self.grad_clip(grads)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/e1d0a8eac81bfce74977e2670201b85ff4f3f91b#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL122' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 12245813</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: e1d0a8eac81bfce74977e2670201b85ff4f3f91b</div><div id='time'> Time: 2022-05-17</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: Adagrad</div><div id='n_method'> N Class Name: Adagrad</div><div id='m_method'> M Method Name: apply_gradients(2)</div><div id='n_method'> N Method Name: apply_gradients(2)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 125</div><div id='m_end'> M End Line: 138</div><div id='n_start'> N Start Line: 128</div><div id='n_end'> N End Line: 145</div><BR>