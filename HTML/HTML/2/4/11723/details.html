<html><h3>Pattern ID :11723
</h3><img src='39545899.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        out = self.relu(out)
        spx = out
        
        outs = <a id="change">[]</a>
        for i in range(self.nums):
            if i==0 or self.stype==&quotstage&quot:
                sp = spx[:, i*self.width: (i+1)*self.width]
            else:
                sp = sp + spx[:, i*self.width: (i+1)*self.width]
            sp = self.convs[i](sp)
            sp = self.relu(self.bns[i](sp))
            <a id="change">outs.append(</a>sp<a id="change">)</a>
        if self.stype==&quotnormal&quot or self.stride==1:
            outs.append(spx[:, self.nums*self.width: (self.nums+1)*self.width])
        elif self.stype==&quotstage&quot:
            outs.append(self.pool(spx[:, self.nums*self.width: (self.nums+1)*self.width]))</code></pre><h3>After Change</h3><pre><code class='java'>
          else:
            out = jt.concat((out, sp), 1)
        if self.scale != 1 and self.stype==&quotnormal&quot:
          out<a id="change"> = </a><a id="change">jt.concat(</a>(out, spx[self.nums]),1<a id="change">)</a>
        elif self.scale != 1 and self.stype==&quotstage&quot:
          out = jt.concat((out, self.pool(spx[self.nums])),1)

        out = self.conv3(out)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/jittor/jittor/commit/c7b78f570e490793fee8e15b3bfcbec08683cdfe#diff-6195af628d95815512dea88a439379dbf51c55b3ac169d46df265908c1afb2b1L56' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 39545899</div><div id='project'> Project Name: jittor/jittor</div><div id='commit'> Commit Name: c7b78f570e490793fee8e15b3bfcbec08683cdfe</div><div id='time'> Time: 2021-04-03</div><div id='author'> Author: randonlang@gmail.com</div><div id='file'> File Name: python/jittor/models/res2net.py</div><div id='m_class'> M Class Name: Bottle2neck</div><div id='n_method'> N Class Name: Bottle2neck</div><div id='m_method'> M Method Name: execute(2)</div><div id='n_method'> N Method Name: execute(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: python/jittor/models/res2net.py</div><div id='n_file'> N File Name: python/jittor/models/res2net.py</div><div id='m_start'> M Start Line: 56</div><div id='m_end'> M End Line: 72</div><div id='n_start'> N Start Line: 67</div><div id='n_end'> N End Line: 84</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                                                                      interest_points,
                                                                      output_list,
                                                                      gradient_tape=g)
        outputs_jacobians_approx = <a id="change">[]</a>
        for output in outputs:  &#47&#47 Per model&quots output tensor
            output = tf.reshape(output, shape=[output.shape[0], -1])

            ipts_jac_trace_approx = []
            for ipt in tqdm(interest_points_tensors):  &#47&#47 Per Interest point activation tensor
                trace_jv = []
                for j in range(n_iter):  &#47&#47 Approximation iterations
                    &#47&#47 Getting a random vector with normal distribution
                    v = tf.random.normal(shape=output.shape)
                    f_v = tf.reduce_sum(v * output)

                    with g.stop_recording():
                        &#47&#47 Computing the jacobian approximation by getting the gradient of (output * v)
                        jac_v = g.gradient(f_v, ipt, unconnected_gradients=tf.UnconnectedGradients.ZERO)
                        jac_v = tf.reshape(jac_v, [jac_v.shape[0], -1])
                        jac_trace_approx = tf.reduce_mean(tf.reduce_sum(tf.pow(jac_v, 2.0)))

                        &#47&#47 If the change to the mean Jacobian approximation is insignificant we stop the calculation
                        if j &gt; MIN_JACOBIANS_ITER:
                            delta = np.mean([jac_trace_approx, *trace_jv]) - np.mean(trace_jv)
                            if np.abs(delta) / (np.abs(np.mean(trace_jv)) + 1e-6) &lt; JACOBIANS_COMP_TOLERANCE:
                                trace_jv.append(jac_trace_approx)
                                break

                        trace_jv.append(jac_trace_approx)
                ipts_jac_trace_approx.append(2 * tf.reduce_mean(trace_jv) / output.shape[-1])  &#47&#47 Get averaged squared jacobian trace approximation
            <a id="change">outputs_jacobians_approx.append(</a>ipts_jac_trace_approx<a id="change">)</a>

        mean_per_point = tf.reduce_mean(outputs_jacobians_approx, axis=0)  &#47&#47 Get mean of jacobian approx of all model outputs
        if norm_weights:
            return _normalize_weights(mean_per_point, all_outputs_indices, alpha)</code></pre><h3>After Change</h3><pre><code class='java'>
            Logger.critical("Can&quott concat model&quots outputs for gradients calculation since the shape of the first axis "  &#47&#47 pragma: no cover
                            "is not equal in all outputs.")

        output<a id="change"> = </a><a id="change">tf.concat(</a>r_outputs<a id="change">, axis=1)</a>

        ipts_jac_trace_approx = []
        for ipt in tqdm(interest_points_tensors):  &#47&#47 Per Interest point activation tensor
            trace_jv = []</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sony/model_optimization/commit/8489a444227d29f03755d326a710095dabdce7fc#diff-231946b7c3045ac9a8fddbfd5cae4ef7e0e05a7a0a16aab8a4d561e522cce80fL100' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 39545890</div><div id='project'> Project Name: sony/model_optimization</div><div id='commit'> Commit Name: 8489a444227d29f03755d326a710095dabdce7fc</div><div id='time'> Time: 2023-01-04</div><div id='author'> Author: ofirg6@gmail.com</div><div id='file'> File Name: model_compression_toolkit/core/keras/back2framework/model_gradients.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: keras_iterative_approx_jacobian_trace(8)</div><div id='n_method'> N Method Name: keras_iterative_approx_jacobian_trace(8)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: model_compression_toolkit/core/keras/back2framework/model_gradients.py</div><div id='n_file'> N File Name: model_compression_toolkit/core/keras/back2framework/model_gradients.py</div><div id='m_start'> M Start Line: 139</div><div id='m_end'> M End Line: 174</div><div id='n_start'> N Start Line: 141</div><div id='n_end'> N End Line: 181</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
				pvars.append(to_numpy(pvar).item())
				mses.append(to_numpy(mse).item())
		
		val_pvars = <a id="change">[]</a>
		inputs = self.raw_time_series[:, 0, :].clone().unsqueeze(1).to(self.model.device)
		for _ in range(100):
			val_x_pred = self.model.get_prediction_trace(inputs)
			pvar = PVarianceLoss()(val_x_pred, self.raw_time_series.to(val_x_pred.device))
			<a id="change">val_pvars.append(</a>to_numpy(pvar).item()<a id="change">)</a>
		print(f"Validation PVariance: {np.mean(val_pvars):.3f}")
		return x_pred, self.raw_time_series

	def compute_learning_signals(self, error: torch.Tensor):</code></pre><h3>After Change</h3><pre><code class='java'>
			self.eprop.on_batch_begin(self)
			inputs = self.true_time_series[:, 0, :].clone().unsqueeze(1).to(self.model.device)
			x_pred = self.model.get_prediction_trace(inputs)
			x_pred<a id="change"> = </a><a id="change">torch.concat(</a>[inputs, x_pred]<a id="change">, dim=1)</a>
			self.current_training_state = self.current_training_state.update(pred_batch=x_pred)
			self.eprop.on_batch_end(self)
			self.eprop.on_train_end(self)
			</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/neurotorch/neurotorch/commit/9ba260c32491b2e046fbd60f8599ede10b9e6273#diff-bfbe6bfd7f09fee9a898ae4ce6ce338f6bfc05e7db28469ba6540dfae09a766dL106' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 39545888</div><div id='project'> Project Name: neurotorch/neurotorch</div><div id='commit'> Commit Name: 9ba260c32491b2e046fbd60f8599ede10b9e6273</div><div id='time'> Time: 2023-02-01</div><div id='author'> Author: 93488840+AnthoDrouin@users.noreply.github.com</div><div id='file'> File Name: src/neurotorch/learning_algorithms/debug_e_prop_v5.py</div><div id='m_class'> M Class Name: SimplifiedEpropFinal</div><div id='n_method'> N Class Name: SimplifiedEpropFinal</div><div id='m_method'> M Method Name: train(4)</div><div id='n_method'> N Method Name: train(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/neurotorch/learning_algorithms/debug_e_prop_v5.py</div><div id='n_file'> N File Name: src/neurotorch/learning_algorithms/debug_e_prop_v5.py</div><div id='m_start'> M Start Line: 116</div><div id='m_end'> M End Line: 145</div><div id='n_start'> N Start Line: 126</div><div id='n_end'> N End Line: 146</div><BR>