<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            dataiter = iter(dataloader)
            &#47&#47 Set encoder and decoder to be in training mode

            for step in range(1, num_batches_per_epoch<a id="change"> + </a>1):
                batch<a id="change"> = </a><a id="change">next(</a>dataiter<a id="change">)</a>
                &#47&#47 Construct batch (currently just using Torch&quots default batch-creator)
                contexts, targets, traj_ts_info, extra_context = self.unpack_batch(batch)

                &#47&#47 Use an algorithm-specific augmentation strategy to augment either</code></pre><h3>After Change</h3><pre><code class='java'>

        loss_record = []
        num_batches_per_epoch = math.ceil(len(dataset)/self.batch_size)
        <a id="change">assert </a>len(dataloader) == num_batches_per_epoch, \
            "num_batches_per_dataset doesn&quott represent actual length of dataloader"
        assert num_batches_per_epoch &gt; 0, \
            f"num_batches_per_epoch is incorrectly 0: len(ds)={len(dataset)}, bs={self.batch_size}"</code></pre>