<html><h3>Pattern ID :3850
</h3><img src='14616236.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                fused_optims.append((path, sharded_module.fused_optimizer))
        else:
            for name, child in module.named_children():
                <a id="change">self._shard_modules_impl(
                    child</a>,
                    path + "." + name if path else name,
                    fused_optims<a id="change">,
                )</a>

    def _init_parameters(self, module: nn.Module) -&gt; None:
        @torch.no_grad()
        def init_parameters(module: nn.Module) -&gt; None:</code></pre><h3>After Change</h3><pre><code class='java'>
            )
            return module

        for name, <a id="change">child</a> in module.named_children():
            child = <a id="change">self._shard_modules_impl(
                child</a>,
                path + "." + name if path else name<a id="change">,
            )</a>
            setattr(module, name, child)

        return module
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 2</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/f13d0c06f6492b058beeb9fe7e2b9e2bbaa5f0f2#diff-088378575010e7b9e27ea06507ba52fcc8604bcf9cb107f4939a9a49b0d52c89L342' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14616236</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: f13d0c06f6492b058beeb9fe7e2b9e2bbaa5f0f2</div><div id='time'> Time: 2022-11-07</div><div id='author'> Author: dstaay@meta.com</div><div id='file'> File Name: torchrec/distributed/model_parallel.py</div><div id='m_class'> M Class Name: DistributedModelParallel</div><div id='n_method'> N Class Name: DistributedModelParallel</div><div id='m_method'> M Method Name: _shard_modules_impl(3)</div><div id='n_method'> N Method Name: _shard_modules_impl(4)</div><div id='m_parent_class'> M Parent Class: FusedOptimizerModule,nn.Module</div><div id='n_parent_class'> N Parent Class: FusedOptimizerModule,nn.Module</div><div id='m_file'> M File Name: torchrec/distributed/model_parallel.py</div><div id='n_file'> N File Name: torchrec/distributed/model_parallel.py</div><div id='m_start'> M Start Line: 342</div><div id='m_end'> M End Line: 371</div><div id='n_start'> N Start Line: 352</div><div id='n_end'> N End Line: 374</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            if isinstance(sharded_module, FusedOptimizerModule):
                fused_optims.append((path, sharded_module.fused_optimizer))
        else:
            for name, <a id="change">child</a> in module.named_children():
                <a id="change">self._shard_modules_impl(
                    </a>child,
                    path + "." + name if path else name,
                    fused_optims<a id="change">,
                )</a>

    def _init_parameters(self, module: nn.Module) -&gt; None:
        @torch.no_grad()
        def init_parameters(module: nn.Module) -&gt; None:</code></pre><h3>After Change</h3><pre><code class='java'>
            )
            return module

        for name, <a id="change">child</a> in module.named_children():
            child = <a id="change">self._shard_modules_impl(
                </a>child,
                path + "." + name if path else name<a id="change">,
            )</a>
            setattr(module, name, child)

        return module
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/f13d0c06f6492b058beeb9fe7e2b9e2bbaa5f0f2#diff-088378575010e7b9e27ea06507ba52fcc8604bcf9cb107f4939a9a49b0d52c89L338' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14616234</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: f13d0c06f6492b058beeb9fe7e2b9e2bbaa5f0f2</div><div id='time'> Time: 2022-11-07</div><div id='author'> Author: dstaay@meta.com</div><div id='file'> File Name: torchrec/distributed/model_parallel.py</div><div id='m_class'> M Class Name: DistributedModelParallel</div><div id='n_method'> N Class Name: DistributedModelParallel</div><div id='m_method'> M Method Name: _shard_modules_impl(3)</div><div id='n_method'> N Method Name: _shard_modules_impl(4)</div><div id='m_parent_class'> M Parent Class: FusedOptimizerModule,nn.Module</div><div id='n_parent_class'> N Parent Class: FusedOptimizerModule,nn.Module</div><div id='m_file'> M File Name: torchrec/distributed/model_parallel.py</div><div id='n_file'> N File Name: torchrec/distributed/model_parallel.py</div><div id='m_start'> M Start Line: 342</div><div id='m_end'> M End Line: 371</div><div id='n_start'> N Start Line: 352</div><div id='n_end'> N End Line: 374</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        path: str,
        fused_optims: List[Tuple[str, KeyedOptimizer]],
    ) -&gt; None:
        for name, <a id="change">child</a> in module.named_children():
            curr_path = path + name
            sharded_params = self._plan.get_plan_for_module(curr_path)
            sharder_key = sharder_name(type(child))
            if sharded_params:
                sharded_child = self._sharder_map[sharder_key].shard(
                    child,
                    sharded_params,
                    self._env,
                    self.device,
                )
                setattr(module, name, sharded_child)
                if isinstance(sharded_child, FusedOptimizerModule):
                    fused_optims.append((curr_path, sharded_child.fused_optimizer))
            else:
                <a id="change">self._shard_modules_impl(
                    </a>child,
                    curr_path + ".",
                    fused_optims<a id="change">,
                )</a>

    def _init_parameters(self, module: nn.Module) -&gt; None:
        @torch.no_grad()
        def init_parameters(module: nn.Module) -&gt; None:</code></pre><h3>After Change</h3><pre><code class='java'>
            if isinstance(sharded_module, FusedOptimizerModule):
                fused_optims.append((path, sharded_module.fused_optimizer))
        else:
            for name, <a id="change">child</a> in module.named_children():
                <a id="change">self._shard_modules_impl(
                    </a>child,
                    path + "." + name if path else name,
                    fused_optims<a id="change">,
                )</a>

    def _init_parameters(self, module: nn.Module) -&gt; None:
        @torch.no_grad()
        def init_parameters(module: nn.Module) -&gt; None:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/b72ed51545d0682547e903f4a921f8d3082ef888#diff-088378575010e7b9e27ea06507ba52fcc8604bcf9cb107f4939a9a49b0d52c89L303' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14616231</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: b72ed51545d0682547e903f4a921f8d3082ef888</div><div id='time'> Time: 2022-03-12</div><div id='author'> Author: jasonjk@fb.com</div><div id='file'> File Name: torchrec/distributed/model_parallel.py</div><div id='m_class'> M Class Name: DistributedModelParallel</div><div id='n_method'> N Class Name: DistributedModelParallel</div><div id='m_method'> M Method Name: _shard_modules_impl(4)</div><div id='n_method'> N Method Name: _shard_modules_impl(4)</div><div id='m_parent_class'> M Parent Class: FusedOptimizerModule,nn.Module</div><div id='n_parent_class'> N Parent Class: FusedOptimizerModule,nn.Module</div><div id='m_file'> M File Name: torchrec/distributed/model_parallel.py</div><div id='n_file'> N File Name: torchrec/distributed/model_parallel.py</div><div id='m_start'> M Start Line: 309</div><div id='m_end'> M End Line: 330</div><div id='n_start'> N Start Line: 309</div><div id='n_end'> N End Line: 336</div><BR>