<html><h3>Pattern ID :11306
</h3><img src='38461066.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def __getitem__(self, idx):
        &#47&#47 get index data
        data = self.data.iloc[self.data_index.index_start.iloc[idx] : self.data_index.index_end.iloc[idx]<a id="change"> + 1</a>].copy()

        &#47&#47 todo: handle missings -&gt; fill them up with strategy
        &#47&#47 determine data window
        sequence_length = len(data)
        max_prediction_length = self.max_prediction_length
        if self.randomize_length is not None:
            &#47&#47 modify sequence length
            sequence_length_prob, encode_length_probability = Beta(*self.randomize_length).sample(torch.Size([2]))
            sequence_length = int(max(1, Binomial(sequence_length, sequence_length_prob).sample()))
            max_prediction_length = int(max(1, Binomial(max_prediction_length, encode_length_probability).sample()))
            if sequence_length &lt; len(data):
                data = data.iloc[<a id="change">-sequence_length:</a>]  &#47&#47 select subset of sequence

        encode_length = min(max(0, sequence_length - max_prediction_length), self.max_encode_length)
        decode_length = sequence_length - encode_length</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 todo: handle missings -&gt; fill them up with strategy
        &#47&#47 determine data window
        sequence_length = len(data)
        <a id="change">assert </a>sequence_length &gt;= self.min_prediction_length
        &#47&#47 determine prediction/decode length and encode length
        decode_length = min(
            data.iloc[-1]["__time_idx__"] - (self.min_prediction_idx - 1), self.max_prediction_length, sequence_length</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/jdb78/pytorch-forecasting/commit/79cfec0818dbe78d8773534e6ce8f5fd578c3c3a#diff-e34740341caadea4baf7d22baa64e7e6ec9f8c5df41233e6a11682dc68310a89L149' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 38461066</div><div id='project'> Project Name: jdb78/pytorch-forecasting</div><div id='commit'> Commit Name: 79cfec0818dbe78d8773534e6ce8f5fd578c3c3a</div><div id='time'> Time: 2020-06-22</div><div id='author'> Author: beitner.jan@bcg.com</div><div id='file'> File Name: temporal_fusion_transformer_pytorch/data.py</div><div id='m_class'> M Class Name: TimeSeriesDataSet</div><div id='n_method'> N Class Name: TimeSeriesDataSet</div><div id='m_method'> M Method Name: __getitem__(2)</div><div id='n_method'> N Method Name: __getitem__(2)</div><div id='m_parent_class'> M Parent Class: Dataset</div><div id='n_parent_class'> N Parent Class: Dataset</div><div id='m_file'> M File Name: temporal_fusion_transformer_pytorch/data.py</div><div id='n_file'> N File Name: temporal_fusion_transformer_pytorch/data.py</div><div id='m_start'> M Start Line: 149</div><div id='m_end'> M End Line: 164</div><div id='n_start'> N Start Line: 186</div><div id='n_end'> N End Line: 220</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    if padding_mode == &quotreplicate&quot:
        &#47&#47 replication padding has some strange constraints...
        assert len(tensor.shape) - dim &lt;= 2
        padding = padding[<a id="change">:(len(tensor.shape) - 2) * 2</a>]

    tensor_ = F.pad(tensor, padding, padding_mode, padding_value)
</code></pre><h3>After Change</h3><pre><code class='java'>

    tensor = torch.movedim(tensor, dim, len(source_shape)-1)
    dim_last_shape = tensor.shape
    <a id="change">assert </a>tensor.shape[-1] == source_shape[dim]

    &#47&#47 we need reshape instead of view for batches like B x C x H x W
    tensor = tensor.reshape(-1, 1, source_shape[dim])</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/matthias-k/deepgaze/commit/a2f6037f9ae20086ff19775583ed036167449172#diff-8cfdb877f7ff2043ef6f90638b5f54cc4cd035e95961a7b9da1ddebee120c34fL120' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 38461064</div><div id='project'> Project Name: matthias-k/deepgaze</div><div id='commit'> Commit Name: a2f6037f9ae20086ff19775583ed036167449172</div><div id='time'> Time: 2022-06-20</div><div id='author'> Author: matthias.kuemmerer@bethgelab.org</div><div id='file'> File Name: deepgaze_pytorch/layers.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: gaussian_filter_1d(7)</div><div id='n_method'> N Method Name: gaussian_filter_1d(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: deepgaze_pytorch/layers.py</div><div id='n_file'> N File Name: deepgaze_pytorch/layers.py</div><div id='m_start'> M Start Line: 130</div><div id='m_end'> M End Line: 160</div><div id='n_start'> N Start Line: 128</div><div id='n_end'> N End Line: 164</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                                                    _cinput[:,feat_id]-pos2)
                    _cinput[:,feat_id] = torch.min(pos1*torch.ones(_cinput.size(0),1), 
                                                        _cinput[:,feat_id])
                    return _cinput[:,<a id="change">:</a>next_feat_id<a id="change">+1</a>] + offsets
                else:
                    raise ValueError(&quotinput tensor and embeddings_per_feat do not match. Double check inputs.&quot)
            </code></pre><h3>After Change</h3><pre><code class='java'>
            return self.qr_bucket_size
        
    def shard_tensor(self, _input: Tensor, rank: int) -&gt; Tensor:
        <a id="change">assert </a>_input.dim() == 2 \
            and _input.size(1) == len(self.embeddings_per_feat)
        if _input.device != self.device:
            _input = _input.to(self.device)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/hpcaitech/freqcacheembedding/commit/c457a4a7eae219f07d82bb57179f70e084c2948f#diff-5eb5715d975b7975822b38af2e5bd3c8cd410d833addf110e32fc3ca3e425e81L123' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 38461065</div><div id='project'> Project Name: hpcaitech/freqcacheembedding</div><div id='commit'> Commit Name: c457a4a7eae219f07d82bb57179f70e084c2948f</div><div id='time'> Time: 2022-07-19</div><div id='author'> Author: e0496101@u.nus.edu</div><div id='file'> File Name: recsys/modules/embeddings/parallel_mix_vocab_embedding.py</div><div id='m_class'> M Class Name: LoadBalanceManager</div><div id='n_method'> N Class Name: LoadBalanceManager</div><div id='m_method'> M Method Name: shard_tensor(3)</div><div id='n_method'> N Method Name: shard_tensor(3)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: recsys/modules/embeddings/parallel_mix_vocab_embedding.py</div><div id='n_file'> N File Name: recsys/modules/embeddings/parallel_mix_vocab_embedding.py</div><div id='m_start'> M Start Line: 130</div><div id='m_end'> M End Line: 166</div><div id='n_start'> N Start Line: 132</div><div id='n_end'> N End Line: 180</div><BR>