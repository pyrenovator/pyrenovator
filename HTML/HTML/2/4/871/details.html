<html><h3>Pattern ID :871
</h3><img src='4275569.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 implicit grad
    implicit_grad = torch.autograd.grad(loss, path[1].trainable_parameters())
    for i in range(1, len(path)-1):
        implicit_grad = darts_helper(implicit_grad, path[i], <a id="change">path[i+1]</a>, config)

    return [add_with_none(dg, ig) for dg, ig in zip(direct_grad, implicit_grad)]
</code></pre><h3>After Change</h3><pre><code class='java'>
    eps = R / to_vec(vector).norm()

    &#47&#47 positive
    <a id="change">for </a>p, v in zip(curr.trainable_parameters(), vector)<a id="change">:
        </a><a id="change">p.data.add_(</a>v.data<a id="change">, alpha=eps)</a>
    loss_p = curr.training_step(curr.cur_batch)
    grad_p = torch.autograd.grad(loss_p, prev.trainable_parameters())

    &#47&#47 negative</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/leopard-ai/betty/commit/5542eb6dd30fc3b008a0626760a339f3fe31088e#diff-fee71c6e11eca0a047fb000e6ce9c0986274faf27ef8f81ac06ee49870b9ef48L6' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4275569</div><div id='project'> Project Name: leopard-ai/betty</div><div id='commit'> Commit Name: 5542eb6dd30fc3b008a0626760a339f3fe31088e</div><div id='time'> Time: 2022-06-03</div><div id='author'> Author: sangkeuc@andrew.cmu.edu</div><div id='file'> File Name: betty/hypergradient/darts.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: darts(3)</div><div id='n_method'> N Method Name: darts(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: betty/hypergradient/darts.py</div><div id='n_file'> N File Name: betty/hypergradient/darts.py</div><div id='m_start'> M Start Line: 6</div><div id='m_end'> M End Line: 49</div><div id='n_start'> N Start Line: 6</div><div id='n_end'> N End Line: 36</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = <a id="change">group[&quotlr&quot]</a>
            lamda = group[&quotlamda&quot]

            &#47&#47 group.setdefault(&quotparams_old&quot, group[&quotparams&quot])
</code></pre><h3>After Change</h3><pre><code class='java'>
    @torch.no_grad()
    def step(self, global_params, device):
        for group in self.param_groups:
            <a id="change">for </a>p, <a id="change">g</a> in zip(group[&quotparams&quot], global_params)<a id="change">:
                </a>g = g.to(device)
                d_p = p.grad.data + group[&quotmu&quot] * (p.data - g.data)
                <a id="change">p.data.add_(</a>d_p<a id="change">, alpha=-group[&quotlr&quot])</a>
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tsingz0/pfl-non-iid/commit/330ce2d6169fd4a54cec28895418bd38c206b6a4#diff-a99c19163c19c03eede69bdda22f1a2d923b8eb7d0f4b0463d797d66d5fc8e87L89' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4275568</div><div id='project'> Project Name: tsingz0/pfl-non-iid</div><div id='commit'> Commit Name: 330ce2d6169fd4a54cec28895418bd38c206b6a4</div><div id='time'> Time: 2021-03-24</div><div id='author'> Author: 2719584131@qq.com</div><div id='file'> File Name: system/flcore/optimizers/fedoptimizer.py</div><div id='m_class'> M Class Name: PerturbedGradientDescent</div><div id='n_method'> N Class Name: PerturbedGradientDescent</div><div id='m_method'> M Method Name: step(3)</div><div id='n_method'> N Method Name: step(1)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: system/flcore/optimizers/fedoptimizer.py</div><div id='n_file'> N File Name: system/flcore/optimizers/fedoptimizer.py</div><div id='m_start'> M Start Line: 89</div><div id='m_end'> M End Line: 108</div><div id='n_start'> N Start Line: 116</div><div id='n_end'> N End Line: 121</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                                      allow_unused=allow_unused)

    &#47&#47 implicit grad
    implicit_grad = torch.autograd.grad(loss, <a id="change">path[1]</a>.trainable_parameters())
    for i in range(1, len(path)-1):
        implicit_grad = darts_helper(implicit_grad, path[i], path[i+1], config)
</code></pre><h3>After Change</h3><pre><code class='java'>
    eps = R / to_vec(vector).norm()

    &#47&#47 positive
    <a id="change">for </a>p, <a id="change">v</a> in zip(curr.trainable_parameters(), vector)<a id="change">:
        </a><a id="change">p.data.add_(</a>v.data<a id="change">, alpha=eps)</a>
    loss_p = curr.training_step(curr.cur_batch)
    grad_p = torch.autograd.grad(loss_p, prev.trainable_parameters())

    &#47&#47 negative</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leopard-ai/betty/commit/5542eb6dd30fc3b008a0626760a339f3fe31088e#diff-fee71c6e11eca0a047fb000e6ce9c0986274faf27ef8f81ac06ee49870b9ef48L6' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4275571</div><div id='project'> Project Name: leopard-ai/betty</div><div id='commit'> Commit Name: 5542eb6dd30fc3b008a0626760a339f3fe31088e</div><div id='time'> Time: 2022-06-03</div><div id='author'> Author: sangkeuc@andrew.cmu.edu</div><div id='file'> File Name: betty/hypergradient/darts.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: darts(3)</div><div id='n_method'> N Method Name: darts(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: betty/hypergradient/darts.py</div><div id='n_file'> N File Name: betty/hypergradient/darts.py</div><div id='m_start'> M Start Line: 6</div><div id='m_end'> M End Line: 49</div><div id='n_start'> N Start Line: 6</div><div id='n_end'> N End Line: 36</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            graph.srcdata[&quoth&quot] = feat_src
            graph.apply_edges(lambda edges: {&quoth&quot: edges.data[&quot_edge_weight&quot] * 2})
            graph.update_all(aggregate_fn, fn.sum(msg=&quotm&quot, out=&quoth&quot))
            rst = <a id="change">graph.dstdata[&quoth&quot]</a>
            rst = th.matmul(feat_src, self.weight1) + th.matmul(rst, self.weight2)


            if self.activation is not None:</code></pre><h3>After Change</h3><pre><code class='java'>

            &#47&#47 aggregate first then mult W
            hg.srcdata[&quoth&quot] = feat_src
            <a id="change">for e</a> in hg.canonical_etypes<a id="change">:
                </a>stype, etype, dtype = e
                sub_graph = hg[stype, etype, dtype]
                sub_graph.update_all(aggregate_fn, fn.sum(msg=&quotm&quot, out=&quotout&quot))
                temp = hg.ndata[&quotout&quot].pop(dtype)
                if isinstance(temp, dict):
                    temp = temp[dtype]
                if outputs.get(dtype) is None:
                    outputs[dtype] = temp
                else:
                    <a id="change">outputs[dtype].add_(</a>temp<a id="change">)</a>

            def _apply(ntype, h):

                h = th.matmul(feat[ntype], self.weight1) + th.matmul(h, self.weight2)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/bupt-gamma/openhgnn/commit/8673a0f313e6be0f6f7ccaf65aaf993769e4adf9#diff-429162e7bcdfbbc409aabd0e6317a9f8bd71ecc0fdb9f27c08cab2cd0cda454cL156' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4275574</div><div id='project'> Project Name: bupt-gamma/openhgnn</div><div id='commit'> Commit Name: 8673a0f313e6be0f6f7ccaf65aaf993769e4adf9</div><div id='time'> Time: 2021-05-23</div><div id='author'> Author: theheavenszhao@outlook.com</div><div id='file'> File Name: openhgnn/models/RSHN.py</div><div id='m_class'> M Class Name: GraphConv</div><div id='n_method'> N Class Name: GraphConv</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: openhgnn/models/RSHN.py</div><div id='n_file'> N File Name: openhgnn/models/RSHN.py</div><div id='m_start'> M Start Line: 170</div><div id='m_end'> M End Line: 179</div><div id='n_start'> N Start Line: 155</div><div id='n_end'> N End Line: 186</div><BR>