<html><h3>Pattern ID :39817
</h3><img src='113356448.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    parse_aliases = False
    parse_claims = False

    <a id="change">with </a>bz2<a id="change">.open(wikidata_file, mode="rb") as file:
        </a>line = file.readline()
        cnt = 0
        while line and (not limit or cnt &lt; limit):
            if cnt % 1000000 == 0:
                print(
                    datetime.datetime.now(), "processed", cnt, "lines of WikiData JSON dump"
                )
            clean_line = line.strip()
            if clean_line.endswith(b","):
                clean_line = clean_line[:-1]
            if len(clean_line) &gt; 1:
                obj = json.loads(clean_line)
                entry_type = obj["type"]

                if entry_type == "item":
                    &#47&#47 filtering records on their properties (currently disabled to get ALL data)
                    &#47&#47 keep = False
                    keep = True

                    claims = obj["claims"]
                    if parse_claims:
                        for prop, value_set in prop_filter.items():
                            claim_property = claims.get(prop, None)
                            if claim_property:
                                for cp in claim_property:
                                    cp_id = (
                                        cp["mainsnak"]
                                        .get("datavalue", {})
                                        .get("value", {})
                                        .get("id")
                                    )
                                    cp_rank = cp["rank"]
                                    if cp_rank != "deprecated" and cp_id in value_set:
                                        keep = True

                    if keep:
                        unique_id = obj["id"]

                        if to_print:
                            print("ID:", unique_id)
                            print("type:", entry_type)

                        &#47&#47 parsing all properties that refer to other entities
                        if parse_properties:
                            for prop, claim_property in claims.items():
                                cp_dicts = [
                                    cp["mainsnak"]["datavalue"].get("value")
                                    for cp in claim_property
                                    if cp["mainsnak"].get("datavalue")
                                ]
                                cp_values = [
                                    cp_dict.get("id")
                                    for cp_dict in cp_dicts
                                    if isinstance(cp_dict, dict)
                                    if cp_dict.get("id") is not None
                                ]
                                if cp_values:
                                    if to_print:
                                        print("prop:", prop, cp_values)

                        found_link = False
                        if parse_sitelinks:
                            site_value = obj["sitelinks"].get(site_filter, None)
                            if site_value:
                                site = site_value["title"]
                                if to_print:
                                    print(site_filter, ":", site)
                                title_to_id[site] = unique_id
                                found_link = True

                        if parse_labels:
                            labels = obj["labels"]
                            if labels:
                                lang_label = labels.get(lang, None)
                                if lang_label:
                                    if to_print:
                                        print(
                                            "label (" + lang + "):", lang_label["value"]
                                        )

                        if found_link and parse_descriptions:
                            descriptions = obj["descriptions"]
                            if descriptions:
                                lang_descr = descriptions.get(lang, None)
                                if lang_descr:
                                    if to_print:
                                        print(
                                            "description (" + lang + "):",
                                            lang_descr["value"],
                                        )
                                    id_to_descr[unique_id] = lang_descr["value"]

                        if parse_aliases:
                            aliases = obj["aliases"]
                            if aliases:
                                lang_aliases = aliases.get(lang, None)
                                if lang_aliases:
                                    for item in lang_aliases:
                                        if to_print:
                                            print(
                                                "alias (" + lang + "):", item["value"]
                                            )

                        if to_print:
                            print()
            line<a id="change"> = </a>file.readline()
            cnt += 1
        print(datetime.datetime.now(), "processed", cnt, "lines of WikiData JSON dump")
</code></pre><h3>After Change</h3><pre><code class='java'>
    parse_claims = False

    with gzip.open(wikidata_file, mode=&quotrb&quot) as file:
        <a id="change">for </a>cnt, line in <a id="change">enumerate(</a>file<a id="change">):
            </a>if limit and cnt &gt;= limit:
                break
            if cnt % 500000 == 0:
                logger.info("processed {} lines of WikiData dump".format(cnt))</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/explosion/spaCy/commit/a6830d60e8c4acc6f378a3b4e6c48e851e729408#diff-4ff027b00c6616a42dedaf91f8b1258a197c3f1f471d3615b6309227d0918e9dL12' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 113356448</div><div id='project'> Project Name: explosion/spaCy</div><div id='commit'> Commit Name: a6830d60e8c4acc6f378a3b4e6c48e851e729408</div><div id='time'> Time: 2019-09-13</div><div id='author'> Author: euan.dowers@barcelonagse.eu</div><div id='file'> File Name: bin/wiki_entity_linking/wikidata_processor.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: read_wikidata_entities_json(5)</div><div id='n_method'> N Method Name: read_wikidata_entities_json(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: bin/wiki_entity_linking/wikidata_processor.py</div><div id='n_file'> N File Name: bin/wiki_entity_linking/wikidata_processor.py</div><div id='m_start'> M Start Line: 13</div><div id='m_end'> M End Line: 141</div><div id='n_start'> N Start Line: 12</div><div id='n_end'> N End Line: 137</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        anno_files = [s for s in dir_items(task_dir, &quot.txt&quot)
            if s.endswith(&quot_&quot + osp.basename(self._path))]
        for ann_filename in anno_files:
            <a id="change">with </a>open<a id="change">(osp.join(task_dir, ann_filename), encoding=&quotutf-8&quot) as f:
                </a>label = ann_filename[:ann_filename.rfind(&quot_&quot)]
                label_id<a id="change"> = </a>self._get_label_id(label)
                for line in f:
                    item, present = line.rsplit(maxsplit=1)
                    if present == &quot1&quot:</code></pre><h3>After Change</h3><pre><code class='java'>
    def _load_annotations(self):
        annotations = {}
        task_dir = osp.dirname(self._path)
        <a id="change">for </a>label_id, label in <a id="change">enumerate(</a>self._categories[AnnotationType.label]<a id="change">):
            </a>ann_file = osp.join(task_dir, f&quot{label.name}_{self._subset}.txt&quot)
            if not osp.isfile(ann_file):
                continue
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/openvinotoolkit/datumaro/commit/4dba6e4eafd99aed88b44908cd9c0e32be0bf272#diff-4dcdddf38e9e5559f858f2ffcd5ad300b51ce9877dbaee2878bd9c8b215b4e97L101' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 113356450</div><div id='project'> Project Name: openvinotoolkit/datumaro</div><div id='commit'> Commit Name: 4dba6e4eafd99aed88b44908cd9c0e32be0bf272</div><div id='time'> Time: 2021-10-14</div><div id='author'> Author: maxim.zhiltsov@intel.com</div><div id='file'> File Name: datumaro/plugins/voc_format/extractor.py</div><div id='m_class'> M Class Name: VocClassificationExtractor</div><div id='n_method'> N Class Name: VocClassificationExtractor</div><div id='m_method'> M Method Name: _load_annotations(1)</div><div id='n_method'> N Method Name: _load_annotations(1)</div><div id='m_parent_class'> M Parent Class: _VocExtractor</div><div id='n_parent_class'> N Parent Class: _VocExtractor</div><div id='m_file'> M File Name: datumaro/plugins/voc_format/extractor.py</div><div id='n_file'> N File Name: datumaro/plugins/voc_format/extractor.py</div><div id='m_start'> M Start Line: 102</div><div id='m_end'> M End Line: 115</div><div id='n_start'> N Start Line: 99</div><div id='n_end'> N End Line: 112</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                h = F.dropout(h, p=self.dropout, training=False)

        edge_weight = F.embedding(g.edata[dgl.ETYPE].long(), h)
        <a id="change">with </a>g<a id="change">.local_scope():
            </a>h = self.feats
            edge_weight<a id="change"> = </a>self.linear_e1(edge_weight)
            x = self.nn_conv1(g, h, edge_weight=edge_weight)
            x = F.dropout(x, p=self.dropout, training=False)
            if self.num_node_layer == 2:</code></pre><h3>After Change</h3><pre><code class='java'>
            h = F.dropout(h, p=self.dropout, training=False)

        edge_weight = {}
        <a id="change">for </a>i, e in <a id="change">enumerate(</a>hg.canonical_etypes<a id="change">):
            </a>edge_weight[e] = h[i].expand(hg.num_edges(e), -1)
        temp = self.h_n_dict()
        n_feats = {}
        for n in hg.ntypes:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/bupt-gamma/openhgnn/commit/5b978d8c10bf24304d51be5f4c2c306d5720b7b3#diff-429162e7bcdfbbc409aabd0e6317a9f8bd71ecc0fdb9f27c08cab2cd0cda454cL48' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 113356437</div><div id='project'> Project Name: bupt-gamma/openhgnn</div><div id='commit'> Commit Name: 5b978d8c10bf24304d51be5f4c2c306d5720b7b3</div><div id='time'> Time: 2021-04-14</div><div id='author'> Author: theheavenszhao@outlook.com</div><div id='file'> File Name: openhgnn/models/RSHN.py</div><div id='m_class'> M Class Name: RSHN</div><div id='n_method'> N Class Name: RSHN</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: BaseModel</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: openhgnn/models/RSHN.py</div><div id='n_file'> N File Name: openhgnn/models/RSHN.py</div><div id='m_start'> M Start Line: 49</div><div id='m_end'> M End Line: 74</div><div id='n_start'> N Start Line: 65</div><div id='n_end'> N End Line: 97</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def _predict(self, vocab_id):
        predicted = self.predicted
        <a id="change">with </a>torch<a id="change">.inference_mode():
            </a>pred_word = self.actions[vocab_id]
            if pred_word in self.gen_stop_toks \
                    or len(pred_word) &lt; 1 \
                    or len(self.predicted) &gt; self.env_max_length:
                return predicted, True, self.tokenizer.convert_tokens_to_string(predicted)
            else:
                predicted<a id="change"> += </a>[pred_word]
                return predicted, False, self.tokenizer.convert_tokens_to_string(predicted)
</code></pre><h3>After Change</h3><pre><code class='java'>
        predicted_list = {}
        predicted_list_end = {}
        with torch.inference_mode():
            <a id="change">for </a>i, (v_id, predicted, predicted_end) in <a id="change">enumerate(</a>zip(vocab_id, self.predicted, self.predicted_end)<a id="change">):
                </a>predicted_list_end[i] = False
                if not predicted_end:
                    pred_word = self.actions[v_id]
                    if pred_word in self.gen_stop_toks \</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/voidful/textrl/commit/285b19d7c0829ec1176b5c61e6ed3960744d307c#diff-34bd53460ee7026ed47dc10c97fc4e1a427ed0dd952c7f338ba96998b0c18c69L85' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 113356443</div><div id='project'> Project Name: voidful/textrl</div><div id='commit'> Commit Name: 285b19d7c0829ec1176b5c61e6ed3960744d307c</div><div id='time'> Time: 2023-02-06</div><div id='author'> Author: voidful.stack@gmail.com</div><div id='file'> File Name: textrl/environment.py</div><div id='m_class'> M Class Name: TextRLEnv</div><div id='n_method'> N Class Name: TextRLEnv</div><div id='m_method'> M Method Name: _predict(2)</div><div id='n_method'> N Method Name: _predict(2)</div><div id='m_parent_class'> M Parent Class: gym.Env</div><div id='n_parent_class'> N Parent Class: gym.Env</div><div id='m_file'> M File Name: textrl/environment.py</div><div id='n_file'> N File Name: textrl/environment.py</div><div id='m_start'> M Start Line: 86</div><div id='m_end'> M End Line: 95</div><div id='n_start'> N Start Line: 98</div><div id='n_end'> N End Line: 121</div><BR>