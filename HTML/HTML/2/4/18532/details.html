<html><h3>Pattern ID :18532
</h3><img src='60489086.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            if exists(attn_bias):
                attn_bias = repeat(attn_bias, &quotb h i j -&gt; (b x) h i j&quot, x = w)

            tie_dim = w<a id="change"> if </a>self.global_query_attn<a id="change"> else </a>None
            w_out = self.attn(w_x, mask = w_mask, attn_bias = attn_bias, tie_dim = tie_dim)
            w_out = rearrange(w_out, &quot(b w) h d -&gt; b h w d&quot, h = h, w = w)
</code></pre><h3>After Change</h3><pre><code class='java'>
        attn_bias = None
        if exists(self.edges_to_attn_bias) and exists(edges):
            attn_bias = self.edges_to_attn_bias(edges)
            attn_bias<a id="change"> = </a><a id="change">repeat(</a>attn_bias, <a id="change">&quotb h i j -&gt; (b x) h i j&quot</a><a id="change">, x = axial_dim)</a>

        tie_dim = axial_dim if self.global_query_attn else None

        out = self.attn(x, mask = mask, attn_bias = attn_bias, tie_dim = tie_dim)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/alphafold2/commit/a53a3eb56d47562dfe5d49a28782fa584676f4f1#diff-ec920ef561468dc34d1b12371582e439c6a12a81ed9904c9608bb9935e895824L220' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60489086</div><div id='project'> Project Name: lucidrains/alphafold2</div><div id='commit'> Commit Name: a53a3eb56d47562dfe5d49a28782fa584676f4f1</div><div id='time'> Time: 2021-07-27</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: alphafold2_pytorch/alphafold2.py</div><div id='m_class'> M Class Name: AxialAttention</div><div id='n_method'> N Class Name: AxialAttention</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: alphafold2_pytorch/alphafold2.py</div><div id='n_file'> N File Name: alphafold2_pytorch/alphafold2.py</div><div id='m_start'> M Start Line: 220</div><div id='m_end'> M End Line: 262</div><div id='n_start'> N Start Line: 220</div><div id='n_end'> N End Line: 249</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                                  for i in range(length) ])
            &#47&#47 the angle for placing oxygen is opposite to psi of current res.
            &#47&#47 psi not available for last one so pi/4 taken for now
            correction = torch.tensor([ -np.pi<a id="change"> if </a>i&lt; length-1<a id="change"> else </a>0 for i in range(length)], device=psis.device)
            new_coords[:, :, 3] = nerf_torch(new_coords[:, :, 0], 
                                             new_coords[:, :, 1], 
                                             new_coords[:, :, 2], </code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 psi not available for last one so pi/4 taken for now
            bond_lens  = repeat(torch.tensor(BB_BUILD_INFO["BONDLENS"]["c-o"]), &quot -&gt; b&quot, b=length).to(psis.device)
            bond_angs  = repeat(torch.tensor(BB_BUILD_INFO["BONDANGS"]["ca-c-o"]), &quot -&gt; b&quot, b=length).to(psis.device)
            correction<a id="change"> = </a><a id="change">repeat(</a>torch.tensor(-np.pi), <a id="change">&quot -&gt; b&quot</a><a id="change">, b=length)</a>.to(psis.device) 
            new_coords[:, :, 3] = nerf_torch(new_coords[:, :, 0], 
                                             new_coords[:, :, 1], 
                                             new_coords[:, :, 2], </code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/alphafold2/commit/7cab59f3037b77862bb24ea0dc1001a84462e82e#diff-f9b2e05be1f80257f9a80ec7cc1290e3337a54fcc62aeee31b23a4cb827f12efL216' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60489087</div><div id='project'> Project Name: lucidrains/alphafold2</div><div id='commit'> Commit Name: 7cab59f3037b77862bb24ea0dc1001a84462e82e</div><div id='time'> Time: 2021-02-07</div><div id='author'> Author: ericalcaide1@gmail.com</div><div id='file'> File Name: alphafold2_pytorch/utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: sidechain_container(4)</div><div id='n_method'> N Method Name: sidechain_container(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: alphafold2_pytorch/utils.py</div><div id='n_file'> N File Name: alphafold2_pytorch/utils.py</div><div id='m_start'> M Start Line: 240</div><div id='m_end'> M End Line: 250</div><div id='n_start'> N Start Line: 242</div><div id='n_end'> N End Line: 252</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 weighted sum of values and combine heads

        aggregate_einsum_note = &quotb h i j, b h j d -&gt; b h i d&quot<a id="change"> if </a>not exists(nbhd_indices)<a id="change"> else </a>&quotb h i j, b h i j d -&gt; b h i d&quot
        out = einsum(aggregate_einsum_note, attn, v)
        out = rearrange(out, &quotb h n d -&gt; b n (h d)&quot)
        out = self.to_out(out)</code></pre><h3>After Change</h3><pre><code class='java'>
            rel_coors = batched_index_select(rel_coors, nbhd_indices, dim = 2)
        else:
            k = repeat(k, &quotb h j d -&gt; b h n j d&quot, n = n)
            v<a id="change"> = </a><a id="change">repeat(</a>v, <a id="change">&quotb h j d -&gt; b h n j d&quot</a><a id="change">, n = n)</a>

        rel_dist_pos_emb = self.to_pos_emb(rel_dist)

        &#47&#47 inject position into values</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/en-transformer/commit/a8449dd45f8b7b872705db7a4053d3470ee4739b#diff-f288a1692c1c58a186cb9ac763046f632757db1647271b97852e59e3fc5696b9L152' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60489084</div><div id='project'> Project Name: lucidrains/en-transformer</div><div id='commit'> Commit Name: a8449dd45f8b7b872705db7a4053d3470ee4739b</div><div id='time'> Time: 2021-03-27</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: en_transformer/en_transformer.py</div><div id='m_class'> M Class Name: EquivariantAttention</div><div id='n_method'> N Class Name: EquivariantAttention</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: en_transformer/en_transformer.py</div><div id='n_file'> N File Name: en_transformer/en_transformer.py</div><div id='m_start'> M Start Line: 164</div><div id='m_end'> M End Line: 254</div><div id='n_start'> N Start Line: 177</div><div id='n_end'> N End Line: 256</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            v = batched_index_select(v, indices, dim = 2)
            qk_rel = batched_index_select(qk_rel, indices, dim = 2)
            rel_pos_emb = batched_index_select(rel_pos_emb, indices, dim = 2)
            mask = batched_index_select(mask, indices, dim = 2)<a id="change"> if </a>exists(mask)<a id="change"> else </a>None

        &#47&#47 add relative positional embeddings to value
</code></pre><h3>After Change</h3><pre><code class='java'>

            dist, indices = rel_dist.topk(num_neighbors, largest = False)

            indices_with_heads<a id="change"> = </a><a id="change">repeat(</a>indices, <a id="change">&quotb i j -&gt; b h i j&quot</a><a id="change">, h = h)</a>

            v = batched_index_select(v, indices_with_heads, dim = 3)
            qk_rel = batched_index_select(qk_rel, indices_with_heads, dim = 3)
            rel_pos_emb = batched_index_select(rel_pos_emb, indices_with_heads, dim = 3)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/point-transformer-pytorch/commit/99bc3958138d8c9d3b882e4ac50b1a18a86160fe#diff-86745e6c9b49a24c7fba780e904e254c675b41beaecc25803a17adc67f0035e9L65' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60489085</div><div id='project'> Project Name: lucidrains/point-transformer-pytorch</div><div id='commit'> Commit Name: 99bc3958138d8c9d3b882e4ac50b1a18a86160fe</div><div id='time'> Time: 2022-01-13</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: point_transformer_pytorch/multihead_point_transformer_pytorch.py</div><div id='m_class'> M Class Name: MultiheadPointTransformerLayer</div><div id='n_method'> N Class Name: MultiheadPointTransformerLayer</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: point_transformer_pytorch/multihead_point_transformer_pytorch.py</div><div id='n_file'> N File Name: point_transformer_pytorch/multihead_point_transformer_pytorch.py</div><div id='m_start'> M Start Line: 92</div><div id='m_end'> M End Line: 127</div><div id='n_start'> N Start Line: 92</div><div id='n_end'> N End Line: 133</div><BR>