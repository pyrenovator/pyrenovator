<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    optimizer = init_optimizer(model.parameters(), **optimizer_kwargs(args))
    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=args.stepsize, gamma=args.gamma)

    <a id="change">if </a>args.fixbase_epoch &gt; 0:
        if hasattr(model, &quotclassifier&quot) and isinstance(model.classifier, nn.Module):
            optimizer_tmp = init_optimizer(model.classifier.parameters(), **optimizer_kwargs(args))
        else:
            print("Warn: model has no attribute &quotclassifier&quot and fixbase_epoch is reset to 0")
            args.fixbase_epoch = 0
        <a id="change">raise </a>NotImplementedError

    if args.load_weights and check_isfile(args.load_weights):
        &#47&#47 load pretrained weights but ignore layers that don&quott match in size</code></pre><h3>After Change</h3><pre><code class='java'>

    if args.fixbase_epoch &gt; 0:
        print("Train {} for {} epochs while keeping other layers frozen".format(args.open_layers, args.fixbase_epoch))
        initial_optim_state<a id="change"> = </a>optimizer.state_dict()

        for epoch in range(args.fixbase_epoch):
            start_train_time = time.time()
            train(epoch, model, criterion, optimizer, trainloader, use_gpu, fixbase=True)
            train_time += round(time.time() - start_train_time)

        print("Done. All layers are open to train for {} epochs".format(args.max_epoch))
        <a id="change">optimizer.load_state_dict(</a>initial_optim_state<a id="change">)</a>

    for epoch in range(args.start_epoch, args.max_epoch):
        start_train_time = time.time()
        train(epoch, model, criterion, optimizer, trainloader, use_gpu)</code></pre>