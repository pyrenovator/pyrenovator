<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    Uses hugginface generate (https://huggingface.co/transformers/main_classes/model.html?highlight=generate&#47&#47transformers.TFPreTrainedModel.generate)
    With tokenizer.padding size = left, otherwise generation is random (issue https://github.com/huggingface/transformers/issues/3021)
    
    encodings_dict<a id="change"> = </a><a id="change">tokenizer(
        </a>prompts<a id="change">, truncation=True, max_length=constants.MAX_SEQ_LEN)</a>
    prompts_ids = torch.tensor(
        encodings_dict[&quotinput_ids&quot], device=device, dtype=torch.long)
    first_idx = len(prompts_ids[0]) if first_idx else 0
</code></pre><h3>After Change</h3><pre><code class='java'>
    With tokenizer.padding size = left, otherwise generation is random (issue https://github.com/huggingface/transformers/issues/3021)
    
    assert len(prompts) == 1, "Generate function assumes one prompt"
    encodings_dict = <a id="change">tokenizer(
        </a>prompts<a id="change">)</a>
    &#47&#47 Truncates tokens from the end of the sequence.
    sliced_inputs = [encodings_dict[&quotinput_ids&quot][0][-constants.MAX_SEQ_LEN:]]
    prompts_ids<a id="change"> = </a>torch.tensor(
        sliced_inputs, device=device, dtype=torch.long)
    first_idx = len(prompts_ids[0]) if first_idx else 0
</code></pre>