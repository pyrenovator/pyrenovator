<html><h3>Pattern ID :5994
</h3><img src='20991691.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    train_loss = train_loss + l1_loss + ssim_loss * 50 + duration_loss * 4 + pitch_loss * 4 + energy_loss * 4
                    if step_counter &gt; postnet_start_steps:
                        train_loss = train_loss + glow_loss
                    if <a id="change">step_counter &gt; kl_start_steps and step_counter &gt; encoder_pretraining_steps</a>:
                        train_loss = train_loss + kl_loss * min(0.05 + 0.00001 * (step_counter - kl_start_steps),
                                                                0.2)  &#47&#47 linear increase over 15k steps
</code></pre><h3>After Change</h3><pre><code class='java'>
            del train_loss
            step_counter += 1
            scaler.unscale_(optimizer)
            if <a id="change">step_counter &gt; postnet_start_steps</a>:
                <a id="change">scaler.unscale_(</a>optimizer_postflow<a id="change">)</a>
            torch.nn.utils.clip_grad_norm_(net.parameters(), 0.8, error_if_nonfinite=False)
            scaler.step(optimizer)
            if step_counter &gt; postnet_start_steps:
                scaler.step(optimizer_postflow)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/00201cac339db5bddb187d80f4d1526f57a281e8#diff-241ea0a4da614fa4b0dbd748ea10aae1c34265d57f46001c62ec2994245ee0f7L51' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 20991691</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 00201cac339db5bddb187d80f4d1526f57a281e8</div><div id='time'> Time: 2022-12-18</div><div id='author'> Author: lux.florian@gmail.com</div><div id='file'> File Name: TrainingInterfaces/Text_to_Spectrogram/PortaSpeech/portaspeech_train_loop.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train_loop(17)</div><div id='n_method'> N Method Name: train_loop(18)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: TrainingInterfaces/Text_to_Spectrogram/PortaSpeech/portaspeech_train_loop.py</div><div id='n_file'> N File Name: TrainingInterfaces/Text_to_Spectrogram/PortaSpeech/portaspeech_train_loop.py</div><div id='m_start'> M Start Line: 51</div><div id='m_end'> M End Line: 232</div><div id='n_start'> N Start Line: 72</div><div id='n_end'> N End Line: 219</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 Writer Loss to file
        writer.add_scalar("Train/Loss", loss.item(), index + epoch * batches + 1)
        if <a id="change">index % config.print_frequency == 0 and index != 0</a>:
            progress.display(index)

</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 enable preload
    train_prefetcher.reset()
    batch_data = train_prefetcher.next()
    while <a id="change">batch_data is not None</a>:
        &#47&#47 measure data loading time
        data_time.update(time.time() - end)

        lr = batch_data["lr"].to(config.device, non_blocking=True)
        hr = batch_data["hr"].to(config.device, non_blocking=True)

        &#47&#47 Initialize the generator gradient
        model.zero_grad()

        &#47&#47 Mixed precision training
        with amp.autocast():
            sr = model(lr)
            loss = pixel_criterion(sr, hr)

        &#47&#47 Gradient zoom
        scaler.scale(loss).backward()
        <a id="change">scaler.unscale_(</a>optimizer<a id="change">)</a>
        &#47&#47 Update generator weight
        scaler.step(optimizer)
        scaler.update()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lornatang/esrgan-pytorch/commit/087e0c9bc621989889918b52b7c0dba9485c5fd6#diff-f4329e1175710811d12324bbaab9881a31dff8276b3c7d45269651c3f66f2b77L151' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 20991693</div><div id='project'> Project Name: lornatang/esrgan-pytorch</div><div id='commit'> Commit Name: 087e0c9bc621989889918b52b7c0dba9485c5fd6</div><div id='time'> Time: 2022-03-06</div><div id='author'> Author: liuchangyu1111@gmail.com</div><div id='file'> File Name: train_rrdbnet.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train(8)</div><div id='n_method'> N Method Name: train(8)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: train_rrdbnet.py</div><div id='n_file'> N File Name: train_rrdbnet.py</div><div id='m_start'> M Start Line: 165</div><div id='m_end'> M End Line: 200</div><div id='n_start'> N Start Line: 185</div><div id='n_end'> N End Line: 235</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 Writer Loss to file
        writer.add_scalar("Train/Loss", loss.item(), index + epoch * batches + 1)
        if <a id="change">index % config.print_frequency == 0 and index != 0</a>:
            progress.display(index)

</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 enable preload
    train_prefetcher.reset()
    batch_data = train_prefetcher.next()
    while <a id="change">batch_data is not None</a>:
        &#47&#47 measure data loading time
        data_time.update(time.time() - end)

        lr = batch_data["lr"].to(config.device, non_blocking=True)
        hr = batch_data["hr"].to(config.device, non_blocking=True)

        &#47&#47 Initialize the generator gradient
        model.zero_grad()

        &#47&#47 Mixed precision training
        with amp.autocast():
            sr = model(lr)
            loss = pixel_criterion(sr, hr)

        &#47&#47 Gradient zoom
        scaler.scale(loss).backward()
        <a id="change">scaler.unscale_(</a>optimizer<a id="change">)</a>
        &#47&#47 Update generator weight
        scaler.step(optimizer)
        scaler.update()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lornatang/srgan-pytorch/commit/231bd74d21d7f532fd746f4a1cb8fb3bc008c933#diff-e654c2e7420a4ea31af0ee350621a8dabfd9ce87571c367a37e621a3f083d92fL135' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 20991694</div><div id='project'> Project Name: lornatang/srgan-pytorch</div><div id='commit'> Commit Name: 231bd74d21d7f532fd746f4a1cb8fb3bc008c933</div><div id='time'> Time: 2022-03-03</div><div id='author'> Author: liuchangyu1111@gmail.com</div><div id='file'> File Name: train_srresnet.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train(8)</div><div id='n_method'> N Method Name: train(8)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: train_srresnet.py</div><div id='n_file'> N File Name: train_srresnet.py</div><div id='m_start'> M Start Line: 149</div><div id='m_end'> M End Line: 184</div><div id='n_start'> N Start Line: 174</div><div id='n_end'> N End Line: 224</div><BR>