<html><h3>Pattern ID :11888
</h3><img src='39974769.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        b = bh // h

        Wsq = expand_dim(self.linear_sort_q, 0, b).reshape(bh, dim, dim_sort)
        Wsk = <a id="change">expand_dim(</a>self.linear_sort_k, <a id="change">0</a>, b<a id="change">)</a>.reshape(bh, dim, dim_sort)
        nsk = expand_dim(self.null_sort_k, 0, b).reshape(bh, 1, dim_sort)

        q_r = torch.cat((cumavg(q, dim=1), q), dim=-1)</code></pre><h3>After Change</h3><pre><code class='java'>
        sq = b_qi @ Wsq
        sk = b_ki @ Wsk

        sk<a id="change"> = </a>F.pad(sk, (0<a id="change">, 0, 1, 0</a>))

        R = torch.einsum(&quotbie,bje-&gt;bij&quot, sq, sk)
        return mask_reordering_matrix(R)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/sinkhorn-transformer/commit/a3d36f7cdf5b9a134787caf42ce18b5b28b0ddc5#diff-2ed19ae7feb552dc52b4c7a7c89a078c0fd371c922789f894af09ff01f35eccbL354' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 39974769</div><div id='project'> Project Name: lucidrains/sinkhorn-transformer</div><div id='commit'> Commit Name: a3d36f7cdf5b9a134787caf42ce18b5b28b0ddc5</div><div id='time'> Time: 2020-04-10</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='m_class'> M Class Name: CausalAttentionSortNet</div><div id='n_method'> N Class Name: CausalAttentionSortNet</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='n_file'> N File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='m_start'> M Start Line: 354</div><div id='m_end'> M End Line: 371</div><div id='n_start'> N Start Line: 365</div><div id='n_end'> N End Line: 380</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        b_q = bucket(buckets, q) if self.n_sortcut == 0 else bucket(1, q)
        b_k = bucket(buckets, k)

        Wsq = <a id="change">expand_dim(</a>self.linear_sort_q, <a id="change">0</a>, b<a id="change">)</a>.reshape(bh, dim, dim_sort)
        Wsk = expand_dim(self.linear_sort_k, 0, b).reshape(bh, dim, dim_sort)        

        b_qi, b_ki = b_q.mean(dim=2), b_k.mean(dim=2)</code></pre><h3>After Change</h3><pre><code class='java'>
        Wsq, Wsk, pos_q, pos_k = map(partial(expand_batch_and_merge_head, b), (self.linear_sort_q, self.linear_sort_k, self.q_pos_emb, self.k_pos_emb))

        b_qi = torch.cat((b_q.mean(dim=2), pos_q), dim=-1)
        b_ki<a id="change"> = </a>torch.cat((b_k.mean(dim=2)<a id="change">, pos_k</a>), dim=-1)

        sq = b_qi @ Wsq
        sk = b_ki @ Wsk</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/sinkhorn-transformer/commit/a3d36f7cdf5b9a134787caf42ce18b5b28b0ddc5#diff-2ed19ae7feb552dc52b4c7a7c89a078c0fd371c922789f894af09ff01f35eccbL201' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 39974771</div><div id='project'> Project Name: lucidrains/sinkhorn-transformer</div><div id='commit'> Commit Name: a3d36f7cdf5b9a134787caf42ce18b5b28b0ddc5</div><div id='time'> Time: 2020-04-10</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='m_class'> M Class Name: AttentionSortNet</div><div id='n_method'> N Class Name: AttentionSortNet</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='n_file'> N File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='m_start'> M Start Line: 203</div><div id='m_end'> M End Line: 211</div><div id='n_start'> N Start Line: 212</div><div id='n_end'> N End Line: 220</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        else:
            lq, q = split_index_fn(q)
            lk = expand_dim(k, 1, self.local_attn_heads)
            lv = <a id="change">expand_dim(</a>v, <a id="change">1</a>, self.local_attn_heads<a id="change">)</a>

        has_local, has_global = map(lambda x: x.shape[1] &gt; 0, (lq, q))

        if has_local:</code></pre><h3>After Change</h3><pre><code class='java'>
            (lk, k), (lv, v) = map(split_kv_fn, (k, v))

            local_expand_heads_fn = lambda t: expand_dim(t, 1, self.local_attn_heads, unsqueeze=False)
            lk<a id="change">, lv = </a>map(local_expand_heads_fn, (lk<a id="change">, lv</a>))

            k, v = map(lambda t: t.squeeze(1), (k, v))
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/linear-attention-transformer/commit/75a6cefd9d7facce1ff162dc70138a6e32358f3c#diff-eee6cf669463e9683bcf9643c4b19a493056f98ded32854287144d47f02a7032L295' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 39974763</div><div id='project'> Project Name: lucidrains/linear-attention-transformer</div><div id='commit'> Commit Name: 75a6cefd9d7facce1ff162dc70138a6e32358f3c</div><div id='time'> Time: 2020-06-29</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='m_class'> M Class Name: SelfAttention</div><div id='n_method'> N Class Name: SelfAttention</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='n_file'> N File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='m_start'> M Start Line: 304</div><div id='m_end'> M End Line: 320</div><div id='n_start'> N Start Line: 308</div><div id='n_end'> N End Line: 327</div><BR>