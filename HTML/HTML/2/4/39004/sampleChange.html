<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            ]
            output_shape = [x.shape[0], self.weight.shape[0]] + output_shape
            empty = _NewEmptyTensorOp.apply(x, output_shape)
            <a id="change">if </a>self.training:
                &#47&#47 https://github.com/pytorch/pytorch/issues/12013
                assert not isinstance(
                    self.norm, torch.nn.SyncBatchNorm
                ), "SyncBatchNorm does not support empty inputs!"

                &#47&#47 This is to make DDP happy.
                &#47&#47 DDP expects all workers to have gradient w.r.t the same set of parameters.
                _dummy = sum(x.view(-1)[0] for x in <a id="change">self.parameters()</a>) * 0.0
                return empty + _dummy
            else:
                return empty</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 2. features needed by exporting module to torchscript are added in PyTorch 1.6 or
        &#47&#47 later version, `Conv2d` in these PyTorch versions has already supported empty inputs.
        if not torch.jit.is_scripting():
            <a id="change">with </a><a id="change">warnings.catch_warnings(record=True):
                </a>if x.numel() == 0 and self.training:
                    &#47&#47 https://github.com/pytorch/pytorch/issues/12013
                    assert not isinstance(
                        self.norm, torch.nn.SyncBatchNorm</code></pre>