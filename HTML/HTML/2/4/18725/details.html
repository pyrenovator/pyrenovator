<html><h3>Pattern ID :18725
</h3><img src='60922709.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        new_col_name = node + "_idx"
        col_map[node] = new_col_name
        &#47&#47 add new Idx to dataframe
        df[new_col_name]<a id="change"> = </a><a id="change">df[config["node_columns"][i]].map(</a>mapping[key]<a id="change">)</a>
        if not heterogeneous:
            offset = len(mapping[key])
    print("Tokenize column map:", col_map)
    return mapping, col_map</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 key in dict: node + "_2idx" stores a unique mapping from real id -&gt; indexed id  mapping
    &#47&#47 column new_col_name: node + "_idx" stores the indexed id
    col_map = {}
    node_types = <a id="change">config["node_types"]</a>
    node_type_columns = get_node_type_columns(config["node_columns"])
    for i, node in enumerate(node_types):
        key = str(node + "_2idx")
        columns = node_type_columns[node]</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/oap-project/cloudtik/commit/71c226134f43b1edac83fbb548a65c52cfae2304#diff-3fd654c4c098e1a50e7a409663f22058b8d3b516a20b9b6e9e8fe73db31fe200L35' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60922709</div><div id='project'> Project Name: oap-project/cloudtik</div><div id='commit'> Commit Name: 71c226134f43b1edac83fbb548a65c52cfae2304</div><div id='time'> Time: 2023-06-28</div><div id='author'> Author: haifeng.chen@intel.com</div><div id='file'> File Name: python/cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/tokenizer.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: tokenize_node_ids(3)</div><div id='n_method'> N Method Name: tokenize_node_ids(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: python/cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/tokenizer.py</div><div id='n_file'> N File Name: python/cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/tokenizer.py</div><div id='m_start'> M Start Line: 35</div><div id='m_end'> M End Line: 41</div><div id='n_start'> N Start Line: 84</div><div id='n_end'> N End Line: 90</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            )
        else:
            train_dataset = datasets.load_dataset(data_args.dataset_name, "queries")[&quottrain&quot]
            train_dataset<a id="change"> = </a><a id="change">train_dataset.map(
                </a>TrainProcessor(tokenizer)<a id="change">,
                batched=False,
                num_proc=12,
                remove_columns=train_dataset.column_names,
                desc="Running tokenizer on train dataset",
            )</a>
            train_dataset = TrainDataset(data_args, train_dataset, tokenizer)
    else:
        train_dataset = None</code></pre><h3>After Change</h3><pre><code class='java'>
            encode_dataset.encode_data = encode_dataset.encode_data\
                .shard(data_args.encode_num_shard, data_args.encode_shard_index)
        else:
            encode_dataset = <a id="change">datasets.load_dataset(data_args.dataset_name, data_args.dataset_split)[&quottrain&quot]</a>\
                .shard(data_args.encode_num_shard, data_args.encode_shard_index)
            encode_dataset = encode_dataset.map(
                PROCESSOR_INFO[data_args.dataset_name][data_args.dataset_split](tokenizer),</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/texttron/tevatron/commit/cf0234cee7ad2a0ba472d040d82989cdcbc5a30c#diff-fca5a9224975d85e3d7e47f26917a2a268314f5898ca6a7d51ff5096b996eeb4L28' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60922710</div><div id='project'> Project Name: texttron/tevatron</div><div id='commit'> Commit Name: cf0234cee7ad2a0ba472d040d82989cdcbc5a30c</div><div id='time'> Time: 2021-09-10</div><div id='author'> Author: x93ma@edu.uwaterloo.ca</div><div id='file'> File Name: examples/wikipedia-nq/run.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(0)</div><div id='n_method'> N Method Name: main(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/wikipedia-nq/run.py</div><div id='n_file'> N File Name: examples/wikipedia-nq/run.py</div><div id='m_start'> M Start Line: 74</div><div id='m_end'> M End Line: 148</div><div id='n_start'> N Start Line: 74</div><div id='n_end'> N End Line: 147</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def f(xs):
      return _pad_reshape_batch(_add_mask(xs, 1), flat_batch_size, num_devices)

    ds<a id="change"> = </a><a id="change">ds.map(</a>f<a id="change">, num_parallel_calls=tf.data.AUTOTUNE)</a>

  if cache == "batched":
    ds = ds.cache()
</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 `batch_size_per_device*num_devices - 1` additional examples).
    padding_example = tf.nest.map_structure(
        lambda spec: tf.zeros(spec.shape, spec.dtype)[None], ds.element_spec)
    <a id="change">padding_example["mask"]</a> = [0.]
    padding_dataset = tf.data.Dataset.from_tensor_slices(padding_example)
    ds = ds.concatenate(
        padding_dataset.repeat(batch_size_per_device * num_devices - 1))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/google/uncertainty-baselines/commit/5ef136b370a87f81dc0d0f1f57757d0432b2bc6a#diff-e4b4b686345ec38b48dee362ed9131cd71626908a9b2dce0a6ad0ebb71cd4a83L217' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60922714</div><div id='project'> Project Name: google/uncertainty-baselines</div><div id='commit'> Commit Name: 5ef136b370a87f81dc0d0f1f57757d0432b2bc6a</div><div id='time'> Time: 2022-05-17</div><div id='author'> Author: dusenberrymw@google.com</div><div id='file'> File Name: baselines/jft/input_utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: get_data(15)</div><div id='n_method'> N Method Name: get_data(15)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: baselines/jft/input_utils.py</div><div id='n_file'> N File Name: baselines/jft/input_utils.py</div><div id='m_start'> M Start Line: 291</div><div id='m_end'> M End Line: 344</div><div id='n_start'> N Start Line: 267</div><div id='n_end'> N End Line: 324</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            )
        else:
            train_dataset = datasets.load_dataset(data_args.dataset_name, "queries")[&quottrain&quot]
            train_dataset<a id="change"> = </a><a id="change">train_dataset.map(
                </a>TrainProcessor(tokenizer)<a id="change">,
                batched=False,
                num_proc=12,
                remove_columns=train_dataset.column_names,
                desc="Running tokenizer on train dataset",
            )</a>
            train_dataset = TrainDataset(data_args, train_dataset, tokenizer)
    else:
        train_dataset = None</code></pre><h3>After Change</h3><pre><code class='java'>
            encode_dataset.encode_data = encode_dataset.encode_data\
                .shard(data_args.encode_num_shard, data_args.encode_shard_index)
        else:
            encode_dataset = <a id="change">datasets.load_dataset(data_args.dataset_name, data_args.dataset_split)[&quottrain&quot]</a>\
                .shard(data_args.encode_num_shard, data_args.encode_shard_index)
            encode_dataset = encode_dataset.map(
                PROCESSOR_INFO[data_args.dataset_name][data_args.dataset_split](tokenizer),</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/texttron/tevatron/commit/415cce1b92441afb3215dafcc4e7ad3b85d5a26a#diff-fca5a9224975d85e3d7e47f26917a2a268314f5898ca6a7d51ff5096b996eeb4L28' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60922718</div><div id='project'> Project Name: texttron/tevatron</div><div id='commit'> Commit Name: 415cce1b92441afb3215dafcc4e7ad3b85d5a26a</div><div id='time'> Time: 2021-09-09</div><div id='author'> Author: x93ma@edu.uwaterloo.ca</div><div id='file'> File Name: examples/wikipedia-nq/run.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(0)</div><div id='n_method'> N Method Name: main(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/wikipedia-nq/run.py</div><div id='n_file'> N File Name: examples/wikipedia-nq/run.py</div><div id='m_start'> M Start Line: 74</div><div id='m_end'> M End Line: 148</div><div id='n_start'> N Start Line: 74</div><div id='n_end'> N End Line: 147</div><BR>