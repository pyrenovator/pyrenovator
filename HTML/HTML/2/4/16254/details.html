<html><h3>Pattern ID :16254
</h3><img src='54491243.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        with open(path, &quotr&quot) as f:
            for line in f:
                self.lines.append(line.strip(&quot\n&quot))
                tokens = <a id="change">Tokenizer.tokenize(
                    line</a>, dictionary<a id="change">, add_if_not_exist=False,
                    append_eos=self.append_eos, reverse_order=self.reverse_order,
                )</a> + 1  &#47&#47 +1 for Lua compatibility
                self.tokens_list.append(tokens)
                self._sizes.append(len(tokens))
        self._sizes = np.array(self._sizes)</code></pre><h3>After Change</h3><pre><code class='java'>
        with open(path, &quotr&quot) as f:
            for line in f:
                self.lines.append(line.strip(&quot\n&quot))
                tokens = <a id="change">Tokenizer.tokenize(
                    line</a>, dictionary<a id="change">, add_if_not_exist=False,
                    append_eos=self.append_eos, reverse_order=self.reverse_order,
                )</a>.long()
                self.tokens_list.append(tokens)
                self.sizes.append(len(tokens))
        self.sizes = np.array(self.sizes)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 2</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/kssteven418/i-bert/commit/ff68a9ef501e7286501dba1719024dfaaab4b473#diff-31f6924187b58b04f39a9a867be62ed42150c833c4f5d40a6418197e0385bc43L135' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 54491243</div><div id='project'> Project Name: kssteven418/i-bert</div><div id='commit'> Commit Name: ff68a9ef501e7286501dba1719024dfaaab4b473</div><div id='time'> Time: 2018-06-15</div><div id='author'> Author: myleott@fb.com</div><div id='file'> File Name: fairseq/data/indexed_dataset.py</div><div id='m_class'> M Class Name: IndexedRawTextDataset</div><div id='n_method'> N Class Name: IndexedRawTextDataset</div><div id='m_method'> M Method Name: read_data(3)</div><div id='n_method'> N Method Name: read_data(3)</div><div id='m_parent_class'> M Parent Class: IndexedDataset</div><div id='n_parent_class'> N Parent Class: IndexedDataset</div><div id='m_file'> M File Name: fairseq/data/indexed_dataset.py</div><div id='n_file'> N File Name: fairseq/data/indexed_dataset.py</div><div id='m_start'> M Start Line: 144</div><div id='m_end'> M End Line: 149</div><div id='n_start'> N Start Line: 135</div><div id='n_end'> N End Line: 140</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            list: A list of string representing converted tokens.
        
        split_tokens = []
        for <a id="change">token</a> in self.basic_tokenizer.tokenize(text):
            for sub_token in <a id="change">self.wordpiece_tokenizer.tokenize(</a>token<a id="change">)</a>:
                split_tokens.append(sub_token)
        return split_tokens
</code></pre><h3>After Change</h3><pre><code class='java'>
        
        split_tokens = []
        if self.do_basic_tokenize:
            for <a id="change">token</a> in self.basic_tokenizer.tokenize(
                    text, never_split=self.all_special_tokens):
                &#47&#47 If the token is part of the never_split set
                if token in self.basic_tokenizer.never_split:
                    split_tokens.append(token)
                else:
                    split_tokens += <a id="change">self.wordpiece_tokenizer.tokenize(</a>token<a id="change">)</a>
        else:
            split_tokens = self.wordpiece_tokenizer.tokenize(text)
        return split_tokens
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/paddlepaddle/paddlenlp/commit/ff437f6c19bd26b103c767a369efb4edaf785e0b#diff-e6adbd3d80fbaaf297c051172a802bdcb34f1de64fee69edc1e86b8a99603590L142' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 54491239</div><div id='project'> Project Name: paddlepaddle/paddlenlp</div><div id='commit'> Commit Name: ff437f6c19bd26b103c767a369efb4edaf785e0b</div><div id='time'> Time: 2022-08-19</div><div id='author'> Author: yyb0576@163.com</div><div id='file'> File Name: paddlenlp/transformers/electra/tokenizer.py</div><div id='m_class'> M Class Name: ElectraTokenizer</div><div id='n_method'> N Class Name: ElectraTokenizer</div><div id='m_method'> M Method Name: _tokenize(2)</div><div id='n_method'> N Method Name: _tokenize(2)</div><div id='m_parent_class'> M Parent Class: PretrainedTokenizer</div><div id='n_parent_class'> N Parent Class: PretrainedTokenizer</div><div id='m_file'> M File Name: paddlenlp/transformers/electra/tokenizer.py</div><div id='n_file'> N File Name: paddlenlp/transformers/electra/tokenizer.py</div><div id='m_start'> M Start Line: 153</div><div id='m_end'> M End Line: 156</div><div id='n_start'> N Start Line: 172</div><div id='n_end'> N End Line: 183</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            List[str]: A list of string representing converted tokens.
        
        split_tokens = []
        for <a id="change">token</a> in self.basic_tokenizer.tokenize(text):
            for sub_token in <a id="change">self.wordpiece_tokenizer.tokenize(</a>token<a id="change">)</a>:
                split_tokens.append(sub_token)
        return split_tokens
</code></pre><h3>After Change</h3><pre><code class='java'>
        
        split_tokens = []
        if self.do_basic_tokenize:
            for <a id="change">token</a> in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):
                &#47&#47 If the token is part of the never_split set
                if token in self.basic_tokenizer.never_split:
                    split_tokens.append(token)
                else:
                    split_tokens += <a id="change">self.wordpiece_tokenizer.tokenize(</a>token<a id="change">)</a>
        else:
            split_tokens = self.wordpiece_tokenizer.tokenize(text)
        return split_tokens
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/paddlepaddle/paddlenlp/commit/17d066448b8d97d719cc56c2191a6c02fd41ec3c#diff-fdcaea65a319d87dd3fd22f674f68a3567b72b404db9e3ed18ca312cd6af1da9L117' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 54491238</div><div id='project'> Project Name: paddlepaddle/paddlenlp</div><div id='commit'> Commit Name: 17d066448b8d97d719cc56c2191a6c02fd41ec3c</div><div id='time'> Time: 2023-04-04</div><div id='author'> Author: 709153940@qq.com</div><div id='file'> File Name: paddlenlp/transformers/ppminilm/tokenizer.py</div><div id='m_class'> M Class Name: PPMiniLMTokenizer</div><div id='n_method'> N Class Name: PPMiniLMTokenizer</div><div id='m_method'> M Method Name: _tokenize(2)</div><div id='n_method'> N Method Name: _tokenize(2)</div><div id='m_parent_class'> M Parent Class: PretrainedTokenizer</div><div id='n_parent_class'> N Parent Class: PretrainedTokenizer</div><div id='m_file'> M File Name: paddlenlp/transformers/ppminilm/tokenizer.py</div><div id='n_file'> N File Name: paddlenlp/transformers/ppminilm/tokenizer.py</div><div id='m_start'> M Start Line: 128</div><div id='m_end'> M End Line: 131</div><div id='n_start'> N Start Line: 141</div><div id='n_end'> N End Line: 151</div><BR>