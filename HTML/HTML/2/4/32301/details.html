<html><h3>Pattern ID :32301
</h3><img src='94462555.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if not AllToAllStatus.initialized or AllToAllStatus.world_size &lt;= 1:
            return (input,)
        output = [
            <a id="change">torch.empty(AllToAllStatus.scatter_tensor_shape, device=input.device).contiguous()</a>
            for _ in range(AllToAllStatus.num_split)
        ]
        tutel_custom_kernel.nccl_all_to_all_scatter_async(input, output, False)
        <a id="change">return </a>tuple(output)

    @staticmethod
    def backward(ctx: Any, *grad_output) -&gt; Tensor:</code></pre><h3>After Change</h3><pre><code class='java'>
            return (input,)
        ctx.input_shape = input.shape
        output_shape = torch.Size([
            x<a id="change"> if i != AllToAllStatus.split_dim</a><a id="change"> else </a>x // AllToAllStatus.num_split
            for i, x in enumerate(ctx.input_shape)
        ])
        ctx.num_slices_per_split = ctx.input_shape[:AllToAllStatus.split_dim].numel()</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/microsoft/tutel/commit/e51df1ca64be59eae3691bc1c64b20a201de1009#diff-97cb6b6b4a46f907579b92c457a8eaec4a13ed75c520b40ae8911d4a59440021L110' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 94462555</div><div id='project'> Project Name: microsoft/tutel</div><div id='commit'> Commit Name: e51df1ca64be59eae3691bc1c64b20a201de1009</div><div id='time'> Time: 2022-01-31</div><div id='author'> Author: ziyyang@microsoft.com</div><div id='file'> File Name: tutel/impls/communicate.py</div><div id='m_class'> M Class Name: AllToAllScatterAsync</div><div id='n_method'> N Class Name: AllToAllScatterAsync</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: torch.autograd.Function</div><div id='n_parent_class'> N Parent Class: torch.autograd.Function</div><div id='m_file'> M File Name: tutel/impls/communicate.py</div><div id='n_file'> N File Name: tutel/impls/communicate.py</div><div id='m_start'> M Start Line: 118</div><div id='m_end'> M End Line: 125</div><div id='n_start'> N Start Line: 110</div><div id='n_end'> N End Line: 118</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    if op == MovementOps.SHRINK:
      return x[tuple(slice(p[0], p[1], None) for p in arg)]
    elif op == MovementOps.STRIDED:
      <a id="change">return </a><a id="change">x.contiguous()</a>.as_strided([x[0] for x in arg], [x[1] for x in arg])
    else:
      return getattr(x, op.name.lower())(arg)
</code></pre><h3>After Change</h3><pre><code class='java'>
  def unary_op(x, op): return CPUBuffer.fxn_for_op[op](x)
  def binary_op(x, op, y): return CPUBuffer.fxn_for_op[op](x, y)
  def reduce_op(x, op, new_shape): return CPUBuffer.fxn_for_op[op](x, new_shape)
  def movement_op(x, op, arg=None): return CPUBuffer.fxn_for_op[op](x, arg)<a id="change"> if op in CPUBuffer.fxn_for_op</a><a id="change"> else </a>getattr(x, op.name.lower())(arg)

  def processing_op(x,op,w,C):
    assert op == ProcessingOps.CONV, f"{op} isn&quott supported"</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/geohot/tinygrad/commit/c0050fab8ff0bc667e40da11980f4ac4c21affda#diff-f95ebbe862be0c2efafe626875b5d1e1f16f69b87b564849abbfa2ee7b4adb68L38' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 94462554</div><div id='project'> Project Name: geohot/tinygrad</div><div id='commit'> Commit Name: c0050fab8ff0bc667e40da11980f4ac4c21affda</div><div id='time'> Time: 2022-10-28</div><div id='author'> Author: geohot@gmail.com</div><div id='file'> File Name: tinygrad/llops/ops_cpu.py</div><div id='m_class'> M Class Name: CPUBuffer</div><div id='n_method'> N Class Name: CPUBuffer</div><div id='m_method'> M Method Name: movement_op(3)</div><div id='n_method'> N Method Name: movement_op(3)</div><div id='m_parent_class'> M Parent Class: np.ndarray,GenericExecAST</div><div id='n_parent_class'> N Parent Class: np.ndarray,GenericExecAST</div><div id='m_file'> M File Name: tinygrad/llops/ops_cpu.py</div><div id='n_file'> N File Name: tinygrad/llops/ops_cpu.py</div><div id='m_start'> M Start Line: 39</div><div id='m_end'> M End Line: 46</div><div id='n_start'> N Start Line: 37</div><div id='n_end'> N End Line: 37</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    q, k, v = map(lambda x: x.contiguous(), (q, k, v))

    q = q.contiguous()
    k = <a id="change">k.contiguous()</a>
    v = v.contiguous()
    bias = bias.contiguous()

    <a id="change">return </a>memory_efficient_attention(q, k, v, bias)
</code></pre><h3>After Change</h3><pre><code class='java'>
    assert memory_efficient_attention is not None

    q, k, v = map(lambda x: x.contiguous(), (q, k, v))
    bias = bias.contiguous()<a id="change"> if bias is not None</a><a id="change"> else </a>None

    out = memory_efficient_attention(q, k, v, bias)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tfernd/sd-fused/commit/ba5181b593ffa0f5fbcd8aa024d78ce88013bdb3#diff-ae835bdb7ff7ae313826b95d84c2c847e7cb7a9e1cfe7725bd67a31ab56dae61L12' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 94462557</div><div id='project'> Project Name: tfernd/sd-fused</div><div id='commit'> Commit Name: ba5181b593ffa0f5fbcd8aa024d78ce88013bdb3</div><div id='time'> Time: 2022-11-23</div><div id='author'> Author: ThalesFDFernandes@gmai.com</div><div id='file'> File Name: sd_fused/layers/blocks/attention/compute/flash_attention.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: flash_attention(4)</div><div id='n_method'> N Method Name: flash_attention(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: sd_fused/layers/blocks/attention/compute/flash_attention.py</div><div id='n_file'> N File Name: sd_fused/layers/blocks/attention/compute/flash_attention.py</div><div id='m_start'> M Start Line: 22</div><div id='m_end'> M End Line: 31</div><div id='n_start'> N Start Line: 23</div><div id='n_end'> N End Line: 27</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def forward(ctx: Any, *input) -&gt; Tensor:
        if not AllToAllStatus.initialized or AllToAllStatus.world_size &lt;= 1:
            return input[0]
        output = <a id="change">torch.empty(AllToAllStatus.gather_tensor_shape, device=input[0].device).contiguous()</a>
        tutel_custom_kernel.nccl_all_to_all_gather_async(input, output, False)
        <a id="change">return </a>output

    @staticmethod
    def backward(ctx: Any, grad_output: Tensor) -&gt; Tuple[Tensor]:</code></pre><h3>After Change</h3><pre><code class='java'>
            return input[0]
        ctx.input_shape = input[0].shape
        output_shape = torch.Size([
            x<a id="change"> if i != AllToAllStatus.split_dim</a><a id="change"> else </a>x * AllToAllStatus.num_split
            for i, x in enumerate(ctx.input_shape)
        ])
        ctx.num_slices_per_split = ctx.input_shape[:AllToAllStatus.split_dim].numel()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/tutel/commit/e51df1ca64be59eae3691bc1c64b20a201de1009#diff-97cb6b6b4a46f907579b92c457a8eaec4a13ed75c520b40ae8911d4a59440021L137' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 94462556</div><div id='project'> Project Name: microsoft/tutel</div><div id='commit'> Commit Name: e51df1ca64be59eae3691bc1c64b20a201de1009</div><div id='time'> Time: 2022-01-31</div><div id='author'> Author: ziyyang@microsoft.com</div><div id='file'> File Name: tutel/impls/communicate.py</div><div id='m_class'> M Class Name: AllToAllGatherAsync</div><div id='n_method'> N Class Name: AllToAllGatherAsync</div><div id='m_method'> M Method Name: forward(1)</div><div id='n_method'> N Method Name: forward(1)</div><div id='m_parent_class'> M Parent Class: torch.autograd.Function</div><div id='n_parent_class'> N Parent Class: torch.autograd.Function</div><div id='m_file'> M File Name: tutel/impls/communicate.py</div><div id='n_file'> N File Name: tutel/impls/communicate.py</div><div id='m_start'> M Start Line: 138</div><div id='m_end'> M End Line: 142</div><div id='n_start'> N Start Line: 129</div><div id='n_end'> N End Line: 137</div><BR>