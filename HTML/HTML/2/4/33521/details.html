<html><h3>Pattern ID :33521
</h3><img src='96556833.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        size = [fixed_size[1], fixed_size[0]]
    else:
        min_size = torch.min(im_shape).to(dtype=torch.float32)
        max_size = <a id="change">torch.max(</a>im_shape<a id="change">)</a>.to(dtype=torch.float32)
        scale<a id="change"> = </a>torch.min(self_min_size / min_size, self_max_size / max_size)

        if torchvision._is_tracing():
            scale_factor = _fake_cast_onnx(scale)</code></pre><h3>After Change</h3><pre><code class='java'>
    else:
        im_shape = torch.tensor(image.shape[-2:])

    ratio = torch.min(new_shape[0] / im_shape[0], new_shape[1]<a id="change"> / </a>im_shape[1])

    ratio_h = <a id="change">torch.round(im_shape[0] * ratio).to(dtype=torch.int32)</a>
    ratio_w = torch.round(im_shape[1] * ratio).to(dtype=torch.int32)

    if torchvision._is_tracing():
        new_unpad = _tracing_item_onnx(ratio_h), _tracing_item_onnx(ratio_w)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/zhiqwang/yolov5-rt-stack/commit/cd1a6ec7cda09de0dc92962a37ecb4f723a8dfeb#diff-60b40f8280cc07f4b1f5ff27c62b172cf2350df49462357850d9ccb60554c3c8L67' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96556833</div><div id='project'> Project Name: zhiqwang/yolov5-rt-stack</div><div id='commit'> Commit Name: cd1a6ec7cda09de0dc92962a37ecb4f723a8dfeb</div><div id='time'> Time: 2022-02-03</div><div id='author'> Author: 92794867+q3394101@users.noreply.github.com</div><div id='file'> File Name: yolort/models/transform.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _resize_image_and_masks(3)</div><div id='n_method'> N Method Name: _resize_image_and_masks(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: yolort/models/transform.py</div><div id='n_file'> N File Name: yolort/models/transform.py</div><div id='m_start'> M Start Line: 257</div><div id='m_end'> M End Line: 286</div><div id='n_start'> N Start Line: 67</div><div id='n_end'> N End Line: 77</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        actions = self.policy.eval(states.to(self.device))
        actions = actions + self._noise_policy.sample([actions.shape[0]])
        actions = torch.min(actions, self._high)
        actions<a id="change"> = </a><a id="change">torch.max(</a>actions, self._low<a id="change">)</a>
        return actions.to("cpu")

    def _train(self):
        if self._should_train():</code></pre><h3>After Change</h3><pre><code class='java'>
    def _choose_actions(self, states: State, actions: Action):
        &#47&#47 choose vae action
        mean, log_var = self.vae.encode(states.to(self.device), actions.to(self.device))
        z = mean + (0.5 * log_var).exp()<a id="change"> * </a>torch.randn_like(log_var)
        vae_action = Action(self.vae.decode(states, z))

        &#47&#47 choose normal action
        actions = self.policy.eval(states.to(self.device), <a id="change">vae_action.to(</a>self.device<a id="change">)</a>)
        actions = actions + self._noise_policy.sample([actions.shape[0]])
        return actions.to("cpu")
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/syuntoku14/pytorch-rl-il/commit/a70365be6b670db00fccd89990fb4b28914e10c3#diff-3f0c90e189e890a2f3c8f34e3a5ec14f93ae51d3ef5135e30c19d88f88bfbb72L82' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96556835</div><div id='project'> Project Name: syuntoku14/pytorch-rl-il</div><div id='commit'> Commit Name: a70365be6b670db00fccd89990fb4b28914e10c3</div><div id='time'> Time: 2020-03-17</div><div id='author'> Author: syuntoku14@gmail.com</div><div id='file'> File Name: rlil/agents/bcq.py</div><div id='m_class'> M Class Name: BCQ</div><div id='n_method'> N Class Name: BCQ</div><div id='m_method'> M Method Name: _choose_actions(3)</div><div id='n_method'> N Method Name: _choose_actions(2)</div><div id='m_parent_class'> M Parent Class: Agent</div><div id='n_parent_class'> N Parent Class: Agent</div><div id='m_file'> M File Name: rlil/agents/bcq.py</div><div id='n_file'> N File Name: rlil/agents/bcq.py</div><div id='m_start'> M Start Line: 82</div><div id='m_end'> M End Line: 86</div><div id='n_start'> N Start Line: 77</div><div id='n_end'> N End Line: 84</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        heatmap = cv2.resize(heatmap[0], _input.shape[2:])

        heatmap = heatmap - np.min(heatmap)
        heatmap<a id="change"> = </a>heatmap / <a id="change">np.max(</a>heatmap<a id="change">)</a>
        return heatmap
</code></pre><h3>After Change</h3><pre><code class='java'>

    def grad_cam(self, _input: torch.FloatTensor, _class: List[int]) -&gt; np.ndarray:
        if isinstance(_class, int):
            _class = [_class]<a id="change"> * </a>len(_input)
        _class = <a id="change">torch.tensor(_class).to(</a>_input.device<a id="change">)</a>
        feats = self._model.get_fm(_input).detach()   &#47&#47 (N,C,H,W)
        feats.requires_grad_()
        _output: torch.FloatTensor = self._model.pool(feats)
        _output: torch.FloatTensor = self._model.flatten(_output)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/ain-soph/trojanzoo/commit/2bf7c2a2e8acba2592ee17d60d1a59b7bd1bbfe5#diff-2e5a199c407507ee95051b9d3c697f33888e4d96925f51e508939244c2f34dbeL206' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96556839</div><div id='project'> Project Name: ain-soph/trojanzoo</div><div id='commit'> Commit Name: 2bf7c2a2e8acba2592ee17d60d1a59b7bd1bbfe5</div><div id='time'> Time: 2020-11-24</div><div id='author'> Author: ain-soph@live.com</div><div id='file'> File Name: trojanzoo/model/imagemodel.py</div><div id='m_class'> M Class Name: ImageModel</div><div id='n_method'> N Class Name: ImageModel</div><div id='m_method'> M Method Name: grad_cam(3)</div><div id='n_method'> N Method Name: grad_cam(3)</div><div id='m_parent_class'> M Parent Class: Model</div><div id='n_parent_class'> N Parent Class: Model</div><div id='m_file'> M File Name: trojanzoo/model/imagemodel.py</div><div id='n_file'> N File Name: trojanzoo/model/imagemodel.py</div><div id='m_start'> M Start Line: 206</div><div id='m_end'> M End Line: 222</div><div id='n_start'> N Start Line: 190</div><div id='n_end'> N End Line: 212</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            Dict: Statistics to be monitored.
            Tensor: Weight value.
        
        text = text[:, : <a id="change">text_lengths.max()</a>]  &#47&#47 for data-parallel
        speech = speech[:, : speech_lengths.max()]  &#47&#47 for data-parallel

        &#47&#47 Add eos at the last of sequence
        xs<a id="change"> = </a>F.pad(text, [0, 1], "constant", self.padding_idx)
        for i, l in enumerate(text_lengths):
            xs[i, l] = self.eos
        ilens = text_lengths + 1</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 eos is [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

        &#47&#47 make labels for stop prediction
        labels = <a id="change">make_pad_mask(speech_lengths - 1).to(</a>speech.device, speech.dtype<a id="change">)</a>
        labels = F.pad(labels, [0, 1], "constant", 1.0)

        &#47&#47 calculate tacotron2 outputs
        after_outs, before_outs, logits, att_ws = self._forward(text, text_lengths, speech, speech_lengths, speaker_embeddings)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/24ca2ac33029f434e47969661b0a878773639ab8#diff-f8d05eb08ac38b212e3d3c6104a5e5618a8484d7eb2df04df282d6599e5473daL158' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 96556840</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 24ca2ac33029f434e47969661b0a878773639ab8</div><div id='time'> Time: 2021-08-12</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: TrainingInterfaces/Text_to_Spectrogram/Tacotron2/Tacotron2.py</div><div id='m_class'> M Class Name: Tacotron2</div><div id='n_method'> N Class Name: Tacotron2</div><div id='m_method'> M Method Name: forward(6)</div><div id='n_method'> N Method Name: forward(6)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: TrainingInterfaces/Text_to_Spectrogram/Tacotron2/Tacotron2.py</div><div id='n_file'> N File Name: TrainingInterfaces/Text_to_Spectrogram/Tacotron2/Tacotron2.py</div><div id='m_start'> M Start Line: 179</div><div id='m_end'> M End Line: 208</div><div id='n_start'> N Start Line: 183</div><div id='n_end'> N End Line: 199</div><BR>