<html><h3>Pattern ID :39551
</h3><img src='112634485.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    for i in range(x.size(0)):
        mask_temp = Masks.get_ff_mask(height, width)
        mask_temp = torch.from_numpy(mask_temp)
        mask[i,:,:,:] = <a id="change">temp[i,:,:,:]</a> * mask_temp
    if x.is_cuda:
        mask = mask.cuda()
    result = x * (1. - mask)</code></pre><h3>After Change</h3><pre><code class='java'>
    if x.is_cuda:
        mask = mask.cuda()

    <a id="change">if </a>config[&quotmask_type&quot] == &quothole&quot:
        result = x * (1. - mask)
    elif config[&quotmask_type&quot] == &quotmosaic&quot:
        &#47&#47 TODO: Matching the mosaic patch size and the mask size
        mosaic_unit_size = config[&quotmosaic_unit_size&quot]
        downsampled_image = F.interpolate(x, scale_factor=1. / mosaic_unit_size, mode=&quotnearest&quot)
        upsampled_image<a id="change"> = </a><a id="change">F.interpolate(</a>downsampled_image<a id="change">, size=(height, width), mode=&quotnearest&quot)</a>
        result = upsampled_image * mask + x * (1. - mask)
    else:
        raise NotImplementedError(&quotNot implemented mask type.&quot)
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/sayednadim/global-and-local-attention-based-free-form-image-inpainting/commit/2e453ae0b658395a88acb8db67115db86d9274ea#diff-4a9d05cc3eadd7aa0b347854fe63c55a626198aae56f6d3482457196186e5d60L40' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 112634485</div><div id='project'> Project Name: sayednadim/global-and-local-attention-based-free-form-image-inpainting</div><div id='commit'> Commit Name: 2e453ae0b658395a88acb8db67115db86d9274ea</div><div id='time'> Time: 2020-08-16</div><div id='author'> Author: smnadimuddin@gmail.com</div><div id='file'> File Name: model/mask.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: mask_image(2)</div><div id='n_method'> N Method Name: mask_image(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: model/mask.py</div><div id='n_file'> N File Name: model/mask.py</div><div id='m_start'> M Start Line: 40</div><div id='m_end'> M End Line: 48</div><div id='n_start'> N Start Line: 56</div><div id='n_end'> N End Line: 72</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            camera_elevation = 2.  &#47&#47 Camera is elevated on 2m

            &#47&#47 the binary occlusion mask is inverse for the trasys planetary dataset
            <a id="change">output[ChannelEnum.BINARY_OCCLUSION_MAP]</a> = ~output[ChannelEnum.BINARY_OCCLUSION_MAP]

            &#47&#47 the encoded elevation map of the trasys dataset measures the orthogonal distance
            &#47&#47 from the camera to the terrain</code></pre><h3>After Change</h3><pre><code class='java'>

                    &#47&#47 we need to improve this solution
                    &#47&#47 the torchvision image transformation do not work well with elevation maps (unbounded values)
                    <a id="change">if </a>trasys:
                        if value.dtype == torch.bool:
                            value = value.to(dtype=torch.float)

                        interpolation_input = value.unsqueeze(dim=0).unsqueeze(dim=0)
                        interpolation_output = <a id="change">F.interpolate(</a>interpolation_input<a id="change">, size=self.config["size"])</a>
                        value<a id="change"> = </a>interpolation_output.squeeze()
                    else:
                        &#47&#47 the transform to a PIL image does not work for bool tensors
                        if value.dtype == torch.bool:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/mstoelzle/solving-occlusion/commit/72b8500ba6ff2908728c12814484aa167e46326b#diff-d05cb7733c20f89c5b7ab6f5586e0ec80c2ce069c78312dd5bb6c5765d762ec7L29' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 112634466</div><div id='project'> Project Name: mstoelzle/solving-occlusion</div><div id='commit'> Commit Name: 72b8500ba6ff2908728c12814484aa167e46326b</div><div id='time'> Time: 2020-10-06</div><div id='author'> Author: maximilian@stoelzle.ch</div><div id='file'> File Name: src/datasets/base_dataset.py</div><div id='m_class'> M Class Name: BaseDataset</div><div id='n_method'> N Class Name: BaseDataset</div><div id='m_method'> M Method Name: prepare_item(3)</div><div id='n_method'> N Method Name: prepare_item(3)</div><div id='m_parent_class'> M Parent Class: VisionDataset</div><div id='n_parent_class'> N Parent Class: VisionDataset</div><div id='m_file'> M File Name: src/datasets/base_dataset.py</div><div id='n_file'> N File Name: src/datasets/base_dataset.py</div><div id='m_start'> M Start Line: 37</div><div id='m_end'> M End Line: 102</div><div id='n_start'> N Start Line: 39</div><div id='n_end'> N End Line: 115</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            video = torch.cat([image.unsqueeze(2), video], dim=2)  &#47&#47 1 x 64 x 21 x 16 x 16
        else:
            video = image.unsqueeze(2)
        video = video.view(*<a id="change">video.shape[:3]</a>, -1).permute(0, 2, 3, 1)  &#47&#47 B x T x (H x W) x C -&gt; 1 x 21 x (16*16) x 64
        video = self.attention(video)  &#47&#47 1 x 21 x 256 x 64
        return video
</code></pre><h3>After Change</h3><pre><code class='java'>
        s = None
        if len(self.encoder) &gt; 0:
            for block, remapper in zip(self.encoder, self.remapper):
                <a id="change">if </a>block.scaler is not None:
                    prev_s<a id="change"> = </a><a id="change">nn.functional.interpolate(</a>video<a id="change">, scale_factor=(1, 0.5, 0.5), recompute_scale_factor=False)</a>
                else:
                    prev_s = video
                video = block(video, s)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/laion-ai/phenaki/commit/e510eaef70feb098d2c1b5fac8b34291c1955c9e#diff-d2675e29d21c30767dbf4b16980547aac6cc99b46ce44781ec89d09ee7193467L60' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 112634449</div><div id='project'> Project Name: laion-ai/phenaki</div><div id='commit'> Commit Name: e510eaef70feb098d2c1b5fac8b34291c1955c9e</div><div id='time'> Time: 2022-10-17</div><div id='author'> Author: d6582533@gmail.com</div><div id='file'> File Name: vivq.py</div><div id='m_class'> M Class Name: Encoder</div><div id='n_method'> N Class Name: Encoder</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: vivq.py</div><div id='n_file'> N File Name: vivq.py</div><div id='m_start'> M Start Line: 62</div><div id='m_end'> M End Line: 70</div><div id='n_start'> N Start Line: 90</div><div id='n_end'> N End Line: 104</div><BR>