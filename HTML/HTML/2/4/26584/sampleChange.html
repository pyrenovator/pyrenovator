<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    assert _TENSOR_MODEL_PARALLEL_GROUP is None, \
        &quottensor model parallel group is already initialized&quot
    &#47&#47 Build the model-parallel groups.
    <a id="change">for </a>i in range(data_parallel_size_)<a id="change">:
        </a>ranks = range(i * tensor_model_parallel_size_, (i + 1) * tensor_model_parallel_size_)
        group = dist.new_group(ranks)
        if rank in ranks:
            _TENSOR_MODEL_PARALLEL_GROUP<a id="change"> = </a>group

    if dist.get_rank() == 0:
        print(&quot&gt; initialize tensor model parallel with size {}&quot.format(tensor_model_parallel_size_))</code></pre><h3>After Change</h3><pre><code class='java'>
        _logger = colossalai.logging.get_dist_logger()
        _logger.error(
            "use fastfold.distributed.init_dap instead of torch.distributed.init_process_group!")
        <a id="change">exit(</a>-1<a id="change">)</a>

    &#47&#47 set distributed environ for single device launch
    if &quotRANK&quot not in os.environ:
        set_distributed_environ(&quotWORLD_SIZE&quot, 1)</code></pre>