<html><h3>Pattern ID :2895
</h3><img src='11340988.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.layers = nn.Sequential()
        for index, output_size in enumerate(output_sizes):
            &#47&#47 self.layers.add_module("linear_%d" % index, LazyLinear(output_size))
            <a id="change">self.layers.add_module(</a>"linear_%d" % index, nn.LazyLinear(output_size)<a id="change">)</a>
        &#47&#47 self.layers.add_module("relu", nn.ReLU6())

    def forward(self, input):
        input = input.to(device)</code></pre><h3>After Change</h3><pre><code class='java'>
class LazyMLP(nn.Module):
    def __init__(self, output_sizes):
        super().__init__()
        num_layers<a id="change"> = </a>len(output_sizes)
        self._layers_ordered_dict = OrderedDict()
        for index, output_size in enumerate(output_sizes):
            self._layers_ordered_dict["linear_" + str(index)] = nn.LazyLinear(output_size)
            &#47&#47 self.layers.add_module("linear_%d" % index, LazyLinear(output_size))
            <a id="change">if </a>index &lt; (num_layers<a id="change"> - </a>1):
                self._layers_ordered_dict["relu_" + str(index)] = nn.ReLU()
        self.layers = nn.Sequential(self._layers_ordered_dict)
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/wwmark/meshgraphnets/commit/9be9ef87016e6502ff5ef60a988866e0d7fc9ecb#diff-2a1838fd89edbf7fc22eb003fbdcbe4ac68927faa9e4cbd1df4afa4bbc9bda9aL36' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 11340988</div><div id='project'> Project Name: wwmark/meshgraphnets</div><div id='commit'> Commit Name: 9be9ef87016e6502ff5ef60a988866e0d7fc9ecb</div><div id='time'> Time: 2021-10-08</div><div id='author'> Author: ruoheng.ma@gmail.com</div><div id='file'> File Name: encode_process_decode.py</div><div id='m_class'> M Class Name: LazyMLP</div><div id='n_method'> N Class Name: LazyMLP</div><div id='m_method'> M Method Name: __init__(2)</div><div id='n_method'> N Method Name: __init__(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: encode_process_decode.py</div><div id='n_file'> N File Name: encode_process_decode.py</div><div id='m_start'> M Start Line: 36</div><div id='m_end'> M End Line: 42</div><div id='n_start'> N Start Line: 36</div><div id='n_end'> N End Line: 43</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        if new_module is not None:
            changed = True
            <a id="change">module.add_module(</a>name, new_module<a id="change">)</a>

        &#47&#47 recursively apply to child
        changed |= patch_dropout_layers(child)
    return changed</code></pre><h3>After Change</h3><pre><code class='java'>
    if not inplace:
        module = deepcopy(module)

    changed<a id="change"> = </a>_patch_dropout(module=module, prob=prob, consistent=consistent, **consistent_kwargs)
    <a id="change">if not changed</a>:
        raise MisconfigurationException("The model should contain at least one dropout layer.")
    return module
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pietrolesci/energizer/commit/280d4ae9cedfd0fa1ca2c52437963e8887e658cc#diff-a3c4b15783c8838afdfbf160adc8dc50df3c7447e488c8ce966c61c737fbc86fL155' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 11341001</div><div id='project'> Project Name: pietrolesci/energizer</div><div id='commit'> Commit Name: 280d4ae9cedfd0fa1ca2c52437963e8887e658cc</div><div id='time'> Time: 2021-12-31</div><div id='author'> Author: pietrolesci@outlook.com</div><div id='file'> File Name: energizer/inference/utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: patch_dropout_layers(4)</div><div id='n_method'> N Method Name: patch_dropout_layers(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: energizer/inference/utils.py</div><div id='n_file'> N File Name: energizer/inference/utils.py</div><div id='m_start'> M Start Line: 155</div><div id='m_end'> M End Line: 195</div><div id='n_start'> N Start Line: 179</div><div id='n_end'> N End Line: 208</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.layers = nn.Sequential()
        for index, output_size in enumerate(output_sizes):
            &#47&#47 self.layers.add_module("linear_%d" % index, LazyLinear(output_size))
            <a id="change">self.layers.add_module(</a>"linear_%d" % index, nn.LazyLinear(output_size)<a id="change">)</a>
        &#47&#47 self.layers.add_module("relu", nn.ReLU6())

    def forward(self, input):
        input = input.to(device)</code></pre><h3>After Change</h3><pre><code class='java'>
class LazyMLP(nn.Module):
    def __init__(self, output_sizes):
        super().__init__()
        num_layers<a id="change"> = </a>len(output_sizes)
        self._layers_ordered_dict = OrderedDict()
        for index, output_size in enumerate(output_sizes):
            self._layers_ordered_dict["linear_" + str(index)] = nn.LazyLinear(output_size)
            &#47&#47 self.layers.add_module("linear_%d" % index, LazyLinear(output_size))
            <a id="change">if </a>index &lt; (num_layers<a id="change"> - </a>1):
                self._layers_ordered_dict["relu_" + str(index)] = nn.ReLU()
        self.layers = nn.Sequential(self._layers_ordered_dict)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/wwmark/meshgraphnets/commit/230815cb6b38dd30b8d25bec0083efb8358d68e2#diff-f98409e68daccfa078c64611e308738c8aa5cb93dd0104f710141b7552af93b6L34' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 11340995</div><div id='project'> Project Name: wwmark/meshgraphnets</div><div id='commit'> Commit Name: 230815cb6b38dd30b8d25bec0083efb8358d68e2</div><div id='time'> Time: 2021-10-08</div><div id='author'> Author: ruoheng.ma@gmail.com</div><div id='file'> File Name: encode_process_decode_max_pooling.py</div><div id='m_class'> M Class Name: LazyMLP</div><div id='n_method'> N Class Name: LazyMLP</div><div id='m_method'> M Method Name: __init__(2)</div><div id='n_method'> N Method Name: __init__(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: encode_process_decode_max_pooling.py</div><div id='n_file'> N File Name: encode_process_decode_max_pooling.py</div><div id='m_start'> M Start Line: 36</div><div id='m_end'> M End Line: 42</div><div id='n_start'> N Start Line: 36</div><div id='n_end'> N End Line: 43</div><BR>