<html><h3>Pattern ID :31463
</h3><img src='92103725.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        Returns:
            dict: grouped_dict like {G1: object, G2: object}
        
        <a id="change">if </a><a id="change">isinstance(</a>getattr(self, "group", None), Callable<a id="change">)</a>:
            grouped_dict = self.group(ungrouped_dict, *args, **kwargs)
            if self.ens is not None:
                ens_dict = {}
                for key, value in grouped_dict.items():
                    ens_dict[key] = self.ens(value)
                grouped_dict<a id="change"> = </a>ens_dict
            return grouped_dict
        else:
            raise NotImplementedError(f"Please specify valid group_func.")</code></pre><h3>After Change</h3><pre><code class='java'>
        job_l = []
        for key, value in grouped_dict.items():
            key_l.append(key)
            job_l.append(<a id="change">delayed(Group.reduce)(</a>self, value<a id="change">)</a>)
        return dict(zip(key_l, Parallel(n_jobs=n_jobs, verbose=verbose)(job_l)))

</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/microsoft/qlib/commit/cca43cf102c6c18958d1363e22cc6855aaaeb473#diff-bc51d7af2362dd467d193f450f88cfe7f795976ce51c7f43b4c7137ee35b8058L33' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 92103725</div><div id='project'> Project Name: microsoft/qlib</div><div id='commit'> Commit Name: cca43cf102c6c18958d1363e22cc6855aaaeb473</div><div id='time'> Time: 2021-04-11</div><div id='author'> Author: afe.young@gmail.com</div><div id='file'> File Name: qlib/model/ens/group.py</div><div id='m_class'> M Class Name: Group</div><div id='n_method'> N Class Name: Group</div><div id='m_method'> M Method Name: __call__(4)</div><div id='n_method'> N Method Name: __call__(2)</div><div id='m_parent_class'> M Parent Class: Serializable</div><div id='n_parent_class'> N Parent Class: Serializable</div><div id='m_file'> M File Name: qlib/model/ens/group.py</div><div id='n_file'> N File Name: qlib/model/ens/group.py</div><div id='m_start'> M Start Line: 33</div><div id='m_end'> M End Line: 44</div><div id='n_start'> N Start Line: 38</div><div id='n_end'> N End Line: 58</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            kwargs = tree_map(unwrap, kwargs)
            o = func(*args, **kwargs)

        <a id="change">if </a><a id="change">isinstance(</a>o, MetaTensor<a id="change">)</a>:
            target._meta_data<a id="change"> = </a>o
            return target
        else:
            return o</code></pre><h3>After Change</h3><pre><code class='java'>

            o = func(*tree_map(unwrap, args), **tree_map(unwrap, kwargs))
            if isinstance(o, (tuple, list)):
                return <a id="change">type(o)(</a>wrap<a id="change">(y, i=i) for i, y in enumerate(o))</a>
            return wrap(o)

    @classmethod
    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/hpcaitech/colossalai/commit/ed8f60b93b9dfe88635baf40501ac80de84fa18c#diff-5419d5476461b80894361023ecb9ef4b709086eb880cd32ed01a5021c92f331fL209' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 92103739</div><div id='project'> Project Name: hpcaitech/colossalai</div><div id='commit'> Commit Name: ed8f60b93b9dfe88635baf40501ac80de84fa18c</div><div id='time'> Time: 2023-03-14</div><div id='author'> Author: lhx0217@gmail.com</div><div id='file'> File Name: colossalai/utils/model/experimental.py</div><div id='m_class'> M Class Name: LazyTensor</div><div id='n_method'> N Class Name: LazyTensor</div><div id='m_method'> M Method Name: __torch_function__(5)</div><div id='n_method'> N Method Name: __torch_function__(5)</div><div id='m_parent_class'> M Parent Class: torch.Tensor</div><div id='n_parent_class'> N Parent Class: torch.Tensor</div><div id='m_file'> M File Name: colossalai/utils/model/experimental.py</div><div id='n_file'> N File Name: colossalai/utils/model/experimental.py</div><div id='m_start'> M Start Line: 211</div><div id='m_end'> M End Line: 247</div><div id='n_start'> N Start Line: 184</div><div id='n_end'> N End Line: 245</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            self.freeze_before_training(pl_module)
            self.pl_module = pl_module  &#47&#47 save pl_module ref for downstream configuration convenience
            &#47&#47 TODO: connect FTS strategy adapter to user-provided pl_module here
            <a id="change">if </a><a id="change">isinstance(</a>strategy, DDPFullyShardedNativeStrategy<a id="change">)</a>:
                if is_overridden("configure_sharded_model", self.pl_module):
                    rank_zero_info(
                        "You have overridden the `LightningModule.configure_sharded_model` hook. Fine-Tuning Scheduler"
                        " will validate that you have wrapped the provided model in a manner that aligns with the"
                        " defined fine-tuning schedule phases. If you would like to have Fine-Tuning Scheduler"
                        " automatically wrap your model in a fine-tuning phase-aligned according to a given auto wrap"
                        " policy, avoid overriding `configure_sharded_model` in your module and provide the desired"
                        " auto wrap policy."
                    )
                    csm_func<a id="change"> = </a>self._wrapped_configure_sharded_model(self.pl_module.configure_sharded_model)
                    setattr(self.pl_module, "configure_sharded_model", csm_func)
                else:
                    setattr(self.pl_module, "configure_sharded_model", self._phase_aligned_configure_sharded_model)</code></pre><h3>After Change</h3><pre><code class='java'>
                    f" &quot{strategy.strategy_name}&quot because ``allow_untested`` is ``True``."  &#47&#47 type: ignore[attr-defined]
                )
                rank_zero_warn(warn_msg)
        self.strategy_adapter = <a id="change">STRATEGY_ADAPTERS.get(strategy.strategy_name, StrategyAdapter)()</a>
        self.strategy_adapter.connect(self)
        if self.gen_ft_sched_only:
            if trainer.is_global_zero:
                assert trainer.log_dir is not None</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/speediedan/finetuning-scheduler/commit/61bf5064dd01abff7800013244a0ecac4d3335a9#diff-1e40f5edc419001c8841d4ee49ba3fe40f5c1a9f4122eff482a4037740bd7683L443' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 92103738</div><div id='project'> Project Name: speediedan/finetuning-scheduler</div><div id='commit'> Commit Name: 61bf5064dd01abff7800013244a0ecac4d3335a9</div><div id='time'> Time: 2022-12-04</div><div id='author'> Author: danny.dale@gmail.com</div><div id='file'> File Name: src/finetuning_scheduler/fts.py</div><div id='m_class'> M Class Name: FinetuningScheduler</div><div id='n_method'> N Class Name: FinetuningScheduler</div><div id='m_method'> M Method Name: setup(4)</div><div id='n_method'> N Method Name: setup(4)</div><div id='m_parent_class'> M Parent Class: ScheduleImplMixin,ScheduleParsingMixin,CallbackDepMixin,BaseFinetuning</div><div id='n_parent_class'> N Parent Class: ScheduleImplMixin,ScheduleParsingMixin,CallbackDepMixin,BaseFinetuning</div><div id='m_file'> M File Name: src/finetuning_scheduler/fts.py</div><div id='n_file'> N File Name: src/finetuning_scheduler/fts.py</div><div id='m_start'> M Start Line: 468</div><div id='m_end'> M End Line: 522</div><div id='n_start'> N Start Line: 463</div><div id='n_end'> N End Line: 506</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        metrics_summary = {}
        for metric_cls in self.metrics:
            metric = metric_cls(device=self.device)
            <a id="change">if </a><a id="change">isinstance(</a>self.calculated_metrics[metric.name], dict<a id="change">)</a>:
                metrics_summary[metric.name] = {}
                for k, v in self.calculated_metrics[metric.name].items():
                    metrics_summary[metric.name][k]<a id="change"> = </a>np.nanmean(
                        self.calculated_metrics[metric.name][k]
                    )
            else:</code></pre><h3>After Change</h3><pre><code class='java'>
        Returns the specified metrics
        result = []
        for metric in metrics:
            result.append(<a id="change">getattr(self, metric)()</a>)
        return result

</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/biasvariancelabs/aitlas/commit/c5d3452f6986bb725b1e32c029e1b7b065acf4ad#diff-c0d71207f64739512046cec6efdc4d6e7d3cbd93d90bb13154c3dfebe606609eL37' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 92103729</div><div id='project'> Project Name: biasvariancelabs/aitlas</div><div id='commit'> Commit Name: c5d3452f6986bb725b1e32c029e1b7b065acf4ad</div><div id='time'> Time: 2021-03-15</div><div id='author'> Author: ivan.kitanovski@gmail.com</div><div id='file'> File Name: aitlas/base/metrics.py</div><div id='m_class'> M Class Name: RunningScore</div><div id='n_method'> N Class Name: RunningScore</div><div id='m_method'> M Method Name: get_scores(2)</div><div id='n_method'> N Method Name: get_scores(1)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: aitlas/base/metrics.py</div><div id='n_file'> N File Name: aitlas/base/metrics.py</div><div id='m_start'> M Start Line: 38</div><div id='m_end'> M End Line: 51</div><div id='n_start'> N Start Line: 81</div><div id='n_end'> N End Line: 86</div><BR>