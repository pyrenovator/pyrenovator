<html><h3>Pattern ID :6313
</h3><img src='21900910.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        raise NotImplementedError

    def backward_G(self):
        <a id="change">raise </a>NotImplementedError

    def optimize_parameters(self, steps):
        raise NotImplementedError</code></pre><h3>After Change</h3><pre><code class='java'>
        else:
            self.loss_G_recon = self.criterionRecon(self.Sfake_B, self.Tfake_B) * self.opt.lambda_recon
            fake = self.Sfake_B
        pred_fake<a id="change"> = </a>self.netD(fake)
        self.loss_G_gan<a id="change"> = </a>self.criterionGAN(pred_fake, True, for_discriminator=False) * self.opt.lambda_gan
        if self.opt.lambda_distill &gt; 0:
            self.loss_G_distill = self.calc_distill_loss() * self.opt.lambda_distill
        else:
            self.loss_G_distill = 0
        self.loss_G = self.loss_G_gan + self.loss_G_recon + self.loss_G_distill
        <a id="change">self.loss_G.backward()</a>

    def optimize_parameters(self, steps):
        raise NotImplementedError
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/mit-han-lab/gan-compression/commit/06742a5fbb7f738bc4910d526608cbd368b787fb#diff-a3e864eac09e902e6213bb6104d54ccb5c997a09665a9ad5ef1cfa228444ab37L174' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 21900910</div><div id='project'> Project Name: mit-han-lab/gan-compression</div><div id='commit'> Commit Name: 06742a5fbb7f738bc4910d526608cbd368b787fb</div><div id='time'> Time: 2020-12-15</div><div id='author'> Author: lmxyy1999@foxmail.com</div><div id='file'> File Name: distillers/base_resnet_distiller.py</div><div id='m_class'> M Class Name: BaseResnetDistiller</div><div id='n_method'> N Class Name: BaseResnetDistiller</div><div id='m_method'> M Method Name: backward_G(1)</div><div id='n_method'> N Method Name: backward_G(1)</div><div id='m_parent_class'> M Parent Class: BaseModel</div><div id='n_parent_class'> N Parent Class: BaseModel</div><div id='m_file'> M File Name: distillers/base_resnet_distiller.py</div><div id='n_file'> N File Name: distillers/base_resnet_distiller.py</div><div id='m_start'> M Start Line: 194</div><div id='m_end'> M End Line: 194</div><div id='n_start'> N Start Line: 174</div><div id='n_end'> N End Line: 187</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.done_mask_ph = tf.placeholder(tf.float32, [None])

    def update(self, ob_no, next_ob_no, re_n, terminal_n):
        <a id="change">raise </a>NotImplementedError
</code></pre><h3>After Change</h3><pre><code class='java'>

        target_q_t = self.__calc_target_vals(next_ob_no, re_n, terminal_n)

        q_t<a id="change"> = </a>torch.sum(self.q_t_values(ob_no) * torch_one_hot(act_t_ph, self.ac_dim), dim=1)

        &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47 

        &#47&#47 TODO compute the Bellman error (i.e. TD error between q_t and target_q_t)
        &#47&#47 Note that this scalar-valued tensor later gets passed into the optimizer, to be minimized
        &#47&#47 HINT: use reduce mean of huber_loss (from infrastructure/dqn_utils.py) instead of squared error
        total_error<a id="change"> = </a>torch.mean(huber_loss(q_t, target_q_t)) 

        &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
        
        &#47&#47 train_fn will be called in order to train the critic (by minimizing the TD error)
        self.optimizer.zero_grad()
        <a id="change">total_error.backward()</a>
        nn.utils.clip_grad_norm_(self.q_t_values.parameters(), self.grad_norm_clipping)

        self.optimizer.step()
        return total_error</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/erfanmhi/deep-reinforcement-learning-cs285-pytorch/commit/5311fe388264d564443693329dbacf832ed6d349#diff-99d25cebe5f630dfc37c66d0e5760fa4276afded9471909da2939a013ff16ca4L113' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 21900958</div><div id='project'> Project Name: erfanmhi/deep-reinforcement-learning-cs285-pytorch</div><div id='commit'> Commit Name: 5311fe388264d564443693329dbacf832ed6d349</div><div id='time'> Time: 2020-08-24</div><div id='author'> Author: mhi.erfan1@gmail.com</div><div id='file'> File Name: hw4/cs285/critics/dqn_critic.py</div><div id='m_class'> M Class Name: DQNCritic</div><div id='n_method'> N Class Name: DQNCritic</div><div id='m_method'> M Method Name: update(7)</div><div id='n_method'> N Method Name: update(5)</div><div id='m_parent_class'> M Parent Class: BaseCritic</div><div id='n_parent_class'> N Parent Class: BaseCritic</div><div id='m_file'> M File Name: hw4/cs285/critics/dqn_critic.py</div><div id='n_file'> N File Name: hw4/cs285/critics/dqn_critic.py</div><div id='m_start'> M Start Line: 114</div><div id='m_end'> M End Line: 114</div><div id='n_start'> N Start Line: 76</div><div id='n_end'> N End Line: 104</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 Build the comutation graph.
        out = self.layer(x)
        <a id="change">raise </a>NotImplementedError()

        &#47&#47 TODO: and now? sum()? 1? ...
</code></pre><h3>After Change</h3><pre><code class='java'>
        self.gen_dummy_layer()
        &#47&#47 Build the comutation graph
        out = self.layer(x)
        t<a id="change"> = </a>out.sum()
        one<a id="change"> = </a>t / t
        <a id="change">one.backward()</a>

        &#47&#47 one = dummy_function.apply(self.layer(x))
        &#47&#47 &#47&#47 to the backward lady.
        &#47&#47 one.backward()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/ffcd1ff663179d9e19976bf79dce7f9fc7780a06#diff-abb40350f2417629a7e6b990b50e066e726bc29213b40df52561c2fe062d5d19L64' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 21900953</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: ffcd1ff663179d9e19976bf79dce7f9fc7780a06</div><div id='time'> Time: 2020-01-02</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: pipeline/communication/send_the_layer_not_the_gradeint.py</div><div id='m_class'> M Class Name: DummyLayerHelper</div><div id='n_method'> N Class Name: DummyLayerHelper</div><div id='m_method'> M Method Name: recompute_and_backwards(2)</div><div id='n_method'> N Method Name: recompute_and_backwards(2)</div><div id='m_parent_class'> M Parent Class: Module</div><div id='n_parent_class'> N Parent Class: Module</div><div id='m_file'> M File Name: pipeline/communication/send_the_layer_not_the_gradeint.py</div><div id='n_file'> N File Name: pipeline/communication/send_the_layer_not_the_gradeint.py</div><div id='m_start'> M Start Line: 70</div><div id='m_end'> M End Line: 70</div><div id='n_start'> N Start Line: 107</div><div id='n_end'> N End Line: 112</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                os.makedirs(self.savepath_dict[d])

    def train_one_epoch(self, **kwargs):
        <a id="change">raise </a>NotImplementedError

    def calc_val_loss(self, **kwargs):
        raise NotImplementedError</code></pre><h3>After Change</h3><pre><code class='java'>
        else:
            _metrics = [0] * len(self.metrics_names)
            for i, _batch in enumerate(data_loader):
                timg<a id="change"> = </a>self._get_data_by_name(_batch, &quotimage&quot)
                self.optimizer.zero_grad()

                loss<a id="change"> = </a>self.calc_loss_one_batch(self.model(timg), timg, **kwargs)
                <a id="change">loss[0].backward()</a>

                &#47&#47 Adjust learning weights
                self.optimizer.step()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/royerlab/cytoself/commit/d6114662473b57f7810ce92d8e438b14d2bf9e42#diff-523a645fa31e54102ea3d8c4549d9c4caee9ee4e75e32ac26e816f9dccd684fdL153' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 21900963</div><div id='project'> Project Name: royerlab/cytoself</div><div id='commit'> Commit Name: d6114662473b57f7810ce92d8e438b14d2bf9e42</div><div id='time'> Time: 2022-05-23</div><div id='author'> Author: liamiiliil@gmail.com</div><div id='file'> File Name: cytoself/trainer/basetrainer.py</div><div id='m_class'> M Class Name: BaseTrainer</div><div id='n_method'> N Class Name: BaseTrainer</div><div id='m_method'> M Method Name: train_one_epoch(2)</div><div id='n_method'> N Method Name: train_one_epoch(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: cytoself/trainer/basetrainer.py</div><div id='n_file'> N File Name: cytoself/trainer/basetrainer.py</div><div id='m_start'> M Start Line: 154</div><div id='m_end'> M End Line: 154</div><div id='n_start'> N Start Line: 182</div><div id='n_end'> N End Line: 202</div><BR>