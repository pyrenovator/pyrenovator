<html><h3>Pattern ID :17246
</h3><img src='57403370.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 setting the device

        if not exists(accelerator) and not exists(device):
            diffusion_prior_device<a id="change"> = </a>next(<a id="change">diffusion_prior.parameters()</a>).device
            self.print(f&quotaccelerator not given, and device not specified: defaulting to device of diffusion prior parameters - {diffusion_prior_device}&quot)
            self.device = diffusion_prior_device
        else:</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 mixed precision checks

        <a id="change">if </a><a id="change">(
            exists(self.accelerator) 
            and self.accelerator.distributed_type == DistributedType.DEEPSPEED 
            and self.diffusion_prior.clip is not None
            )</a>:
            &#47&#47 Then we need to make sure clip is using the correct precision or else deepspeed will error
            cast_type_map = {
                "fp16": torch.half,</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/dalle2-pytorch/commit/f9423d308b6f36e51152c2c45045ff4ebb308287#diff-617450527162fa367141dbf45e8b201673573479820af0ffc56ba93b7f70947fL177' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 57403370</div><div id='project'> Project Name: lucidrains/dalle2-pytorch</div><div id='commit'> Commit Name: f9423d308b6f36e51152c2c45045ff4ebb308287</div><div id='time'> Time: 2022-07-20</div><div id='author'> Author: 51308183+nousr@users.noreply.github.com</div><div id='file'> File Name: dalle2_pytorch/trainer.py</div><div id='m_class'> M Class Name: DiffusionPriorTrainer</div><div id='n_method'> N Class Name: DiffusionPriorTrainer</div><div id='m_method'> M Method Name: __init__(10)</div><div id='n_method'> N Method Name: __init__(12)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dalle2_pytorch/trainer.py</div><div id='n_file'> N File Name: dalle2_pytorch/trainer.py</div><div id='m_start'> M Start Line: 182</div><div id='m_end'> M End Line: 240</div><div id='n_start'> N Start Line: 177</div><div id='n_end'> N End Line: 246</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

                &#47&#47 This is to make DDP happy.
                &#47&#47 DDP expects all workers to have gradient w.r.t the same set of parameters.
                _dummy<a id="change"> = </a>sum(x.view(-1)[0] for x in <a id="change">self.parameters()</a>) * 0.0
                return empty + _dummy
            else:
                return empty</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 later version, `Conv2d` in these PyTorch versions has already supported empty inputs.
        if not torch.jit.is_scripting():
            with warnings.catch_warnings(record=True):
                <a id="change">if </a><a id="change">x.numel() == 0 and self.training</a>:
                    &#47&#47 https://github.com/pytorch/pytorch/issues/12013
                    assert not isinstance(
                        self.norm, torch.nn.SyncBatchNorm</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/stevewongv/instanceshadowdetection/commit/52f5f2cdcdb670b07efe4086abf503d9d50753c5#diff-013791abb9a2879485a664b3062b408cce57c163b4ec14151e126fdcad50bfb1L60' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 57403368</div><div id='project'> Project Name: stevewongv/instanceshadowdetection</div><div id='commit'> Commit Name: 52f5f2cdcdb670b07efe4086abf503d9d50753c5</div><div id='time'> Time: 2022-07-28</div><div id='author'> Author: steve.w.git@icloud.com</div><div id='file'> File Name: detectron2/layers/wrappers.py</div><div id='m_class'> M Class Name: Conv2d</div><div id='n_method'> N Class Name: Conv2d</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: torch.nn.Conv2d</div><div id='n_parent_class'> N Parent Class: torch.nn.Conv2d</div><div id='m_file'> M File Name: detectron2/layers/wrappers.py</div><div id='n_file'> N File Name: detectron2/layers/wrappers.py</div><div id='m_start'> M Start Line: 61</div><div id='m_end'> M End Line: 87</div><div id='n_start'> N Start Line: 105</div><div id='n_end'> N End Line: 115</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            adjoint_params = tuple(kwargs[&quotadjoint_params&quot])
        except KeyError:
            try:
                adjoint_params<a id="change"> = </a>tuple(<a id="change">func.parameters()</a>)
            except AttributeError:
                raise ValueError("If using the adjoint, then either `adjoint_params` must be specified, or `func` "
                                 "must be a `torch.nn.Module`, to collect the parameters that gradients are calculated "</code></pre><h3>After Change</h3><pre><code class='java'>

        for buffer in X.buffers():
            &#47&#47 Compare based on id to avoid PyTorch not playing well with using `in` on tensors.
            <a id="change">if </a><a id="change">buffer.requires_grad and id(buffer) not in _adjoint_params</a>:
                warnings.warn("One of the inputs to the control path X requires gradients but is not listed in "
                              "`options[&quotadjoint_params&quot]`. This is probably a mistake: it will not receive a gradient "
                              "when using the adjoint method. Either have the input not require gradients (if that "</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/patrick-kidger/torchcde/commit/a7b8403a215d7433eb14abe647b39dc098b9933d#diff-afeeb2ccc4125b3a37dd6f91249fadcea8fd708e5a69f6373ae61bff84db78a6L131' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 57403372</div><div id='project'> Project Name: patrick-kidger/torchcde</div><div id='commit'> Commit Name: a7b8403a215d7433eb14abe647b39dc098b9933d</div><div id='time'> Time: 2021-02-06</div><div id='author'> Author: 33688385+patrick-kidger@users.noreply.github.com</div><div id='file'> File Name: torchcde/solver.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: cdeint(5)</div><div id='n_method'> N Method Name: cdeint(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: torchcde/solver.py</div><div id='n_file'> N File Name: torchcde/solver.py</div><div id='m_start'> M Start Line: 179</div><div id='m_end'> M End Line: 200</div><div id='n_start'> N Start Line: 180</div><div id='n_end'> N End Line: 208</div><BR>