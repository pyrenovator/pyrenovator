<html><h3>Pattern ID :9466
</h3><img src='33815531.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        i, j = q.shape[-2], k.shape[-2]
        causal_mask = torch.ones(i, j, device = q.device, dtype = torch.bool).triu(j - i + 1)
        causal_mask_chunks = causal_mask.split(q_bucket_size, dim = 0)
        causal_mask_chunks = list(<a id="change">map(</a>lambda t: t.split(k_bucket_size, dim = -1), causal_mask_chunks<a id="change">)</a>)

    if exists(attn_bias):
        i, j = attn_bias.shape[-2:]</code></pre><h3>After Change</h3><pre><code class='java'>
            q_start_index = q_index * q_bucket_size
            k_start_index = k_index * k_bucket_size

            if <a id="change">causal and k_start_index &gt; (q_start_index + q_chunk.shape[-2] - 1)</a>:
                &#47&#47 if chunk is to be all masked out causally, skip
                continue

            attn_bias_chunk = attn_bias_chunks[q_index][k_index] if exists(attn_bias) else None

            exp_weight_chunk, weighted_value_chunk, weight_max_chunk = summarize_qkv_fn(
                q_chunk,
                k_chunk,
                v_chunk,
                mask_chunk,
                attn_bias_chunk,
                causal,
                (q_start_index<a id="change">, k_start_index</a>)
            )

            exp_weights.append(exp_weight_chunk)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/memory-efficient-attention-pytorch/commit/4be82443e060be7224be5e8247c097fcc84aa72d#diff-7de1bf8844a9aafc0b616bb3f7f2bc3701cbdab6390a8d8096e5dfea36da1708L87' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33815531</div><div id='project'> Project Name: lucidrains/memory-efficient-attention-pytorch</div><div id='commit'> Commit Name: 4be82443e060be7224be5e8247c097fcc84aa72d</div><div id='time'> Time: 2022-03-20</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memory_efficient_attention_pytorch/memory_efficient_attention.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: memory_efficient_attention(9)</div><div id='n_method'> N Method Name: memory_efficient_attention(9)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: memory_efficient_attention_pytorch/memory_efficient_attention.py</div><div id='n_file'> N File Name: memory_efficient_attention_pytorch/memory_efficient_attention.py</div><div id='m_start'> M Start Line: 87</div><div id='m_end'> M End Line: 136</div><div id='n_start'> N Start Line: 114</div><div id='n_end'> N End Line: 136</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        i, j = q.shape[-2], k.shape[-2]
        causal_mask = torch.ones(i, j, device = q.device, dtype = torch.bool).triu(j - i + 1)
        causal_mask_chunks = causal_mask.split(q_bucket_size, dim = 0)
        causal_mask_chunks = list(<a id="change">map(</a>lambda t: t.split(k_bucket_size, dim = -1), causal_mask_chunks<a id="change">)</a>)

    if exists(attn_bias):
        i, j = attn_bias.shape[-2:]</code></pre><h3>After Change</h3><pre><code class='java'>
        for k_index, (k_chunk, v_chunk, mask_chunk) in enumerate(zip(k_chunks, v_chunks, mask_chunks)):
            k_start_index = k_index * k_bucket_size

            if <a id="change">causal and k_start_index &gt; (q_start_index + q_chunk.shape[-2] - 1)</a>:
                &#47&#47 if chunk is to be all masked out causally, skip
                continue

            attn_bias_chunk = attn_bias_chunks[q_index][k_index] if exists(attn_bias) else None

            exp_weight_chunk, weighted_value_chunk = summarize_qkv_fn(
                q_chunk,
                k_chunk,
                v_chunk,
                mask_chunk,
                attn_bias_chunk,
                causal,
                (q_start_index<a id="change">, k_start_index</a>)
            )

            exp_weights.append(exp_weight_chunk)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/memory-efficient-attention-pytorch/commit/4be82443e060be7224be5e8247c097fcc84aa72d#diff-da6685a180d8c8c636b32d699e32ddc64180690f91b63dc75a91d3846cb1bb37L75' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33815530</div><div id='project'> Project Name: lucidrains/memory-efficient-attention-pytorch</div><div id='commit'> Commit Name: 4be82443e060be7224be5e8247c097fcc84aa72d</div><div id='time'> Time: 2022-03-20</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memory_efficient_attention_pytorch/memory_efficient_cosine_sim_attention.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: numerically_unstable_memory_efficient_attention(9)</div><div id='n_method'> N Method Name: numerically_unstable_memory_efficient_attention(9)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: memory_efficient_attention_pytorch/memory_efficient_cosine_sim_attention.py</div><div id='n_file'> N File Name: memory_efficient_attention_pytorch/memory_efficient_cosine_sim_attention.py</div><div id='m_start'> M Start Line: 94</div><div id='m_end'> M End Line: 128</div><div id='n_start'> N Start Line: 107</div><div id='n_end'> N End Line: 128</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.blocks = nn.ModuleList([ReversibleBlock(f, g) for (f, g) in blocks])

    def forward(self, x, arg_route = (True, True), **kwargs):
        f_args, g_args = <a id="change">map(</a>lambda route: kwargs if route else {}, arg_route<a id="change">)</a>
        block_kwargs = {&quotf_args&quot: f_args, &quotg_args&quot: g_args}
        return _ReversibleFunction.apply(x, self.blocks, block_kwargs)
</code></pre><h3>After Change</h3><pre><code class='java'>

        layers_and_args = list(zip(blocks, args))

        if <a id="change">self.training and self.layer_dropout &gt; 0</a>:
            layers_and_args = layer_drop(layers_and_args, self.layer_dropout)
            blocks<a id="change">, args</a> = map(lambda ind: list(map(itemgetter(ind), layers_and_args)), (0, 1))

        out =  _ReversibleFunction.apply(x, blocks, args)
        return torch.stack(out.chunk(2, dim=-1)).sum(dim=0)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/linear-attention-transformer/commit/fa23ce09a98a63d26116e3935ad5902cf705255d#diff-29d3c048298bdaa9a9670921a4026430e5b09b55bc1da20d65da18bd5c85f575L118' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33815529</div><div id='project'> Project Name: lucidrains/linear-attention-transformer</div><div id='commit'> Commit Name: fa23ce09a98a63d26116e3935ad5902cf705255d</div><div id='time'> Time: 2020-06-04</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: linear_attention_transformer/reversible.py</div><div id='m_class'> M Class Name: ReversibleSequence</div><div id='n_method'> N Class Name: ReversibleSequence</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: linear_attention_transformer/reversible.py</div><div id='n_file'> N File Name: linear_attention_transformer/reversible.py</div><div id='m_start'> M Start Line: 118</div><div id='m_end'> M End Line: 121</div><div id='n_start'> N Start Line: 161</div><div id='n_end'> N End Line: 174</div><BR>