<html><h3>Pattern ID :8002
</h3><img src='28454983.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 fcn_out: (..., *ny)
        self.yshape = y0.shape
        self.ynumel = y0.numel()
        yndims<a id="change"> = </a>y0.ndim - ts.ndim + 1
        self.ydims = tuple(<a id="change">range(</a>-1, -yndims - 1, -1<a id="change">)</a>)  &#47&#47 the dimension indices of *ny
        self.y0 = y0

        &#47&#47 get the slices to expand from (...,) to (..., *1)
        ylast_slices = (Ellipsis,)<a id="change"> + </a>(None,) * yndims
        self.ylast_slices = ylast_slices

        direction = ts[1] - ts[0]  &#47&#47 (...,)</code></pre><h3>After Change</h3><pre><code class='java'>
        self.func = lambda t, y: sgn * fcn(sgn * t, y.reshape(self.yshape), *params).reshape(-1)
        self.dtype = y0.dtype
        self.device = y0.device
        n = <a id="change">torch.numel(</a>y0<a id="change">)</a>
        self.K = torch.empty((self.n_stages + 1, n), dtype=self.dtype, device=self.device)

        &#47&#47 convert the predefined tensors into the dtype and device</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/xitorch/xitorch/commit/d91c4d6e56fb40b9f91fda53f69871e97dcb6abe#diff-1483d3ea142df8ec6c0e99905625e2421b3c2854fb87878e7255748ac68ae5a4L38' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 28454983</div><div id='project'> Project Name: xitorch/xitorch</div><div id='commit'> Commit Name: d91c4d6e56fb40b9f91fda53f69871e97dcb6abe</div><div id='time'> Time: 2021-12-30</div><div id='author'> Author: firman.kasim@gmail.com</div><div id='file'> File Name: xitorch/_impls/integrate/ivp/adaptive_rk.py</div><div id='m_class'> M Class Name: RKAdaptiveStepSolver</div><div id='n_method'> N Class Name: RKAdaptiveStepSolver</div><div id='m_method'> M Method Name: setup(5)</div><div id='n_method'> N Method Name: setup(5)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: xitorch/_impls/integrate/ivp/adaptive_rk.py</div><div id='n_file'> N File Name: xitorch/_impls/integrate/ivp/adaptive_rk.py</div><div id='m_start'> M Start Line: 54</div><div id='m_end'> M End Line: 77</div><div id='n_start'> N Start Line: 38</div><div id='n_end'> N End Line: 50</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    device = torch.device(&quotcuda&quot if backend == &quotnccl&quot else &quotcpu&quot)

    if backend == &quotnccl&quot:
        world_size<a id="change"> = </a>get_world_size()
        total_size = len(bytearray(nncore.dumps(data)))<a id="change"> * </a>world_size

        pynvml.nvmlInit()
        for i in <a id="change">range(</a>world_size<a id="change">)</a>:
            handle = pynvml.nvmlDeviceGetHandleByIndex(i)
            meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)
            if meminfo.free &lt; total_size:</code></pre><h3>After Change</h3><pre><code class='java'>
    buffer = nncore.dumps(data)
    storage = torch.ByteStorage.from_buffer(buffer)
    data_tensor = torch.ByteTensor(storage, device=device)
    size_tensor = torch.LongTensor([<a id="change">data_tensor.numel()</a>], device=device)
    return data_tensor, size_tensor

</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/yeliudev/nncore/commit/7b1c1711f2805cd55f6fdd396743454017170d76#diff-fdf562e6b578fc92f4c74e59141f61b874fc8722e8eb5d843ef38c4c81d99321L37' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 28454986</div><div id='project'> Project Name: yeliudev/nncore</div><div id='commit'> Commit Name: 7b1c1711f2805cd55f6fdd396743454017170d76</div><div id='time'> Time: 2021-04-28</div><div id='author'> Author: yeliudev@outlook.com</div><div id='file'> File Name: nncore/engine/comm.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _serialize_to_tensor(2)</div><div id='n_method'> N Method Name: _serialize_to_tensor(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: nncore/engine/comm.py</div><div id='n_file'> N File Name: nncore/engine/comm.py</div><div id='m_start'> M Start Line: 38</div><div id='m_end'> M End Line: 59</div><div id='n_start'> N Start Line: 43</div><div id='n_end'> N End Line: 46</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 broadcast to this shape
    &#47&#47 This will work if the original tensor shape was any dimensions as long as the first dimension matches the
    &#47&#47 encoding tensor shape
    shape<a id="change"> = </a>list(tensor.shape)
    num_channels = shape.pop(ch_axis)
    encoding = encoding<a id="change"> * </a>torch.ones(shape + [num_channels]).to(tensor.device)

    &#47&#47 we permute the resulting tensor back to OIHW/IOHW shape
    permute_dims = list(<a id="change">range(</a>len(shape)<a id="change">)</a>)
    permute_dims.insert(ch_axis, len(shape))
    encoding = encoding.permute(permute_dims)
</code></pre><h3>After Change</h3><pre><code class='java'>

    assert len(encoding.shape) &lt;= 1 &#47&#47 Should be 1-dimensional tensor

    if <a id="change">encoding.numel()</a> == 1:
        return encoding

    &#47&#47 Shape of encoding should match the channel dimension of the input</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/quic/aimet/commit/2062c78e97c677c749f400c4e84cb5da741b3e6c#diff-b2f96b89478a93e69873eb98010efb6e08d1224ae44c6034c14355f04497a669L68' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 28454987</div><div id='project'> Project Name: quic/aimet</div><div id='commit'> Commit Name: 2062c78e97c677c749f400c4e84cb5da741b3e6c</div><div id='time'> Time: 2023-04-11</div><div id='author'> Author: quic_kyunggeu@quicinc.com</div><div id='file'> File Name: TrainingExtensions/torch/src/python/aimet_torch/quantsim_straight_through_grad.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: broadcast_to_tensor(3)</div><div id='n_method'> N Method Name: broadcast_to_tensor(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: TrainingExtensions/torch/src/python/aimet_torch/quantsim_straight_through_grad.py</div><div id='n_file'> N File Name: TrainingExtensions/torch/src/python/aimet_torch/quantsim_straight_through_grad.py</div><div id='m_start'> M Start Line: 78</div><div id='m_end'> M End Line: 93</div><div id='n_start'> N Start Line: 78</div><div id='n_end'> N End Line: 90</div><BR>