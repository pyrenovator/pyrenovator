<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        mask = torch.Tensor([[x is not None for x in lb] for lb in label_batch])
        labels = [[0 if x is None else x for x in lb] for lb in label_batch]
        <a id="change">if </a>scaler is not None:
            labels = scaler.transform(labels)  &#47&#47 subtract mean, divide by std
        labels = torch.Tensor(labels)
</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 Log and/or add to tensorboard
        if (n_iter // args.batch_size) % args.log_frequency == 0 and (logger is not None or writer is not None):
            lr<a id="change"> = </a>scheduler.get_lr()[0]
            pnorm = compute_pnorm(model)
            gnorm = compute_gnorm(model)
            loss_avg = loss_sum / iter_count
            loss_sum, iter_count = 0, 0

            if logger is not None:
                logger.debug("Loss = {:.4e}, PNorm = {:.4f}, GNorm = {:.4f}, lr = {:.4f}".format(loss_avg, pnorm, gnorm, lr))

            if writer is not None:
                writer.add_scalar(&quottrain_loss&quot, loss_avg, n_iter)
                writer.add_scalar(&quotparam_norm&quot, pnorm, n_iter)
                writer.add_scalar(&quotgradient_norm&quot, gnorm, n_iter)
                <a id="change">writer.add_scalar(&quotlearning_rate&quot</a>, lr, n_iter<a id="change">)</a>

    return n_iter

</code></pre>