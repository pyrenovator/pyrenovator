<html><h3>Pattern ID :26481
</h3><img src='79462013.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        for _, local_heads in zip(range(depth), n_local_attn_heads):
            layer = nn.ModuleList([
                PreNorm(dim, SelfAttention(dim, heads, causal, one_kv_head = one_kv_head, blindspot_size = blindspot_size, n_local_attn_heads = local_heads, local_attn_window_size = local_attn_window_size, psi_fn = psi_fn)),
                PreNorm(dim, <a id="change">Chunk(</a>ff_chunks, <a id="change">FeedForward(</a>dim<a id="change">)</a><a id="change">, along_dim = 1)</a>)
            ])
            layers.append(layer)
</code></pre><h3>After Change</h3><pre><code class='java'>
            layer_num = ind + 1
            use_pkm = layer_num in cast_tuple(pkm_layers)

            parallel_net = <a id="change">Chunk(</a>ff_chunks, <a id="change">FeedForward(</a>dim<a id="change">)</a><a id="change">, along_dim = 1)</a> if not use_pkm else PKM(dim)

            layer = nn.ModuleList([
                PreNorm(dim, SelfAttention(dim, heads, causal, one_kv_head = one_kv_head, blindspot_size = blindspot_size, n_local_attn_heads = local_heads, local_attn_window_size = local_attn_window_size, psi_fn = psi_fn)),</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/linear-attention-transformer/commit/ffeb7c194ff9a8737e209d1f11e7de55ff55f64d#diff-eee6cf669463e9683bcf9643c4b19a493056f98ded32854287144d47f02a7032L314' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 79462013</div><div id='project'> Project Name: lucidrains/linear-attention-transformer</div><div id='commit'> Commit Name: ffeb7c194ff9a8737e209d1f11e7de55ff55f64d</div><div id='time'> Time: 2020-06-08</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='m_class'> M Class Name: LinearAttentionTransformer</div><div id='n_method'> N Class Name: LinearAttentionTransformer</div><div id='m_method'> M Method Name: __init__(17)</div><div id='n_method'> N Method Name: __init__(15)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='n_file'> N File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='m_start'> M Start Line: 326</div><div id='m_end'> M End Line: 326</div><div id='n_start'> N Start Line: 314</div><div id='n_end'> N End Line: 335</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        for _, local_heads in zip(range(depth), n_local_attn_heads):
            layer = nn.ModuleList([
                PreNorm(dim, SelfAttention(dim, heads, causal, one_kv_head = one_kv_head, blindspot_size = blindspot_size, n_local_attn_heads = local_heads, local_attn_window_size = local_attn_window_size, psi_fn = psi_fn)),
                <a id="change">Chunk(</a>ff_chunks, PreNorm(dim, <a id="change">FeedForward(</a>dim<a id="change">)</a>)<a id="change">, along_dim = 1)</a>
            ])
            layers.append(layer)

        execute_type = ReversibleSequence if reversible else SequentialSequence</code></pre><h3>After Change</h3><pre><code class='java'>
        for _, local_heads in zip(range(depth), n_local_attn_heads):
            layer = nn.ModuleList([
                PreNorm(dim, SelfAttention(dim, heads, causal, one_kv_head = one_kv_head, blindspot_size = blindspot_size, n_local_attn_heads = local_heads, local_attn_window_size = local_attn_window_size, psi_fn = psi_fn)),
                PreNorm(dim, <a id="change">Chunk(</a>ff_chunks, <a id="change">FeedForward(</a>dim<a id="change">)</a><a id="change">, along_dim = 1)</a>)
            ])
            layers.append(layer)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/linear-attention-transformer/commit/3ec94bd694f68c2eb6e69dbc4b65c6602175c929#diff-eee6cf669463e9683bcf9643c4b19a493056f98ded32854287144d47f02a7032L300' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 79462016</div><div id='project'> Project Name: lucidrains/linear-attention-transformer</div><div id='commit'> Commit Name: 3ec94bd694f68c2eb6e69dbc4b65c6602175c929</div><div id='time'> Time: 2020-06-08</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='m_class'> M Class Name: LinearAttentionTransformer</div><div id='n_method'> N Class Name: LinearAttentionTransformer</div><div id='m_method'> M Method Name: __init__(14)</div><div id='n_method'> N Method Name: __init__(14)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='n_file'> N File Name: linear_attention_transformer/linear_attention_transformer.py</div><div id='m_start'> M Start Line: 313</div><div id='m_end'> M End Line: 313</div><div id='n_start'> N Start Line: 313</div><div id='n_end'> N End Line: 313</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        for _ in range(depth):
            layers.append(nn.ModuleList([
                SinkhornSelfAttention(dim, causal = causal, heads = heads, buckets = buckets, sinkhorn_iter = sinkhorn_iter, n_sortcut = n_sortcut, temperature = temperature),
                <a id="change">Chunk(</a>ff_chunks, <a id="change">FeedForward(</a>dim<a id="change">)</a><a id="change">, along_dim=1)</a>
            ]))
        self.layers = ReversibleSequence(layers)
    def forward(self, x, input_mask = None):
        x = torch.cat([x, x], dim = -1)</code></pre><h3>After Change</h3><pre><code class='java'>
        for _ in range(depth):
            layers.append(nn.ModuleList([
                WithNorm(nn.LayerNorm, dim, SinkhornSelfAttention(dim, causal = causal, heads = heads, buckets = buckets, sinkhorn_iter = sinkhorn_iter, n_sortcut = n_sortcut, temperature = temperature)),
                <a id="change">Chunk(</a>ff_chunks, WithNorm(nn.LayerNorm, dim, <a id="change">FeedForward(</a>dim<a id="change">)</a>)<a id="change">, along_dim=1)</a>
            ]))
        self.layers = ReversibleSequence(layers)
    def forward(self, x, input_mask = None):
        x = torch.cat([x, x], dim = -1)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/sinkhorn-transformer/commit/75c1346f2112fdeadd5567fe5391ad05a8a1a467#diff-2ed19ae7feb552dc52b4c7a7c89a078c0fd371c922789f894af09ff01f35eccbL277' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 79462018</div><div id='project'> Project Name: lucidrains/sinkhorn-transformer</div><div id='commit'> Commit Name: 75c1346f2112fdeadd5567fe5391ad05a8a1a467</div><div id='time'> Time: 2020-04-03</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='m_class'> M Class Name: SinkhornTransformer</div><div id='n_method'> N Class Name: SinkhornTransformer</div><div id='m_method'> M Method Name: __init__(10)</div><div id='n_method'> N Method Name: __init__(10)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='n_file'> N File Name: sinkhorn_transformer/sinkhorn_transformer.py</div><div id='m_start'> M Start Line: 279</div><div id='m_end'> M End Line: 284</div><div id='n_start'> N Start Line: 279</div><div id='n_end'> N End Line: 284</div><BR>