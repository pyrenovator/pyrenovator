<html><h3>Pattern ID :18538
</h3><img src='60490459.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47  only update H if H_new doesn&quott contain any infs or nans
        H_vec = np.reshape(H_new, (H_new.size,1))
        if <a id="change">np.all( </a>(np.logical_or(np.isinf(H_vec),np.isnan(H_vec))) == False<a id="change"> )</a>: 
            self.H = H_new
            self.updates += 1
            if damped: </code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 H_vec = np.reshape(H_new, (H_new.size,1))
        &#47&#47 if np.all( (np.logical_or(np.isinf(H_vec),np.isnan(H_vec))) == False ): 
        
        H_vec<a id="change"> = </a>torch.reshape(H_new, (torch.numel(H_new),1))
        notInf_flag = torch.all(<a id="change">torch.isinf(H_vec) == False</a>)
        notNan_flag = torch.all(torch.isnan(H_vec) == False)
        dbg_print_1("notInf_flag = {}".format( notInf_flag ) )
        dbg_print_1("notNan_flag = {}".format( notNan_flag ) )
        &#47&#47 if torch.all( (torch.logical_or(torch.isfinite(H_vec),torch.isnan(H_vec))) == False ): 
        if <a id="change">notInf_flag and notNan_flag</a>:
            self.H = H_new.cpu().numpy()
            self.updates += 1
            if damped: </code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/sun-umn/pygranso/commit/cde0984020653fa77d975293fe823580189b5cce#diff-899e5a52aa0b4a436a19d73a9dc7c6dc1606079cad9c810a91f659310db9558aL91' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60490459</div><div id='project'> Project Name: sun-umn/pygranso</div><div id='commit'> Commit Name: cde0984020653fa77d975293fe823580189b5cce</div><div id='time'> Time: 2021-09-08</div><div id='author'> Author: 52502144+Buyun-Liang@users.noreply.github.com</div><div id='file'> File Name: private/bfgsHessianInverse.py</div><div id='m_class'> M Class Name: H_obj_struct</div><div id='n_method'> N Class Name: H_obj_struct</div><div id='m_method'> M Method Name: update(5)</div><div id='n_method'> N Method Name: update(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: private/bfgsHessianInverse.py</div><div id='n_file'> N File Name: private/bfgsHessianInverse.py</div><div id='m_start'> M Start Line: 92</div><div id='m_end'> M End Line: 97</div><div id='n_start'> N Start Line: 91</div><div id='n_end'> N End Line: 105</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

            causal_mask_chunk = causal_mask_chunks[q_index][k_index] if causal else None

            if exists(causal_mask_chunk) and <a id="change">torch.all(</a>causal_mask_chunk<a id="change">)</a>:
                &#47&#47 if chunk is to be all masked out causally, skip
                continue
</code></pre><h3>After Change</h3><pre><code class='java'>

        for k_index, (k_chunk, v_chunk, mask_chunk) in enumerate(zip(k_chunks, v_chunks, mask_chunks)):
            q_start_index = q_index * q_bucket_size
            k_start_index<a id="change"> = </a>k_index * k_bucket_size

            if <a id="change">causal and k_start_index &gt; (q_start_index + q_chunk.shape[-2] - 1)</a>:
                &#47&#47 if chunk is to be all masked out causally, skip
                continue
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/memory-efficient-attention-pytorch/commit/4be82443e060be7224be5e8247c097fcc84aa72d#diff-7de1bf8844a9aafc0b616bb3f7f2bc3701cbdab6390a8d8096e5dfea36da1708L77' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60490465</div><div id='project'> Project Name: lucidrains/memory-efficient-attention-pytorch</div><div id='commit'> Commit Name: 4be82443e060be7224be5e8247c097fcc84aa72d</div><div id='time'> Time: 2022-03-20</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memory_efficient_attention_pytorch/memory_efficient_attention.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: memory_efficient_attention(9)</div><div id='n_method'> N Method Name: memory_efficient_attention(9)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: memory_efficient_attention_pytorch/memory_efficient_attention.py</div><div id='n_file'> N File Name: memory_efficient_attention_pytorch/memory_efficient_attention.py</div><div id='m_start'> M Start Line: 87</div><div id='m_end'> M End Line: 136</div><div id='n_start'> N Start Line: 114</div><div id='n_end'> N End Line: 136</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

            causal_mask_chunk = causal_mask_chunks[q_index][k_index] if causal else None

            if exists(causal_mask_chunk) and <a id="change">torch.all(</a>causal_mask_chunk<a id="change">)</a>:
                &#47&#47 if chunk is to be all masked out causally, skip
                continue
</code></pre><h3>After Change</h3><pre><code class='java'>

    out = []
    for q_index, q_chunk in enumerate(q_chunks):
        q_start_index<a id="change"> = </a>q_index * q_bucket_size
        exp_weights = []
        weighted_values = []        

        for k_index, (k_chunk, v_chunk, mask_chunk) in enumerate(zip(k_chunks, v_chunks, mask_chunks)):
            k_start_index = k_index * k_bucket_size

            if <a id="change">causal and k_start_index &gt; (q_start_index + q_chunk.shape[-2] - 1)</a>:
                &#47&#47 if chunk is to be all masked out causally, skip
                continue
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/memory-efficient-attention-pytorch/commit/4be82443e060be7224be5e8247c097fcc84aa72d#diff-da6685a180d8c8c636b32d699e32ddc64180690f91b63dc75a91d3846cb1bb37L75' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60490464</div><div id='project'> Project Name: lucidrains/memory-efficient-attention-pytorch</div><div id='commit'> Commit Name: 4be82443e060be7224be5e8247c097fcc84aa72d</div><div id='time'> Time: 2022-03-20</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memory_efficient_attention_pytorch/memory_efficient_cosine_sim_attention.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: numerically_unstable_memory_efficient_attention(9)</div><div id='n_method'> N Method Name: numerically_unstable_memory_efficient_attention(9)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: memory_efficient_attention_pytorch/memory_efficient_cosine_sim_attention.py</div><div id='n_file'> N File Name: memory_efficient_attention_pytorch/memory_efficient_cosine_sim_attention.py</div><div id='m_start'> M Start Line: 94</div><div id='m_end'> M End Line: 128</div><div id='n_start'> N Start Line: 107</div><div id='n_end'> N End Line: 128</div><BR>