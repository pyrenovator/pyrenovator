<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            mean_entity = mean_entity / (real_number)
            tensor_list.append(mean_entity.reshape(1, -1))
        tensor_mean = torch.cat(tensor_list, dim=0)
        <a id="change">print(&quottensor_mean:&quot</a>, tensor_mean<a id="change">)</a> if debug else None

        &#47&#47 print(&quotout.shape:&quot, out.shape) if debug else None
        &#47&#47 out = out[:, :self.real_entities_size, :]</code></pre><h3>After Change</h3><pre><code class='java'>
        print(&quotmask.shape:&quot, mask.shape) if debug else None

        &#47&#47 mask: [batch_size, max_entities]
        device = next(<a id="change">self.parameters()</a>).device
        mask = mask.to(device)

        &#47&#47 assert the input shape is : batch_seq_size x entities_size x embeding_size
        &#47&#47 note: because the feature size of entity is not equal to 256, so it can not fed into transformer directly.
        &#47&#47 thus, we add a embedding layer to transfer it to right size.
        &#47&#47 x is batch_entities_tensor (dim = 3). Shape: batch_seq_size x entities_size x embeding_size
        x = self.embedd(x)
        print(&quotx.shape:&quot, x.shape) if debug else None

        &#47&#47 mask for transformer need a special format
        mask_seq_len = mask.shape[-1]
        tran_mask<a id="change"> = </a>mask.unsqueeze(1)

        &#47&#47 tran_mask: [batch_seq_size x max_entities x max_entities]
        tran_mask = tran_mask.repeat(1, mask_seq_len, 1)</code></pre>