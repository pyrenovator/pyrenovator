<html><h3>Pattern ID :30558
</h3><img src='90362145.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 IterableDataset that joins several individual datasets by taking a
        &#47&#47 random number of samples from each. (or we could just have tiny,
        &#47&#47 one-trajectory shards)
        dataloader<a id="change"> = </a><a id="change">DataLoader(</a>dataset<a id="change">, num_workers=wds_workers,
                                batch_size=None)</a>

        loss_record = []

        if self.scheduler_cls is not None:</code></pre><h3>After Change</h3><pre><code class='java'>
            f"Training for {n_epochs} epochs, each of {batches_per_epoch} "
            f"batches (batch size {self.batch_size})")

        for epoch_num in <a id="change">range(</a>1, n_epochs + 1<a id="change">)</a>:
            loss_meter = AverageMeter()
            &#47&#47 Set encoder and decoder to be in training mode

            for step, batch in enumerate(dataloader):
                &#47&#47 Construct batch (currently just using Torch&quots default batch-creator)
                contexts, targets, traj_ts_info, extra_context = self.unpack_batch(batch)

                &#47&#47 Use an algorithm-specific augmentation strategy to augment either
                &#47&#47 just context, or both context and targets
                contexts, targets = self._prep_tensors(contexts), self._prep_tensors(targets)
                extra_context = self._prep_tensors(extra_context)
                traj_ts_info = self._prep_tensors(traj_ts_info)
                &#47&#47 Note: preprocessing might be better to do on CPU if, in future, we can parallelize doing so
                contexts = self._preprocess(contexts)
                if self.preprocess_target:
                    targets = self._preprocess(targets)
                contexts, targets = self.augmenter(contexts, targets)
                extra_context = self._preprocess_extra_context(extra_context)


                &#47&#47 These will typically just use the forward() function for the encoder, but can optionally
                &#47&#47 use a specific encode_context and encode_target if one is implemented
                encoded_contexts = self.encoder.encode_context(contexts, traj_ts_info)
                encoded_targets = self.encoder.encode_target(targets, traj_ts_info)
                &#47&#47 Typically the identity function
                extra_context = self.encoder.encode_extra_context(extra_context, traj_ts_info)

                &#47&#47 Use an algorithm-specific decoder to "decode" the representations into a loss-compatible tensor
                &#47&#47 As with encode, these will typically just use forward()
                decoded_contexts = self.decoder.decode_context(encoded_contexts, traj_ts_info, extra_context)
                decoded_targets = self.decoder.decode_target(encoded_targets, traj_ts_info, extra_context)

                &#47&#47 Optionally add to the batch before loss. By default, this is an identity operation, but
                &#47&#47 can also implement momentum queue logic
                decoded_contexts, decoded_targets = self.batch_extender(decoded_contexts, decoded_targets)

                &#47&#47 Use an algorithm-specific loss function. Typically this only requires decoded_contexts and
                &#47&#47 decoded_targets, but VAE requires encoded_contexts, so we pass it in here

                loss = self.loss_calculator(decoded_contexts, decoded_targets, encoded_contexts)
                loss_item = loss.item()
                assert not np.isnan(loss_item), "Loss is not NAN"
                loss_meter.update(loss_item)

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                del loss  &#47&#47 so we don&quott use again

                gradient_norm, weight_norm = self._calculate_norms()

                &#47&#47 FIXME(sam): don&quott plot this every batch, plot it every k
                &#47&#47 batches (will make log files much smaller & let us stream
                &#47&#47 them to head node on Ray cluster)
                loss_meter.update(loss_item)
                logger.record(&quotloss&quot, loss_item)
                logger.record(&quotgradient_norm&quot, gradient_norm.item())
                logger.record(&quotweight_norm&quot, weight_norm.item())
                logger.record(&quotepoch&quot, epoch_num)
                logger.record(&quotwithin_epoch_step&quot, step)
                logger.record(&quotbatches_trained&quot, batches_trained)
                logger.dump(step=batches_trained)
                batches_trained += 1

            assert batches_trained &gt; 0, \
                "went through training loop with no batches---empty dataset?"

            if self.scheduler is not None:
                self.scheduler.step()
            loss_record.append(loss_meter.avg)

            &#47&#47 save checkpoint on last epoch, or at regular interval
            should_save_checkpoint = (<a id="change">epoch_num == n_epochs</a> or
                                      epoch_num % self.save_interval == 0)
            if should_save_checkpoint:
                most_recent_encoder_checkpoint_path = os.path.join(</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/humancompatibleai/eirli/commit/579e0a8ca8ba0c924b8c8f393d9445915448617d#diff-da75b71a2421881ac87f16bdedbc66a6eee21a60650cb052fb123cfe1436b72fL220' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 90362145</div><div id='project'> Project Name: humancompatibleai/eirli</div><div id='commit'> Commit Name: 579e0a8ca8ba0c924b8c8f393d9445915448617d</div><div id='time'> Time: 2020-11-19</div><div id='author'> Author: sam@qxcv.net</div><div id='file'> File Name: src/il_representations/algos/representation_learner.py</div><div id='m_class'> M Class Name: RepresentationLearner</div><div id='n_method'> N Class Name: RepresentationLearner</div><div id='m_method'> M Method Name: learn(4)</div><div id='n_method'> N Method Name: learn(4)</div><div id='m_parent_class'> M Parent Class: BaseEnvironmentLearner</div><div id='n_parent_class'> N Parent Class: BaseEnvironmentLearner</div><div id='m_file'> M File Name: src/il_representations/algos/representation_learner.py</div><div id='n_file'> N File Name: src/il_representations/algos/representation_learner.py</div><div id='m_start'> M Start Line: 220</div><div id='m_end'> M End Line: 354</div><div id='n_start'> N Start Line: 220</div><div id='n_end'> N End Line: 343</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 IterableDataset that joins several individual datasets by taking a
        &#47&#47 random number of samples from each. (or we could just have tiny,
        &#47&#47 one-trajectory shards)
        dataloader<a id="change"> = </a><a id="change">DataLoader(</a>dataset<a id="change">, num_workers=wds_workers,
                                batch_size=None)</a>

        loss_record = []

        if self.scheduler_cls is not None:</code></pre><h3>After Change</h3><pre><code class='java'>
            f"Training for {n_epochs} epochs, each of {batches_per_epoch} "
            f"batches (batch size {self.batch_size})")

        for epoch_num in <a id="change">range(</a>1, n_epochs + 1<a id="change">)</a>:
            loss_meter = AverageMeter()
            &#47&#47 Set encoder and decoder to be in training mode

            for step, batch in enumerate(dataloader):
                &#47&#47 Construct batch (currently just using Torch&quots default batch-creator)
                contexts, targets, traj_ts_info, extra_context = self.unpack_batch(batch)

                &#47&#47 Use an algorithm-specific augmentation strategy to augment either
                &#47&#47 just context, or both context and targets
                contexts, targets = self._prep_tensors(contexts), self._prep_tensors(targets)
                extra_context = self._prep_tensors(extra_context)
                traj_ts_info = self._prep_tensors(traj_ts_info)
                &#47&#47 Note: preprocessing might be better to do on CPU if, in future, we can parallelize doing so
                contexts = self._preprocess(contexts)
                if self.preprocess_target:
                    targets = self._preprocess(targets)
                contexts, targets = self.augmenter(contexts, targets)
                extra_context = self._preprocess_extra_context(extra_context)


                &#47&#47 These will typically just use the forward() function for the encoder, but can optionally
                &#47&#47 use a specific encode_context and encode_target if one is implemented
                encoded_contexts = self.encoder.encode_context(contexts, traj_ts_info)
                encoded_targets = self.encoder.encode_target(targets, traj_ts_info)
                &#47&#47 Typically the identity function
                extra_context = self.encoder.encode_extra_context(extra_context, traj_ts_info)

                &#47&#47 Use an algorithm-specific decoder to "decode" the representations into a loss-compatible tensor
                &#47&#47 As with encode, these will typically just use forward()
                decoded_contexts = self.decoder.decode_context(encoded_contexts, traj_ts_info, extra_context)
                decoded_targets = self.decoder.decode_target(encoded_targets, traj_ts_info, extra_context)

                &#47&#47 Optionally add to the batch before loss. By default, this is an identity operation, but
                &#47&#47 can also implement momentum queue logic
                decoded_contexts, decoded_targets = self.batch_extender(decoded_contexts, decoded_targets)

                &#47&#47 Use an algorithm-specific loss function. Typically this only requires decoded_contexts and
                &#47&#47 decoded_targets, but VAE requires encoded_contexts, so we pass it in here

                loss = self.loss_calculator(decoded_contexts, decoded_targets, encoded_contexts)
                loss_item = loss.item()
                assert not np.isnan(loss_item), "Loss is not NAN"
                loss_meter.update(loss_item)

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                del loss  &#47&#47 so we don&quott use again

                gradient_norm, weight_norm = self._calculate_norms()

                &#47&#47 FIXME(sam): don&quott plot this every batch, plot it every k
                &#47&#47 batches (will make log files much smaller & let us stream
                &#47&#47 them to head node on Ray cluster)
                loss_meter.update(loss_item)
                logger.record(&quotloss&quot, loss_item)
                logger.record(&quotgradient_norm&quot, gradient_norm.item())
                logger.record(&quotweight_norm&quot, weight_norm.item())
                logger.record(&quotepoch&quot, epoch_num)
                logger.record(&quotwithin_epoch_step&quot, step)
                logger.record(&quotbatches_trained&quot, batches_trained)
                logger.dump(step=batches_trained)
                batches_trained += 1

            assert batches_trained &gt; 0, \
                "went through training loop with no batches---empty dataset?"

            if self.scheduler is not None:
                self.scheduler.step()
            loss_record.append(loss_meter.avg)

            &#47&#47 save checkpoint on last epoch, or at regular interval
            should_save_checkpoint = (<a id="change">epoch_num == n_epochs</a> or
                                      epoch_num % self.save_interval == 0)
            if should_save_checkpoint:
                most_recent_encoder_checkpoint_path = os.path.join(</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/humancompatibleai/eirli/commit/ccdc28141c799043c5385b20f1201bc6971e462b#diff-da75b71a2421881ac87f16bdedbc66a6eee21a60650cb052fb123cfe1436b72fL215' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 90362150</div><div id='project'> Project Name: humancompatibleai/eirli</div><div id='commit'> Commit Name: ccdc28141c799043c5385b20f1201bc6971e462b</div><div id='time'> Time: 2020-11-19</div><div id='author'> Author: sam@qxcv.net</div><div id='file'> File Name: src/il_representations/algos/representation_learner.py</div><div id='m_class'> M Class Name: RepresentationLearner</div><div id='n_method'> N Class Name: RepresentationLearner</div><div id='m_method'> M Method Name: learn(4)</div><div id='n_method'> N Method Name: learn(4)</div><div id='m_parent_class'> M Parent Class: BaseEnvironmentLearner</div><div id='n_parent_class'> N Parent Class: BaseEnvironmentLearner</div><div id='m_file'> M File Name: src/il_representations/algos/representation_learner.py</div><div id='n_file'> N File Name: src/il_representations/algos/representation_learner.py</div><div id='m_start'> M Start Line: 220</div><div id='m_end'> M End Line: 354</div><div id='n_start'> N Start Line: 220</div><div id='n_end'> N End Line: 343</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        dl_test = dataset.prepare("test", col_set=["feature", "label"], data_key=DataHandlerLP.DK_I)
        dl_test.config(fillna_type="ffill+bfill")
        test_loader<a id="change"> = </a><a id="change">DataLoader(</a>dl_test<a id="change">, batch_size=self.batch_size, num_workers=self.n_jobs)</a>
        self.model.eval()
        preds = []
</code></pre><h3>After Change</h3><pre><code class='java'>
        sample_num = x_values.shape[0]
        preds = []

        for begin in <a id="change">range(</a>sample_num<a id="change">)</a>[:: self.batch_size]:

            if <a id="change">sample_num - begin &lt; self.batch_size</a>:
                end = sample_num
            else:
                end = begin + self.batch_size</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/qlib/commit/bee031af68cd0864c8329de13608c2d4feb58fc1#diff-fe0bd624f58dd8fe7dd95d97a1d8ef4de2800a5a92cb439e19aee34c6628b9bfL209' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 90362149</div><div id='project'> Project Name: microsoft/qlib</div><div id='commit'> Commit Name: bee031af68cd0864c8329de13608c2d4feb58fc1</div><div id='time'> Time: 2021-07-21</div><div id='author'> Author: yl3851@uw.edu</div><div id='file'> File Name: qlib/contrib/model/pytorch_localformer.py</div><div id='m_class'> M Class Name: LocalformerModel</div><div id='n_method'> N Class Name: LocalformerModel</div><div id='m_method'> M Method Name: predict(3)</div><div id='n_method'> N Method Name: predict(2)</div><div id='m_parent_class'> M Parent Class: Model</div><div id='n_parent_class'> N Parent Class: Model</div><div id='m_file'> M File Name: qlib/contrib/model/pytorch_localformer.py</div><div id='n_file'> N File Name: qlib/contrib/model/pytorch_localformer.py</div><div id='m_start'> M Start Line: 209</div><div id='m_end'> M End Line: 223</div><div id='n_start'> N Start Line: 223</div><div id='n_end'> N End Line: 244</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        dl_test = dataset.prepare("test", col_set=["feature", "label"], data_key=DataHandlerLP.DK_I)
        dl_test.config(fillna_type="ffill+bfill")
        test_loader<a id="change"> = </a><a id="change">DataLoader(</a>dl_test<a id="change">, batch_size=self.batch_size, num_workers=self.n_jobs)</a>
        self.model.eval()
        preds = []
</code></pre><h3>After Change</h3><pre><code class='java'>
        sample_num = x_values.shape[0]
        preds = []

        for begin in <a id="change">range(</a>sample_num<a id="change">)</a>[:: self.batch_size]:

            if <a id="change">sample_num - begin &lt; self.batch_size</a>:
                end = sample_num
            else:
                end = begin + self.batch_size</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/qlib/commit/bee031af68cd0864c8329de13608c2d4feb58fc1#diff-82a48ba3edeb02bda1232a9cf7a21b13b2ab82fa992719af2e4494d6fe969887L206' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 90362156</div><div id='project'> Project Name: microsoft/qlib</div><div id='commit'> Commit Name: bee031af68cd0864c8329de13608c2d4feb58fc1</div><div id='time'> Time: 2021-07-21</div><div id='author'> Author: yl3851@uw.edu</div><div id='file'> File Name: qlib/contrib/model/pytorch_transformer.py</div><div id='m_class'> M Class Name: TransformerModel</div><div id='n_method'> N Class Name: TransformerModel</div><div id='m_method'> M Method Name: predict(3)</div><div id='n_method'> N Method Name: predict(2)</div><div id='m_parent_class'> M Parent Class: Model</div><div id='n_parent_class'> N Parent Class: Model</div><div id='m_file'> M File Name: qlib/contrib/model/pytorch_transformer.py</div><div id='n_file'> N File Name: qlib/contrib/model/pytorch_transformer.py</div><div id='m_start'> M Start Line: 206</div><div id='m_end'> M End Line: 220</div><div id='n_start'> N Start Line: 222</div><div id='n_end'> N End Line: 243</div><BR>