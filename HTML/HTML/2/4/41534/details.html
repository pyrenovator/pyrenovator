<html><h3>Pattern ID :41534
</h3><img src='116641862.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                if callback(locals(), globals()) is False:
                    break

            episode_reward<a id="change">, episode_timesteps</a> = self.collect_rollouts(self.env, n_episodes=1,
                                                                      action_noise_std=self.action_noise_std,
                                                                      deterministic=False, callback=None,
                                                                      learning_starts=self.learning_starts,</code></pre><h3>After Change</h3><pre><code class='java'>
        evaluations = []
        start_time = time.time()
        eval_env = self._get_eval_env(eval_env)
        obs<a id="change"> = </a><a id="change">self.env.reset()</a>

        while self.num_timesteps &lt; total_timesteps:

            if callback is not None:
                &#47&#47 Only stop training if return value is False, not when it is None.
                if callback(locals(), globals()) is False:
                    break

            rollout = self.collect_rollouts(self.env, n_episodes=self.n_episodes_rollout,
                                            n_steps=self.train_freq, action_noise_std=self.action_noise_std,
                                            deterministic=False, callback=None,
                                            learning_starts=self.learning_starts,
                                            num_timesteps=self.num_timesteps,
                                            replay_buffer=self.replay_buffer,
                                            obs=obs)
            &#47&#47 Unpack
            episode_reward<a id="change">, episode_timesteps, n_episodes, obs</a> = rollout

            episode_num += n_episodes
            self.num_timesteps += episode_timesteps</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/dlr-rm/stable-baselines3/commit/322399e8fefc9dceed5a13bb389ef374168e32c5#diff-5c42912597f7d995bbb7bf4d154d4e6a0171371036a631278ad5d1fa6e549b2cL193' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116641862</div><div id='project'> Project Name: dlr-rm/stable-baselines3</div><div id='commit'> Commit Name: 322399e8fefc9dceed5a13bb389ef374168e32c5</div><div id='time'> Time: 2019-09-25</div><div id='author'> Author: antonin.raffin@dlr.de</div><div id='file'> File Name: torchy_baselines/td3/td3.py</div><div id='m_class'> M Class Name: TD3</div><div id='n_method'> N Class Name: TD3</div><div id='m_method'> M Method Name: learn(9)</div><div id='n_method'> N Method Name: learn(9)</div><div id='m_parent_class'> M Parent Class: BaseRLModel</div><div id='n_parent_class'> N Parent Class: BaseRLModel</div><div id='m_file'> M File Name: torchy_baselines/td3/td3.py</div><div id='n_file'> N File Name: torchy_baselines/td3/td3.py</div><div id='m_start'> M Start Line: 193</div><div id='m_end'> M End Line: 219</div><div id='n_start'> N Start Line: 196</div><div id='n_end'> N End Line: 229</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                if callback(locals(), globals()) is False:
                    break

            episode_reward<a id="change">, episode_timesteps</a> = self.collect_rollouts(self.env, n_episodes=1,
                                                                      action_noise_std=self.action_noise_std,
                                                                      deterministic=False, callback=None,
                                                                      learning_starts=self.learning_starts,</code></pre><h3>After Change</h3><pre><code class='java'>
        evaluations = []
        start_time = time.time()
        eval_env = self._get_eval_env(eval_env)
        obs<a id="change"> = </a><a id="change">self.env.reset()</a>

        while self.num_timesteps &lt; total_timesteps:

            if callback is not None:
                &#47&#47 Only stop training if return value is False, not when it is None.
                if callback(locals(), globals()) is False:
                    break

            rollout = self.collect_rollouts(self.env, n_episodes=self.n_episodes_rollout,
                                            n_steps=self.train_freq, action_noise_std=self.action_noise_std,
                                            deterministic=False, callback=None,
                                            learning_starts=self.learning_starts,
                                            num_timesteps=self.num_timesteps,
                                            replay_buffer=self.replay_buffer,
                                            obs=obs)
            &#47&#47 Unpack
            episode_reward<a id="change">, episode_timesteps, n_episodes, obs</a> = rollout

            self.num_timesteps += episode_timesteps
            episode_num += n_episodes</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/dlr-rm/stable-baselines3/commit/322399e8fefc9dceed5a13bb389ef374168e32c5#diff-ace7082ddb02987587afd18b50d1bebc77d97a1c15fdef1fe5d602ffe5993fd2L217' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116641861</div><div id='project'> Project Name: dlr-rm/stable-baselines3</div><div id='commit'> Commit Name: 322399e8fefc9dceed5a13bb389ef374168e32c5</div><div id='time'> Time: 2019-09-25</div><div id='author'> Author: antonin.raffin@dlr.de</div><div id='file'> File Name: torchy_baselines/sac/sac.py</div><div id='m_class'> M Class Name: SAC</div><div id='n_method'> N Class Name: SAC</div><div id='m_method'> M Method Name: learn(9)</div><div id='n_method'> N Method Name: learn(9)</div><div id='m_parent_class'> M Parent Class: BaseRLModel</div><div id='n_parent_class'> N Parent Class: BaseRLModel</div><div id='m_file'> M File Name: torchy_baselines/sac/sac.py</div><div id='n_file'> N File Name: torchy_baselines/sac/sac.py</div><div id='m_start'> M Start Line: 221</div><div id='m_end'> M End Line: 247</div><div id='n_start'> N Start Line: 223</div><div id='n_end'> N End Line: 256</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

                self.actor.load_from_vector(params)

                episode_reward<a id="change">, episode_timesteps</a> = self.collect_rollouts(self.env, n_episodes=1,
                                                                          action_noise_std=self.action_noise_std,
                                                                          deterministic=False, callback=None,
                                                                          learning_starts=self.learning_starts,</code></pre><h3>After Change</h3><pre><code class='java'>
        evaluations = []
        start_time = time.time()
        eval_env = self._get_eval_env(eval_env)
        obs<a id="change"> = </a><a id="change">self.env.reset()</a>

        while self.num_timesteps &lt; total_timesteps:

            self.fitnesses = []
            self.es_params = self.es.ask(self.pop_size)

            if callback is not None:
                &#47&#47 Only stop training if return value is False, not when it is None.
                if callback(locals(), globals()) is False:
                    break

            if self.num_timesteps &gt; 0:
                &#47&#47 self.train(episode_timesteps)
                &#47&#47 Gradient steps for half of the population
                for i in range(self.n_grad):
                    &#47&#47 set params
                    self.actor.load_from_vector(self.es_params[i])
                    self.actor_target.load_from_vector(self.es_params[i])
                    self.actor.optimizer = th.optim.Adam(self.actor.parameters(), lr=self.learning_rate)

                    &#47&#47 In the paper: 2 * actor_steps // self.n_grad
                    &#47&#47 In the original implementation: actor_steps // self.n_grad
                    &#47&#47 Difference with TD3 implementation:
                    &#47&#47 the target critic is updated in the train_critic()
                    &#47&#47 instead of the train_actor() and no policy delay
                    &#47&#47 Issue with this update style: the bigger the population, the slower the code
                    if self.update_style == &quotoriginal&quot:
                        self.train_critic(actor_steps // self.n_grad, tau=self.tau)
                        self.train_actor(actor_steps, tau_actor=self.tau, tau_critic=0.0)
                    elif self.update_style == &quotoriginal_td3&quot:
                        self.train_critic(actor_steps // self.n_grad, tau=0.0)
                        self.train_actor(actor_steps, tau_actor=self.tau, tau_critic=self.tau)
                    else:
                        &#47&#47 Closer to td3: with policy delay
                        if self.update_style == &quottd3_like&quot:
                            n_training_steps = actor_steps
                        else:
                            &#47&#47 scales with a bigger population
                            &#47&#47 but less training steps per agent
                            n_training_steps = 2 * (actor_steps // self.n_grad)
                        for it in range(n_training_steps):
                            &#47&#47 Sample replay buffer
                            replay_data = self.replay_buffer.sample(self.batch_size)
                            self.train_critic(replay_data=replay_data)

                            &#47&#47 Delayed policy updates
                            if it % self.policy_delay == 0:
                                self.train_actor(replay_data=replay_data, tau_actor=self.tau, tau_critic=self.tau)

                    &#47&#47 Get the params back in the population
                    self.es_params[i] = self.actor.parameters_to_vector()

            &#47&#47 Evaluate agent
            if 0 &lt; eval_freq &lt;= timesteps_since_eval and eval_env is not None:
                timesteps_since_eval %= eval_freq

                self.actor.load_from_vector(self.es.mu)

                mean_reward, _ = evaluate_policy(self, eval_env, n_eval_episodes)
                evaluations.append(mean_reward)

                if self.verbose &gt; 0:
                    print("Eval num_timesteps={}, mean_reward={:.2f}".format(self.num_timesteps, evaluations[-1]))
                    print("FPS: {:.2f}".format(self.num_timesteps / (time.time() - start_time)))

            actor_steps = 0
            &#47&#47 evaluate all actors
            for params in self.es_params:

                self.actor.load_from_vector(params)

                rollout = self.collect_rollouts(self.env, n_episodes=self.n_episodes_rollout,
                                                n_steps=-1, action_noise_std=self.action_noise_std,
                                                deterministic=False, callback=None,
                                                learning_starts=self.learning_starts,
                                                num_timesteps=self.num_timesteps,
                                                replay_buffer=self.replay_buffer,
                                                obs=obs)

                &#47&#47 Unpack
                episode_reward<a id="change">, episode_timesteps, n_episodes, obs</a> = rollout

                episode_num += n_episodes
                self.num_timesteps += episode_timesteps</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/dlr-rm/stable-baselines3/commit/322399e8fefc9dceed5a13bb389ef374168e32c5#diff-3058c36c77f0574f580eb3320d8421e429d0b4f233214b283498816ef3332c36L56' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 116641860</div><div id='project'> Project Name: dlr-rm/stable-baselines3</div><div id='commit'> Commit Name: 322399e8fefc9dceed5a13bb389ef374168e32c5</div><div id='time'> Time: 2019-09-25</div><div id='author'> Author: antonin.raffin@dlr.de</div><div id='file'> File Name: torchy_baselines/cem_rl/cem_rl.py</div><div id='m_class'> M Class Name: CEMRL</div><div id='n_method'> N Class Name: CEMRL</div><div id='m_method'> M Method Name: learn(9)</div><div id='n_method'> N Method Name: learn(9)</div><div id='m_parent_class'> M Parent Class: TD3</div><div id='n_parent_class'> N Parent Class: TD3</div><div id='m_file'> M File Name: torchy_baselines/cem_rl/cem_rl.py</div><div id='n_file'> N File Name: torchy_baselines/cem_rl/cem_rl.py</div><div id='m_start'> M Start Line: 60</div><div id='m_end'> M End Line: 141</div><div id='n_start'> N Start Line: 62</div><div id='n_end'> N End Line: 149</div><BR>