<html><h3>Pattern ID :11594
</h3><img src='39267095.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            for j, entity in enumerate(batch):
                if j &gt;= real_number_tensor[i]:
                    break        
                mean_entity<a id="change"> = </a>mean_entity + entity
            mean_entity = mean_entity / (real_number)
            tensor_list.append(<a id="change">mean_entity.reshape(</a>1, -1<a id="change">)</a>)
        tensor_mean = torch.cat(tensor_list, dim=0)
        print(&quottensor_mean:&quot, tensor_mean) if debug else None
</code></pre><h3>After Change</h3><pre><code class='java'>
        print(&quotentity_num:&quot, entity_num) if debug else None

        &#47&#47 generate the mask for transformer
        mask = <a id="change">torch.arange(</a>0, self.max_entities<a id="change">)</a>.float()
        mask = mask.repeat(batch_size, 1)
        mask = mask &lt; entity_num.unsqueeze(dim=1)

        print(&quotmask:&quot, mask) if debug else None
        print(&quotmask.shape:&quot, mask.shape) if debug else None

        &#47&#47 mask: [batch_size, max_entities]
        device = next(self.parameters()).device
        mask = mask.to(device)

        &#47&#47 assert the input shape is : batch_seq_size x entities_size x embeding_size
        &#47&#47 note: because the feature size of entity is not equal to 256, so it can not fed into transformer directly.
        &#47&#47 thus, we add a embedding layer to transfer it to right size.
        &#47&#47 x is batch_entities_tensor (dim = 3). Shape: batch_seq_size x entities_size x embeding_size
        x = self.embedd(x)
        print(&quotx.shape:&quot, x.shape) if debug else None

        &#47&#47 mask for transformer need a special format
        mask_seq_len = mask.shape[-1]
        tran_mask = mask.unsqueeze(1)

        &#47&#47 tran_mask: [batch_seq_size x max_entities x max_entities]
        tran_mask = tran_mask.repeat(1, mask_seq_len, 1)

        &#47&#47 out: [batch_seq_size x entities_size x embeding_size]
        out = self.transformer(x, mask=tran_mask)
        print(&quotout.shape:&quot, out.shape) if debug else None

        &#47&#47 entity_embeddings: [batch_seq_size x entities_size x conv1_output_size]
        entity_embeddings = F.relu(self.conv1(F.relu(out).transpose(1, 2))).transpose(1, 2)
        print(&quotentity_embeddings.shape:&quot, entity_embeddings.shape) if debug else None

        &#47&#47 AlphaStar: The mean of the transformer output across across the units (masked by the missing entries) 
        &#47&#47 is fed through a linear layer of size 256 and a ReLU to yield `embedded_entity`
        masked_out<a id="change"> = </a>out * mask.unsqueeze(2)

        &#47&#47 sum over across the units
        &#47&#47 masked_out: [batch_seq_size x entities_size x embeding_size]</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/liuruoze/mini-alphastar/commit/612d42a7bf5ef827e1e919198d839fce106155cd#diff-71869e70f9cb835282d541d2d3529d25b6ed6795c79bc770e860e9318619f6b1L716' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 39267095</div><div id='project'> Project Name: liuruoze/mini-alphastar</div><div id='commit'> Commit Name: 612d42a7bf5ef827e1e919198d839fce106155cd</div><div id='time'> Time: 2021-11-25</div><div id='author'> Author: liuruoze@163.com</div><div id='file'> File Name: alphastarmini/core/arch/entity_encoder.py</div><div id='m_class'> M Class Name: EntityEncoder</div><div id='n_method'> N Class Name: EntityEncoder</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: alphastarmini/core/arch/entity_encoder.py</div><div id='n_file'> N File Name: alphastarmini/core/arch/entity_encoder.py</div><div id='m_start'> M Start Line: 721</div><div id='m_end'> M End Line: 758</div><div id='n_start'> N Start Line: 716</div><div id='n_end'> N End Line: 778</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            y /= max_label
            batch_fake_images = netG(z, y)
            raw_fake_images[tmp:(tmp+batch_size)] = batch_fake_images.cpu().detach().numpy()
            raw_fake_counts[tmp:(tmp+batch_size)]<a id="change"> = </a>y.cpu().view(-1).detach().numpy()
            tmp += batch_size

    &#47&#47remove extra entries
    raw_fake_images = raw_fake_images[0:NFAKE]
    raw_fake_counts = raw_fake_counts[0:NFAKE]

    return raw_fake_images, <a id="change">raw_fake_counts.reshape(</a>-1<a id="change">)</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
        tmp = 0
        while tmp &lt; NFAKE:
            z = torch.randn(batch_size, GAN_Latent_Length, dtype=torch.float).to(device)
            labels = np.random.choice(<a id="change">np.arange(</a>num_classes<a id="change">)</a>,size=batch_size,replace=True)
            raw_fake_labels[tmp:(tmp+batch_size)] = labels
            labels = torch.from_numpy(labels).type(torch.long).to(device)
            batch_fake_images = netG(z, labels)
            fake_images[tmp:(tmp+batch_size)] = batch_fake_images.cpu().detach().numpy()
            tmp += batch_size

    &#47&#47remove extra entries
    fake_images = fake_images[0:NFAKE]
    raw_fake_labels = raw_fake_labels[0:NFAKE]
    raw_fake_labels<a id="change"> = </a>raw_fake_labels.astype(np.float)

    &#47&#47convert class labels to raw labels
    raw_fake_labels = np.array([class2label[raw_fake_labels[i]] for i in range(NFAKE)])</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/ubcdingxin/improved_ccgan/commit/8a85572c67f2b5e51be8e71eb77edbead26b2c0a#diff-df3c440c73c50b64aefbe69083440a66533d0d3b3c5f056547c895503b9d9f9aL134' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 39267191</div><div id='project'> Project Name: ubcdingxin/improved_ccgan</div><div id='commit'> Commit Name: 8a85572c67f2b5e51be8e71eb77edbead26b2c0a</div><div id='time'> Time: 2020-03-11</div><div id='author'> Author: dingx92@gmail.com</div><div id='file'> File Name: CellCounting/Train_cDCGAN.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: SampcDCGAN(7)</div><div id='n_method'> N Method Name: SampcDCGAN(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: CellCounting/Train_cDCGAN.py</div><div id='n_file'> N File Name: CellCounting/Train_cDCGAN.py</div><div id='m_start'> M Start Line: 134</div><div id='m_end'> M End Line: 159</div><div id='n_start'> N Start Line: 136</div><div id='n_end'> N End Line: 165</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 my_shape.append(embedding_size)
        &#47&#47 position_embeddings = position_embeddings.view(my_shape)

        position_embeddings<a id="change"> = </a>torch.take_along_dim(embeddings_table, final_mat.flatten().unsqueeze(1), dim=0)
        position_embeddings = <a id="change">position_embeddings.reshape(</a>*final_mat.shape, embeddings_table.shape[-1]<a id="change">)</a>  &#47&#47 [seq_len, seq_len, hdsz]
        self.register_buffer(&quotposition_embeddings&quot, position_embeddings)
        &#47&#47 self.post = nn.Embedding.from_pretrained(position_embeddings, freeze=True)
</code></pre><h3>After Change</h3><pre><code class='java'>
        super(RelativePositionsEncoding, self).__init__()
        &#47&#47 生成相对位置矩阵
        vocab_size = max_relative_position * 2 + 1
        distance_mat = torch.arange(klen)[None, :] - <a id="change">torch.arange(</a>qlen<a id="change">)</a>[:, None]  &#47&#47 列数-行数, [query_len, key_len]
        distance_mat_clipped = torch.clamp(distance_mat, -max_relative_position, max_relative_position)
        final_mat = distance_mat_clipped + max_relative_position

        &#47&#47 sinusoid_encoding编码的位置矩阵
        embeddings_table = get_sinusoid_encoding_table(vocab_size, embedding_size)

        &#47&#47 实现方式1
        &#47&#47 flat_relative_positions_matrix = final_mat.view(-1)
        &#47&#47 one_hot_relative_positions_matrix = torch.nn.functional.one_hot(flat_relative_positions_matrix, num_classes=vocab_size).float()
        &#47&#47 position_embeddings = torch.matmul(one_hot_relative_positions_matrix, embeddings_table)
        &#47&#47 my_shape = list(final_mat.size())
        &#47&#47 my_shape.append(embedding_size)
        &#47&#47 position_embeddings = position_embeddings.view(my_shape)

        &#47&#47 实现方式2
        &#47&#47 position_embeddings = torch.take_along_dim(embeddings_table, final_mat.flatten().unsqueeze(1), dim=0)
        &#47&#47 position_embeddings = position_embeddings.reshape(*final_mat.shape, embeddings_table.shape[-1])  &#47&#47 [seq_len, seq_len, hdsz]
        &#47&#47 self.register_buffer(&quotposition_embeddings&quot, position_embeddings)
        
        &#47&#47 实现方式3
        position_embeddings<a id="change"> = </a>nn.Embedding.from_pretrained(embeddings_table, freeze=True)(final_mat)
        self.register_buffer(&quotposition_embeddings&quot, position_embeddings)

    def forward(self, qlen, klen):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tongjilibo/bert4torch/commit/a2b2e99808769f4b3713f554a41a7d88a45ebfde#diff-423158f6414177c50deaf8ff8d75202474ddccb543fca22c762f1c3c0e973f8eL319' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 39267188</div><div id='project'> Project Name: tongjilibo/bert4torch</div><div id='commit'> Commit Name: a2b2e99808769f4b3713f554a41a7d88a45ebfde</div><div id='time'> Time: 2022-03-19</div><div id='author'> Author: tongjilibo@163.com</div><div id='file'> File Name: bert4torch/layers.py</div><div id='m_class'> M Class Name: RelativePositionsEncoding</div><div id='n_method'> N Class Name: RelativePositionsEncoding</div><div id='m_method'> M Method Name: __init__(5)</div><div id='n_method'> N Method Name: __init__(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: bert4torch/layers.py</div><div id='n_file'> N File Name: bert4torch/layers.py</div><div id='m_start'> M Start Line: 323</div><div id='m_end'> M End Line: 340</div><div id='n_start'> N Start Line: 357</div><div id='n_end'> N End Line: 382</div><BR>