<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                out = _stateless.functional_call(self._model, {n: p for n, p in zip(keys, params)}, x)
                return out
            self._j_list: tuple[torch.Tensor] = torch.autograd.functional.jacobian(func, values, create_graph=True)
            self._j_list = <a id="change">[j.squeeze(1).flatten(start_dim=1) for j in self._j_list]</a> &#47&#47 remove hidden and predict bias dimensions

            &#47&#47 vectorized hessian (https://github.com/pytorch/pytorch/issues/49171)
            def loss(*params):</code></pre><h3>After Change</h3><pre><code class='java'>
                    return out.square().sum()
                self._h_list: tuple[torch.Tensor] = torch.autograd.functional.hessian(func, tuple(self._model.parameters()), create_graph=False)
                self._h_list = [self._h_list[i][i] for i in range(len(self._h_list))] &#47&#47 filter j-th element
                self._h_list<a id="change"> = </a>[<a id="change">h.flatten(end_dim=len(self._h_list[i].shape)-len(d_p_list[i].shape)-1).flatten(start_dim=1)</a> for i, h in enumerate(self._h_list)] &#47&#47 (NC)x(BCHW)

            self.gna_update(
                params_with_grad,</code></pre>