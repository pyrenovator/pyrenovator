<html><h3>Pattern ID :25567
</h3><img src='77734947.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        **kwargs: Any,
    ) -&gt; Tuple[tf.Tensor, tf.Tensor]:

        seq_len<a id="change"> = </a><a id="change">tf.shape(</a>x<a id="change">)</a>[1]

        x = self.embedding(x, **kwargs)  &#47&#47 (batch_size, target_seq_len, d_model)
        x *= tf.math.sqrt(tf.cast(self.d_model, x.dtype))
        x<a id="change"> += </a>tf.cast(self.pos_encoding[:, :seq_len, :], dtype=x.dtype)

        x = self.dropout(x, **kwargs)
</code></pre><h3>After Change</h3><pre><code class='java'>
            output = output + self.dropout(self.position_feed_forward[i](normed_output, **kwargs), **kwargs)

        &#47&#47 (batch_size, seq_len, d_model)
        <a id="change">return </a>self.layer_norm(output, **kwargs)
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/mindee/doctr/commit/9530f81d15395006b4844299236bdadba11c1dde#diff-e1106a3628d5365c5fe2e4fec7304ea8e40c58a5439f23d8f20513e722955178L160' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 77734947</div><div id='project'> Project Name: mindee/doctr</div><div id='commit'> Commit Name: 9530f81d15395006b4844299236bdadba11c1dde</div><div id='time'> Time: 2022-07-01</div><div id='author'> Author: felixdittrich92@gmail.com</div><div id='file'> File Name: doctr/models/recognition/transformer/tensorflow.py</div><div id='m_class'> M Class Name: Decoder</div><div id='n_method'> N Class Name: Decoder</div><div id='m_method'> M Method Name: call(5)</div><div id='n_method'> N Method Name: call(5)</div><div id='m_parent_class'> M Parent Class: NestedObject,layers.Layer</div><div id='n_parent_class'> N Parent Class: tf.keras.layers.Layer</div><div id='m_file'> M File Name: doctr/models/recognition/transformer/tensorflow.py</div><div id='n_file'> N File Name: doctr/models/recognition/transformer/tensorflow.py</div><div id='m_start'> M Start Line: 251</div><div id='m_end'> M End Line: 265</div><div id='n_start'> N Start Line: 160</div><div id='n_end'> N End Line: 179</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        **kwargs: Any,
    ) -&gt; Tuple[tf.Tensor, tf.Tensor]:

        batch_size = <a id="change">tf.shape(</a>q<a id="change">)</a>[0]

        q = self.wq(q, **kwargs)  &#47&#47 (batch_size, seq_len, d_model)
        k = self.wk(k, **kwargs)  &#47&#47 (batch_size, seq_len, d_model)
        v = self.wv(v, **kwargs)  &#47&#47 (batch_size, seq_len, d_model)

        q = self.split_heads(q, batch_size)  &#47&#47 (batch_size, num_heads, seq_len_q, depth)
        k = self.split_heads(k, batch_size)  &#47&#47 (batch_size, num_heads, seq_len_k, depth)
        v<a id="change"> = </a>self.split_heads(v, batch_size)  &#47&#47 (batch_size, num_heads, seq_len_v, depth)

        &#47&#47 scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)
        &#47&#47 attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)
        scaled_attention = scaled_dot_product_attention(q, k, v, mask)

        scaled_attention<a id="change"> = </a>tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  &#47&#47 (batch, seq_len_q, num_heads, depth)

        concat_attention = tf.reshape(scaled_attention,
                                      (batch_size, -1, self.d_model))  &#47&#47 (batch_size, seq_len_q, d_model)</code></pre><h3>After Change</h3><pre><code class='java'>
        x = tf.transpose(x, perm=[0, 2, 1, 3])
        x = tf.reshape(x, shape=[batch_size, -1, self.num_heads * self.d_k])

        <a id="change">return </a>self.output_linear(x, **kwargs)


class Decoder(layers.Layer, NestedObject):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/mindee/doctr/commit/9530f81d15395006b4844299236bdadba11c1dde#diff-e1106a3628d5365c5fe2e4fec7304ea8e40c58a5439f23d8f20513e722955178L126' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 77734954</div><div id='project'> Project Name: mindee/doctr</div><div id='commit'> Commit Name: 9530f81d15395006b4844299236bdadba11c1dde</div><div id='time'> Time: 2022-07-01</div><div id='author'> Author: felixdittrich92@gmail.com</div><div id='file'> File Name: doctr/models/recognition/transformer/tensorflow.py</div><div id='m_class'> M Class Name: MultiHeadAttention</div><div id='n_method'> N Class Name: MultiHeadAttention</div><div id='m_method'> M Method Name: call(5)</div><div id='n_method'> N Method Name: call(5)</div><div id='m_parent_class'> M Parent Class: NestedObject,layers.Layer</div><div id='n_parent_class'> N Parent Class: tf.keras.layers.Layer</div><div id='m_file'> M File Name: doctr/models/recognition/transformer/tensorflow.py</div><div id='n_file'> N File Name: doctr/models/recognition/transformer/tensorflow.py</div><div id='m_start'> M Start Line: 135</div><div id='m_end'> M End Line: 156</div><div id='n_start'> N Start Line: 107</div><div id='n_end'> N End Line: 121</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    matmul_qk = tf.matmul(q, k, transpose_b=True)  &#47&#47 (..., seq_len_q, seq_len_k)
    &#47&#47 scale matmul_qk
    dk<a id="change"> = </a>tf.cast(<a id="change">tf.shape(</a>k<a id="change">)</a>[-1], q.dtype)
    scaled_attention_logits<a id="change"> = </a>matmul_qk / tf.math.sqrt(dk)
    &#47&#47 add the mask to the scaled tensor.
    if mask is not None:
        scaled_attention_logits += (tf.cast(mask, dtype=q.dtype) * -1e9)</code></pre><h3>After Change</h3><pre><code class='java'>
    if mask is not None:
        scores = tf.where(mask == 0, -1e9, scores)
    p_attn = tf.nn.softmax(scores, axis=-1)
    <a id="change">return </a>tf.matmul(p_attn, value), p_attn


class PositionwiseFeedForward(layers.Layer, NestedObject):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/mindee/doctr/commit/9530f81d15395006b4844299236bdadba11c1dde#diff-e1106a3628d5365c5fe2e4fec7304ea8e40c58a5439f23d8f20513e722955178L70' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 77734959</div><div id='project'> Project Name: mindee/doctr</div><div id='commit'> Commit Name: 9530f81d15395006b4844299236bdadba11c1dde</div><div id='time'> Time: 2022-07-01</div><div id='author'> Author: felixdittrich92@gmail.com</div><div id='file'> File Name: doctr/models/recognition/transformer/tensorflow.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: scaled_dot_product_attention(4)</div><div id='n_method'> N Method Name: scaled_dot_product_attention(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: doctr/models/recognition/transformer/tensorflow.py</div><div id='n_file'> N File Name: doctr/models/recognition/transformer/tensorflow.py</div><div id='m_start'> M Start Line: 88</div><div id='m_end'> M End Line: 99</div><div id='n_start'> N Start Line: 63</div><div id='n_end'> N End Line: 67</div><BR>