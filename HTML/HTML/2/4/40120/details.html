<html><h3>Pattern ID :40120
</h3><img src='114055916.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
def shutdown_init_context():
    global zero_init_enabled

    <a id="change">if </a>not zero_init_enabled:
        return

    def _disable_class(cls):
        cls.__init__ = cls._old_init

    &#47&#47 Replace .__init__() for all existing subclasses of torch.nn.Module
    for subclass in get_all_subclasses(torch.nn.modules.module.Module):
        _disable_class(subclass)

    &#47&#47 putting methods back the way we found them
    torch.nn.modules.module.Module.__init_subclass__<a id="change"> = </a>torch.nn.modules.module.Module._old_init_subclass
    torch.nn.modules.module.Module.apply = torch.nn.modules.module.Module._old_apply

    torch.Tensor.__new__ = torch.Tensor.__old_new__
    torch.empty = _orig_torch_empty
    torch.zeros<a id="change"> = </a>_orig_torch_zeros
    torch.ones = _orig_torch_ones
    torch.full = _orig_torch_full
</code></pre><h3>After Change</h3><pre><code class='java'>
    This function is used to initialize deepspeed engine inside the context of Init.
    We need to remove the wrappers but keep the list of contexts.
    
    <a id="change">global zero_init_context</a>
    for ctx in zero_init_context:
        ctx.remove_wrappers()

</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 5</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/microsoft/deepspeed/commit/717c30203e48cb4bd195d4f12df78f6b373662ad#diff-bc45426db58250294594100cfdf3d73ecb653d879cabee404e38edc4eb4c9ecbL462' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 114055916</div><div id='project'> Project Name: microsoft/deepspeed</div><div id='commit'> Commit Name: 717c30203e48cb4bd195d4f12df78f6b373662ad</div><div id='time'> Time: 2023-04-13</div><div id='author'> Author: 81312776+tohtana@users.noreply.github.com</div><div id='file'> File Name: deepspeed/runtime/zero/partition_parameters.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: shutdown_init_context(0)</div><div id='n_method'> N Method Name: shutdown_init_context(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: deepspeed/runtime/zero/partition_parameters.py</div><div id='n_file'> N File Name: deepspeed/runtime/zero/partition_parameters.py</div><div id='m_start'> M Start Line: 462</div><div id='m_end'> M End Line: 490</div><div id='n_start'> N Start Line: 520</div><div id='n_end'> N End Line: 524</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        _, val_dataset = init_data(&quotscisummnet&quot, config, multitask)

    &#47&#47ncls 
    <a id="change">if </a>args[2] == &quotncls_mixed&quot:
        ncls_train<a id="change">, _ = </a>init_data(&quotncls&quot, config)
        ncls_train = ncls_train.head(19200)
        train_dataset = pd.concat([train_dataset, ncls_train],ignore_index=True)
        train_dataset<a id="change"> = </a>train_dataset.sample(frac=1).reset_index(drop=True)

    &#47&#47print("FULL Dataset: {}".format(df.shape))
    </code></pre><h3>After Change</h3><pre><code class='java'>

def main(args):
   
    <a id="change">global config</a>

    &#47&#47Initialize distributed training 
    accelerator = Accelerator()
    device = accelerator.device</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thudm/multilingual-glm/commit/003f258e788fac1ca5ca1120a8af53a399cf1c48#diff-492199c08af6beb08984f18cd77e4cf95d0c7808bf19ec0f367ef9bb9c867f08L191' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 114055964</div><div id='project'> Project Name: thudm/multilingual-glm</div><div id='commit'> Commit Name: 003f258e788fac1ca5ca1120a8af53a399cf1c48</div><div id='time'> Time: 2022-07-26</div><div id='author'> Author: smy930923@qq.com</div><div id='file'> File Name: mt5/finetune_mt5.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: mt5/finetune_mt5.py</div><div id='n_file'> N File Name: mt5/finetune_mt5.py</div><div id='m_start'> M Start Line: 203</div><div id='m_end'> M End Line: 254</div><div id='n_start'> N Start Line: 307</div><div id='n_end'> N End Line: 370</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    global tokenized_column_names

    <a id="change">if </a>this_tokenizer:
        with this_tokenizer.as_target_tokenizer():
            d<a id="change"> = </a>X.apply(
                lambda x: tokenize_row(
                    x,
                    this_tokenizer,
                    prefix=("",) if task is SUMMARIZATION else None,
                    task=task,
                    custom_hpo_args=custom_hpo_args,
                ),
                axis=1,
                result_type="expand",
            )
    else:
        this_tokenizer<a id="change"> = </a>AutoTokenizer.from_pretrained(
            custom_hpo_args.model_path, use_fast=True
        )
        d = X.apply(</code></pre><h3>After Change</h3><pre><code class='java'>
):
    import pandas

    <a id="change">global tokenized_column_names</a>

    with tokenizer.as_target_tokenizer():
        d = X.apply(
            lambda x: tokenize_row(</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/flaml/commit/cb9c7b0d1613eae1b573736a6ad0faf80b9054b2#diff-fe144f6e83b0f68676641733ca0821bdf9076b7fb6e13322d4b16d06bd27de84L168' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 114055933</div><div id='project'> Project Name: microsoft/flaml</div><div id='commit'> Commit Name: cb9c7b0d1613eae1b573736a6ad0faf80b9054b2</div><div id='time'> Time: 2022-01-16</div><div id='author'> Author: liususan091219@users.noreply.github.com</div><div id='file'> File Name: flaml/nlp/utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: tokenize_onedataframe(5)</div><div id='n_method'> N Method Name: tokenize_onedataframe(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: flaml/nlp/utils.py</div><div id='n_file'> N File Name: flaml/nlp/utils.py</div><div id='m_start'> M Start Line: 177</div><div id='m_end'> M End Line: 209</div><div id='n_start'> N Start Line: 196</div><div id='n_end'> N End Line: 216</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def __init__(self, G=None, netlib=None, **kwargs):
        self.G = G
        <a id="change">if </a>self.G is None and hasattr(G, "netlib"):
            self.netlib<a id="change"> = </a>G.netlib
        else:
            if not netlib:
                import networkx as netlib
            self.netlib<a id="change"> = </a>netlib
        keys = [
            "node_feature",
            "node_label",</code></pre><h3>After Change</h3><pre><code class='java'>
    

    def __init__(self, G=None, netlib=None, **kwargs):
        <a id="change">global _netlib</a>
        self.G = G
        if netlib:
            _netlib = netlib
        keys = [</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/snap-stanford/deepsnap/commit/610bdadfd0d2dd94a4ef55318e78919a5153f14b#diff-6e00332a84b17561d6f88c10cc82271c84ecfd1b67966a9866553cf6ae1d24aaL31' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 114055981</div><div id='project'> Project Name: snap-stanford/deepsnap</div><div id='commit'> Commit Name: 610bdadfd0d2dd94a4ef55318e78919a5153f14b</div><div id='time'> Time: 2020-12-05</div><div id='author'> Author: zecheng@stanford.edu</div><div id='file'> File Name: deepsnap/graph.py</div><div id='m_class'> M Class Name: Graph</div><div id='n_method'> N Class Name: Graph</div><div id='m_method'> M Method Name: __init__(3)</div><div id='n_method'> N Method Name: __init__(3)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: deepsnap/graph.py</div><div id='n_file'> N File Name: deepsnap/graph.py</div><div id='m_start'> M Start Line: 32</div><div id='m_end'> M End Line: 39</div><div id='n_start'> N Start Line: 31</div><div id='n_end'> N End Line: 35</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 Only create groups if they don&quott already exist
    &#47&#47 Need to check conditions outside the group creation loop because of the way torch.dist group creation works
    <a id="change">if </a>group_name not in _EXPERT_DATA_PARALLEL_GROUP and group_name not in _EXPERT_PARALLEL_GROUP:
        for j in range(model_parallel_size_):
            for i in range(expert_parallel_size_):
                ranks<a id="change"> = </a>range(i * model_parallel_size_ + j,
                              world_size,
                              expert_parallel_size_ * model_parallel_size_)
                group = torch.distributed.new_group(ranks)
                if rank in list(ranks):
                    _EXPERT_DATA_PARALLEL_GROUP[group_name] = group

                for i in range(dp_world_size // expert_parallel_size_):
                    ranks<a id="change"> = </a>range(i * num_ep * model_parallel_size_ + j,
                                  (i + 1) * expert_parallel_size_ * model_parallel_size_,
                                  model_parallel_size_)
                    group = torch.distributed.new_group(ranks)</code></pre><h3>After Change</h3><pre><code class='java'>
        f"Creating deepspeed groups with model parallel size {model_parallel_size_}, expert parallel size {expert_parallel_size_}, world size {world_size}, dp world size {dp_world_size}",
        [0])

    <a id="change">global _EXPERT_PARALLEL_GROUP, _EXPERT_DATA_PARALLEL_GROUP</a>

    &#47&#47 Get world size and rank. Ensure some consistencies.
    _DATA_PARALLEL_GROUP = mpu.get_data_parallel_group()
    _MODEL_PARALLEL_GROUP = mpu.get_model_parallel_group()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/deepspeed/commit/1e61c7a860e7ac63cd4024a654b4f319063b9374#diff-48e0c3e9a85b5cf697755f9f3a6baa65fc0b8b08059c528581379adaf4295e72L160' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 114055979</div><div id='project'> Project Name: microsoft/deepspeed</div><div id='commit'> Commit Name: 1e61c7a860e7ac63cd4024a654b4f319063b9374</div><div id='time'> Time: 2022-03-17</div><div id='author'> Author: shjwudp@gmail.com</div><div id='file'> File Name: deepspeed/utils/groups.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _create_expert_data_and_model_parallel(2)</div><div id='n_method'> N Method Name: _create_expert_data_and_model_parallel(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: deepspeed/utils/groups.py</div><div id='n_file'> N File Name: deepspeed/utils/groups.py</div><div id='m_start'> M Start Line: 176</div><div id='m_end'> M End Line: 219</div><div id='n_start'> N Start Line: 222</div><div id='n_end'> N End Line: 259</div><BR>