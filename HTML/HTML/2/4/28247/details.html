<html><h3>Pattern ID :28247
</h3><img src='83411853.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            a_dist.log_prob(a_dist.sample()).sum(-1, keepdim=True).clamp(-100.0, 100.0)
        )
    alpha_loss = -(log_alpha * (logp_a + target_entropy).detach()).mean()
    <a id="change">optimizer.zero_grad()</a>
    alpha_loss.backward()
    optimizer.step()
    logs["losses/alpha_loss"] = alpha_loss.item()
    logs["alpha"] = log_alpha.exp().item()</code></pre><h3>After Change</h3><pre><code class='java'>
    with torch.no_grad():
        s_rep = agent.encoder(o)

    <a id="change">for </a>i in range(agent.ensemble_size)<a id="change">:
        </a>with torch.no_grad():
            a_dist<a id="change"> = </a>agent.actors[i](s_rep)
        if discrete:
            logp_a = (a_dist.probs * torch.log_softmax(a_dist.logits, dim=1)).sum(-1)
        else:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/jakegrigsby/super_sac/commit/819313070dbb7c72886cafb948c401c78eb03861#diff-ed592a42afa845d29a0da80d3c28fc31230301bf13d0d27e271c158cb7728875L185' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 83411853</div><div id='project'> Project Name: jakegrigsby/super_sac</div><div id='commit'> Commit Name: 819313070dbb7c72886cafb948c401c78eb03861</div><div id='time'> Time: 2021-10-12</div><div id='author'> Author: jcg6dn@virginia.edu</div><div id='file'> File Name: uafbc/learning.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: alpha_update(9)</div><div id='n_method'> N Method Name: alpha_update(9)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: uafbc/learning.py</div><div id='n_file'> N File Name: uafbc/learning.py</div><div id='m_start'> M Start Line: 185</div><div id='m_end'> M End Line: 198</div><div id='n_start'> N Start Line: 191</div><div id='n_end'> N End Line: 211</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            a_dist.log_prob(a_dist.sample()).sum(-1, keepdim=True).clamp(-100.0, 100.0)
        )
    alpha_loss = -(log_alpha * (logp_a + target_entropy).detach()).mean()
    <a id="change">optimizer.zero_grad()</a>
    alpha_loss.backward()
    optimizer.step()
    logs["losses/alpha_loss"] = alpha_loss.item()
    logs["alpha"] = log_alpha.exp().item()</code></pre><h3>After Change</h3><pre><code class='java'>
    with torch.no_grad():
        s_rep = agent.encoder(o)

    <a id="change">for i</a> in range(agent.ensemble_size)<a id="change">:
        </a>with torch.no_grad():
            a_dist = agent.actors[i](s_rep)
        if discrete:
            logp_a = (a_dist.probs * torch.log_softmax(a_dist.logits, dim=1)).sum(-1)
        else:
            logp_a = (
                a_dist.log_prob(a_dist.sample())
                .sum(-1, keepdim=True)
                .clamp(-100.0, 100.0)
            )
        alpha_loss = -(log_alphas[i] * (logp_a + target_entropy).detach()).mean()
        optimizers[i].zero_grad()
        alpha_loss.backward()
        optimizers[i].step()
        logs[f"losses/alpha_loss_{i}"] = alpha_loss.item()
        logs[f"alphas/alpha_{i}"]<a id="change"> = </a>log_alphas[i].exp().item()
    return logs

</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/jakegrigsby/super_sac/commit/819313070dbb7c72886cafb948c401c78eb03861#diff-ed592a42afa845d29a0da80d3c28fc31230301bf13d0d27e271c158cb7728875L165' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 83411854</div><div id='project'> Project Name: jakegrigsby/super_sac</div><div id='commit'> Commit Name: 819313070dbb7c72886cafb948c401c78eb03861</div><div id='time'> Time: 2021-10-12</div><div id='author'> Author: jcg6dn@virginia.edu</div><div id='file'> File Name: uafbc/learning.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: alpha_update(9)</div><div id='n_method'> N Method Name: alpha_update(9)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: uafbc/learning.py</div><div id='n_file'> N File Name: uafbc/learning.py</div><div id='m_start'> M Start Line: 185</div><div id='m_end'> M End Line: 198</div><div id='n_start'> N Start Line: 191</div><div id='n_end'> N End Line: 211</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
			hh_list.append(hh)
		preds = torch.stack(output_list, dim=1)
		loss = criterion(preds, targets)
		<a id="change">optimizer.zero_grad()</a>
		loss.backward()
		optimizer.step()
		p_bar.set_description(f"Loss: {loss.item():.4f}")
	return torch.squeeze(preds)</code></pre><h3>After Change</h3><pre><code class='java'>
			
			instantaneous_eligibility_trace = torch.zeros_like(param)
			grad_outputs = torch.eye(out.shape[-1], device=targets.device)
			<a id="change">for g_idx</a> in range(grad_outputs.shape[0])<a id="change">:
				</a>param.grad.zero_()
				instantaneous_eligibility_trace[g_idx]<a id="change"> = </a>torch.autograd.grad(out[:, g_idx], param, retain_graph=True)[0][g_idx]
			eligibility_trace.append(instantaneous_eligibility_trace)
		preds = torch.stack(output_list, dim=1)
		mean_error = targets - preds</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/neurotorch/neurotorch/commit/4028f8f7fcb44b43d6235c1d12c734f809b1e629#diff-e6c8fac3a6216017c8602da4fb676a8fa4863da3f367703767d4a4defbe277adL44' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 83411855</div><div id='project'> Project Name: neurotorch/neurotorch</div><div id='commit'> Commit Name: 4028f8f7fcb44b43d6235c1d12c734f809b1e629</div><div id='time'> Time: 2022-11-28</div><div id='author'> Author: 50332514+JeremieGince@users.noreply.github.com</div><div id='file'> File Name: src/neurotorch/learning_algorithms/debug_e_prop.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: dummy_train(1)</div><div id='n_method'> N Method Name: dummy_train(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/neurotorch/learning_algorithms/debug_e_prop.py</div><div id='n_file'> N File Name: src/neurotorch/learning_algorithms/debug_e_prop.py</div><div id='m_start'> M Start Line: 51</div><div id='m_end'> M End Line: 64</div><div id='n_start'> N Start Line: 45</div><div id='n_end'> N End Line: 76</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def step_on_computed_grads(self, old_lrs=None):
        self.optimizer.step()
        <a id="change">self.optimizer.zero_grad()</a>

        &#47&#47 Restore old LRs, to avoid messing up scheduler.
        if old_lrs:
            pgs = self.optimizer.param_groups</code></pre><h3>After Change</h3><pre><code class='java'>
        self.optimizer.step()

        &#47&#47 self.optimizer.zero_grad()
        <a id="change">for pg</a> in self.optimizer.param_groups<a id="change">:
            </a>for p in pg[&quotparams&quot]:
                p.grad<a id="change"> = </a>None

        &#47&#47 Restore old LRs, to avoid messing up scheduler.
        if old_lrs:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/82db7dac9576244e71fce2b614f8191a9cc59616#diff-ca14f2b62469a9993ffe1ba3727263af0e1283e8361a660da888fd7afaa76b82L120' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 83411856</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 82db7dac9576244e71fce2b614f8191a9cc59616</div><div id='time'> Time: 2020-09-03</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: pipeline/training/interface.py</div><div id='m_class'> M Class Name: GradNormStepper</div><div id='n_method'> N Class Name: GradNormStepper</div><div id='m_method'> M Method Name: step_on_computed_grads(2)</div><div id='n_method'> N Method Name: step_on_computed_grads(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: pipeline/training/interface.py</div><div id='n_file'> N File Name: pipeline/training/interface.py</div><div id='m_start'> M Start Line: 122</div><div id='m_end'> M End Line: 122</div><div id='n_start'> N Start Line: 126</div><div id='n_end'> N End Line: 131</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

def train():
    model.train()
    <a id="change">optimizer.zero_grad()</a>
    for batch in train_loader:
        batch.to(device)
        emb = model(batch)
        loss = F.nll_loss(emb[batch.node_label_index],</code></pre><h3>After Change</h3><pre><code class='java'>
    model = model_cls(num_node_features, args.hidden_dim, num_classes, args).to(device)
    opt = build_optimizer(args, model.parameters())

    <a id="change">for epoch</a> in range(args.epochs)<a id="change">:
        </a>total_loss = 0
        model.train()
        for batch in train_loader:
            batch.to(device)
            opt.zero_grad()
            pred = model(batch)
            label<a id="change"> = </a>batch.node_label
            loss = model.loss(pred[batch.node_label_index], label[batch.node_label_index])
            loss.backward()
            opt.step()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/snap-stanford/deepsnap/commit/7bd48169aea6ac9922ecbb0a7b1fa88b3809586e#diff-4780709f76611adc90c2e1b697215120ba78852cb10ec7fbbf7d1a4c963c16b5L75' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 83411857</div><div id='project'> Project Name: snap-stanford/deepsnap</div><div id='commit'> Commit Name: 7bd48169aea6ac9922ecbb0a7b1fa88b3809586e</div><div id='time'> Time: 2020-10-26</div><div id='author'> Author: youjiaxuan@gmail.com</div><div id='file'> File Name: examples/node_classification_cora.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train(7)</div><div id='n_method'> N Method Name: train(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/node_classification_cora.py</div><div id='n_file'> N File Name: examples/node_classification_cora.py</div><div id='m_start'> M Start Line: 76</div><div id='m_end'> M End Line: 86</div><div id='n_start'> N Start Line: 120</div><div id='n_end'> N End Line: 145</div><BR>