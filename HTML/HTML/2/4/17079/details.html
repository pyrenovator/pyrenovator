<html><h3>Pattern ID :17079
</h3><img src='57219961.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                block_wrapper(<a id="change">Attention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)</a>),
                block_wrapper(FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout)),
            ]))
</code></pre><h3>After Change</h3><pre><code class='java'>
        for idx in range(depth):
            use_knn_attention = (idx + 1) in memorizing_layers

            if <a id="change">use_knn_attention</a>:
                attn = KNNAttention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout, num_retrieved_memories = num_retrieved_memories)
            else:
                attn<a id="change"> = </a><a id="change">Attention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)</a>

            self.layers.append(nn.ModuleList([
                block_wrapper(attn),
                block_wrapper(FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout)),</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/memorizing-transformers-pytorch/commit/2efd604bcf4adb00d205f88f5c5fedce34ececb8#diff-698d7ac098881cd330cfb9868699bcdf92d002f508c0204f7e40400664b526d1L242' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 57219961</div><div id='project'> Project Name: lucidrains/memorizing-transformers-pytorch</div><div id='commit'> Commit Name: 2efd604bcf4adb00d205f88f5c5fedce34ececb8</div><div id='time'> Time: 2022-03-24</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memorizing_transformers_pytorch/memorizing_transformers_pytorch.py</div><div id='m_class'> M Class Name: MemorizingTransformer</div><div id='n_method'> N Class Name: MemorizingTransformer</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: memorizing_transformers_pytorch/memorizing_transformers_pytorch.py</div><div id='n_file'> N File Name: memorizing_transformers_pytorch/memorizing_transformers_pytorch.py</div><div id='m_start'> M Start Line: 242</div><div id='m_end'> M End Line: 242</div><div id='n_start'> N Start Line: 247</div><div id='n_end'> N End Line: 260</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                Residual(PreNorm(dim, <a id="change">Attention(dim = dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)</a>)),
                Residual(PreNorm(dim, FeedForward(dim = dim, dropout = ff_dropout)))
            ]))
</code></pre><h3>After Change</h3><pre><code class='java'>
        self.layers = nn.ModuleList([])
        shared_kv_proj = None

        for <a id="change">_</a> in range(depth):
            attn = <a id="change">Attention(dim = dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)</a>
            ff = FeedForward(dim = dim, dropout = ff_dropout)

            shared_kv_proj = default(shared_kv_proj, attn.to_kv)
            attn.to_kv<a id="change"> = </a>shared_kv_proj

            self.layers.append(nn.ModuleList([
                Residual(PreNorm(dim, attn)),</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/feedback-transformer-pytorch/commit/8044063db1a5fc092c9ce68998dc1e301d9f7818#diff-6b348c286a3e8b8e064ec9c64a1f61ec6d6b66e7f14bae4c1b91af070d30c72eL169' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 57219960</div><div id='project'> Project Name: lucidrains/feedback-transformer-pytorch</div><div id='commit'> Commit Name: 8044063db1a5fc092c9ce68998dc1e301d9f7818</div><div id='time'> Time: 2021-02-02</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: feedback_transformer_pytorch/feedback_transformer_pytorch.py</div><div id='m_class'> M Class Name: FeedbackTransformer</div><div id='n_method'> N Class Name: FeedbackTransformer</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: feedback_transformer_pytorch/feedback_transformer_pytorch.py</div><div id='n_file'> N File Name: feedback_transformer_pytorch/feedback_transformer_pytorch.py</div><div id='m_start'> M Start Line: 192</div><div id='m_end'> M End Line: 204</div><div id='n_start'> N Start Line: 194</div><div id='n_end'> N End Line: 212</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            EinopsToAndFrom(
                "b c l",
                "b l c",
                <a id="change">Attention(
                    features=channels,
                    num_heads=attention_heads,
                    head_features=attention_features,
                )</a>,
            )
            if use_attention
            else nn.Identity()</code></pre><h3>After Change</h3><pre><code class='java'>

        if use_attention:
            assert exists(attention_heads) and exists(attention_features)
            self.attention<a id="change"> = </a>EinopsToAndFrom(
                "b c l",
                "b l c",
                <a id="change">Attention(
                    features=channels,
                    num_heads=attention_heads,
                    head_features=attention_features,
                )</a>,
            )

        self.post_block = ResnetBlock1d(</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/archinetai/audio-diffusion-pytorch/commit/67e9fd7b2dbb371fc606ed48550825a03fd0e507#diff-da1345932fc05af66ecce445bdc039bfd662c73dfe7326e045c689fbb3092d67L904' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 57219965</div><div id='project'> Project Name: archinetai/audio-diffusion-pytorch</div><div id='commit'> Commit Name: 67e9fd7b2dbb371fc606ed48550825a03fd0e507</div><div id='time'> Time: 2022-07-21</div><div id='author'> Author: flavio.schneider.97@gmail.com</div><div id='file'> File Name: audio_diffusion_pytorch/models.py</div><div id='m_class'> M Class Name: BottleneckBlock1d</div><div id='n_method'> N Class Name: BottleneckBlock1d</div><div id='m_method'> M Method Name: __init__(0)</div><div id='n_method'> N Method Name: __init__(0)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: audio_diffusion_pytorch/models.py</div><div id='n_file'> N File Name: audio_diffusion_pytorch/models.py</div><div id='m_start'> M Start Line: 930</div><div id='m_end'> M End Line: 941</div><div id='n_start'> N Start Line: 930</div><div id='n_end'> N End Line: 942</div><BR>