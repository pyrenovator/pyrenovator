<html><h3>Pattern ID :3756
</h3><img src='14126231.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

                p_t = p
                param_state = self.state[p]
                <a id="change">if &quotparam_old&quot not in param_state</a>:
                    param_state[&quotparam_old&quot] = torch.clone(p).detach()
                else:
                    p_t<a id="change"> = </a>param_state[&quotparam_old&quot]

                d_p = p.grad.data + lamda*(p-p_t)
                p.add_(d_p, alpha=-lr)</code></pre><h3>After Change</h3><pre><code class='java'>
            for p, g in zip(group[&quotparams&quot], global_params):
                g = g.to(device)
                d_p = p.grad.data + group[&quotmu&quot] * (p.data - g.data)
                <a id="change">p.data.add_(</a>d_p<a id="change">, alpha=-group[&quotlr&quot])</a>
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tsingz0/pfl-non-iid/commit/330ce2d6169fd4a54cec28895418bd38c206b6a4#diff-a99c19163c19c03eede69bdda22f1a2d923b8eb7d0f4b0463d797d66d5fc8e87L89' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14126231</div><div id='project'> Project Name: tsingz0/pfl-non-iid</div><div id='commit'> Commit Name: 330ce2d6169fd4a54cec28895418bd38c206b6a4</div><div id='time'> Time: 2021-03-24</div><div id='author'> Author: 2719584131@qq.com</div><div id='file'> File Name: system/flcore/optimizers/fedoptimizer.py</div><div id='m_class'> M Class Name: PerturbedGradientDescent</div><div id='n_method'> N Class Name: PerturbedGradientDescent</div><div id='m_method'> M Method Name: step(3)</div><div id='n_method'> N Method Name: step(1)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: system/flcore/optimizers/fedoptimizer.py</div><div id='n_file'> N File Name: system/flcore/optimizers/fedoptimizer.py</div><div id='m_start'> M Start Line: 89</div><div id='m_end'> M End Line: 108</div><div id='n_start'> N Start Line: 116</div><div id='n_end'> N End Line: 121</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    runs with success, but needs further validation and possibly optimization for lower runtime impact.

    
    <a id="change">if drop_prob == 0.</a> or not training:
        return x
    _, _, height, width = x.shape
    total_size = width * height
    clipped_block_size<a id="change"> = </a>min(block_size, min(width, height))
    &#47&#47 seed_drop_rate, the gamma parameter
    seed_drop_rate = gamma_scale * drop_prob * total_size / clipped_block_size ** 2 / (
            (width - block_size + 1) *</code></pre><h3>After Change</h3><pre><code class='java'>
    if with_noise:
        normal_noise = torch.randn((1, C, H, W), dtype=x.dtype, device=x.device) if batchwise else torch.randn_like(x)
        if inplace:
            <a id="change">x.mul_(block_mask).add_(</a>normal_noise * (1 - block_mask)<a id="change">)</a>
        else:
            x = x * block_mask + normal_noise * (1 - block_mask)
    else:
        normalize_scale = (block_mask.numel() / block_mask.to(dtype=torch.float32).sum().add(1e-7)).to(x.dtype)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/feng-lab/pytorch-image-models/commit/1904ed8fecdb3f37818378421350315d2abf1224#diff-9431741d608429ed80ae9a820816fda52965a3227c7819c89a44ac86ecac9db5L24' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14126229</div><div id='project'> Project Name: feng-lab/pytorch-image-models</div><div id='commit'> Commit Name: 1904ed8fecdb3f37818378421350315d2abf1224</div><div id='time'> Time: 2020-05-13</div><div id='author'> Author: rwightman@gmail.com</div><div id='file'> File Name: timm/models/layers/drop.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: drop_block_2d(7)</div><div id='n_method'> N Method Name: drop_block_2d(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: timm/models/layers/drop.py</div><div id='n_file'> N File Name: timm/models/layers/drop.py</div><div id='m_start'> M Start Line: 25</div><div id='m_end'> M End Line: 62</div><div id='n_start'> N Start Line: 26</div><div id='n_end'> N End Line: 69</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        

        <a id="change">if group_id in self._averaged_gradients</a>:
            self._averaged_gradients[group_id].append(tensor)
        else:
            self._averaged_gradients[group_id]<a id="change"> = </a>[tensor]

    def reset_average_gradients_by_group(self, group_id: int) -&gt; None:
        </code></pre><h3>After Change</h3><pre><code class='java'>
        :type tensor: torch.Tensor

        
        <a id="change">self._averaged_gradients[group_id][tensor_idx].add_(</a>tensor<a id="change">)</a>

    def reset_average_gradients_by_group(self, group_id: int) -&gt; None:
        
        Reset the bookkeeping data structure for averaged gradients to an empty list</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/hpcaitech/colossalai/commit/ae86a29e2379314da3cb4abf95b5306db6156794#diff-32183142e2b10bda287379bffaf8dbc2e1fbc4ebaefb31b1e2b6e5dab55bb001L42' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14126225</div><div id='project'> Project Name: hpcaitech/colossalai</div><div id='commit'> Commit Name: ae86a29e2379314da3cb4abf95b5306db6156794</div><div id='time'> Time: 2023-02-15</div><div id='author'> Author: 100389977+yhna940@users.noreply.github.com</div><div id='file'> File Name: colossalai/zero/sharded_optim/bookkeeping/gradient_store.py</div><div id='m_class'> M Class Name: GradientStore</div><div id='n_method'> N Class Name: GradientStore</div><div id='m_method'> M Method Name: add_average_gradient_by_group(4)</div><div id='n_method'> N Method Name: add_average_gradient_by_group(3)</div><div id='m_parent_class'> M Parent Class: BaseStore</div><div id='n_parent_class'> N Parent Class: BaseStore</div><div id='m_file'> M File Name: colossalai/zero/sharded_optim/bookkeeping/gradient_store.py</div><div id='n_file'> N File Name: colossalai/zero/sharded_optim/bookkeeping/gradient_store.py</div><div id='m_start'> M Start Line: 53</div><div id='m_end'> M End Line: 58</div><div id='n_start'> N Start Line: 60</div><div id='n_end'> N End Line: 73</div><BR>