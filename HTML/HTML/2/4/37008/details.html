<html><h3>Pattern ID :37008
</h3><img src='105276452.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    assert dy.shape == pos.shape

                    assert conf["partial_charges"].attrs["units"] == "e"
                    pq<a id="change"> = </a><a id="change">pt.tensor(</a>conf["partial_charges"]<a id="change">, dtype=pt.float32)</a>
                    assert pq.shape == z.shape

                    assert conf["dipole_moment"].attrs["units"] == "e*Ã…"
                    dp = pt.tensor(conf["dipole_moment"], dtype=pt.float32)</code></pre><h3>After Change</h3><pre><code class='java'>

            &#47&#47 Iterate over the molecules
            for i_mol, (mol_id, mol) in tqdm(
                <a id="change">enumerate(</a>mols<a id="change">)</a>,
                desc="Molecules",
                total=len(mols),
                leave=False,
            ):

                &#47&#47 Subsample molecules
                if i_mol % self.subsample_molecules != 0:
                    continue

                z = pt.tensor(mol["atomic_numbers"], dtype=pt.long)
                fq = pt.tensor(mol["formal_charges"], dtype=pt.long)
                q = fq.sum()

                for pos, y, neg_dy, pq, dp in load_confs(mol, n_atoms=len(z)):

                    &#47&#47 Skip samples with large forces
                    if self.max_gradient:
                        if neg_dy.norm(dim=1).max() &gt; float(self.max_gradient):
                            continue

                    &#47&#47 Create a sample
                    args = dict(
                        z=z, pos=pos, y=y.view(1, 1), neg_dy=neg_dy, q=q, pq=pq, dp=dp
                    )
                    if mol_ids:
                        args["mol_id"]<a id="change"> = </a>mol_id
                    data = Data(**args)

                    if self.pre_filter is not None and not self.pre_filter(data):</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/torchmd/torchmd-net/commit/d23e6500f2cef1fa56d6c99ce5fdb983f1379bca#diff-56de3e28dd2b438171f4af7de59cc66305304d98e21b24faf4d744698d8c1aa3L72' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 105276452</div><div id='project'> Project Name: torchmd/torchmd-net</div><div id='commit'> Commit Name: d23e6500f2cef1fa56d6c99ce5fdb983f1379bca</div><div id='time'> Time: 2022-10-28</div><div id='author'> Author: peastman@stanford.edu</div><div id='file'> File Name: torchmdnet/datasets/ace.py</div><div id='m_class'> M Class Name: Ace</div><div id='n_method'> N Class Name: Ace</div><div id='m_method'> M Method Name: sample_iter(2)</div><div id='n_method'> N Method Name: sample_iter(1)</div><div id='m_parent_class'> M Parent Class: Dataset</div><div id='n_parent_class'> N Parent Class: Dataset</div><div id='m_file'> M File Name: torchmdnet/datasets/ace.py</div><div id='n_file'> N File Name: torchmdnet/datasets/ace.py</div><div id='m_start'> M Start Line: 72</div><div id='m_end'> M End Line: 123</div><div id='n_start'> N Start Line: 144</div><div id='n_end'> N End Line: 206</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                optimizer.step() 

            if batch % args.log_interval == 0 and batch &gt; 0:
                loss<a id="change"> = </a><a id="change">torch.tensor(</a>total_loss / args.log_interval<a id="change">)</a>
                train_loss.update(loss)

                total_loss = 0.
            </code></pre><h3>After Change</h3><pre><code class='java'>
    with tqdm(total=len(train_loader), 
              desc=&quotEpoch {:2d}/{:2d}&quot.format(epoch + 1, args.epochs),
              disable=not args.verbose) as t:
        for batch_idx, (data, target) in <a id="change">enumerate(</a>train_loader<a id="change">)</a>:
            if args.cuda:
                data, target = data.cuda(), target.cuda()
            data, target = torch.squeeze(data), torch.squeeze(target)
            &#47&#47 Starting each batch, we detach the hidden state from how it was 
            &#47&#47 previously produced. If we didn&quott, the model would try backpropagating 
            &#47&#47 all the way to start of the dataset.
            model.zero_grad()
            if args.model == &quotTransformer&quot:
                output = model(data)
                output = output.view(-1, args.ntokens)
            else:
                hidden = repackage_hidden(hidden)
                output, hidden = model(data, hidden)
            loss<a id="change"> = </a>criterion(output, target)

            loss.backward()
            train_loss.update(loss)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/gpauloski/kfac_pytorch/commit/22a07a821dbaeffdcc00bb4614c8967b44803eeb#diff-bdffb13cddd47e5bff2b5c74cc5a6b24986275164f7b0399c894841d3699ad66L261' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 105276405</div><div id='project'> Project Name: gpauloski/kfac_pytorch</div><div id='commit'> Commit Name: 22a07a821dbaeffdcc00bb4614c8967b44803eeb</div><div id='time'> Time: 2020-04-09</div><div id='author'> Author: gpauloski@yahoo.com</div><div id='file'> File Name: examples/pytorch_wikitext_rnn.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train(10)</div><div id='n_method'> N Method Name: train(9)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/pytorch_wikitext_rnn.py</div><div id='n_file'> N File Name: examples/pytorch_wikitext_rnn.py</div><div id='m_start'> M Start Line: 264</div><div id='m_end'> M End Line: 309</div><div id='n_start'> N Start Line: 246</div><div id='n_end'> N End Line: 279</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            out_probs_i.append(max_logit.item())
            ind = ind + c
        out_tokens.append(cons_tokens)
        out_probs_i<a id="change"> = </a><a id="change">torch.tensor(</a>out_probs_i<a id="change">)</a>
        out_probs.append(out_probs_i)

    out_tokens = pad_sequence(out_tokens, batch_first=True, padding_value=0)
    out_probs = pad_sequence(out_probs, batch_first=True, padding_value=0)</code></pre><h3>After Change</h3><pre><code class='java'>
            max_indices, return_counts=True)
        out_probs_i = torch.zeros(len(counts), device=logits.device)
        ind = 0
        for i, c in <a id="change">enumerate(</a>counts<a id="change">)</a>:
            max_logit = max_logits[ind:ind + c].max()
            out_probs_i[i]<a id="change"> = </a>max_logit
            ind = ind + c
        out_tokens.append(cons_tokens)
        out_probs.append(out_probs_i)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/as-ideas/deepphonemizer/commit/5580ad4ec56f70c19460fafddc989b89d8330c19#diff-37642134bd2fb19b0f715149b4a82f0e17b8d3aa00f3a59b22187a0caac869e1L38' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 105276441</div><div id='project'> Project Name: as-ideas/deepphonemizer</div><div id='commit'> Commit Name: 5580ad4ec56f70c19460fafddc989b89d8330c19</div><div id='time'> Time: 2021-06-21</div><div id='author'> Author: c.schaefer.home@gmail.com</div><div id='file'> File Name: dp/model/utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: get_dedup_tokens(1)</div><div id='n_method'> N Method Name: get_dedup_tokens(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: dp/model/utils.py</div><div id='n_file'> N File Name: dp/model/utils.py</div><div id='m_start'> M Start Line: 61</div><div id='m_end'> M End Line: 71</div><div id='n_start'> N Start Line: 61</div><div id='n_end'> N End Line: 70</div><BR>