<html><h3>Pattern ID :26415
</h3><img src='79245459.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        mask = rearrange(mask, &quotb j -&gt; b 1 1 j&quot)
        weight = weight.masked_fill(~mask, mask_value)

    <a id="change">if </a><a id="change">exists(</a>causal_mask<a id="change">)</a>:
        weight = weight.masked_fill(causal_mask, mask_value)

    weight_max = weight.amax(dim = -1, keepdim = True).detach()</code></pre><h3>After Change</h3><pre><code class='java'>
&#47&#47 memory efficient attention

def summarize_qkv_chunk(q, k, v, mask, attn_bias_chunk, causal, qk_start_indices):
    q_start_index, k_start_index, q_chunk_size, k_chunk_size, device = *qk_start_indices, q.shape[-2], <a id="change">k.shape[-2]</a>, q.device

    weight = einsum(&quotb h i d, b h j d -&gt; b h i j&quot, q, k)
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/memory-efficient-attention-pytorch/commit/4be82443e060be7224be5e8247c097fcc84aa72d#diff-7de1bf8844a9aafc0b616bb3f7f2bc3701cbdab6390a8d8096e5dfea36da1708L52' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 79245459</div><div id='project'> Project Name: lucidrains/memory-efficient-attention-pytorch</div><div id='commit'> Commit Name: 4be82443e060be7224be5e8247c097fcc84aa72d</div><div id='time'> Time: 2022-03-20</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memory_efficient_attention_pytorch/memory_efficient_attention.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: summarize_qkv_chunk(7)</div><div id='n_method'> N Method Name: summarize_qkv_chunk(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: memory_efficient_attention_pytorch/memory_efficient_attention.py</div><div id='n_file'> N File Name: memory_efficient_attention_pytorch/memory_efficient_attention.py</div><div id='m_start'> M Start Line: 52</div><div id='m_end'> M End Line: 67</div><div id='n_start'> N Start Line: 52</div><div id='n_end'> N End Line: 72</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if padding &gt; 0:
            quad_q, quad_k, lin_q, lin_k, v = map(lambda t: F.pad(t, (0, 0, 0, padding), value = 0.), (quad_q, quad_k, lin_q, lin_k, v))

            <a id="change">if </a><a id="change">exists(</a>mask<a id="change">)</a>:
                mask = F.pad(mask, (0, padding), value = False)

        &#47&#47 group along sequence</code></pre><h3>After Change</h3><pre><code class='java'>
        j - sequence dimension (target)
        

        b, n, device, g = <a id="change">x.shape[0]</a>, x.shape[-2], x.device, self.group_size

        &#47&#47 prenorm
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/flash-pytorch/commit/2a0bf49fc80567fbec4d8c88fbf94cd6cc9dce73#diff-6408ce30d3c8730249ca81344fdd04a1dd01ed7b072e84e9d752276e93661682L237' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 79245458</div><div id='project'> Project Name: lucidrains/flash-pytorch</div><div id='commit'> Commit Name: 2a0bf49fc80567fbec4d8c88fbf94cd6cc9dce73</div><div id='time'> Time: 2022-03-28</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: flash_pytorch/flash_pytorch.py</div><div id='m_class'> M Class Name: FLASH</div><div id='n_method'> N Class Name: FLASH</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: flash_pytorch/flash_pytorch.py</div><div id='n_file'> N File Name: flash_pytorch/flash_pytorch.py</div><div id='m_start'> M Start Line: 286</div><div id='m_end'> M End Line: 291</div><div id='n_start'> N Start Line: 254</div><div id='n_end'> N End Line: 287</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        mask = rearrange(mask, &quotb j -&gt; b 1 1 j&quot)
        weight = weight.masked_fill(~mask, mask_value)

    <a id="change">if </a><a id="change">exists(</a>causal_mask<a id="change">)</a>:
        weight = weight.masked_fill(causal_mask, mask_value)

    exp_weight = weight.exp()</code></pre><h3>After Change</h3><pre><code class='java'>
&#47&#47 memory efficient attention

def summarize_qkv_chunk(q, k, v, mask, attn_bias_chunk, causal, qk_start_indices):
    q_start_index, k_start_index, q_chunk_size, k_chunk_size, device = *qk_start_indices, q.shape[-2], <a id="change">k.shape[-2]</a>, q.device

    weight = einsum(&quotb h i d, b h j d -&gt; b h i j&quot, q, k)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/memory-efficient-attention-pytorch/commit/4be82443e060be7224be5e8247c097fcc84aa72d#diff-da6685a180d8c8c636b32d699e32ddc64180690f91b63dc75a91d3846cb1bb37L53' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 79245461</div><div id='project'> Project Name: lucidrains/memory-efficient-attention-pytorch</div><div id='commit'> Commit Name: 4be82443e060be7224be5e8247c097fcc84aa72d</div><div id='time'> Time: 2022-03-20</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memory_efficient_attention_pytorch/memory_efficient_cosine_sim_attention.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: summarize_qkv_chunk(7)</div><div id='n_method'> N Method Name: summarize_qkv_chunk(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: memory_efficient_attention_pytorch/memory_efficient_cosine_sim_attention.py</div><div id='n_file'> N File Name: memory_efficient_attention_pytorch/memory_efficient_cosine_sim_attention.py</div><div id='m_start'> M Start Line: 53</div><div id='m_end'> M End Line: 68</div><div id='n_start'> N Start Line: 53</div><div id='n_end'> N End Line: 73</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        images = None

        for (sigma, gamma), (sigma_next, gamma_next) in tqdm(sigma_schedule, desc = &quotsampling time step&quot):
            <a id="change">if </a>not <a id="change">exists(</a>images<a id="change">)</a>:
                &#47&#47 images start off as the noise based off the first sigma
                images = get_noise(sigma)
</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 images is noise at the beginning

        init_sigma = <a id="change">sigmas[0]</a>

        images = init_sigma * torch.randn(shape, device = self.device)

        &#47&#47 gradually denoise</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/denoising-diffusion-pytorch/commit/5db64fec4bc1c34dac5ecdde5d9e6ffdc7b32c97#diff-0e457471c6076af51b06926f05e9bb40034688d60e789577e04f6f49aa1971cdL147' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 79245463</div><div id='project'> Project Name: lucidrains/denoising-diffusion-pytorch</div><div id='commit'> Commit Name: 5db64fec4bc1c34dac5ecdde5d9e6ffdc7b32c97</div><div id='time'> Time: 2022-06-28</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: denoising_diffusion_pytorch/elucidated_diffusion.py</div><div id='m_class'> M Class Name: ElucidatedDiffusion</div><div id='n_method'> N Class Name: ElucidatedDiffusion</div><div id='m_method'> M Method Name: sample(2)</div><div id='n_method'> N Method Name: sample(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: denoising_diffusion_pytorch/elucidated_diffusion.py</div><div id='n_file'> N File Name: denoising_diffusion_pytorch/elucidated_diffusion.py</div><div id='m_start'> M Start Line: 152</div><div id='m_end'> M End Line: 181</div><div id='n_start'> N Start Line: 144</div><div id='n_end'> N End Line: 184</div><BR>