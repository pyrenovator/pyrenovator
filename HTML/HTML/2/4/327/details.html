<html><h3>Pattern ID :327
</h3><img src='2165281.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        x_lazy = x.detach().clone().to(device=device)
        bn = bn.to(device=device)
        for i in range(10):
            z_lazy = <a id="change">bn(</a>x_lazy<a id="change">)</a>
            torch._lazy.mark_step()

        torch.testing.assert_close(z.cpu(), z_lazy.cpu())
        assert metrics.counter_value("IrNodeReused_torch::lazy::TSNativeBatchNormForward") &gt;= 7</code></pre><h3>After Change</h3><pre><code class='java'>
        for i in range(10):
            &#47&#47 BatchNorm2d does extra checks on dimensions which SymInts don&quott support yet
            &#47&#47 so we call `torch.ops.aten.native_batch_norm` to bypass the checks.
            z<a id="change">, _, _</a> = torch.ops.aten.native_batch_norm(x, weight, bias, None, None, True, 0.1, 1e-5)

        device = "lazy"
        x_lazy = x.detach().clone().to(device=device)
        weight_lazy = weight.detach().clone().to(device=device)
        bias_lazy = bias.detach().clone().to(device=device)
        for i in range(10):
            z_lazy<a id="change">, _, _</a> = torch.ops.aten.native_batch_norm(x_lazy, weight_lazy, bias_lazy, None, None, True, 0.1, 1e-5)
            torch._lazy.mark_step()

        torch.testing.assert_close(z.cpu(), z_lazy.cpu())</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/d332724071704939e1c50704f6bc62bb6c990383#diff-e320ff3c48c9f6505f3b0d1819a98262797d390acf79d5a574efbe91b966476bL106' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2165281</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: d332724071704939e1c50704f6bc62bb6c990383</div><div id='time'> Time: 2022-06-13</div><div id='author'> Author: korovaikon@gmail.com</div><div id='file'> File Name: test/lazy/test_reuse_ir.py</div><div id='m_class'> M Class Name: TestLazyReuseIr</div><div id='n_method'> N Class Name: TestLazyReuseIr</div><div id='m_method'> M Method Name: testBatchNorm(1)</div><div id='n_method'> N Method Name: testBatchNorm(1)</div><div id='m_parent_class'> M Parent Class: TestCase</div><div id='n_parent_class'> N Parent Class: TestCase</div><div id='m_file'> M File Name: test/lazy/test_reuse_ir.py</div><div id='n_file'> N File Name: test/lazy/test_reuse_ir.py</div><div id='m_start'> M Start Line: 106</div><div id='m_end'> M End Line: 115</div><div id='n_start'> N Start Line: 106</div><div id='n_end'> N End Line: 120</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        x_lazy = x.detach().clone().to(device=device)
        bn = bn.to(device=device)
        for i in range(10):
            z_lazy = <a id="change">bn(</a>x_lazy<a id="change">)</a>
            torch._lazy.mark_step()

        torch.testing.assert_close(z.cpu(), z_lazy.cpu())
        assert metrics.counter_value("IrNodeReused_torch::lazy::TSNativeBatchNormForward") &gt;= 7</code></pre><h3>After Change</h3><pre><code class='java'>
        for i in range(10):
            &#47&#47 BatchNorm2d does extra checks on dimensions which SymInts don&quott support yet
            &#47&#47 so we call `torch.ops.aten.native_batch_norm` to bypass the checks.
            z<a id="change">, _, _</a> = torch.ops.aten.native_batch_norm(x, weight, bias, None, None, True, 0.1, 1e-5)

        device = "lazy"
        x_lazy = x.detach().clone().to(device=device)
        weight_lazy = weight.detach().clone().to(device=device)
        bias_lazy = bias.detach().clone().to(device=device)
        for i in range(10):
            z_lazy<a id="change">, _, _</a> = torch.ops.aten.native_batch_norm(x_lazy, weight_lazy, bias_lazy, None, None, True, 0.1, 1e-5)
            torch._lazy.mark_step()

        torch.testing.assert_close(z.cpu(), z_lazy.cpu())</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/f7ee061638aa2011191caeff4438fa8aff5bfec3#diff-e320ff3c48c9f6505f3b0d1819a98262797d390acf79d5a574efbe91b966476bL104' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2165280</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: f7ee061638aa2011191caeff4438fa8aff5bfec3</div><div id='time'> Time: 2022-06-20</div><div id='author'> Author: ezyang@fb.com</div><div id='file'> File Name: test/lazy/test_reuse_ir.py</div><div id='m_class'> M Class Name: TestLazyReuseIr</div><div id='n_method'> N Class Name: TestLazyReuseIr</div><div id='m_method'> M Method Name: testBatchNorm(1)</div><div id='n_method'> N Method Name: testBatchNorm(1)</div><div id='m_parent_class'> M Parent Class: TestCase</div><div id='n_parent_class'> N Parent Class: TestCase</div><div id='m_file'> M File Name: test/lazy/test_reuse_ir.py</div><div id='n_file'> N File Name: test/lazy/test_reuse_ir.py</div><div id='m_start'> M Start Line: 106</div><div id='m_end'> M End Line: 115</div><div id='n_start'> N Start Line: 106</div><div id='n_end'> N End Line: 120</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        x_lazy = x.detach().clone().to(device=device)
        bn = bn.to(device=device)
        for i in range(10):
            z_lazy = <a id="change">bn(</a>x_lazy<a id="change">)</a>
            torch._lazy.mark_step()

        torch.testing.assert_close(z.cpu(), z_lazy.cpu())
        assert metrics.counter_value("IrNodeReused_torch::lazy::TSNativeBatchNormForward") &gt;= 7</code></pre><h3>After Change</h3><pre><code class='java'>
        for i in range(10):
            &#47&#47 BatchNorm2d does extra checks on dimensions which SymInts don&quott support yet
            &#47&#47 so we call `torch.ops.aten.native_batch_norm` to bypass the checks.
            z<a id="change">, _, _</a> = torch.ops.aten.native_batch_norm(x, weight, bias, None, None, True, 0.1, 1e-5)

        device = "lazy"
        x_lazy = x.detach().clone().to(device=device)
        weight_lazy = weight.detach().clone().to(device=device)
        bias_lazy = bias.detach().clone().to(device=device)
        for i in range(10):
            z_lazy<a id="change">, _, _</a> = torch.ops.aten.native_batch_norm(x_lazy, weight_lazy, bias_lazy, None, None, True, 0.1, 1e-5)
            torch._lazy.mark_step()

        torch.testing.assert_close(z.cpu(), z_lazy.cpu())</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/d332724071704939e1c50704f6bc62bb6c990383#diff-e320ff3c48c9f6505f3b0d1819a98262797d390acf79d5a574efbe91b966476bL104' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2165283</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: d332724071704939e1c50704f6bc62bb6c990383</div><div id='time'> Time: 2022-06-13</div><div id='author'> Author: korovaikon@gmail.com</div><div id='file'> File Name: test/lazy/test_reuse_ir.py</div><div id='m_class'> M Class Name: TestLazyReuseIr</div><div id='n_method'> N Class Name: TestLazyReuseIr</div><div id='m_method'> M Method Name: testBatchNorm(1)</div><div id='n_method'> N Method Name: testBatchNorm(1)</div><div id='m_parent_class'> M Parent Class: TestCase</div><div id='n_parent_class'> N Parent Class: TestCase</div><div id='m_file'> M File Name: test/lazy/test_reuse_ir.py</div><div id='n_file'> N File Name: test/lazy/test_reuse_ir.py</div><div id='m_start'> M Start Line: 106</div><div id='m_end'> M End Line: 115</div><div id='n_start'> N Start Line: 106</div><div id='n_end'> N End Line: 120</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        x = torch.randn(16, 3, 224, 224, device=device)
        bn = torch.nn.BatchNorm2d(3).to(device=device)
        for i in range(10):
            z = <a id="change">bn(</a>x<a id="change">)</a>

        device = "lazy"
        x_lazy = x.detach().clone().to(device=device)
        bn = bn.to(device=device)</code></pre><h3>After Change</h3><pre><code class='java'>
        for i in range(10):
            &#47&#47 BatchNorm2d does extra checks on dimensions which SymInts don&quott support yet
            &#47&#47 so we call `torch.ops.aten.native_batch_norm` to bypass the checks.
            z<a id="change">, _, _</a> = torch.ops.aten.native_batch_norm(x, weight, bias, None, None, True, 0.1, 1e-5)

        device = "lazy"
        x_lazy = x.detach().clone().to(device=device)
        weight_lazy = weight.detach().clone().to(device=device)
        bias_lazy = bias.detach().clone().to(device=device)
        for i in range(10):
            z_lazy<a id="change">, _, _</a> = torch.ops.aten.native_batch_norm(x_lazy, weight_lazy, bias_lazy, None, None, True, 0.1, 1e-5)
            torch._lazy.mark_step()

        torch.testing.assert_close(z.cpu(), z_lazy.cpu())</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/8ef6356f267c75276ea23b51163274cd5fffc0ce#diff-e320ff3c48c9f6505f3b0d1819a98262797d390acf79d5a574efbe91b966476bL104' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 2165282</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 8ef6356f267c75276ea23b51163274cd5fffc0ce</div><div id='time'> Time: 2022-06-15</div><div id='author'> Author: korovaikon@gmail.com</div><div id='file'> File Name: test/lazy/test_reuse_ir.py</div><div id='m_class'> M Class Name: TestLazyReuseIr</div><div id='n_method'> N Class Name: TestLazyReuseIr</div><div id='m_method'> M Method Name: testBatchNorm(1)</div><div id='n_method'> N Method Name: testBatchNorm(1)</div><div id='m_parent_class'> M Parent Class: TestCase</div><div id='n_parent_class'> N Parent Class: TestCase</div><div id='m_file'> M File Name: test/lazy/test_reuse_ir.py</div><div id='n_file'> N File Name: test/lazy/test_reuse_ir.py</div><div id='m_start'> M Start Line: 106</div><div id='m_end'> M End Line: 115</div><div id='n_start'> N Start Line: 106</div><div id='n_end'> N End Line: 120</div><BR>