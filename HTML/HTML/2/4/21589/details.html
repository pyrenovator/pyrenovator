<html><h3>Pattern ID :21589
</h3><img src='68979588.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def load_resized_weights(self, source_layer, method="nearest"):
        if isinstance(source_layer, dict):
            source_tt = <a id="change">source_layer["positional_embedding:0"]</a>  &#47&#47 weights
            &#47&#47 source_tt = source_layer["pos_emb:0"]  &#47&#47 weights
        else:
            source_tt = source_layer.bb  &#47&#47 layer</code></pre><h3>After Change</h3><pre><code class='java'>
        tt = backend.numpy_image_resize(tt, [self.kv_hh, self.kv_ww], method=method)  &#47&#47 [num_heads, self.kv_hh, self.kv_ww, self.query_hh * self.query_ww]
        tt = np.reshape(tt, [num_heads, self.kv_hh * self.kv_ww, self.query_hh * self.query_ww])
        tt = np.transpose(tt, [0, 2, 1])  &#47&#47 [num_heads, self.query_hh * self.query_ww, self.kv_hh * self.kv_ww]
        <a id="change">self.set_weights([</a>tt<a id="change"></a>]<a id="change">)</a>


def light_mhsa_with_multi_head_relative_position_embedding(
    inputs, num_heads=4, key_dim=0, sr_ratio=1, qkv_bias=False, pos_emb=None, use_bn=False, out_shape=None, out_weight=True, out_bias=False, dropout=0, name=""</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/2ba27b0132168f3590dd4b3bead9edc15a70ba7d#diff-f52477ea03b2a8f18695211ce6217a2ea1e5dd5afaa7adfd75a231c5a04692bcL59' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 68979588</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 2ba27b0132168f3590dd4b3bead9edc15a70ba7d</div><div id='time'> Time: 2023-02-11</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/cmt/cmt.py</div><div id='m_class'> M Class Name: BiasPositionalEmbedding</div><div id='n_method'> N Class Name: BiasPositionalEmbedding</div><div id='m_method'> M Method Name: load_resized_weights(3)</div><div id='n_method'> N Method Name: load_resized_weights(3)</div><div id='m_parent_class'> M Parent Class: layers.Layer</div><div id='n_parent_class'> N Parent Class: keras.layers.Layer</div><div id='m_file'> M File Name: keras_cv_attention_models/cmt/cmt.py</div><div id='n_file'> N File Name: keras_cv_attention_models/cmt/cmt.py</div><div id='m_start'> M Start Line: 59</div><div id='m_end'> M End Line: 74</div><div id='n_start'> N Start Line: 61</div><div id='n_end'> N End Line: 79</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def load_resized_weights(self, source_layer, method="nearest"):
        &#47&#47 For input 224 --&gt; [128, 27], convert to 480 --&gt; [128, 30]
        if isinstance(source_layer, dict):
            source_pos_emb_h = <a id="change">source_layer["r_height:0"]</a>  &#47&#47 weights
            source_pos_emb_w = source_layer["r_width:0"]  &#47&#47 weights
        else:
            source_pos_emb_h = source_layer.pos_emb_h  &#47&#47 layer</code></pre><h3>After Change</h3><pre><code class='java'>
        resize_w = backend.numpy_image_resize(image_like_w, target_shape=(1, self.pos_emb_w.shape[1]), method=method)[0]
        resize_w = np.transpose(resize_w, [1, 0])

        <a id="change">self.set_weights([</a>resize_h, resize_w<a id="change"></a>]<a id="change">)</a>

    def show_pos_emb(self, base_size=4):
        import matplotlib.pyplot as plt
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/e05e233f369a1d58f912872b1581a80d15cacc3f#diff-4163380308eb60d4185b09d8359be777af3cee4f5ffc9acbf77fc54a76f12e5bL112' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 68979589</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: e05e233f369a1d58f912872b1581a80d15cacc3f</div><div id='time'> Time: 2023-02-07</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/botnet/botnet.py</div><div id='m_class'> M Class Name: RelativePositionalEmbedding</div><div id='n_method'> N Class Name: RelativePositionalEmbedding</div><div id='m_method'> M Method Name: load_resized_weights(3)</div><div id='n_method'> N Method Name: load_resized_weights(3)</div><div id='m_parent_class'> M Parent Class: layers.Layer</div><div id='n_parent_class'> N Parent Class: keras.layers.Layer</div><div id='m_file'> M File Name: keras_cv_attention_models/botnet/botnet.py</div><div id='n_file'> N File Name: keras_cv_attention_models/botnet/botnet.py</div><div id='m_start'> M Start Line: 115</div><div id='m_end'> M End Line: 126</div><div id='n_start'> N Start Line: 110</div><div id='n_end'> N End Line: 124</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def load_resized_weights(self, source_layer, method="nearest"):
        if isinstance(source_layer, dict):
            source_bb = <a id="change">source_layer["positional_embedding:0"]</a>  &#47&#47 weights
        else:
            source_bb = source_layer.bb  &#47&#47 layer
        hh = ww = int(tf.math.sqrt(float(source_bb.shape[0])))</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 target_hh = target_ww = int(float(self.bb.shape[0]) ** 0.5)
        tt = backend.numpy_image_resize(ss, target_shape=[self.k_blocks_h, self.k_blocks_w], method=method)  &#47&#47 [target_hh, target_ww, num_heads]
        tt = tt.reshape((self.bb.shape))  &#47&#47 [target_hh * target_ww, num_heads]
        <a id="change">self.set_weights([</a>tt<a id="change"></a>]<a id="change">)</a>

    def show_pos_emb(self, rows=1, base_size=2):
        import matplotlib.pyplot as plt
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/c870bf2e8d3e6b8b0e969d5468d550085414c0cd#diff-419ea771811e38fb2e106cf19724ad992d4d6f2fbae3c511c734548a78b6d63bL64' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 68979586</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: c870bf2e8d3e6b8b0e969d5468d550085414c0cd</div><div id='time'> Time: 2023-02-05</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/levit/levit.py</div><div id='m_class'> M Class Name: MultiHeadPositionalEmbedding</div><div id='n_method'> N Class Name: MultiHeadPositionalEmbedding</div><div id='m_method'> M Method Name: load_resized_weights(3)</div><div id='n_method'> N Method Name: load_resized_weights(3)</div><div id='m_parent_class'> M Parent Class: layers.Layer</div><div id='n_parent_class'> N Parent Class: keras.layers.Layer</div><div id='m_file'> M File Name: keras_cv_attention_models/levit/levit.py</div><div id='n_file'> N File Name: keras_cv_attention_models/levit/levit.py</div><div id='m_start'> M Start Line: 66</div><div id='m_end'> M End Line: 74</div><div id='n_start'> N Start Line: 73</div><div id='n_end'> N End Line: 82</div><BR>