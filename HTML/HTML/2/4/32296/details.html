<html><h3>Pattern ID :32296
</h3><img src='94449552.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def forward(ctx: Any, group: dist.ProcessGroup, input: Tensor):
        if get_world_size(group) &lt;= 1:
            return input
        output = <a id="change">torch.clone(input).contiguous()</a>
        dist.all_reduce(output, op=torch.distributed.ReduceOp.SUM)
        return output

    @staticmethod</code></pre><h3>After Change</h3><pre><code class='java'>
            return input
        ctx.input_shape = input.shape
        ctx.leading_dim = 0
        chunks = <a id="change">[x.contiguous() for x in torch.chunk(input, chunks=ctx.num_nodes, dim=ctx.leading_dim)]</a>
        assert len(chunks) == ctx.num_nodes
        output<a id="change"> = </a>torch.empty_like(chunks[0])
        dist.reduce_scatter(output=output, input_list=list(chunks))
        return output
    @staticmethod</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/microsoft/tutel/commit/6b434d936b8d892725a0ff0020d2a41f6aa43a3e#diff-97cb6b6b4a46f907579b92c457a8eaec4a13ed75c520b40ae8911d4a59440021L85' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 94449552</div><div id='project'> Project Name: microsoft/tutel</div><div id='commit'> Commit Name: 6b434d936b8d892725a0ff0020d2a41f6aa43a3e</div><div id='time'> Time: 2021-11-15</div><div id='author'> Author: weicu@microsoft.com</div><div id='file'> File Name: tutel/impls/communicate.py</div><div id='m_class'> M Class Name: PostAllreduceSum</div><div id='n_method'> N Class Name: PostAllreduceSum</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: torch.autograd.Function</div><div id='n_parent_class'> N Parent Class: torch.autograd.Function</div><div id='m_file'> M File Name: tutel/impls/communicate.py</div><div id='n_file'> N File Name: tutel/impls/communicate.py</div><div id='m_start'> M Start Line: 85</div><div id='m_end'> M End Line: 89</div><div id='n_start'> N Start Line: 93</div><div id='n_end'> N End Line: 103</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def forward(ctx: Any, *input) -&gt; Tensor:
        if not AllToAllStatus.initialized or AllToAllStatus.world_size &lt;= 1:
            return input[0]
        output = <a id="change">torch.empty(AllToAllStatus.gather_tensor_shape, device=input[0].device).contiguous()</a>
        tutel_custom_kernel.nccl_all_to_all_gather_async(input, output, False)
        return output

    @staticmethod</code></pre><h3>After Change</h3><pre><code class='java'>
        if not AllToAllStatus.initialized:
            return input[0]
        ctx.input_shape = input[0].shape
        output_shape<a id="change"> = </a>torch.Size(<a id="change">[
            x if i != AllToAllStatus.split_dim else x * AllToAllStatus.num_split
            for i, x in enumerate(ctx.input_shape)
        ]</a>)
        ctx.num_slices_per_split = ctx.input_shape[:AllToAllStatus.split_dim].numel()
        return tutel_custom_kernel.nccl_all_to_all_gather_async(input, output_shape, ctx.num_slices_per_split, False)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/tutel/commit/e51df1ca64be59eae3691bc1c64b20a201de1009#diff-97cb6b6b4a46f907579b92c457a8eaec4a13ed75c520b40ae8911d4a59440021L137' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 94449554</div><div id='project'> Project Name: microsoft/tutel</div><div id='commit'> Commit Name: e51df1ca64be59eae3691bc1c64b20a201de1009</div><div id='time'> Time: 2022-01-31</div><div id='author'> Author: ziyyang@microsoft.com</div><div id='file'> File Name: tutel/impls/communicate.py</div><div id='m_class'> M Class Name: AllToAllGatherAsync</div><div id='n_method'> N Class Name: AllToAllGatherAsync</div><div id='m_method'> M Method Name: forward(1)</div><div id='n_method'> N Method Name: forward(1)</div><div id='m_parent_class'> M Parent Class: torch.autograd.Function</div><div id='n_parent_class'> N Parent Class: torch.autograd.Function</div><div id='m_file'> M File Name: tutel/impls/communicate.py</div><div id='n_file'> N File Name: tutel/impls/communicate.py</div><div id='m_start'> M Start Line: 138</div><div id='m_end'> M End Line: 142</div><div id='n_start'> N Start Line: 129</div><div id='n_end'> N End Line: 137</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        if dim != [ind for ind in range(len(dim))]:
            &#47&#47 put the desired dimension(s) at the front
            zeros = <a id="change">zeros.permute(*dim, *[ind for ind in range(len(zeros.shape)) if ind not in dim]).contiguous()</a>

    return zeros.float() / float(total)

</code></pre><h3>After Change</h3><pre><code class='java'>
    permute_order = sorted(
        ((d, len(dim) - i - 1) for i, d in enumerate(dim)), reverse=True
    )
    permute<a id="change"> = </a><a id="change">[d[1] for d in permute_order]</a>

    if permute != [i for i in range(len(permute))]:
        &#47&#47 need to permute to get desired dimensions at the front
        zeros = zeros.permute(*permute).contiguous()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/neuralmagic/sparseml/commit/36de5fae4791e2e00a61b2459d51438920ce504d#diff-36c01261adf2d45e67e47fb91d781955f5d63d0760d79dbeff936f6161a8ee75L201' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 94449549</div><div id='project'> Project Name: neuralmagic/sparseml</div><div id='commit'> Commit Name: 36de5fae4791e2e00a61b2459d51438920ce504d</div><div id='time'> Time: 2020-02-16</div><div id='author'> Author: mark@neuralmagic.com</div><div id='file'> File Name: neuralmagicML/utils/helpers.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: tensor_sparsity(2)</div><div id='n_method'> N Method Name: tensor_sparsity(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: neuralmagicML/utils/helpers.py</div><div id='n_file'> N File Name: neuralmagicML/utils/helpers.py</div><div id='m_start'> M Start Line: 202</div><div id='m_end'> M End Line: 220</div><div id='n_start'> N Start Line: 447</div><div id='n_end'> N End Line: 478</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def backward(ctx: Any, grad_output: Tensor):
        if get_world_size(ctx.group) &lt;= 1:
            return (None, grad_output)
        dinput = <a id="change">torch.clone(grad_output).contiguous()</a>
        dist.all_reduce(dinput, op=torch.distributed.ReduceOp.SUM)
        return (None, dinput)

class PostAllreduceSum(torch.autograd.Function):</code></pre><h3>After Change</h3><pre><code class='java'>
        if get_world_size(ctx.group) &lt;= 1:
            return (None, doutput)
        dinput = torch.empty(ctx.input_shape, device=doutput.device, dtype=doutput.dtype)
        chunks<a id="change"> = </a><a id="change">[x.contiguous() for x in torch.chunk(doutput.view(ctx.num_nodes, -1), chunks=ctx.num_nodes, dim=0)]</a>
        dist.reduce_scatter(output=dinput, input_list=chunks)
        return (None, dinput)

class PostAllreduceSum(torch.autograd.Function):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/tutel/commit/6b434d936b8d892725a0ff0020d2a41f6aa43a3e#diff-97cb6b6b4a46f907579b92c457a8eaec4a13ed75c520b40ae8911d4a59440021L76' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 94449550</div><div id='project'> Project Name: microsoft/tutel</div><div id='commit'> Commit Name: 6b434d936b8d892725a0ff0020d2a41f6aa43a3e</div><div id='time'> Time: 2021-11-15</div><div id='author'> Author: weicu@microsoft.com</div><div id='file'> File Name: tutel/impls/communicate.py</div><div id='m_class'> M Class Name: PreAllreduceSum</div><div id='n_method'> N Class Name: PreAllreduceSum</div><div id='m_method'> M Method Name: backward(2)</div><div id='n_method'> N Method Name: backward(2)</div><div id='m_parent_class'> M Parent Class: torch.autograd.Function</div><div id='n_parent_class'> N Parent Class: torch.autograd.Function</div><div id='m_file'> M File Name: tutel/impls/communicate.py</div><div id='n_file'> N File Name: tutel/impls/communicate.py</div><div id='m_start'> M Start Line: 76</div><div id='m_end'> M End Line: 80</div><div id='n_start'> N Start Line: 83</div><div id='n_end'> N End Line: 88</div><BR>