<html><h3>Pattern ID :13608
</h3><img src='45707721.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        x1 = x1.log_softmax(-1)
        x2 = x2.log_softmax(-1)

    x1_max = <a id="change">x1.max(-1, keepdim=True)[0]</a>
    x2_max = x2.max(-1, keepdim=True)[0]

    x1_stable = x1 - x1_max
    x2_stable = x2 - x2_max</code></pre><h3>After Change</h3><pre><code class='java'>
        torch.Tensor: The pairwise joint product kernel matrix, of shape (batch_size_1, batch_size_2).

    
    eps<a id="change"> = </a><a id="change">torch.finfo(</a>x1.dtype<a id="change">)</a>.eps

    if apply_softmax:
        x1 = x1.log_softmax(-1)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/jgraving/selfsne/commit/8a7af0bd2e24010295ac3852ef01ac042082082d#diff-f067ea572d2a724c22fdaf81c9955e3337f17151f31afd4a0aa865b838c491d1L443' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 45707721</div><div id='project'> Project Name: jgraving/selfsne</div><div id='commit'> Commit Name: 8a7af0bd2e24010295ac3852ef01ac042082082d</div><div id='time'> Time: 2023-05-07</div><div id='author'> Author: jgraving@gmail.com</div><div id='file'> File Name: selfsne/kernels.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: pairwise_joint_product(4)</div><div id='n_method'> N Method Name: pairwise_joint_product(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: selfsne/kernels.py</div><div id='n_file'> N File Name: selfsne/kernels.py</div><div id='m_start'> M Start Line: 445</div><div id='m_end'> M End Line: 455</div><div id='n_start'> N Start Line: 443</div><div id='n_end'> N End Line: 450</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.to_attn_logits = nn.Parameter(torch.eye(dim))

    def forward(self, x):
        remainder = <a id="change">x.shape[-1]</a> % self.pool_size
        if remainder &gt; 0:
            x = F.pad(x, (0, remainder), value = 0)
</code></pre><h3>After Change</h3><pre><code class='java'>
        logits = self.pool_fn(attn_logits)

        if needs_padding:
            mask_value = -<a id="change">torch.finfo(</a>logits.dtype<a id="change">)</a>.max
            logits<a id="change"> = </a>logits.masked_fill(self.pool_fn(mask), mask_value)

        attn = logits.softmax(dim = -1)
        return (x * attn).sum(dim = -1)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/enformer-pytorch/commit/d6c7c92831f85618e38d208d15a411e1259fa4b3#diff-d41824114176cf139e53af298fee855401c92b5fbb8c782f4eb1dd3fbf56fe10L121' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 45707722</div><div id='project'> Project Name: lucidrains/enformer-pytorch</div><div id='commit'> Commit Name: d6c7c92831f85618e38d208d15a411e1259fa4b3</div><div id='time'> Time: 2021-10-30</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: enformer_pytorch/enformer_pytorch.py</div><div id='m_class'> M Class Name: AttentionPool</div><div id='n_method'> N Class Name: AttentionPool</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: enformer_pytorch/enformer_pytorch.py</div><div id='n_file'> N File Name: enformer_pytorch/enformer_pytorch.py</div><div id='m_start'> M Start Line: 122</div><div id='m_end'> M End Line: 128</div><div id='n_start'> N Start Line: 122</div><div id='n_end'> N End Line: 139</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 help auto-solve a frequent area of confusion around input masks in auto-regressive
        &#47&#47 if user supplies a mask that is only off by one from the source sequence, resolve it for them
        mask = kwargs.get(&quotmask&quot, None)
        if mask is not None and <a id="change">mask.shape[1]</a> == x.shape[1]:
            mask = mask[:, :-1]
            kwargs[&quotmask&quot] = mask
</code></pre><h3>After Change</h3><pre><code class='java'>

        if self.mask_prob &gt; 0.:
            rand = torch.randn(inp.shape, device = x.device)
            rand[:, 0] = -<a id="change">torch.finfo(</a>rand.dtype<a id="change">)</a>.max &#47&#47 first token should not be masked out
            num_mask = min(int(seq * self.mask_prob), seq - 1)
            indices = rand.topk(num_mask, dim = -1).indices
            mask<a id="change"> = </a>~torch.zeros_like(inp).scatter(1, indices, 1.).bool()
            kwargs.update(context_mask = mask)

        out = self.net(inp, **kwargs)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/x-transformers/commit/595a4745d532c20b8ebd310256c342e946a4cef7#diff-b70a3173d353a448f8f499d8b685672d9bccac7b78bc1d36b77f84afe33be694L106' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 45707727</div><div id='project'> Project Name: lucidrains/x-transformers</div><div id='commit'> Commit Name: 595a4745d532c20b8ebd310256c342e946a4cef7</div><div id='time'> Time: 2022-11-02</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: x_transformers/autoregressive_wrapper.py</div><div id='m_class'> M Class Name: AutoregressiveWrapper</div><div id='n_method'> N Class Name: AutoregressiveWrapper</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: x_transformers/autoregressive_wrapper.py</div><div id='n_file'> N File Name: x_transformers/autoregressive_wrapper.py</div><div id='m_start'> M Start Line: 107</div><div id='m_end'> M End Line: 118</div><div id='n_start'> N Start Line: 122</div><div id='n_end'> N End Line: 142</div><BR>