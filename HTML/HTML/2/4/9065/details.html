<html><h3>Pattern ID :9065
</h3><img src='32955734.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            a_dist.log_prob(a_dist.sample()).sum(-1, keepdim=True).clamp(-100.0, 100.0)
        )
    alpha_loss = -(log_alpha * (logp_a + target_entropy).detach()).mean()
    <a id="change">optimizer.zero_grad()</a>
    alpha_loss.backward()
    optimizer.step()
    logs["losses/alpha_loss"] = alpha_loss.item()
    logs["alpha"] = log_alpha.exp().item()</code></pre><h3>After Change</h3><pre><code class='java'>
                .sum(-1, keepdim=True)
                .clamp(-100.0, 100.0)
            )
        alpha_loss = -(<a id="change">log_alphas[i]</a> * (logp_a + target_entropy).detach()).mean()
        optimizers[i].zero_grad()
        alpha_loss.backward()
        optimizers[i].step()
        logs[f"losses/alpha_loss_{i}"] = alpha_loss.item()
        logs[f"alphas/alpha_{i}"] = <a id="change">log_alphas[i]</a>.exp().item()
    return logs

</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/jakegrigsby/super_sac/commit/819313070dbb7c72886cafb948c401c78eb03861#diff-ed592a42afa845d29a0da80d3c28fc31230301bf13d0d27e271c158cb7728875L185' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 32955734</div><div id='project'> Project Name: jakegrigsby/super_sac</div><div id='commit'> Commit Name: 819313070dbb7c72886cafb948c401c78eb03861</div><div id='time'> Time: 2021-10-12</div><div id='author'> Author: jcg6dn@virginia.edu</div><div id='file'> File Name: uafbc/learning.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: alpha_update(9)</div><div id='n_method'> N Method Name: alpha_update(9)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: uafbc/learning.py</div><div id='n_file'> N File Name: uafbc/learning.py</div><div id='m_start'> M Start Line: 185</div><div id='m_end'> M End Line: 198</div><div id='n_start'> N Start Line: 191</div><div id='n_end'> N End Line: 211</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            for i in range(self.current_batch * self.batch_size, batches, self.batch_size):
                y_pred = self.model(x[i : i + self.batch_size])
                loss = self.criterion(y_pred, y[i : i + self.batch_size])
                <a id="change">self.optimizer.zero_grad()</a>
                loss.backward()
                self.optimizer.step()
                self.n_updates += 1
        </code></pre><h3>After Change</h3><pre><code class='java'>
        if self.local_epochs &gt; 0:
            for _ in range(self.local_epochs):
                for i in range(0, x.size(0), batch_size):
                    self._local_step(x[i : i + batch_size], <a id="change">y[i : i + batch_size]</a>)
        else:
            perm = torch.randperm(x.size(0))
            self._local_step(x[perm][:batch_size], <a id="change">y[perm]</a>[:batch_size])
    
    def _local_step(self, x:torch.Tensor, y:torch.Tensor) -&gt; None:
        self.model.train()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/makgyver/gossipy/commit/7601b023e9ecd0604a37cf18afd67e14a2044939#diff-36e45624fe64c0d02688d4de471bd229b4bba0d9212ce90bec22a3949a0f2f10L134' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 32955735</div><div id='project'> Project Name: makgyver/gossipy</div><div id='commit'> Commit Name: 7601b023e9ecd0604a37cf18afd67e14a2044939</div><div id='time'> Time: 2022-04-29</div><div id='author'> Author: mak1788@gmail.com</div><div id='file'> File Name: gossipy/model/handler.py</div><div id='m_class'> M Class Name: TorchModelHandler</div><div id='n_method'> N Class Name: TorchModelHandler</div><div id='m_method'> M Method Name: _update(2)</div><div id='n_method'> N Method Name: _update(2)</div><div id='m_parent_class'> M Parent Class: ModelHandler</div><div id='n_parent_class'> N Parent Class: ModelHandler</div><div id='m_file'> M File Name: gossipy/model/handler.py</div><div id='n_file'> N File Name: gossipy/model/handler.py</div><div id='m_start'> M Start Line: 136</div><div id='m_end'> M End Line: 153</div><div id='n_start'> N Start Line: 136</div><div id='n_end'> N End Line: 145</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            a_dist.log_prob(a_dist.sample()).sum(-1, keepdim=True).clamp(-100.0, 100.0)
        )
    alpha_loss = -(log_alpha * (logp_a + target_entropy).detach()).mean()
    <a id="change">optimizer.zero_grad()</a>
    alpha_loss.backward()
    optimizer.step()
    logs["losses/alpha_loss"] = alpha_loss.item()
    logs["alpha"] = log_alpha.exp().item()</code></pre><h3>After Change</h3><pre><code class='java'>
                .sum(-1, keepdim=True)
                .clamp(-100.0, 100.0)
            )
        alpha_loss = -(<a id="change">log_alphas[i]</a> * (logp_a + target_entropy).detach()).mean()
        optimizers[i].zero_grad()
        alpha_loss.backward()
        optimizers[i].step()
        logs[f"losses/alpha_loss_{i}"] = alpha_loss.item()
        logs[f"alphas/alpha_{i}"] = <a id="change">log_alphas[i]</a>.exp().item()
    return logs

</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/jakegrigsby/super_sac/commit/819313070dbb7c72886cafb948c401c78eb03861#diff-ed592a42afa845d29a0da80d3c28fc31230301bf13d0d27e271c158cb7728875L165' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 32955719</div><div id='project'> Project Name: jakegrigsby/super_sac</div><div id='commit'> Commit Name: 819313070dbb7c72886cafb948c401c78eb03861</div><div id='time'> Time: 2021-10-12</div><div id='author'> Author: jcg6dn@virginia.edu</div><div id='file'> File Name: uafbc/learning.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: alpha_update(9)</div><div id='n_method'> N Method Name: alpha_update(9)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: uafbc/learning.py</div><div id='n_file'> N File Name: uafbc/learning.py</div><div id='m_start'> M Start Line: 185</div><div id='m_end'> M End Line: 198</div><div id='n_start'> N Start Line: 191</div><div id='n_end'> N End Line: 211</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 prepare lstm to receive gradient from all losses (Q1_loss, Q2_loss, policy_loss)
        &#47&#47 retain_graph needs to be used because lstm is shared among the three

        <a id="change">self.lstm_optimizer.zero_grad()</a>

        &#47&#47 reduce td error

        self.Q1_optimizer.zero_grad()</code></pre><h3>After Change</h3><pre><code class='java'>

        self.critic_lstm.flatten_parameters()
        critic_h, _ = self.actor_lstm(b.o)
        critic_h_1_T, critic_h_2_Tplus1 = <a id="change">critic_h[:, :-1, :]</a>, <a id="change">critic_h[:, 1:, :]</a>  &#47&#47 T represents num_bptt

        &#47&#47 prepare lstm to receive gradient from all losses (Q1_loss, Q2_loss, policy_loss)
        &#47&#47 retain_graph needs to be used because lstm is shared among the three</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/zhihanyang2022/off-policy-continuous-control/commit/0d405a315e44a0b8df2bfcb89ea02b6979215166#diff-a5c291ec9aab37f6351911dd4dd49334c82a0507245742c0291338a09f74c16aL120' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 32955715</div><div id='project'> Project Name: zhihanyang2022/off-policy-continuous-control</div><div id='commit'> Commit Name: 0d405a315e44a0b8df2bfcb89ea02b6979215166</div><div id='time'> Time: 2021-05-23</div><div id='author'> Author: yangz2@carleton.edu</div><div id='file'> File Name: offpcc/algorithms/sac_lstm.py</div><div id='m_class'> M Class Name: SAC_LSTM</div><div id='n_method'> N Class Name: SAC_LSTM</div><div id='m_method'> M Method Name: update_networks(2)</div><div id='n_method'> N Method Name: update_networks(2)</div><div id='m_parent_class'> M Parent Class: OffPolicyRLAlgorithm</div><div id='n_parent_class'> N Parent Class: OffPolicyRLAlgorithm</div><div id='m_file'> M File Name: offpcc/algorithms/sac_lstm.py</div><div id='n_file'> N File Name: offpcc/algorithms/sac_lstm.py</div><div id='m_start'> M Start Line: 128</div><div id='m_end'> M End Line: 221</div><div id='n_start'> N Start Line: 130</div><div id='n_end'> N End Line: 229</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
  &#47&#47 Update the policy by maximising the clipped PPO objective
  policy_ratio = (trajectories[&quotlog_prob_actions&quot] - trajectories[&quotold_log_prob_actions&quot]).exp()
  policy_loss = -torch.min(policy_ratio * trajectories[&quotadvantages&quot], torch.clamp(policy_ratio, min=1 - ppo_clip, max=1 + ppo_clip) * trajectories[&quotadvantages&quot]).mean()
  <a id="change">actor_optimiser.zero_grad()</a>
  policy_loss.backward()
  actor_optimiser.step()
</code></pre><h3>After Change</h3><pre><code class='java'>
  &#47&#47 Recalculate outputs for subsequent iterations
  if epoch &gt; 0:
    policy, trajectories[&quotvalues&quot] = agent(trajectories[&quotstates&quot])
    trajectories[&quotlog_prob_actions&quot], <a id="change">trajectories[&quotentropies&quot]</a> = policy.log_prob(trajectories[&quotactions&quot].detach()), policy.entropy()

  &#47&#47 Update the policy by maximising the clipped PPO objective
  policy_ratio = (trajectories[&quotlog_prob_actions&quot] - trajectories[&quotold_log_prob_actions&quot]).exp()
  policy_loss = -torch.min(policy_ratio * trajectories[&quotadvantages&quot], torch.clamp(policy_ratio, min=1 - ppo_clip, max=1 + ppo_clip) * trajectories[&quotadvantages&quot]).mean()
  &#47&#47 Fit value function by regression on mean squared error
  value_loss = F.mse_loss(trajectories[&quotvalues&quot], trajectories[&quotrewards_to_go&quot])
  &#47&#47 Add entropy regularisation
  entropy_loss = -<a id="change">trajectories[&quotentropies&quot]</a>.mean()  
  
  agent_optimiser.zero_grad()
  (policy_loss + value_loss_coeff * value_loss + entropy_loss_coeff * entropy_loss).backward()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/kaixhin/imitation-learning/commit/fd3ee1838359dcc6da9836b6249396e595ff90db#diff-2b28d1dcda2cd70d29d3251359adce0fbfde60edb53108cba1a284ee0d39e256L33' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 32955713</div><div id='project'> Project Name: kaixhin/imitation-learning</div><div id='commit'> Commit Name: fd3ee1838359dcc6da9836b6249396e595ff90db</div><div id='time'> Time: 2020-04-16</div><div id='author'> Author: design@kaixhin.com</div><div id='file'> File Name: training.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: ppo_update(7)</div><div id='n_method'> N Method Name: ppo_update(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: training.py</div><div id='n_file'> N File Name: training.py</div><div id='m_start'> M Start Line: 33</div><div id='m_end'> M End Line: 50</div><div id='n_start'> N Start Line: 34</div><div id='n_end'> N End Line: 51</div><BR>