<html><h3>Pattern ID :20385
</h3><img src='66037342.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def train(self, train_dataloader, eval_dataloader):
        optimizer = optim.Adam(filter(lambda p: p.requires_grad,
                                      <a id="change">self.model.parameters()</a>),
                               lr=self.config[&quotlearning_rate&quot],
                               weight_decay=self.config[&quotL2&quot])
        scheduler<a id="change"> = </a>optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, &quotmax&quot, patience=self.config[&quotlr_step&quot],
            factor=self.config[&quotlr_decay&quot],
            threshold=self.config[&quotschedule_threshold&quot])</code></pre><h3>After Change</h3><pre><code class='java'>
            metrics[&quotloss&quot].append(avg_eval_loss)
            if self.config[&quothyper_tune&quot]:
                &#47&#47 use ray tune to checkpoint
                <a id="change">with tune</a><a id="change">.checkpoint_dir(step=epoch) as checkpoint_dir:
                    </a>path = os.path.join(checkpoint_dir, "checkpoint")
                    self.save_model(path)
                &#47&#47 ray tune use loss to determine which params are best
                tune.report(loss=avg_eval_loss, accuracy=avg_eval_acc)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/libcity/bigscity-libcity/commit/022010c6ffc7ff440e276127d6e6341751e03d69#diff-3b0408c84ea1982a6fe3d842dd9eff33a8bba27ca54ad0f448b9f8ae7b2c38e4L24' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 66037342</div><div id='project'> Project Name: libcity/bigscity-libcity</div><div id='commit'> Commit Name: 022010c6ffc7ff440e276127d6e6341751e03d69</div><div id='time'> Time: 2021-04-14</div><div id='author'> Author: 33283819+WenMellors@users.noreply.github.com</div><div id='file'> File Name: trafficdl/executor/traj_loc_pred_executor.py</div><div id='m_class'> M Class Name: TrajLocPredExecutor</div><div id='n_method'> N Class Name: TrajLocPredExecutor</div><div id='m_method'> M Method Name: train(3)</div><div id='n_method'> N Method Name: train(3)</div><div id='m_parent_class'> M Parent Class: AbstractExecutor</div><div id='n_parent_class'> N Parent Class: AbstractExecutor</div><div id='m_file'> M File Name: trafficdl/executor/traj_loc_pred_executor.py</div><div id='n_file'> N File Name: trafficdl/executor/traj_loc_pred_executor.py</div><div id='m_start'> M Start Line: 24</div><div id='m_end'> M End Line: 74</div><div id='n_start'> N Start Line: 33</div><div id='n_end'> N End Line: 71</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        optimizer = self.optimizer_class(actor_parameters, lr=self.lr)
        &#47&#47 make sure critic isn&quott updated!
        if critic2 is not None:
            critic_parameters<a id="change"> = </a>critic1.parameters() + <a id="change">critic2.parameters()</a>
        else:
            critic_parameters = critic1.parameters()
        for var in critic_parameters:
            var.requires_grad = False</code></pre><h3>After Change</h3><pre><code class='java'>
        log_probs = distributions.log_prob(actions)
        entropy = distributions.entropy().mean()

        <a id="change">with T</a><a id="change">.no_grad():
            </a>values1 = critic1(observations, actions)
            if critic2 is not None:
                values2 = critic2(observations, actions)
                values = T.min(values1, values2)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/londonnode/pearl/commit/630c4814f3e238ec120cfabf2023a4bf9156a22d#diff-b68c703d61b55535b734b62646c41dc40f50b287ada3402dbf6b5a6cd37fddceL266' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 66037341</div><div id='project'> Project Name: londonnode/pearl</div><div id='commit'> Commit Name: 630c4814f3e238ec120cfabf2023a4bf9156a22d</div><div id='time'> Time: 2021-09-24</div><div id='author'> Author: rohan.tangri@gmail.com</div><div id='file'> File Name: anvil/updaters/actors.py</div><div id='m_class'> M Class Name: SoftPolicyGradient</div><div id='n_method'> N Class Name: SoftPolicyGradient</div><div id='m_method'> M Method Name: __call__(5)</div><div id='n_method'> N Method Name: __call__(5)</div><div id='m_parent_class'> M Parent Class: BaseActorUpdater</div><div id='n_parent_class'> N Parent Class: BaseActorUpdater</div><div id='m_file'> M File Name: anvil/updaters/actors.py</div><div id='n_file'> N File Name: anvil/updaters/actors.py</div><div id='m_start'> M Start Line: 284</div><div id='m_end'> M End Line: 315</div><div id='n_start'> N Start Line: 287</div><div id='n_end'> N End Line: 295</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

                &#47&#47 This is to make DDP happy.
                &#47&#47 DDP expects all workers to have gradient w.r.t the same set of parameters.
                _dummy<a id="change"> = </a>sum(x.view(-1)[0] for x in <a id="change">self.parameters()</a>) * 0.0
                return empty + _dummy
            else:
                return empty</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 2. features needed by exporting module to torchscript are added in PyTorch 1.6 or
        &#47&#47 later version, `Conv2d` in these PyTorch versions has already supported empty inputs.
        if not torch.jit.is_scripting():
            <a id="change">with warnings</a><a id="change">.catch_warnings(record=True):
                </a>if x.numel() == 0 and self.training:
                    &#47&#47 https://github.com/pytorch/pytorch/issues/12013
                    assert not isinstance(
                        self.norm, torch.nn.SyncBatchNorm</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/stevewongv/instanceshadowdetection/commit/52f5f2cdcdb670b07efe4086abf503d9d50753c5#diff-013791abb9a2879485a664b3062b408cce57c163b4ec14151e126fdcad50bfb1L60' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 66037351</div><div id='project'> Project Name: stevewongv/instanceshadowdetection</div><div id='commit'> Commit Name: 52f5f2cdcdb670b07efe4086abf503d9d50753c5</div><div id='time'> Time: 2022-07-28</div><div id='author'> Author: steve.w.git@icloud.com</div><div id='file'> File Name: detectron2/layers/wrappers.py</div><div id='m_class'> M Class Name: Conv2d</div><div id='n_method'> N Class Name: Conv2d</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: torch.nn.Conv2d</div><div id='n_parent_class'> N Parent Class: torch.nn.Conv2d</div><div id='m_file'> M File Name: detectron2/layers/wrappers.py</div><div id='n_file'> N File Name: detectron2/layers/wrappers.py</div><div id='m_start'> M Start Line: 61</div><div id='m_end'> M End Line: 87</div><div id='n_start'> N Start Line: 105</div><div id='n_end'> N End Line: 115</div><BR>