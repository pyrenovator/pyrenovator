<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            request_objects = []
            for tensor, (tensor_name,
                         receive_ranks) in zip(x, ranks_dict_items):
                <a id="change">assert </a>len(receive_ranks) == 1
                receive_rank = receive_ranks[0]
                tensor_tag = self.tensor_tags[tensor_name] + (self.TOTAL_TAGS * batch_idx)
                if self.verbose:</code></pre><h3>After Change</h3><pre><code class='java'>
    def _recv_tensors_p2p(self, x, batch_idx, ranks_dict_items):
        &#47&#47 FIXME: it is possible that we recived multiple gradients for the same tensor.

        ix = <a id="change">iter(</a>x<a id="change">)</a>

        with torch.no_grad():
            request_objects = []

            for (tensor_name, receive_ranks) in ranks_dict_items:

                if len(receive_ranks) &gt; 1:
                    print(f"rank={self.rank}: recieving {tensor_name} from multiple ranks: {receive_ranks}")
                    &#47&#47 TODO: need to acummulate the result somwhere.

                for receive_rank in receive_ranks:
                    tensor<a id="change"> = next(</a>ix<a id="change">)</a>

                    tensor_tag = self.tensor_tags[tensor_name] + (self.TOTAL_TAGS * batch_idx)
                    if self.verbose:
                        self.logger.info(</code></pre>