<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    if dist.get_rank() == 0:
        print(&quot&gt; initialize tensor model parallel with size {}&quot.format(tensor_model_parallel_size_))
        print(<a id="change">&quot&gt; initialize data parallel with size {}&quot.format(</a>data_parallel_size_<a id="change">)</a>)


def dap_is_initialized():</code></pre><h3>After Change</h3><pre><code class='java'>
        else:
            tensor_model_parallel_size_ = 1

    <a id="change">if </a>torch.torch.distributed.is_initialized():
        _logger = colossalai.logging.get_dist_logger()
        _logger.error(
            "use fastfold.distributed.init_dap instead of torch.distributed.init_process_group!")
        <a id="change">exit(</a>-1<a id="change">)</a>

    &#47&#47 set distributed environ for single device launch
    if &quotRANK&quot not in os.environ:
        set_distributed_environ(&quotWORLD_SIZE&quot, 1)</code></pre>