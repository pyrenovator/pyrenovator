<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            [0.51605093, 0.5707241, 0.47365507, 0.50578886, 0.5633877, 0.4642503, 0.5182081, 0.48763484, 0.49084237]
        )
        assert np.abs(image_slice.flatten() - expected_slice).max() &lt; 1e-2
        <a id="change">assert </a>np.abs(image_from_tuple_slice.flatten() - expected_slice).max() &lt; 1e-2

    @unittest.skipIf(torch_device != "cuda", "This test requires a GPU")
    def test_alt_diffusion_fp16(self):</code></pre><h3>After Change</h3><pre><code class='java'>
    def test_alt_diffusion_pndm(self):
        device = "cpu"  &#47&#47 ensure determinism for the device-dependent torch.Generator

        components = <a id="change">self.get_dummy_components()</a>
        components["scheduler"] = PNDMScheduler(skip_prk_steps=True)
        torch.manual_seed(0)
        text_encoder_config = RobertaSeriesConfig(
            hidden_size=32,
            project_dim=32,
            intermediate_size=37,
            layer_norm_eps=1e-05,
            num_attention_heads=4,
            num_hidden_layers=5,
            vocab_size=5002,
        )
        &#47&#47 TODO: remove after fixing the non-deterministic text encoder
        text_encoder = RobertaSeriesModelWithTransformation(text_encoder_config)
        components["text_encoder"] = text_encoder
        alt_pipe = AltDiffusionPipeline(**components)
        alt_pipe = alt_pipe.to(device)
        alt_pipe.set_progress_bar_config(disable=None)

        inputs = <a id="change">self.get_dummy_inputs(</a>device<a id="change">)</a>
        output = alt_pipe(**inputs)
        image = output.images
        image_slice = image[0, -3:, -3:, -1]
</code></pre>