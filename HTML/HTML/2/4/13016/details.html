<html><h3>Pattern ID :13016
</h3><img src='44009295.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    random.seed(args.seed)
    if args.evaluate:
        args.results_dir = &quot/tmp&quot
    <a id="change">if </a>args.save is &quot&quot:
        args.save<a id="change"> = </a>datetime.now().strftime(&quot%Y-%m-%d_%H-%M-%S&quot)
    save_path = os.path.join(args.results_dir, args.save)
    if not os.path.exists(save_path):
        os.makedirs(save_path)</code></pre><h3>After Change</h3><pre><code class='java'>
    if args.lr_type == &quotcos&quot:
        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs, eta_min = 0, last_epoch=args.start_epoch)
    elif args.lr_type == &quotstep&quot:
        lr_scheduler<a id="change"> = </a><a id="change">torch.optim.lr_scheduler.MultiStepLR(</a>optimizer, args.lr_decay_step<a id="change">, gamma=0.1, last_epoch=-1)</a>
    logging.info(&quotscheduler: %s&quot, lr_scheduler)

    def cosin(i,T,emin=0,emax=0.01):
        "customized cos-lr"</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lmbxmu/rbnn/commit/be209779b9f8deeba9218c8d0deeb2a2623ff42b#diff-51e54fa2edf92734ba1c452ae8ba36f0526eba62a804b6908faeb0378308a387L28' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 44009295</div><div id='project'> Project Name: lmbxmu/rbnn</div><div id='commit'> Commit Name: be209779b9f8deeba9218c8d0deeb2a2623ff42b</div><div id='time'> Time: 2020-05-04</div><div id='author'> Author: 791411501@qq.com</div><div id='file'> File Name: imagenet/main.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(0)</div><div id='n_method'> N Method Name: main(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: imagenet/main.py</div><div id='n_file'> N File Name: imagenet/main.py</div><div id='m_start'> M Start Line: 28</div><div id='m_end'> M End Line: 175</div><div id='n_start'> N Start Line: 30</div><div id='n_end'> N End Line: 185</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    random.seed(args.seed)
    if args.evaluate:
        args.results_dir = &quot/tmp&quot
    <a id="change">if </a>args.save is &quot&quot:
        args.save<a id="change"> = </a>datetime.now().strftime(&quot%Y-%m-%d_%H-%M-%S&quot)
    save_path = os.path.join(args.results_dir, args.save)
    if not os.path.exists(save_path):
        os.makedirs(save_path)</code></pre><h3>After Change</h3><pre><code class='java'>
    if args.lr_type == &quotcos&quot:
        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs, eta_min = 0, last_epoch=args.start_epoch)
    elif args.lr_type == &quotstep&quot:
        lr_scheduler<a id="change"> = </a><a id="change">torch.optim.lr_scheduler.MultiStepLR(</a>optimizer, args.lr_decay_step<a id="change">, gamma=0.1, last_epoch=-1)</a>
    logging.info(&quotscheduler: %s&quot, lr_scheduler)

    def cosin(i,T,emin=0,emax=0.01):
        "customized cos-lr"</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lmbxmu/rbnn/commit/be209779b9f8deeba9218c8d0deeb2a2623ff42b#diff-a6126765e897edb0c8b93ff749585ddf583ca28434cce764a6e8cf6ed11847c6L21' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 44009294</div><div id='project'> Project Name: lmbxmu/rbnn</div><div id='commit'> Commit Name: be209779b9f8deeba9218c8d0deeb2a2623ff42b</div><div id='time'> Time: 2020-05-04</div><div id='author'> Author: 791411501@qq.com</div><div id='file'> File Name: cifar/main.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(0)</div><div id='n_method'> N Method Name: main(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: cifar/main.py</div><div id='n_file'> N File Name: cifar/main.py</div><div id='m_start'> M Start Line: 28</div><div id='m_end'> M End Line: 172</div><div id='n_start'> N Start Line: 30</div><div id='n_end'> N End Line: 178</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    test_sampler = torch.utils.data.distributed.DistributedSampler(test_dataset)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=2, pin_memory=True, sampler=test_sampler)

    <a id="change">if </a>args.evaluate:
        validate(test_loader, model, criterion, local_rank, args)
        return

    for epoch in range(args.start_epoch, args.epochs):
        train_sampler.set_epoch(epoch)
        test_sampler.set_epoch(epoch)

        adjust_learning_rate(optimizer, epoch, args)

        &#47&#47 train for one epoch
        train(train_loader, model, criterion, optimizer, epoch, local_rank, args)

        &#47&#47 evaluate on validation set
        acc1 = validate(val_loader, model, criterion, local_rank, args)

        &#47&#47 remember best acc@1 and save checkpoint
        is_best<a id="change"> = </a>acc1 &gt; best_acc1
        best_acc1 = max(acc1, best_acc1)

        if args.local_rank == 0:</code></pre><h3>After Change</h3><pre><code class='java'>
    criterion = nn.CrossEntropyLoss().cuda(local_rank)
    &#47&#47 =================================
    optimizer = torch.optim.SGD(model.parameters(), 0.1, momentum=0.9, weight_decay=1e-4)
    train_scheduler<a id="change"> = </a><a id="change">optim.lr_scheduler.MultiStepLR(</a>optimizer<a id="change">, milestones=[60, 120, 160], gamma=0.2)</a>

    &#47&#47 3. 加载数据，
    batch_size = int(args.batch_size / nprocs) &#47&#47 需要手动划分 batch_size 为 mini-batch_size
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/rentainhe/pytorch-distributed-training/commit/924a65892510a4cf1352ee408d6186344043f3a4#diff-f080c66d54726c9df3b2a7a496310425104eddad5c79eb4dc09c30d7d71f0a50L56' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 44009291</div><div id='project'> Project Name: rentainhe/pytorch-distributed-training</div><div id='commit'> Commit Name: 924a65892510a4cf1352ee408d6186344043f3a4</div><div id='time'> Time: 2021-01-05</div><div id='author'> Author: 596106517@qq.com</div><div id='file'> File Name: distributed.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main_worker(3)</div><div id='n_method'> N Method Name: main_worker(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: distributed.py</div><div id='n_file'> N File Name: distributed.py</div><div id='m_start'> M Start Line: 60</div><div id='m_end'> M End Line: 111</div><div id='n_start'> N Start Line: 41</div><div id='n_end'> N End Line: 110</div><BR>