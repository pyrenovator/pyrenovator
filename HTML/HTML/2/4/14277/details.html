<html><h3>Pattern ID :14277
</h3><img src='47314913.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        os.environ[&quotDMLC_NUM_SERVER&quot] = str(num_servers)
        os.environ[&quotDMLC_NUM_WORKER&quot] = str(num_workers)

    <a id="change">global executor_shell</a>
    if len(hosts) == 1:
        &#47&#47 single machine
        &#47&#47 TODO: add hostdress validation check
        if enable_PS:</code></pre><h3>After Change</h3><pre><code class='java'>
    parser.add_argument(&quotcommand&quot, nargs=argparse.REMAINDER,
                        help=&quotCommand to be executed.&quot)
    args = parser.parse_args()
    <a id="change">if </a>args.config is None:
        assert args.workers &gt; 0, \
            &quotPlease specify the configuration file or set the number of local workers.&quot
        settings<a id="change"> = </a><a id="change">{</a>&quotnodes&quot: [{
            &quothost&quot: &quotlocalhost&quot,
            &quotservers&quot: args.servers,
            &quotworkers&quot: args.workers,
            &quotchief&quot: True,
        }]<a id="change">}</a>
    else:
        settings = yaml.load(open(args.config).read(), Loader=yaml.FullLoader)
    attributes = set([&quothost&quot, &quotservers&quot, &quotworkers&quot, &quotchief&quot])
    hosts = []</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/hsword/hetu/commit/43f4467347abb96737657a61b2ea49506e240038#diff-e8c461298233e63f2f2901bec55801e35e938dc08318210ba4ca984b2d0495adL150' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 47314913</div><div id='project'> Project Name: hsword/hetu</div><div id='commit'> Commit Name: 43f4467347abb96737657a61b2ea49506e240038</div><div id='time'> Time: 2021-09-02</div><div id='author'> Author: 657671989@qq.com</div><div id='file'> File Name: python/runner.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(0)</div><div id='n_method'> N Method Name: main(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: python/runner.py</div><div id='n_file'> N File Name: python/runner.py</div><div id='m_start'> M Start Line: 158</div><div id='m_end'> M End Line: 192</div><div id='n_start'> N Start Line: 150</div><div id='n_end'> N End Line: 206</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                mp_size = 1
                mp_group = None

        <a id="change">global cuda_device, transport_stream, PARTITION_ACTIVATIONS, buffer_0, buffer_1, buffer_0_offset, buffer_1_offset</a>

        if cuda_device is None:
            see_memory_usage("First Forward Begining", force=True)</code></pre><h3>After Change</h3><pre><code class='java'>
            ]
        ctx.mark_non_differentiable(*non_grad_outputs)

        <a id="change">if </a>torch.is_tensor(outputs):
            all_outputs<a id="change"> += </a><a id="change">[</a>outputs<a id="change"></a>]
            return outputs
        else:
            all_outputs += outputs</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/deepspeed/commit/ec8b1cb0a0a5752bba029da4bdc91616c0f5bec7#diff-a4333224075c38d4a6f6aa97123c10f93e519b39cce1f02ecaa55d881493bb2dL325' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 47314915</div><div id='project'> Project Name: microsoft/deepspeed</div><div id='commit'> Commit Name: ec8b1cb0a0a5752bba029da4bdc91616c0f5bec7</div><div id='time'> Time: 2021-02-12</div><div id='author'> Author: olruwase@microsoft.com</div><div id='file'> File Name: deepspeed/runtime/activation_checkpointing/checkpointing.py</div><div id='m_class'> M Class Name: CheckpointFunction</div><div id='n_method'> N Class Name: CheckpointFunction</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: torch.autograd.Function</div><div id='n_parent_class'> N Parent Class: torch.autograd.Function</div><div id='m_file'> M File Name: deepspeed/runtime/activation_checkpointing/checkpointing.py</div><div id='n_file'> N File Name: deepspeed/runtime/activation_checkpointing/checkpointing.py</div><div id='m_start'> M Start Line: 338</div><div id='m_end'> M End Line: 503</div><div id='n_start'> N Start Line: 373</div><div id='n_end'> N End Line: 564</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 因此需要解码的长度实际是 lengths - 1
        decode_lengths = (caption_lengths - 1).tolist()
        &#47&#47 新建两个张量用于存放 word predicion scores and alphas
        <a id="change">global device</a>
        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)
        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)

        &#47&#47 在每一个时间步根据解码器的前一个状态以及经过attention加权后的encoder输出进行解码</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47     torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),
            &#47&#47     (h[:batch_size_t], c[:batch_size_t]))  &#47&#47 (batch_size_t, decoder_dim)
            &#47&#47teahcer forcing
            <a id="change">if </a>t==1 or (np.random.rand() &lt; self.p) :
                h = self.decode_step(
                    torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),
                    h[:batch_size_t])  &#47&#47 (batch_size_t, decoder_dim)
            else:
                h<a id="change"> = </a>self.decode_step(
                    torch.cat(<a id="change">[</a>self.embedding(torch.argmax(predictions[:batch_size_t, t, :],dim = 1)), attention_weighted_encoding<a id="change"></a>], dim=1),
                    h[:batch_size_t])  &#47&#47 (batch_size_t, decoder_dim)
            preds = self.fc(self.dropout(h))  &#47&#47 (batch_size_t, vocab_size)
            predictions[:batch_size_t, t, :] = preds</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/qs956/latex_ocr_pytorch/commit/0455746d6d3141dfc06cd15fb9cd67a0b9defcfc#diff-62e0d959e50d9f4003df8124a1c19d98c77320268642d433b1cc28c58cf73a85L205' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 47314914</div><div id='project'> Project Name: qs956/latex_ocr_pytorch</div><div id='commit'> Commit Name: 0455746d6d3141dfc06cd15fb9cd67a0b9defcfc</div><div id='time'> Time: 2020-03-21</div><div id='author'> Author: qs956@163.com</div><div id='file'> File Name: model/model.py</div><div id='m_class'> M Class Name: DecoderWithAttention</div><div id='n_method'> N Class Name: DecoderWithAttention</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: model/model.py</div><div id='n_file'> N File Name: model/model.py</div><div id='m_start'> M Start Line: 231</div><div id='m_end'> M End Line: 250</div><div id='n_start'> N Start Line: 214</div><div id='n_end'> N End Line: 271</div><BR>