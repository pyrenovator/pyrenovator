<html><h3>Pattern ID :13368
</h3><img src='45132736.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            optimizer.zero_grad()
            scaler.scale(train_loss).backward()
            if step_counter - 1 == net.switch_on_prenet_step:
                <a id="change">optimizer.add_param_group(</a>{&quotparams&quot: net.dec.prenet.parameters()}<a id="change">)</a>
            step_counter += 1
            torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)
            scaler.step(optimizer)
            scaler.update()</code></pre><h3>After Change</h3><pre><code class='java'>
            optimizer.zero_grad()
            scaler.scale(train_loss).backward()
            if step_counter - 1 == net.switch_on_prenet_step:
                <a id="change">if net.dec.prenet is not None</a>:
                    <a id="change">optimizer.add_param_group(</a>{&quotparams&quot: net.dec.prenet.parameters()}<a id="change">)</a>
            step_counter += 1
            torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)
            scaler.step(optimizer)
            scaler.update()</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/55fece9e6c65368b029e3dc0228737289039daaf#diff-50506526d43ed453ff387fee15511f145ae91f8e336dbcc118883a5d89a1232cL107' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 45132736</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 55fece9e6c65368b029e3dc0228737289039daaf</div><div id='time'> Time: 2021-08-25</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: TrainingInterfaces/Text_to_Spectrogram/Tacotron2/tacotron2_train_loop.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train_loop(12)</div><div id='n_method'> N Method Name: train_loop(12)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: TrainingInterfaces/Text_to_Spectrogram/Tacotron2/tacotron2_train_loop.py</div><div id='n_file'> N File Name: TrainingInterfaces/Text_to_Spectrogram/Tacotron2/tacotron2_train_loop.py</div><div id='m_start'> M Start Line: 161</div><div id='m_end'> M End Line: 161</div><div id='n_start'> N Start Line: 107</div><div id='n_end'> N End Line: 163</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            args.append(DistillationArgs(layer_name, new_layer, layer_name))

            optimizer_arg = checkpoint[&quotconfig&quot][&quotoptimizer&quot][&quotargs&quot]
            <a id="change">self.optimizer.add_param_group(</a>{&quotparams&quot: new_layer.parameters(),
                                            **optimizer_arg}<a id="change">)</a>
        &#47&#47 load state dict
        self.model.update_pruned_layers(args)
        forgiving_state_restore(self.model, checkpoint[&quotstate_dict&quot])
        &#47&#47self.logger.debug(self.model)</code></pre><h3>After Change</h3><pre><code class='java'>
            layer_name = checkpoint[&quottraining_ta_layers&quot][i]
            args.append(DistillationArgs(layer_name, new_layer, layer_name))

            <a id="change">if i == 0</a> and list(filter(lambda x: x.requires_grad, self.model.parameters())) == 0:
                self.logger.debug(&quotCreating new optimizer...&quot)
                self.optimizer = checkpoint[&quotconfig&quot].init_obj(&quotoptimizer&quot, optim_module, new_layer.parameters)
            else:
                optimizer_arg = checkpoint[&quotconfig&quot][&quotoptimizer&quot][&quotargs&quot]
                <a id="change">self.optimizer.add_param_group(</a>{&quotparams&quot: new_layer.parameters(),
                                                **optimizer_arg}<a id="change">)</a>
        &#47&#47 load state dict
        self.model.update_pruned_layers(args)
        forgiving_state_restore(self.model, checkpoint[&quotstate_dict&quot])
        &#47&#47self.logger.debug(self.model)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lehduong/knowledge-distillation-by-replacing-cheap-conv/commit/8aff10acbbb2ad57877dab14f5309cd6628f2c4f#diff-60bd2a7eb048049f580030840b0b7273b0f45a45f557d882b4d695bfeb140c57L82' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 45132732</div><div id='project'> Project Name: lehduong/knowledge-distillation-by-replacing-cheap-conv</div><div id='commit'> Commit Name: 8aff10acbbb2ad57877dab14f5309cd6628f2c4f</div><div id='time'> Time: 2020-02-20</div><div id='author'> Author: oopsxilitol@gmail.com</div><div id='file'> File Name: trainer/takdp_trainer.py</div><div id='m_class'> M Class Name: TAKDPTrainer</div><div id='n_method'> N Class Name: TAKDPTrainer</div><div id='m_method'> M Method Name: _resume_checkpoint(2)</div><div id='n_method'> N Method Name: _resume_checkpoint(2)</div><div id='m_parent_class'> M Parent Class: KDPTrainer</div><div id='n_parent_class'> N Parent Class: KDPTrainer</div><div id='m_file'> M File Name: trainer/takdp_trainer.py</div><div id='n_file'> N File Name: trainer/takdp_trainer.py</div><div id='m_start'> M Start Line: 90</div><div id='m_end'> M End Line: 158</div><div id='n_start'> N Start Line: 90</div><div id='n_end'> N End Line: 150</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                optimizer_arg[&quotlr&quot] = to_be_pruned_layers[i][&quotlr&quot]

            &#47&#47 add new parameters to optimizer
            <a id="change">self.optimizer.add_param_group(</a>{&quotparams&quot: new_layer.parameters(),
                                            **optimizer_arg}<a id="change">)</a>
        &#47&#47 add new blocks to student model
        self.model.update_pruned_layers(args)
        self.logger.info(&quotNumber of trainable parameters after pruning: &quot + str(self.model.dump_trainable_params()))
        self.logger.info(self.model.dump_student_teacher_blocks_info())</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 add new parameters to optimizer
            &#47&#47 if start pruning this epoch and model doesn&quott have any trainable paramters i.e. just have been \
            &#47&#47 promoted to TA then create new optimizer
            <a id="change">if </a>i == 0 and <a id="change">list(filter(lambda x: x.requires_grad, self.model.parameters())) == 0</a>:
                self.logger.debug(&quotCreating new optimizer...&quot)
                self.optimizer = self.config.init_obj(&quotoptimizer&quot, optim_module, new_layer.parameters)
                for param_group in self.optimizer.param_groups:
                    param_group[&quotlr&quot] = optimizer_arg[&quotlr&quot]
            else:
                <a id="change">self.optimizer.add_param_group(</a>{&quotparams&quot: new_layer.parameters(),
                                                **optimizer_arg}<a id="change">)</a>
        &#47&#47 add new blocks to student model
        self.model.update_pruned_layers(args)
        self.logger.info(&quotNumber of trainable parameters after pruning: &quot + str(self.model.dump_trainable_params()))
        self.logger.info(self.model.dump_student_teacher_blocks_info())</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lehduong/knowledge-distillation-by-replacing-cheap-conv/commit/1fc6b3f6f443dc7529911ec7c41579d6826a3e6d#diff-ca0f7afed3ce453389c5c52c54188a3b47b6b76376bdec7b023fead156916909L22' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 45132734</div><div id='project'> Project Name: lehduong/knowledge-distillation-by-replacing-cheap-conv</div><div id='commit'> Commit Name: 1fc6b3f6f443dc7529911ec7c41579d6826a3e6d</div><div id='time'> Time: 2020-02-20</div><div id='author'> Author: oopsxilitol@gmail.com</div><div id='file'> File Name: trainer/kdp_trainer.py</div><div id='m_class'> M Class Name: KDPTrainer</div><div id='n_method'> N Class Name: KDPTrainer</div><div id='m_method'> M Method Name: prune(2)</div><div id='n_method'> N Method Name: prune(2)</div><div id='m_parent_class'> M Parent Class: KnowledgeDistillationTrainer</div><div id='n_parent_class'> N Parent Class: KnowledgeDistillationTrainer</div><div id='m_file'> M File Name: trainer/kdp_trainer.py</div><div id='n_file'> N File Name: trainer/kdp_trainer.py</div><div id='m_start'> M Start Line: 58</div><div id='m_end'> M End Line: 59</div><div id='n_start'> N Start Line: 49</div><div id='n_end'> N End Line: 70</div><BR>