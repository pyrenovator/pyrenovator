<html><h3>Pattern ID :10727
</h3><img src='36958147.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self._iter = iter([{key: data[key].numpy() for key in data} for data in calib_dataloader])

    def get_next(self):
        <a id="change">return </a>next(self._iter, None)


class ORTQuantizer:</code></pre><h3>After Change</h3><pre><code class='java'>
            if self.batch_size == 1:
                featurized_samples = {key: [value] for key, value in next(self._dataset_iter).items()}
            else:
                featurized_samples<a id="change"> = </a><a id="change">defaultdict(</a>list<a id="change">)</a>
                for _ in range(self.batch_size):
                    sample = next(self._dataset_iter)

                    for name, value in sample.items():
                        featurized_samples[name]<a id="change"> += </a>[value]

        except StopIteration:
            pass</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/huggingface/optimum/commit/8a993de9245ee7cb76f4730d9e0a50f9e76eae3a#diff-3688cadccda29282df1b90910924f7a353b8e9cf4d2e50369484c95ee8e17172L56' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 36958147</div><div id='project'> Project Name: huggingface/optimum</div><div id='commit'> Commit Name: 8a993de9245ee7cb76f4730d9e0a50f9e76eae3a</div><div id='time'> Time: 2022-03-24</div><div id='author'> Author: mfuntowicz@users.noreply.github.com</div><div id='file'> File Name: optimum/onnxruntime/quantization.py</div><div id='m_class'> M Class Name: ORTCalibrationDataReader</div><div id='n_method'> N Class Name: ORTCalibrationDataReader</div><div id='m_method'> M Method Name: get_next(1)</div><div id='n_method'> N Method Name: get_next(1)</div><div id='m_parent_class'> M Parent Class: CalibrationDataReader</div><div id='n_parent_class'> N Parent Class: CalibrationDataReader</div><div id='m_file'> M File Name: optimum/onnxruntime/quantization.py</div><div id='n_file'> N File Name: optimum/onnxruntime/quantization.py</div><div id='m_start'> M Start Line: 65</div><div id='m_end'> M End Line: 65</div><div id='n_start'> N Start Line: 56</div><div id='n_end'> N End Line: 76</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        return torch.tensor([ff.getAngle(*at) for at in uqangleatomtypes])

    def make_dihedrals(self, ff, uqdihedralatomtypes):
        <a id="change">return </a>torch.tensor([ff.getDihedral(*at) for at in uqdihedralatomtypes])

    def _unique_impropers(self, impropers, bonds):
        graph = improperGraph(impropers, bonds)</code></pre><h3>After Change</h3><pre><code class='java'>
    def make_dihedrals(self, ff, uqdihedralatomtypes):
        from collections import defaultdict

        dihedrals<a id="change"> = </a><a id="change">defaultdict(</a>lambda: {"idx": [], "params": []}<a id="change">)</a>

        for i, at in enumerate(uqdihedralatomtypes):
            terms = ff.getDihedral(*at)
            for j, term in enumerate(terms):
                dihedrals[j]["idx"].append(i)
                dihedrals[j]["params"].append(term)

        maxterms<a id="change"> = </a>max(dihedrals.keys()) + 1
        newdihedrals = []
        for j in range(maxterms):
            dihedrals[j]["idx"] = torch.tensor(dihedrals[j]["idx"])</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/torchmd/torchmd/commit/3b9546b4305fd06e35b22e29940fe3d7f67bce86#diff-8e94c429360469c7fe6e59ee00e0d1de14b8e54cf5c61ab9a5817158e0f64651L143' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 36958149</div><div id='project'> Project Name: torchmd/torchmd</div><div id='commit'> Commit Name: 3b9546b4305fd06e35b22e29940fe3d7f67bce86</div><div id='time'> Time: 2020-05-21</div><div id='author'> Author: stefdoerr@gmail.com</div><div id='file'> File Name: torchmd/parameters.py</div><div id='m_class'> M Class Name: Parameters</div><div id='n_method'> N Class Name: Parameters</div><div id='m_method'> M Method Name: make_dihedrals(3)</div><div id='n_method'> N Method Name: make_dihedrals(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: torchmd/parameters.py</div><div id='n_file'> N File Name: torchmd/parameters.py</div><div id='m_start'> M Start Line: 144</div><div id='m_end'> M End Line: 144</div><div id='n_start'> N Start Line: 173</div><div id='n_end'> N End Line: 188</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        init_ft_cnt = len(self.strategy_adapter.fts_optim_view(sched[0]["params"]))
        total_ft_cnt = len([p for phase in sched for p in self.strategy_adapter.fts_optim_view(sched[phase]["params"])])
        expected_grad_params = req_grad_opt == init_ft_cnt
        <a id="change">return </a>no_grad_cnt, init_ft_cnt, total_ft_cnt, req_grad_opt, expected_grad_params

    def _validate_opt_init(self) -&gt; None:
        Validate the user-initialized optimizer state (necessary for fine-tuning phase 0) and warn user if</code></pre><h3>After Change</h3><pre><code class='java'>
            if n in self.strategy_adapter.fts_optim_view(sched[0]["params"])
        }
        expected_params_sym_diff = optim_grad_param_set ^ sched_grad_param_set
        p_diff_summary<a id="change"> = </a><a id="change">defaultdict(</a>list<a id="change">)</a>
        if expected_params_sym_diff:
            for n, p in self.pl_module.named_parameters():
                if p in optim_grad_param_set:
                    p_diff_summary["optim_params"].append(n)
                if p in sched_grad_param_set:
                    p_diff_summary["phase_0_params"].append(n)
        p_diff_summary<a id="change"> = </a>{k: self.strategy_adapter.logical_param_translation(v) for k, v in p_diff_summary.items()}
        return no_grad_cnt, init_ft_cnt, total_ft_cnt, p_diff_summary

    @staticmethod</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/speediedan/finetuning-scheduler/commit/956653716799e9bb88b5015f399bda463a408b8b#diff-2d4f76c6f0c8223fab0ac9c5108a651abdb2c3dd1e3bf3f7db29dbe500990ab3L1391' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 36958139</div><div id='project'> Project Name: speediedan/finetuning-scheduler</div><div id='commit'> Commit Name: 956653716799e9bb88b5015f399bda463a408b8b</div><div id='time'> Time: 2023-02-08</div><div id='author'> Author: danny.dale@gmail.com</div><div id='file'> File Name: src/finetuning_scheduler/fts_supporters.py</div><div id='m_class'> M Class Name: ScheduleImplMixin</div><div id='n_method'> N Class Name: ScheduleImplMixin</div><div id='m_method'> M Method Name: _inspect_fts_opt_state(1)</div><div id='n_method'> N Method Name: _inspect_fts_opt_state(1)</div><div id='m_parent_class'> M Parent Class: ABC</div><div id='n_parent_class'> N Parent Class: ABC</div><div id='m_file'> M File Name: src/finetuning_scheduler/fts_supporters.py</div><div id='n_file'> N File Name: src/finetuning_scheduler/fts_supporters.py</div><div id='m_start'> M Start Line: 1400</div><div id='m_end'> M End Line: 1405</div><div id='n_start'> N Start Line: 1407</div><div id='n_end'> N End Line: 1427</div><BR>