<html><h3>Pattern ID :1999
</h3><img src='8884917.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if n &lt; seq_len:
            padding = seq_len - n
            x = F.pad(x, (0, 0, 0, padding), value = 0)
            <a id="change">if </a><a id="change">exists(</a>mask<a id="change">)</a>:
                mask = F.pad(x, (0, padding), value = False)

        qkv = self.to_qkv(x).chunk(3, dim = -1)</code></pre><h3>After Change</h3><pre><code class='java'>

        if n &lt; seq_len:
            padding = seq_len - n
            mask<a id="change"> = </a><a id="change">default(</a>mask, lambda: torch.ones(b, n, device = device).bool()<a id="change">)</a>
            x = F.pad(x, (0, 0, 0, padding), value = 0)
            mask = F.pad(x, (0, padding), value = False)

        qkv = self.to_qkv(x).chunk(3, dim = -1)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/dalle-pytorch/commit/5e2eff659073d32e3c77dafb6459a312d8f92c81#diff-58807b9968341c05944b3adf3de1bfcd9d33660121a78f3ef013fd0e9bf16fdfL100' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8884917</div><div id='project'> Project Name: lucidrains/dalle-pytorch</div><div id='commit'> Commit Name: 5e2eff659073d32e3c77dafb6459a312d8f92c81</div><div id='time'> Time: 2021-02-10</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: dalle_pytorch/attention.py</div><div id='m_class'> M Class Name: SparseConvCausalAttention</div><div id='n_method'> N Class Name: SparseConvCausalAttention</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dalle_pytorch/attention.py</div><div id='n_file'> N File Name: dalle_pytorch/attention.py</div><div id='m_start'> M Start Line: 101</div><div id='m_end'> M End Line: 104</div><div id='n_start'> N Start Line: 100</div><div id='n_end'> N End Line: 102</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        assert not ImagenTrainer.locked, &quotImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)&quot
        assert exists(imagen) ^ exists(imagen_checkpoint_path), &quoteither imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config&quot

        <a id="change">if </a><a id="change">exists(</a>imagen_checkpoint_path<a id="change">)</a>:
            checkpoint_path = Path(imagen_checkpoint_path)
            assert checkpoint_path.exists()
            loaded = torch.load(str(imagen_checkpoint_path))</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 determine filesystem, using fsspec, for saving to local filesystem or cloud

        fs<a id="change"> = </a><a id="change">default(</a>checkpoint_fs, lambda: LocalFileSystem()<a id="change">)</a>
        self.fs = fs

        if self.fs.exists(imagen_checkpoint_path):
            with self.fs.open(imagen_checkpoint_path) as f:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/imagen-pytorch/commit/56fd7e14501d1bf96e02c9e92d2bc9a496615ad1#diff-db403ff86c6fb0812b3ce92a98fb7d76cc888c28dfa3e0a5421bde05febd47bbL191' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8884916</div><div id='project'> Project Name: lucidrains/imagen-pytorch</div><div id='commit'> Commit Name: 56fd7e14501d1bf96e02c9e92d2bc9a496615ad1</div><div id='time'> Time: 2022-07-26</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: imagen_pytorch/trainer.py</div><div id='m_class'> M Class Name: ImagenTrainer</div><div id='n_method'> N Class Name: ImagenTrainer</div><div id='m_method'> M Method Name: __init__(24)</div><div id='n_method'> N Method Name: __init__(23)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: imagen_pytorch/trainer.py</div><div id='n_file'> N File Name: imagen_pytorch/trainer.py</div><div id='m_start'> M Start Line: 221</div><div id='m_end'> M End Line: 341</div><div id='n_start'> N Start Line: 218</div><div id='n_end'> N End Line: 353</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            padding = seq_len - n
            x = F.pad(x, (0, 0, 0, padding), value = 0)

            <a id="change">if </a><a id="change">exists(</a>mask<a id="change">)</a>:
                mask = F.pad(x, (0, padding), value = False)

        qkv = self.to_qkv(x).chunk(3, dim = -1)</code></pre><h3>After Change</h3><pre><code class='java'>

        if n &lt; seq_len:
            padding = seq_len - n
            mask<a id="change"> = </a><a id="change">default(</a>mask, lambda: torch.ones(b, n, device = device).bool()<a id="change">)</a>
            x = F.pad(x, (0, 0, 0, padding), value = 0)
            mask = F.pad(x, (0, padding), value = False)

        qkv = self.to_qkv(x).chunk(3, dim = -1)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/dalle-pytorch/commit/5e2eff659073d32e3c77dafb6459a312d8f92c81#diff-58807b9968341c05944b3adf3de1bfcd9d33660121a78f3ef013fd0e9bf16fdfL194' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8884913</div><div id='project'> Project Name: lucidrains/dalle-pytorch</div><div id='commit'> Commit Name: 5e2eff659073d32e3c77dafb6459a312d8f92c81</div><div id='time'> Time: 2021-02-10</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: dalle_pytorch/attention.py</div><div id='m_class'> M Class Name: SparseAxialCausalAttention</div><div id='n_method'> N Class Name: SparseAxialCausalAttention</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dalle_pytorch/attention.py</div><div id='n_file'> N File Name: dalle_pytorch/attention.py</div><div id='m_start'> M Start Line: 201</div><div id='m_end'> M End Line: 204</div><div id='n_start'> N Start Line: 199</div><div id='n_end'> N End Line: 201</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if padding &gt; 0:
            quad_q, quad_k, lin_q, lin_k, v = map(lambda t: F.pad(t, (0, 0, 0, padding), value = 0.), (quad_q, quad_k, lin_q, lin_k, v))

            <a id="change">if </a><a id="change">exists(</a>mask<a id="change">)</a>:
                mask = F.pad(mask, (0, padding), value = False)

        &#47&#47 group along sequence</code></pre><h3>After Change</h3><pre><code class='java'>
        if padding &gt; 0:
            quad_q, quad_k, lin_q, lin_k, v = map(lambda t: F.pad(t, (0, 0, 0, padding), value = 0.), (quad_q, quad_k, lin_q, lin_k, v))

            mask<a id="change"> = </a><a id="change">default(</a>mask, torch.ones((b, n), device = device, dtype = torch.bool)<a id="change">)</a>
            mask = F.pad(mask, (0, padding), value = False)

        &#47&#47 group along sequence
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/flash-pytorch/commit/2a0bf49fc80567fbec4d8c88fbf94cd6cc9dce73#diff-6408ce30d3c8730249ca81344fdd04a1dd01ed7b072e84e9d752276e93661682L237' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8884912</div><div id='project'> Project Name: lucidrains/flash-pytorch</div><div id='commit'> Commit Name: 2a0bf49fc80567fbec4d8c88fbf94cd6cc9dce73</div><div id='time'> Time: 2022-03-28</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: flash_pytorch/flash_pytorch.py</div><div id='m_class'> M Class Name: FLASH</div><div id='n_method'> N Class Name: FLASH</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: flash_pytorch/flash_pytorch.py</div><div id='n_file'> N File Name: flash_pytorch/flash_pytorch.py</div><div id='m_start'> M Start Line: 286</div><div id='m_end'> M End Line: 291</div><div id='n_start'> N Start Line: 254</div><div id='n_end'> N End Line: 287</div><BR>