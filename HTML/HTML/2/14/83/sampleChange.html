<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    key = tf.transpose(tf.reshape(key, [-1, key.shape[1], num_heads, key_dim]), [0, 2, 1, 3])  &#47&#47 [batch, num_heads, hh * ww, key_dim]
    value = tf.transpose(tf.reshape(value, [-1, value.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  &#47&#47  [batch, num_heads, key_dim, hh * ww]

    attention_scores = <a id="change">keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))(</a><a id="change">[</a>query, key<a id="change"></a>]<a id="change">)</a> * qk_scale  &#47&#47 [batch, num_heads, key_dim, key_dim]
    attention_scores = keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(attention_scores)
    attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores) if attn_dropout &gt; 0 else attention_scores

    &#47&#47 value = [batch, num_heads, key_dim, hh * ww], attention_output = [batch, num_heads, key_dim, hh * ww]
    attention_output = <a id="change">keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))(</a><a id="change">[</a>attention_scores, value<a id="change"></a>]<a id="change">)</a>
    attention_output = tf.transpose(attention_output, perm=[0, 3, 1, 2])  &#47&#47 [batch, hh * ww, num_heads, key_dim]
    attention_output = tf.reshape(attention_output, [-1, inputs.shape[1], inputs.shape[2], num_heads * key_dim])
    &#47&#47 print(f"&gt;&gt;&gt;&gt; {attention_output.shape = }, {attention_scores.shape = }")
</code></pre><h3>After Change</h3><pre><code class='java'>
    _, hh, ww, cc = inputs.shape
    key_dim = key_dim if key_dim &gt; 0 else cc // num_heads
    &#47&#47 qk_scale = float(1.0 / tf.math.sqrt(tf.cast(key_dim, "float32")))
    qk_scale = <a id="change">1.0</a><a id="change"> / </a>(<a id="change">float(</a>key_dim<a id="change">) ** 0.5</a>)
    out_shape = cc if out_shape is None or not out_weight else out_shape
    qkv_out = num_heads * key_dim

    qkv = layers.Dense(qkv_out * 3, use_bias=qkv_bias, name=name and name + "qkv")(inputs)
    qkv = functional.reshape(qkv, [-1, qkv.shape[1] * qkv.shape[2], qkv.shape[-1]])
    value, query, key = functional.split(qkv, 3, axis=-1)  &#47&#47 Matching weights from PyTorch
    query = functional.transpose(functional.reshape(query, [-1, query.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  &#47&#47 [batch, num_heads, key_dim, hh * ww]
    key = functional.transpose(functional.reshape(key, [-1, key.shape[1], num_heads, key_dim]), [0, 2, 1, 3])  &#47&#47 [batch, num_heads, hh * ww, key_dim]
    value = functional.transpose(functional.reshape(value, [-1, value.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  &#47&#47  [batch, num_heads, key_dim, hh * ww]

    &#47&#47 attention_scores = layers.Lambda(lambda xx: functional.matmul(xx[0], xx[1]))([query, key]) * qk_scale  &#47&#47 [batch, num_heads, key_dim, key_dim]
    attention_scores = (query<a id="change"> @ </a>key)<a id="change"> * </a>qk_scale
    attention_scores = layers.Softmax(axis=-1, name=name and name + "attention_scores")(attention_scores)
    attention_scores = layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores) if attn_dropout &gt; 0 else attention_scores

    &#47&#47 value = [batch, num_heads, key_dim, hh * ww], attention_output = [batch, num_heads, key_dim, hh * ww]
    &#47&#47 attention_output = layers.Lambda(lambda xx: functional.matmul(xx[0], xx[1]))([attention_scores, value])
    attention_output = attention_scores<a id="change"> @ </a>value
    attention_output = functional.transpose(attention_output, perm=[0, 3, 1, 2])  &#47&#47 [batch, hh * ww, num_heads, key_dim]
    attention_output = functional.reshape(attention_output, [-1, inputs.shape[1], inputs.shape[2], num_heads * key_dim])
    &#47&#47 print(f"&gt;&gt;&gt;&gt; {attention_output.shape = }, {attention_scores.shape = }")</code></pre>