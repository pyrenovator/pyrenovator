<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        pre_mem, post_mem, max_mem = measure_allocated_memory(
            model, sample, model_device, transfer_to_device_fn, print_details
        )
        <a id="change">results["pre_inference_memory"] = </a>pre_mem
        <a id="change">results["max_inference_memory"] = </a>max_mem
        <a id="change">results["post_inference_memory"] = </a>post_mem
        print_fn(
            f"Allocated GPU memory prior to inference: {pre_mem} ({format_num(pre_mem, bytes=True)})"
        )</code></pre><h3>After Change</h3><pre><code class='java'>
        print_fn(f"Model FLOPs: {flops} ({format_num(flops)})")

    &#47&#47 Measure inference timing
    memory<a id="change"> = </a><a id="change">{}</a>
    timing = {}
    energy = {}
    with torch.no_grad():
        for bs in sorted(set([1, batch_size])):
            s = sample1 if bs == 1 else sample

            &#47&#47 Measure Allocated Memory
            if model_device.type == "cuda":
                pre_mem, post_mem, max_mem = measure_allocated_memory(
                    model, sample, model_device, transfer_to_device_fn, print_details
                )
                <a id="change">memory[f"batch_size_{bs}"] = </a>{
                    "pre_inference_bytes": pre_mem,
                    "max_inference_bytes": max_mem,
                    "post_inference_bytes": post_mem,</code></pre>