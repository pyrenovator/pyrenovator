<html><h3>Pattern ID :9038
</h3><img src='32927142.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        :return: (N, 2, L)
        
        &#47&#47 the source sentences, which are to be fed as the inputs to the encoder
        <a id="change">input_ids_src</a><a id="change">, attention_mask_src</a> = self.encode([
            self.tokenizer.bos_token + " " + sent + " " + self.tokenizer.eos_token  &#47&#47 noqa
            for sent in srcs
        ])
        <a id="change">input_ids_tgt</a><a id="change">, attention_mask_tgt</a> = self.encode([
            &#47&#47 just start with bos_token.
            &#47&#47 why no eos token at the end?
            &#47&#47 A: because the label for eos token, i.e. pad token, is ignored in computing the loss anyways
            &#47&#47 also, this may lead to the model repeating characters
            &#47&#47 refer to: https://discuss.pytorch.org/t/transformer-mask-doesnt-do-anything/79765
            self.tokenizer.bos_token  &#47&#47 noqa
            for _ in srcs
        ])
        inputs_src<a id="change"> = torch</a><a id="change">.stack([input_ids_src</a>, <a id="change">attention_mask_src</a>]<a id="change">, dim=1)</a>
        inputs_tgt<a id="change"> = torch</a><a id="change">.stack([input_ids_tgt</a>, <a id="change">attention_mask_tgt</a>]<a id="change">, dim=1)</a>
        inputs = torch.stack([inputs_src, inputs_tgt], dim=1)
        return inputs

</code></pre><h3>After Change</h3><pre><code class='java'>
class InferInputsBuilder(InputsBuilder):

    def __call__(self, srcs: List[str]) -&gt; torch.Tensor:
        src_inputs = <a id="change">self.src_inputs(</a>srcs<a id="change">)</a>  &#47&#47 (N, 2, L)
        tgt_inputs = self.tgt_inputs(len(srcs))  &#47&#47 (N, 2, L)
        inputs = torch.stack([src_inputs, tgt_inputs], dim=1)  &#47&#47 (N, 2, 2, L)
        return inputs</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 9</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/eubinecto/the-clean-transformer/commit/0b9835241bbe1f98ef8df55dfbeedfda717efc43#diff-a465aad76ce1b8e19a2fdb45e525e21a5b0288e8fccfdfccf8281214ab25ee8aL60' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 32927142</div><div id='project'> Project Name: eubinecto/the-clean-transformer</div><div id='commit'> Commit Name: 0b9835241bbe1f98ef8df55dfbeedfda717efc43</div><div id='time'> Time: 2021-12-11</div><div id='author'> Author: eubinecto</div><div id='file'> File Name: enkorde/builders.py</div><div id='m_class'> M Class Name: InferInputsBuilder</div><div id='n_method'> N Class Name: InferInputsBuilder</div><div id='m_method'> M Method Name: __call__(2)</div><div id='n_method'> N Method Name: __call__(2)</div><div id='m_parent_class'> M Parent Class: InputsBuilder</div><div id='n_parent_class'> N Parent Class: DataBuilder</div><div id='m_file'> M File Name: enkorde/builders.py</div><div id='n_file'> N File Name: enkorde/builders.py</div><div id='m_start'> M Start Line: 60</div><div id='m_end'> M End Line: 74</div><div id='n_start'> N Start Line: 71</div><div id='n_end'> N End Line: 72</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        :return: (N, 2, L)
        
        &#47&#47 the source sentences, which are to be fed as the inputs to the encoder
        input_ids_src<a id="change">, attention_mask_src</a> = self.encode([
            self.tokenizer.bos_token + " " + sent + " " + self.tokenizer.eos_token  &#47&#47 noqa
            for sent in srcs
        ])
        input_ids_tgt<a id="change">, attention_mask_tgt</a> = self.encode([
            &#47&#47 just start with bos_token.
            &#47&#47 why no eos token at the end?
            &#47&#47 A: because the label for eos token, i.e. pad token, is ignored in computing the loss anyways
            &#47&#47 also, this may lead to the model repeating characters
            &#47&#47 refer to: https://discuss.pytorch.org/t/transformer-mask-doesnt-do-anything/79765
            self.tokenizer.bos_token  &#47&#47 noqa
            for _ in srcs
        ])
        inputs_src<a id="change"> = </a><a id="change">torch.stack([</a>input_ids_src, attention_mask_src<a id="change"></a>]<a id="change">, dim=1)</a>
        inputs_tgt<a id="change"> = </a><a id="change">torch.stack([</a>input_ids_tgt, attention_mask_tgt<a id="change"></a>]<a id="change">, dim=1)</a>
        inputs = torch.stack([inputs_src, inputs_tgt], dim=1)
        return inputs

</code></pre><h3>After Change</h3><pre><code class='java'>
class InferInputsBuilder(InputsBuilder):

    def __call__(self, srcs: List[str]) -&gt; torch.Tensor:
        src_inputs = <a id="change">self.src_inputs(</a>srcs<a id="change">)</a>  &#47&#47 (N, 2, L)
        tgt_inputs = self.tgt_inputs(len(srcs))  &#47&#47 (N, 2, L)
        inputs = torch.stack([src_inputs, tgt_inputs], dim=1)  &#47&#47 (N, 2, 2, L)
        return inputs</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/eubinecto/the-clean-transformer/commit/0b9835241bbe1f98ef8df55dfbeedfda717efc43#diff-a465aad76ce1b8e19a2fdb45e525e21a5b0288e8fccfdfccf8281214ab25ee8aL54' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 32927143</div><div id='project'> Project Name: eubinecto/the-clean-transformer</div><div id='commit'> Commit Name: 0b9835241bbe1f98ef8df55dfbeedfda717efc43</div><div id='time'> Time: 2021-12-11</div><div id='author'> Author: eubinecto</div><div id='file'> File Name: enkorde/builders.py</div><div id='m_class'> M Class Name: InferInputsBuilder</div><div id='n_method'> N Class Name: InferInputsBuilder</div><div id='m_method'> M Method Name: __call__(2)</div><div id='n_method'> N Method Name: __call__(2)</div><div id='m_parent_class'> M Parent Class: InputsBuilder</div><div id='n_parent_class'> N Parent Class: DataBuilder</div><div id='m_file'> M File Name: enkorde/builders.py</div><div id='n_file'> N File Name: enkorde/builders.py</div><div id='m_start'> M Start Line: 60</div><div id='m_end'> M End Line: 74</div><div id='n_start'> N Start Line: 71</div><div id='n_end'> N End Line: 72</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        :return: (N, 2, 2, L) - input_ids & attention_mask
        
        &#47&#47 the source sentences, which are to be fed as the inputs to the encoder
        input_ids_src<a id="change">, attention_mask_src</a> = self.encode([
            self.tokenizer.bos_token + " " + sent + " " + self.tokenizer.eos_token  &#47&#47 noqa
            for sent in srcs
        ])
        &#47&#47 the target sentences, which are to be fed as the inputs to the decoder
        input_ids_tgt<a id="change">, attention_mask_tgt</a> = self.encode([
            &#47&#47 starts with bos, but does not end with eos (pad token is ignored anyways)
            self.tokenizer.bos_token + " " + sent  &#47&#47 noqa
            for sent in tgts
        ])
        inputs_src<a id="change"> = </a><a id="change">torch.stack([</a>input_ids_src, attention_mask_src<a id="change"></a>]<a id="change">, dim=1)</a>
        inputs_tgt<a id="change"> = </a><a id="change">torch.stack([</a>input_ids_tgt, attention_mask_tgt<a id="change"></a>]<a id="change">, dim=1)</a>
        inputs = torch.stack([inputs_src, inputs_tgt], dim=1)
        return inputs

</code></pre><h3>After Change</h3><pre><code class='java'>

    def __call__(self, srcs: List[str], tgts: List[str]) -&gt; torch.Tensor:
        assert len(srcs) == len(tgts)
        src_inputs = <a id="change">self.src_inputs(</a>srcs<a id="change">)</a>  &#47&#47 (N, 2, L)
        tgt_inputs = self.tgt_inputs(tgts)  &#47&#47 (N, 2, L)
        inputs = torch.stack([src_inputs, tgt_inputs], dim=1)  &#47&#47 (N, 2, 2, L)
        return inputs</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/eubinecto/dekorde/commit/0b9835241bbe1f98ef8df55dfbeedfda717efc43#diff-a465aad76ce1b8e19a2fdb45e525e21a5b0288e8fccfdfccf8281214ab25ee8aL30' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 32927139</div><div id='project'> Project Name: eubinecto/dekorde</div><div id='commit'> Commit Name: 0b9835241bbe1f98ef8df55dfbeedfda717efc43</div><div id='time'> Time: 2021-12-11</div><div id='author'> Author: eubinecto</div><div id='file'> File Name: enkorde/builders.py</div><div id='m_class'> M Class Name: TrainInputsBuilder</div><div id='n_method'> N Class Name: TrainInputsBuilder</div><div id='m_method'> M Method Name: __call__(3)</div><div id='n_method'> N Method Name: __call__(3)</div><div id='m_parent_class'> M Parent Class: InputsBuilder</div><div id='n_parent_class'> N Parent Class: DataBuilder</div><div id='m_file'> M File Name: enkorde/builders.py</div><div id='n_file'> N File Name: enkorde/builders.py</div><div id='m_start'> M Start Line: 37</div><div id='m_end'> M End Line: 48</div><div id='n_start'> N Start Line: 47</div><div id='n_end'> N End Line: 49</div><BR>