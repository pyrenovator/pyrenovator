<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
):
    _, bb, cc = inputs.shape
    key_dim = key_dim if key_dim &gt; 0 else cc // num_heads
    qk_scale = <a id="change">float(1.0</a><a id="change"> / </a><a id="change">tf.math.sqrt(tf.cast(</a>key_dim, <a id="change">"float32"</a><a id="change">)</a><a id="change">))</a>
    emded_dim = num_heads * key_dim

    &#47&#47 if qkv_bias, just use bias in qkv_dense, and set qv_bias False
    qkv_bias, qv_bias = (True, False) if qkv_bias else (False, qv_bias)</code></pre><h3>After Change</h3><pre><code class='java'>
    _, bb, cc = inputs.shape
    key_dim = key_dim if key_dim &gt; 0 else cc // num_heads
    &#47&#47 qk_scale = float(1.0 / tf.math.sqrt(tf.cast(key_dim, "float32")))
    qk_scale = <a id="change">1.0</a><a id="change"> / </a>(<a id="change">float(</a>key_dim<a id="change">) ** 0.5</a>)
    emded_dim = num_heads * key_dim
    &#47&#47 print(f"&gt;&gt;&gt;&gt; {bb = }, {cc = }, {emded_dim = }")

    &#47&#47 if qkv_bias, just use bias in qkv_dense, and set qv_bias False
    qkv_bias, qv_bias = (True, False) if qkv_bias else (False, qv_bias)

    qkv = layers.Dense(emded_dim * 3, use_bias=qkv_bias, name=name and name + "qkv")(inputs)
    &#47&#47 print(f"&gt;&gt;&gt;&gt; {qkv.shape = }")
    qkv = functional.reshape(qkv, [-1, bb, qkv.shape[-1]])
    query, key, value = functional.split(qkv, 3, axis=-1)
    &#47&#47 query = [batch, num_heads, cls_token + hh * ww, key_dim]
    if qv_bias:
        query = BiasLayer(name=name + "query_bias")(query)
    query = functional.reshape(query, [-1, query.shape[1], num_heads, key_dim])
    query = functional.transpose(query, [0, 2, 1, 3])
    &#47&#47 key = [batch, num_heads, key_dim, cls_token + hh * ww]
    key = functional.transpose(functional.reshape(key, [-1, key.shape[1], num_heads, key_dim]), [0, 2, 3, 1])
    &#47&#47 value = [batch, num_heads, cls_token + hh * ww, key_dim]
    if qv_bias:
        value = BiasLayer(name=name + "value_bias")(value)
    value = functional.reshape(value, [-1, value.shape[1], num_heads, key_dim])
    value = functional.transpose(value, [0, 2, 1, 3])

    &#47&#47 query *= qk_scale
    &#47&#47 [batch, num_heads, cls_token + hh * ww, cls_token + hh * ww]
    &#47&#47 attention_scores = layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([query, key])
    attention_scores = (query<a id="change"> @ </a>key)<a id="change"> * </a>qk_scale
    if use_pos_emb:
        attention_scores = MultiHeadRelativePositionalEmbedding(attn_height=attn_height, name=name and name + "pos_emb")(attention_scores)
    &#47&#47 attention_scores = tf.nn.softmax(attention_scores, axis=-1, name=name and name + "_attention_scores")
    attention_scores = layers.Softmax(axis=-1, name=name and name + "attention_scores")(attention_scores)

    if attn_dropout &gt; 0:
        attention_scores = layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
    &#47&#47 value = [batch, num_heads, cls_token + hh * ww, key_dim]
    &#47&#47 attention_output = tf.matmul(attention_scores, value)  &#47&#47 [batch, num_heads, cls_token + hh * ww, key_dim]
    &#47&#47 attention_output = layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([attention_scores, value])
    attention_output = attention_scores<a id="change"> @ </a>value
    attention_output = functional.transpose(attention_output, perm=[0, 2, 1, 3])
    attention_output = functional.reshape(attention_output, [-1, bb, emded_dim])
    &#47&#47 print(f"&gt;&gt;&gt;&gt; {attention_output.shape = }, {attention_scores.shape = }")</code></pre>