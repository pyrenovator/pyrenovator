<html><h3>Pattern ID :30559
</h3><img src='90364128.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        self.buffer.reset()

        <a id="change">return </a><a id="change">Log(
            actor_loss=np.mean(actor_losses),
            critic_loss=np.mean(critic_losses),
            divergence=actor_log.divergence,
            entropy=actor_log.entropy,
        )</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
    def _fit(self, batch_size: int, actor_epochs: int = 1, critic_epochs: int = 1):
        critic_losses = np.zeros(shape=(critic_epochs))
        actor_losses = np.zeros(shape=(actor_epochs))
        <a id="change">divergences = np</a><a id="change">.zeros(shape=(actor_epochs))</a>
        <a id="change">entropies = np</a><a id="change">.zeros(shape=(actor_epochs))</a>

        &#47&#47 Sample rollout and compute advantages/returns
        trajectories = self.buffer.sample(batch_size, dtype="torch")
        with T.no_grad():
            old_values = self.model.forward_critic(trajectories.observations)
            next_values = self.model.forward_critic(trajectories.next_observations)
            advantages, returns = generalized_advantage_estimate(
                rewards=trajectories.rewards,
                old_values=old_values,
                new_values=next_values,
                dones=trajectories.dones,
                gae_lambda=self.gae_lambda,
                gamma=self.gae_gamma,
            )

        &#47&#47 Train actor for actor_epochs
        for i in range(actor_epochs):
            actor_log = self.actor_updater(
                model=self.model,
                observations=trajectories.observations,
                actions=trajectories.actions,
                advantages=advantages,
            )
            actor_losses[i] = actor_log.loss
            <a id="change">divergences[i] = </a>actor_log.divergence
            <a id="change">entropies[i] = </a>actor_log.entropy

        &#47&#47 Train critic for critic_epochs
        for i in range(critic_epochs):</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 10</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/londonnode/pearl/commit/0d7fa4ab982a1fb9ea6734553d93aa5d65fd87ce#diff-443b8a55dbb22575ba75ea5b560e6a97dbb31bdab5ca6723f781f2436cfcd5feL138' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 90364128</div><div id='project'> Project Name: londonnode/pearl</div><div id='commit'> Commit Name: 0d7fa4ab982a1fb9ea6734553d93aa5d65fd87ce</div><div id='time'> Time: 2021-12-20</div><div id='author'> Author: rohan.tangri@gmail.com</div><div id='file'> File Name: anvilrl/agents/a2c.py</div><div id='m_class'> M Class Name: A2C</div><div id='n_method'> N Class Name: A2C</div><div id='m_method'> M Method Name: _fit(4)</div><div id='n_method'> N Method Name: _fit(4)</div><div id='m_parent_class'> M Parent Class: BaseDeepAgent</div><div id='n_parent_class'> N Parent Class: BaseDeepAgent</div><div id='m_file'> M File Name: anvilrl/agents/a2c.py</div><div id='n_file'> N File Name: anvilrl/agents/a2c.py</div><div id='m_start'> M Start Line: 174</div><div id='m_end'> M End Line: 179</div><div id='n_start'> N Start Line: 138</div><div id='n_end'> N End Line: 183</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        )
        self.buffer.reset()

        <a id="change">return </a><a id="change">Log(divergence=log.divergence, entropy=log.entropy)</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
        self.elitism = elitism

    def _fit(self, epochs: int = 1) -&gt; Log:
        <a id="change">divergences = </a><a id="change">np.zeros(</a>epochs<a id="change">)</a>
        <a id="change">entropies = </a><a id="change">np.zeros(</a>epochs<a id="change">)</a>

        trajectories = self.buffer.all()
        rewards = trajectories.rewards.squeeze()
        if rewards.ndim &gt; 1:
            rewards = rewards.sum(dim=0)
        for <a id="change">i</a> in range(epochs):
            log = self.updater(
                rewards=rewards,
                selection_operator=self.selection_operator,
                crossover_operator=self.crossover_operator,
                mutation_operator=self.mutation_operator,
                selection_settings=self.selection_settings,
                crossover_settings=self.crossover_settings,
                mutation_settings=self.mutation_settings,
                elitism=self.elitism,
            )
            <a id="change">divergences[i] = </a>log.divergence
            <a id="change">entropies[i] = </a>log.entropy
        self.buffer.reset()

        return Log(divergence=divergences.mean(), entropy=entropies.mean())</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/londonnode/pearl/commit/90195a810a7780ac8982f85f2e47d5464164ac41#diff-9481d9d41ebe20514632a6fd73aff5b756ec8fa96de509d0c8c7a6a1ffcd8a43L86' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 90364098</div><div id='project'> Project Name: londonnode/pearl</div><div id='commit'> Commit Name: 90195a810a7780ac8982f85f2e47d5464164ac41</div><div id='time'> Time: 2021-12-20</div><div id='author'> Author: rohan.tangri@gmail.com</div><div id='file'> File Name: anvilrl/agents/ga.py</div><div id='m_class'> M Class Name: GA</div><div id='n_method'> N Class Name: GA</div><div id='m_method'> M Method Name: _fit(2)</div><div id='n_method'> N Method Name: _fit(1)</div><div id='m_parent_class'> M Parent Class: BaseEvolutionAgent</div><div id='n_parent_class'> N Parent Class: BaseEvolutionAgent</div><div id='m_file'> M File Name: anvilrl/agents/ga.py</div><div id='n_file'> N File Name: anvilrl/agents/ga.py</div><div id='m_start'> M Start Line: 86</div><div id='m_end'> M End Line: 103</div><div id='n_start'> N Start Line: 86</div><div id='n_end'> N End Line: 109</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        )
        self.buffer.reset()

        <a id="change">return </a><a id="change">Log(divergence=log.divergence, entropy=log.entropy)</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
        self.learning_rate = learning_rate

    def _fit(self, epochs: int = 1) -&gt; Log:
        <a id="change">divergences = </a><a id="change">np.zeros(</a>epochs<a id="change">)</a>
        <a id="change">entropies = </a><a id="change">np.zeros(</a>epochs<a id="change">)</a>

        trajectories = self.buffer.all()
        rewards = trajectories.rewards.squeeze()
        if rewards.ndim &gt; 1:
            rewards = rewards.sum(axis=0)
        scaled_rewards = scale(rewards)
        learning_rate = self.learning_rate / (
            np.mean(self.population_settings.population_std) * self.env.num_envs
        )
        optimization_direction = np.dot(self.updater.normal_dist.T, scaled_rewards)
        for <a id="change">i</a> in range(epochs):
            log = self.updater(
                learning_rate=learning_rate,
                optimization_direction=optimization_direction,
            )
            <a id="change">divergences[i] = </a>log.divergence
            <a id="change">entropies[i] = </a>log.entropy
        self.buffer.reset()

        return Log(divergence=divergences.mean(), entropy=entropies.mean())</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/londonnode/pearl/commit/90195a810a7780ac8982f85f2e47d5464164ac41#diff-d329d9466d61a46ca78297c04fbb197ee6498d72746d01789c8d16a28bb8247bL69' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 90364096</div><div id='project'> Project Name: londonnode/pearl</div><div id='commit'> Commit Name: 90195a810a7780ac8982f85f2e47d5464164ac41</div><div id='time'> Time: 2021-12-20</div><div id='author'> Author: rohan.tangri@gmail.com</div><div id='file'> File Name: anvilrl/agents/es.py</div><div id='m_class'> M Class Name: ES</div><div id='n_method'> N Class Name: ES</div><div id='m_method'> M Method Name: _fit(2)</div><div id='n_method'> N Method Name: _fit(1)</div><div id='m_parent_class'> M Parent Class: BaseEvolutionAgent</div><div id='n_parent_class'> N Parent Class: BaseEvolutionAgent</div><div id='m_file'> M File Name: anvilrl/agents/es.py</div><div id='n_file'> N File Name: anvilrl/agents/es.py</div><div id='m_start'> M Start Line: 69</div><div id='m_end'> M End Line: 85</div><div id='n_start'> N Start Line: 69</div><div id='n_end'> N End Line: 91</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        self.buffer.reset()

        <a id="change">return </a><a id="change">Log(
            actor_loss=np.mean(actor_losses),
            critic_loss=np.mean(critic_losses),
            divergence=actor_log.divergence,
            entropy=actor_log.entropy,
        )</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
    def _fit(self, batch_size: int, actor_epochs: int = 1, critic_epochs: int = 1):
        critic_losses = np.zeros(shape=(critic_epochs))
        actor_losses = np.zeros(shape=(actor_epochs))
        <a id="change">divergences = </a><a id="change">np.zeros(shape=(actor_epochs))</a>
        <a id="change">entropies = </a><a id="change">np.zeros(shape=(actor_epochs))</a>

        &#47&#47 Sample rollout and compute advantages/returns
        trajectories = self.buffer.sample(batch_size, dtype="torch")
        with T.no_grad():
            old_values = self.model.forward_critic(trajectories.observations)
            next_values = self.model.forward_critic(trajectories.next_observations)
            advantages, returns = generalized_advantage_estimate(
                rewards=trajectories.rewards,
                old_values=old_values,
                new_values=next_values,
                dones=trajectories.dones,
                gae_lambda=self.gae_lambda,
                gamma=self.gae_gamma,
            )

        &#47&#47 Train actor for actor_epochs
        for <a id="change">i</a> in range(actor_epochs):
            actor_log = self.actor_updater(
                model=self.model,
                observations=trajectories.observations,
                actions=trajectories.actions,
                advantages=advantages,
            )
            actor_losses[i] = actor_log.loss
            <a id="change">divergences[i] = </a>actor_log.divergence
            <a id="change">entropies[i] = </a>actor_log.entropy

        &#47&#47 Train critic for critic_epochs
        for i in range(critic_epochs):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/londonnode/pearl/commit/0d7fa4ab982a1fb9ea6734553d93aa5d65fd87ce#diff-443b8a55dbb22575ba75ea5b560e6a97dbb31bdab5ca6723f781f2436cfcd5feL135' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 90364097</div><div id='project'> Project Name: londonnode/pearl</div><div id='commit'> Commit Name: 0d7fa4ab982a1fb9ea6734553d93aa5d65fd87ce</div><div id='time'> Time: 2021-12-20</div><div id='author'> Author: rohan.tangri@gmail.com</div><div id='file'> File Name: anvilrl/agents/a2c.py</div><div id='m_class'> M Class Name: A2C</div><div id='n_method'> N Class Name: A2C</div><div id='m_method'> M Method Name: _fit(4)</div><div id='n_method'> N Method Name: _fit(4)</div><div id='m_parent_class'> M Parent Class: BaseDeepAgent</div><div id='n_parent_class'> N Parent Class: BaseDeepAgent</div><div id='m_file'> M File Name: anvilrl/agents/a2c.py</div><div id='n_file'> N File Name: anvilrl/agents/a2c.py</div><div id='m_start'> M Start Line: 174</div><div id='m_end'> M End Line: 179</div><div id='n_start'> N Start Line: 138</div><div id='n_end'> N End Line: 183</div><BR>