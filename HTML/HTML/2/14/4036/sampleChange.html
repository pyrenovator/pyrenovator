<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    

    def __init__(self, learning_rate=0.01, momentum=0.0, nesterov=False, *args, **kwargs):
        <a id="change">super().__init__(</a>learning_rate, momentum, nesterov, *<a id="change">args, **kwargs)</a>


class Momentum(tf.compat.v1.train.MomentumOptimizer):
    Optimizer that implements the Momentum algorithm. Equivalent to tf.compat.v1.train.MomentumOptimizer</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self, learning_rate=0.01, momentum=0.0, weight_decay=0.0, grad_clip=None):
        self.learning_rate = learning_rate
        self.momentum = momentum
        <a id="change">if weight_decay &lt; 0.0</a>:
            <a id="change">raise ValueError("weight_decay should not smaller than 0.0, but got {}".format(weight_decay</a><a id="change">)</a><a id="change">)</a>
        self.weight_decay<a id="change"> = float(weight_decay</a><a id="change">)</a>
        self.grad_clip<a id="change"> = </a>grad_clip
        self.sgd<a id="change"> = </a>tf.optimizers.SGD(learning_rate=self.learning_rate, momentum=self.momentum, nesterov=False)

    def apply_gradients(self, grads_and_vars):
        if grads_and_vars is None:</code></pre>