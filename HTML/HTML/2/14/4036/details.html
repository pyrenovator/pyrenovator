<html><h3>Pattern ID :4036
</h3><img src='15049829.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    

    def __init__(self, learning_rate=0.01, momentum=0.0, nesterov=False, *args, **kwargs):
        <a id="change">super().__init__(</a>learning_rate, momentum, nesterov, *<a id="change">args, **kwargs)</a>


class Momentum(tf.compat.v1.train.MomentumOptimizer):
    Optimizer that implements the Momentum algorithm. Equivalent to tf.compat.v1.train.MomentumOptimizer</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self, learning_rate=0.01, momentum=0.0, weight_decay=0.0, grad_clip=None):
        self.learning_rate = learning_rate
        self.momentum = momentum
        <a id="change">if weight_decay &lt; 0.0</a>:
            <a id="change">raise ValueError("weight_decay should not smaller than 0.0, but got {}".format(weight_decay</a><a id="change">)</a><a id="change">)</a>
        self.weight_decay<a id="change"> = float(weight_decay</a><a id="change">)</a>
        self.grad_clip<a id="change"> = </a>grad_clip
        self.sgd<a id="change"> = </a>tf.optimizers.SGD(learning_rate=self.learning_rate, momentum=self.momentum, nesterov=False)

    def apply_gradients(self, grads_and_vars):
        if grads_and_vars is None:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 9</div><BR><div id='size'>Non-data size: 11</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL277' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15049829</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e</div><div id='time'> Time: 2022-03-25</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: SGD</div><div id='n_method'> N Class Name: SGD</div><div id='m_method'> M Method Name: __init__(5)</div><div id='n_method'> N Method Name: __init__(4)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: tf.optimizers.SGD</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 277</div><div id='m_end'> M End Line: 277</div><div id='n_start'> N Start Line: 548</div><div id='n_end'> N End Line: 554</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        l1_regularization_strength=0.0, l2_regularization_strength=0.0, beta=0.0,
        l2_shrinkage_regularization_strength=0.0, **kwargs
    ):
        <a id="change">super().__init__(
            </a>learning_rate, learning_rate_power, initial_accumulator_value, l1_regularization_strength,
            l2_regularization_strength, beta, l2_shrinkage_regularization_strength<a id="change">, **kwargs
        )</a>


class Nadam(tf.optimizers.Nadam):
    Optimizer that implements the NAdam algorithm. Equivalent to tf.optimizers.Nadam.</code></pre><h3>After Change</h3><pre><code class='java'>
        self.l2_regularization_strength = l2_regularization_strength
        self.beta = beta
        self.l2_shrinkage_regularization_strength = l2_shrinkage_regularization_strength
        <a id="change">if weight_decay &lt; 0.0</a>:
            <a id="change">raise ValueError("weight_decay should not smaller than 0.0, but got {}".format(</a>weight_decay<a id="change">)</a><a id="change">)</a>
        self.weight_decay<a id="change"> = float(</a>weight_decay<a id="change">)</a>
        self.grad_clip<a id="change"> = </a>grad_clip
        self.ftrl<a id="change"> = </a>tf.optimizers.Ftrl(
            learning_rate=self.learning_rate, learning_rate_power=self.learning_rate_power,
            initial_accumulator_value=self.initial_accumulator_value,
            l1_regularization_strength=self.l1_regularization_strength,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL170' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15049813</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e</div><div id='time'> Time: 2022-03-25</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: Ftrl</div><div id='n_method'> N Class Name: Ftrl</div><div id='m_method'> M Method Name: __init__(10)</div><div id='n_method'> N Method Name: __init__(8)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: tf.optimizers.Ftrl</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 175</div><div id='m_end'> M End Line: 178</div><div id='n_start'> N Start Line: 326</div><div id='n_end'> N End Line: 345</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    

    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, *args, **kwargs):
        <a id="change">super().__init__(</a>learning_rate, beta_1, beta_2, epsilon, *<a id="change">args, **kwargs)</a>


class Ftrl(tf.optimizers.Ftrl):
    Optimizer that implements the FTRL algorithm. Equivalent to tf.optimizers.Ftrl.</code></pre><h3>After Change</h3><pre><code class='java'>
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        self.epsilon = epsilon
        <a id="change">if weight_decay &lt; 0.0</a>:
            <a id="change">raise ValueError("weight_decay should not smaller than 0.0, but got {}".format(</a>weight_decay<a id="change">)</a><a id="change">)</a>
        self.weight_decay<a id="change"> = float(</a>weight_decay<a id="change">)</a>
        self.grad_clip<a id="change"> = </a>grad_clip
        self.adamax<a id="change"> = </a>tf.optimizers.Adamax(
            learning_rate=self.learning_rate, beta_1=self.beta_1, beta_2=self.beta_2, epsilon=self.epsilon
        )
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL131' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15049828</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e</div><div id='time'> Time: 2022-03-25</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: Adamax</div><div id='n_method'> N Class Name: Adamax</div><div id='m_method'> M Method Name: __init__(7)</div><div id='n_method'> N Method Name: __init__(5)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: tf.optimizers.Adamax</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 132</div><div id='m_end'> M End Line: 132</div><div id='n_start'> N Start Line: 248</div><div id='n_end'> N End Line: 258</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    

    def __init__(self, learning_rate=0.001, rho=0.95, epsilon=1e-07, *args, **kwargs):
        <a id="change">super().__init__(</a>learning_rate, rho, epsilon, *<a id="change">args, **kwargs)</a>


class Adagrad(tf.optimizers.Adagrad):
    Optimizer that implements the Adagrad algorithm. Equivalent to tf.optimizers.Adagrad.</code></pre><h3>After Change</h3><pre><code class='java'>
        self.learning_rate = learning_rate
        self.rho = rho
        self.epsilon = epsilon
        <a id="change">if weight_decay &lt; 0.0</a>:
            <a id="change">raise ValueError("weight_decay should not smaller than 0.0, but got {}".format(</a>weight_decay<a id="change">)</a><a id="change">)</a>
        self.weight_decay<a id="change"> = float(</a>weight_decay<a id="change">)</a>
        self.grad_clip<a id="change"> = </a>grad_clip
        self.adadelta<a id="change"> = </a>tf.optimizers.Adadelta(learning_rate=self.learning_rate, rho=self.rho, epsilon=self.epsilon)

    def apply_gradients(self, grads_and_vars):
        if grads_and_vars is None:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL36' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15049812</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e</div><div id='time'> Time: 2022-03-25</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: Adadelta</div><div id='n_method'> N Class Name: Adadelta</div><div id='m_method'> M Method Name: __init__(6)</div><div id='n_method'> N Method Name: __init__(4)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: tf.optimizers.Adadelta</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 37</div><div id='m_end'> M End Line: 37</div><div id='n_start'> N Start Line: 46</div><div id='n_end'> N End Line: 53</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    

    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, *args, **kwargs):
        <a id="change">super().__init__(</a>learning_rate, beta_1, beta_2, epsilon, *<a id="change">args, **kwargs)</a>


class RMSprop(tf.optimizers.RMSprop):
    Optimizer that implements the RMSprop algorithm. Equivalent to tf.optimizers.RMSprop.</code></pre><h3>After Change</h3><pre><code class='java'>
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        self.epsilon = epsilon
        <a id="change">if weight_decay &lt; 0.0</a>:
            <a id="change">raise ValueError("weight_decay should not smaller than 0.0, but got {}".format(</a>weight_decay<a id="change">)</a><a id="change">)</a>
        self.weight_decay<a id="change"> = float(</a>weight_decay<a id="change">)</a>
        self.grad_clip<a id="change"> = </a>grad_clip
        self.nadam<a id="change"> = </a>tf.optimizers.Nadam(
            learning_rate=self.learning_rate, beta_1=self.beta_1, beta_2=self.beta_2, epsilon=self.epsilon
        )
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL209' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15049831</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e</div><div id='time'> Time: 2022-03-25</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: Nadam</div><div id='n_method'> N Class Name: Nadam</div><div id='m_method'> M Method Name: __init__(7)</div><div id='n_method'> N Method Name: __init__(5)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: tf.optimizers.Nadam</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 210</div><div id='m_end'> M End Line: 210</div><div id='n_start'> N Start Line: 404</div><div id='n_end'> N End Line: 414</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    

    def __init__(self, learning_rate=0.001, rho=0.95, epsilon=1e-07, *args, **kwargs):
        <a id="change">super().__init__(</a>learning_rate, rho, epsilon, *<a id="change">args, **kwargs)</a>


class Adam(tf.optimizers.Adam):
    Optimizer that implements the Adam algorithm. Equivalent to tf.optimizers.Adam.</code></pre><h3>After Change</h3><pre><code class='java'>
        self.learning_rate = learning_rate
        self.initial_accumulator = initial_accumulator
        self.epsilon = epsilon
        <a id="change">if weight_decay &lt; 0.0</a>:
            <a id="change">raise ValueError("weight_decay should not smaller than 0.0, but got {}".format(</a>weight_decay<a id="change">)</a><a id="change">)</a>
        self.weight_decay<a id="change"> = float(</a>weight_decay<a id="change">)</a>
        self.grad_clip<a id="change"> = </a>grad_clip
        self.adagrad<a id="change"> = </a>tf.optimizers.Adagrad(
            learning_rate=self.learning_rate, initial_accumulator=self.initial_accumulator, epsilon=self.epsilon
        )
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL67' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15049799</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e</div><div id='time'> Time: 2022-03-25</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: Adagrad</div><div id='n_method'> N Class Name: Adagrad</div><div id='m_method'> M Method Name: __init__(6)</div><div id='n_method'> N Method Name: __init__(4)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: tf.optimizers.Adagrad</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 68</div><div id='m_end'> M End Line: 68</div><div id='n_start'> N Start Line: 111</div><div id='n_end'> N End Line: 120</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    

    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, *args, **kwargs):
        <a id="change">super().__init__(</a>learning_rate, beta_1, beta_2, epsilon, *<a id="change">args, **kwargs)</a>


class Adamax(tf.optimizers.Adamax):
    Optimizer that implements the Adamax algorithm. Equivalent to tf.optimizers.Adamax.</code></pre><h3>After Change</h3><pre><code class='java'>
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        self.epsilon = epsilon
        <a id="change">if weight_decay &lt; 0.0</a>:
            <a id="change">raise ValueError("weight_decay should not smaller than 0.0, but got {}".format(</a>weight_decay<a id="change">)</a><a id="change">)</a>
        self.weight_decay<a id="change"> = float(</a>weight_decay<a id="change">)</a>
        self.grad_clip<a id="change"> = </a>grad_clip
        self.adam<a id="change"> = </a>tf.optimizers.Adam(
            learning_rate=self.learning_rate, beta_1=self.beta_1, beta_2=self.beta_2, epsilon=self.epsilon
        )
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL99' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15049824</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e</div><div id='time'> Time: 2022-03-25</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: Adam</div><div id='n_method'> N Class Name: Adam</div><div id='m_method'> M Method Name: __init__(7)</div><div id='n_method'> N Method Name: __init__(5)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: tf.optimizers.Adam</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 100</div><div id='m_end'> M End Line: 100</div><div id='n_start'> N Start Line: 179</div><div id='n_end'> N End Line: 189</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    

    def __init__(self, learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False, *args, **kwargs):
        <a id="change">super().__init__(</a>learning_rate, rho, momentum, epsilon, centered, *<a id="change">args, **kwargs)</a>


class SGD(tf.optimizers.SGD):
    Gradient descent (with momentum) optimizer. Equivalent to tf.optimizers.SGD.</code></pre><h3>After Change</h3><pre><code class='java'>
        self.momentum = momentum
        self.epsilon = epsilon
        self.centered = centered
        <a id="change">if weight_decay &lt; 0.0</a>:
            <a id="change">raise ValueError("weight_decay should not smaller than 0.0, but got {}".format(</a>weight_decay<a id="change">)</a><a id="change">)</a>
        self.weight_decay<a id="change"> = float(</a>weight_decay<a id="change">)</a>
        self.grad_clip<a id="change"> = </a>grad_clip
        self.rmsprop<a id="change"> = </a>tf.optimizers.RMSprop(
            learning_rate=self.learning_rate, rho=self.rho, momentum=self.momentum, epsilon=self.epsilon,
            centered=self.centered
        )</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL245' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15049827</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e</div><div id='time'> Time: 2022-03-25</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: RMSprop</div><div id='n_method'> N Class Name: RMSprop</div><div id='m_method'> M Method Name: __init__(8)</div><div id='n_method'> N Method Name: __init__(6)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: tf.optimizers.RMSprop</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 246</div><div id='m_end'> M End Line: 246</div><div id='n_start'> N Start Line: 480</div><div id='n_end'> N End Line: 492</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    

    def __init__(self, learning_rate, momentum=0.0, *args, **kwargs):
        <a id="change">super().__init__(</a>learning_rate, momentum, *<a id="change">args, **kwargs)</a>


class Lamb(object):
    Optimizer that implements the Layer-wise Adaptive Moments (LAMB).</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self, learning_rate=0.01, momentum=0.0, nesterov=False, weight_decay=0.0, grad_clip=None):
        self.learning_rate = learning_rate
        self.momentum = momentum
        <a id="change">if weight_decay &lt; 0.0</a>:
            <a id="change">raise ValueError("weight_decay should not smaller than 0.0, but got {}".format(</a>weight_decay<a id="change">)</a><a id="change">)</a>
        self.weight_decay<a id="change"> = float(</a>weight_decay<a id="change">)</a>
        self.grad_clip<a id="change"> = </a>grad_clip
        self.nesterov = nesterov
        self.sgd<a id="change"> = </a>tf.optimizers.SGD(learning_rate=self.learning_rate, momentum=self.momentum, nesterov=self.nesterov)

    def apply_gradients(self, grads_and_vars):
        if grads_and_vars is None:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL313' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15049838</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e</div><div id='time'> Time: 2022-03-25</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: Momentum</div><div id='n_method'> N Class Name: Momentum</div><div id='m_method'> M Method Name: __init__(6)</div><div id='n_method'> N Method Name: __init__(3)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: tf.compat.v1.train.MomentumOptimizer</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 314</div><div id='m_end'> M End Line: 314</div><div id='n_start'> N Start Line: 617</div><div id='n_end'> N End Line: 625</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    

    def __init__(self, learning_rate=0.01, momentum=0.0, nesterov=False, *args, **kwargs):
        <a id="change">super().__init__(</a>learning_rate, momentum, nesterov, *<a id="change">args, **kwargs)</a>


class Momentum(tf.compat.v1.train.MomentumOptimizer):
    Optimizer that implements the Momentum algorithm. Equivalent to tf.compat.v1.train.MomentumOptimizer</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self, learning_rate=0.01, momentum=0.0, weight_decay=0.0, grad_clip=None):
        self.learning_rate = learning_rate
        self.momentum = momentum
        <a id="change">if weight_decay &lt; 0.0</a>:
            <a id="change">raise ValueError("weight_decay should not smaller than 0.0, but got {}".format(</a>weight_decay<a id="change">)</a><a id="change">)</a>
        self.weight_decay<a id="change"> = float(</a>weight_decay<a id="change">)</a>
        self.grad_clip<a id="change"> = </a>grad_clip
        self.sgd<a id="change"> = </a>tf.optimizers.SGD(learning_rate=self.learning_rate, momentum=self.momentum, nesterov=False)

    def apply_gradients(self, grads_and_vars):
        if grads_and_vars is None:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL276' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 15049806</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e</div><div id='time'> Time: 2022-03-25</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: SGD</div><div id='n_method'> N Class Name: SGD</div><div id='m_method'> M Method Name: __init__(5)</div><div id='n_method'> N Method Name: __init__(4)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: tf.optimizers.SGD</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 277</div><div id='m_end'> M End Line: 277</div><div id='n_start'> N Start Line: 548</div><div id='n_end'> N End Line: 554</div><BR>