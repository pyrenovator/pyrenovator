<html><h3>Pattern ID :6026
</h3><img src='21020864.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                eps = pg[&quoteps&quot]

                for p, sp in zip(pg[&quotparams&quot], spg):
                    exp_avg_sq<a id="change"> = opt_state[p][&quotexp_avg_sq&quot]</a>

                    &#47&#47 FIXME: remove assert after this works.
                    step_count = opt_state[p][&quotstep&quot] + 1
                    assert step_count == self.step_count

                    bias_correction2 = 1 - beta2**(step_count)
                    &#47&#47 calculate C coefficient per-element
                    &#47&#47 NOTE: can remove the "data". but whatever.
                    avg_steps_needed<a id="change"> = </a>max_lr<a id="change"> * \
                        </a>(((exp_avg_sq.data<a id="change"> / </a>bias_correction2)<a id="change"> ** 0.5</a>)<a id="change"> + </a>eps)

                    gap = (p - sp).abs()
</code></pre><h3>After Change</h3><pre><code class='java'>
                    &#47&#47 calculate C coefficient per-element
                    &#47&#47 NOTE: can remove the "data". but whatever.
                    avg_steps_needed = max_lr * \
                        (((<a id="change">ra[id(p)]</a>.data / bias_correction2) ** 0.5) + eps)

                    gap = (p - sp).abs()
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 12</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/5ab23421874eadce8bed648b3eebccce714c7271#diff-22e803f6848fe13ddfd6473f05a519c62fd0e31bafe04851f4fcb25b161fa432L63' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 21020864</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 5ab23421874eadce8bed648b3eebccce714c7271</div><div id='time'> Time: 2020-03-17</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: pipeline/gap_aware/adamw_gap_aware.py</div><div id='m_class'> M Class Name: AdamWGapAware</div><div id='n_method'> N Class Name: AdamWGapAware</div><div id='m_method'> M Method Name: apply_on_stashed(2)</div><div id='n_method'> N Method Name: apply_on_stashed(2)</div><div id='m_parent_class'> M Parent Class: GapAwareBase</div><div id='n_parent_class'> N Parent Class: GapAwareBase</div><div id='m_file'> M File Name: pipeline/gap_aware/adamw_gap_aware.py</div><div id='n_file'> N File Name: pipeline/gap_aware/adamw_gap_aware.py</div><div id='m_start'> M Start Line: 70</div><div id='m_end'> M End Line: 93</div><div id='n_start'> N Start Line: 63</div><div id='n_end'> N End Line: 83</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def apply_on_stashed(self, stashed_theta):
         True weights are loaded into the model, and given a stashed theta 
        <a id="change">opt_state</a> = self.optimizer.state

        with torch.no_grad():
            for pg, spg in zip(self.optimizer.param_groups, stashed_theta):
                max_lr = pg[GapAwareBase.MAX_LR_NAME]
                lr = pg[&quotlr&quot]
                if max_lr &lt;= 0:
                    continue
                weight_decay = pg[&quotweight_decay&quot]
                beta1, beta2 = pg[&quotbetas&quot]
                eps = pg[&quoteps&quot]

                for p, sp in zip(pg[&quotparams&quot], spg):
                    exp_avg_sq<a id="change"> = </a><a id="change">opt_state[p][&quotexp_avg_sq&quot]</a>

                    &#47&#47 FIXME: remove assert after this works.
                    step_count = opt_state[p][&quotstep&quot] + 1
                    assert step_count == self.step_count

                    bias_correction2 = 1 - beta2**(step_count)
                    &#47&#47 calculate C coefficient per-element
                    &#47&#47 NOTE: can remove the "data". but whatever.
                    avg_steps_needed<a id="change"> = </a>max_lr<a id="change"> * \
                        </a>(((exp_avg_sq.data<a id="change"> / </a>bias_correction2)<a id="change"> ** 0.5</a>)<a id="change"> + </a>eps)

                    gap = (p - sp).abs()
</code></pre><h3>After Change</h3><pre><code class='java'>
                    &#47&#47 calculate C coefficient per-element
                    &#47&#47 NOTE: can remove the "data". but whatever.
                    avg_steps_needed = max_lr * \
                        (((<a id="change">ra[id(p)]</a>.data / bias_correction2) ** 0.5) + eps)

                    gap = (p - sp).abs()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/5ab23421874eadce8bed648b3eebccce714c7271#diff-22e803f6848fe13ddfd6473f05a519c62fd0e31bafe04851f4fcb25b161fa432L68' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 21020865</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 5ab23421874eadce8bed648b3eebccce714c7271</div><div id='time'> Time: 2020-03-17</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: pipeline/gap_aware/adamw_gap_aware.py</div><div id='m_class'> M Class Name: AdamWGapAware</div><div id='n_method'> N Class Name: AdamWGapAware</div><div id='m_method'> M Method Name: apply_on_stashed(2)</div><div id='n_method'> N Method Name: apply_on_stashed(2)</div><div id='m_parent_class'> M Parent Class: GapAwareBase</div><div id='n_parent_class'> N Parent Class: GapAwareBase</div><div id='m_file'> M File Name: pipeline/gap_aware/adamw_gap_aware.py</div><div id='n_file'> N File Name: pipeline/gap_aware/adamw_gap_aware.py</div><div id='m_start'> M Start Line: 70</div><div id='m_end'> M End Line: 93</div><div id='n_start'> N Start Line: 63</div><div id='n_end'> N End Line: 83</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def apply_on_theta(self, real_theta):

        <a id="change">opt_state</a> = self.optimizer.state

        with torch.no_grad():
            for pg, rpg in zip(self.optimizer.param_groups, real_theta):
                max_lr = pg[GapAwareBase.MAX_LR_NAME]
                lr = pg[&quotlr&quot]
                if max_lr &lt;= 0:
                    continue
                weight_decay = pg[&quotweight_decay&quot]
                beta1, beta2 = pg[&quotbetas&quot]
                eps = pg[&quoteps&quot]

                for p, rp in zip(pg[&quotparams&quot], rpg):
                    exp_avg_sq<a id="change"> = </a><a id="change">opt_state[p][&quotexp_avg_sq&quot]</a>
                    &#47&#47 FIXME: remove assert after this works.
                    step_count = opt_state[p][&quotstep&quot] + 1
                    assert step_count == self.step_count

                    bias_correction2 = 1 - beta2**(step_count)

                    &#47&#47 calculate C coefficient per-element
                    &#47&#47 NOTE: can remove the "data". but whatever.
                    avg_steps_needed<a id="change"> = </a>max_lr<a id="change"> * \
                        </a>(((exp_avg_sq.data<a id="change"> / </a>bias_correction2)<a id="change"> ** 0.5</a>)<a id="change"> + </a>eps)

                    gap = (p - rp).abs()
                    &#47&#47 pg[&quotlr&quot] * p.grad.abs()</code></pre><h3>After Change</h3><pre><code class='java'>
                    &#47&#47 calculate C coefficient per-element
                    &#47&#47 NOTE: can remove the "data". but whatever.
                    avg_steps_needed = max_lr * \
                        (((<a id="change">ra[id(p)]</a>.data / bias_correction2) ** 0.5) + eps)

                    gap = (p - rp).abs()
                    &#47&#47 pg[&quotlr&quot] * p.grad.abs()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/5ab23421874eadce8bed648b3eebccce714c7271#diff-22e803f6848fe13ddfd6473f05a519c62fd0e31bafe04851f4fcb25b161fa432L124' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 21020870</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 5ab23421874eadce8bed648b3eebccce714c7271</div><div id='time'> Time: 2020-03-17</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: pipeline/gap_aware/adamw_gap_aware.py</div><div id='m_class'> M Class Name: AdamWGapAware</div><div id='n_method'> N Class Name: AdamWGapAware</div><div id='m_method'> M Method Name: apply_on_theta(2)</div><div id='n_method'> N Method Name: apply_on_theta(2)</div><div id='m_parent_class'> M Parent Class: GapAwareBase</div><div id='n_parent_class'> N Parent Class: GapAwareBase</div><div id='m_file'> M File Name: pipeline/gap_aware/adamw_gap_aware.py</div><div id='n_file'> N File Name: pipeline/gap_aware/adamw_gap_aware.py</div><div id='m_start'> M Start Line: 126</div><div id='m_end'> M End Line: 149</div><div id='n_start'> N Start Line: 117</div><div id='n_end'> N End Line: 137</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def apply_on_theta(self, real_theta):

        <a id="change">opt_state</a> = self.optimizer.state

        with torch.no_grad():
            for pg, rpg in zip(self.optimizer.param_groups, real_theta):
                max_lr = pg[GapAwareBase.MAX_LR_NAME]
                if max_lr &lt;= 0:
                    continue
                weight_decay = pg[&quotweight_decay&quot]
                beta1, beta2 = pg[&quotbetas&quot]
                eps = pg[&quoteps&quot]

                for p, rp in zip(pg[&quotparams&quot], rpg):
                    exp_avg_sq<a id="change"> = </a><a id="change">opt_state[p][&quotexp_avg_sq&quot]</a>
                    &#47&#47 FIXME: remove assert after this works.
                    step_count = opt_state[p][&quotstep&quot] + 1
                    assert step_count == self.step_count

                    bias_correction2 = 1 - beta2**(step_count)
                    &#47&#47 if p.grad is None:
                    &#47&#47     continue
                    &#47&#47 calculate C coefficient per-element
                    &#47&#47 Note: can remove the "data". but whatever.
                    avg_steps_needed<a id="change"> = </a>max_lr<a id="change"> * \
                        </a>(((exp_avg_sq.data<a id="change"> / </a>bias_correction2)<a id="change"> ** 0.5</a>)<a id="change"> + </a>eps)

                    gap = (p - rp).abs()
                    &#47&#47 pg[&quotlr&quot] * p.grad.abs()</code></pre><h3>After Change</h3><pre><code class='java'>
                    &#47&#47 calculate C coefficient per-element
                    &#47&#47 Note: can remove the "data". but whatever.
                    avg_steps_needed = max_lr * \
                        (((<a id="change">ra[id(p)]</a>.data / bias_correction2) ** 0.5) + eps)

                    gap = (p - rp).abs()
                    &#47&#47 pg[&quotlr&quot] * p.grad.abs()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/5ab23421874eadce8bed648b3eebccce714c7271#diff-65d217c40d70981a1b6f6ed49a6deb9244789b70496f926d6c2a87a525ab3980L91' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 21020918</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 5ab23421874eadce8bed648b3eebccce714c7271</div><div id='time'> Time: 2020-03-17</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: pipeline/gap_aware/adam_gap_aware.py</div><div id='m_class'> M Class Name: AdamGapAware</div><div id='n_method'> N Class Name: AdamGapAware</div><div id='m_method'> M Method Name: apply_on_theta(2)</div><div id='n_method'> N Method Name: apply_on_theta(2)</div><div id='m_parent_class'> M Parent Class: GapAwareBase</div><div id='n_parent_class'> N Parent Class: GapAwareBase</div><div id='m_file'> M File Name: pipeline/gap_aware/adam_gap_aware.py</div><div id='n_file'> N File Name: pipeline/gap_aware/adam_gap_aware.py</div><div id='m_start'> M Start Line: 93</div><div id='m_end'> M End Line: 116</div><div id='n_start'> N Start Line: 84</div><div id='n_end'> N End Line: 103</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def apply_on_stashed(self, stashed_theta):
         True weights are loaded into the model, and given a stashed theta 
        <a id="change">opt_state</a> = self.optimizer.state

        with torch.no_grad():
            for pg, spg in zip(self.optimizer.param_groups, stashed_theta):
                max_lr = pg[GapAwareBase.MAX_LR_NAME]
                if max_lr &lt;= 0:
                    continue
                weight_decay = pg[&quotweight_decay&quot]
                beta1, beta2 = pg[&quotbetas&quot]
                eps = pg[&quoteps&quot]

                for p, sp in zip(pg[&quotparams&quot], spg):
                    exp_avg_sq<a id="change"> = </a><a id="change">opt_state[p][&quotexp_avg_sq&quot]</a>

                    &#47&#47 FIXME: remove assert after this works.
                    step_count = opt_state[p][&quotstep&quot] + 1
                    assert step_count == self.step_count

                    bias_correction2 = 1 - beta2**(step_count)
                    &#47&#47 if p.grad is None:
                    &#47&#47     continue
                    &#47&#47 calculate C coefficient per-element
                    &#47&#47 Note: can remove the "data". but whatever.
                    avg_steps_needed<a id="change"> = </a>max_lr<a id="change"> * \
                        </a>(((exp_avg_sq.data<a id="change"> / </a>bias_correction2)<a id="change"> ** 0.5</a>)<a id="change"> + </a>eps)

                    gap = (p - sp).abs()
                    &#47&#47 pg[&quotlr&quot] * p.grad.abs()</code></pre><h3>After Change</h3><pre><code class='java'>
                    &#47&#47 calculate C coefficient per-element
                    &#47&#47 Note: can remove the "data". but whatever.
                    avg_steps_needed = max_lr * \
                        (((<a id="change">ra[id(p)]</a>.data / bias_correction2) ** 0.5) + eps)

                    gap = (p - sp).abs()
                    &#47&#47 pg[&quotlr&quot] * p.grad.abs()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/5ab23421874eadce8bed648b3eebccce714c7271#diff-65d217c40d70981a1b6f6ed49a6deb9244789b70496f926d6c2a87a525ab3980L37' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 21020894</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 5ab23421874eadce8bed648b3eebccce714c7271</div><div id='time'> Time: 2020-03-17</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: pipeline/gap_aware/adam_gap_aware.py</div><div id='m_class'> M Class Name: AdamGapAware</div><div id='n_method'> N Class Name: AdamGapAware</div><div id='m_method'> M Method Name: apply_on_stashed(2)</div><div id='n_method'> N Method Name: apply_on_stashed(2)</div><div id='m_parent_class'> M Parent Class: GapAwareBase</div><div id='n_parent_class'> N Parent Class: GapAwareBase</div><div id='m_file'> M File Name: pipeline/gap_aware/adam_gap_aware.py</div><div id='n_file'> N File Name: pipeline/gap_aware/adam_gap_aware.py</div><div id='m_start'> M Start Line: 39</div><div id='m_end'> M End Line: 63</div><div id='n_start'> N Start Line: 32</div><div id='n_end'> N End Line: 53</div><BR>