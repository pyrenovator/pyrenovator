<html><h3>Pattern ID :35457
</h3><img src='100667490.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    if word_idx is None:
        raise ValueError(f&quotCouldn\&quott find "{word}" in "{prompt}"&quot)

    <a id="change">for </a>idx, <a id="change">token</a> in <a id="change">enumerate(</a>tokens<a id="change">):
        </a>merge_idxs.append(idx)

        <a id="change">if </a>&quot&lt;/w&gt;&quot in token:
            curr_token<a id="change"> += </a>token[:-4]

            if idx &gt;= word_idx and curr_token == word:
                break

            curr_token = &quot&quot
            curr_idx += 1
            merge_idxs.clear()
        else:
            curr_token<a id="change"> += </a>token
            merge_idxs.append(idx)

    return [x + 1 for x in merge_idxs], word_idx  &#47&#47 Offset by 1.</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 merge together all tokens in the word
            return [first_pos + i + offset_idx for i in range(0, token_len)]

        <a id="change">for </a>idx, w_token in enumerate(word_tokens)<a id="change">:
            &#47&#47 if the word contains more than one token
            </a>if len(w_token) &gt; len(search_tokens):
                &#47&#47 check to see if the extra tokens were from punctuation
                no_punc<a id="change"> = </a>[t for t in w_token if t not in punc_tokens]
                search_no_punc = [t for t in search_tokens if t not in punc_tokens]
                <a id="change">if </a>no_punc and <a id="change">no_punc == search_no_punc</a>:
                    merge_idxs += calc_token_positions(idx, len(search_tokens))
                    word_idx<a id="change"> = </a>idx
            elif w_token == search_tokens:
                merge_idxs<a id="change"> += </a>calc_token_positions(idx, len(search_tokens))
                word_idx = idx
    else:
        merge_idxs.append(word_idx)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 11</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/castorini/daam/commit/854e6ddfdae1781009b31b66b2d5bb15d852fccf#diff-ea4e615a144dec76002186f2cde018c58e9a14487f738b334af1572531829aaeL53' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 100667490</div><div id='project'> Project Name: castorini/daam</div><div id='commit'> Commit Name: 854e6ddfdae1781009b31b66b2d5bb15d852fccf</div><div id='time'> Time: 2022-12-08</div><div id='author'> Author: alotofcatz@gmail.com</div><div id='file'> File Name: daam/utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: compute_token_merge_indices(5)</div><div id='n_method'> N Method Name: compute_token_merge_indices(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: daam/utils.py</div><div id='n_file'> N File Name: daam/utils.py</div><div id='m_start'> M Start Line: 53</div><div id='m_end'> M End Line: 90</div><div id='n_start'> N Start Line: 54</div><div id='n_end'> N End Line: 91</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    if word_idx is None:
        raise ValueError(f&quotCouldn\&quott find "{word}" in "{prompt}"&quot)

    <a id="change">for </a>idx, <a id="change">token</a> in <a id="change">enumerate(</a>tokens<a id="change">):
        </a>merge_idxs.append(idx)

        <a id="change">if </a>&quot&lt;/w&gt;&quot in token:
            curr_token<a id="change"> += </a>token[:-4]

            if idx &gt;= word_idx and curr_token == word:
                break

            curr_token = &quot&quot
            curr_idx += 1
            merge_idxs.clear()
        else:
            curr_token<a id="change"> += </a>token
            merge_idxs.append(idx)

    return [x + 1 for x in merge_idxs], word_idx  &#47&#47 Offset by 1.</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 merge together all tokens in the word
            return [first_pos + i + offset_idx for i in range(0, token_len)]

        <a id="change">for </a>idx, <a id="change">w_token</a> in enumerate(word_tokens)<a id="change">:
            &#47&#47 if the word contains more than one token
            </a>if len(w_token) &gt; len(search_tokens):
                &#47&#47 check to see if the extra tokens were from punctuation
                no_punc<a id="change"> = </a>[t for t in w_token if t not in punc_tokens]
                search_no_punc = [t for t in search_tokens if t not in punc_tokens]
                <a id="change">if </a>no_punc and <a id="change">no_punc == search_no_punc</a>:
                    merge_idxs += calc_token_positions(idx, len(search_tokens))
                    word_idx<a id="change"> = </a>idx
            elif w_token == search_tokens:
                merge_idxs += calc_token_positions(idx, len(search_tokens))
                word_idx<a id="change"> = </a>idx
    else:
        merge_idxs.append(word_idx)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/castorini/daam/commit/854e6ddfdae1781009b31b66b2d5bb15d852fccf#diff-ea4e615a144dec76002186f2cde018c58e9a14487f738b334af1572531829aaeL52' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 100667554</div><div id='project'> Project Name: castorini/daam</div><div id='commit'> Commit Name: 854e6ddfdae1781009b31b66b2d5bb15d852fccf</div><div id='time'> Time: 2022-12-08</div><div id='author'> Author: alotofcatz@gmail.com</div><div id='file'> File Name: daam/utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: compute_token_merge_indices(5)</div><div id='n_method'> N Method Name: compute_token_merge_indices(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: daam/utils.py</div><div id='n_file'> N File Name: daam/utils.py</div><div id='m_start'> M Start Line: 53</div><div id='m_end'> M End Line: 90</div><div id='n_start'> N Start Line: 54</div><div id='n_end'> N End Line: 91</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        updates = {}
        handles = []
        self.sync_handles()
        <a id="change">for </a>i, <a id="change">m</a> in <a id="change">enumerate(</a>self.modules<a id="change">):
            if </a>i % hvd.size() == hvd.rank():
                classname = m.__class__.__name__
                if self.steps % self.TInv == 0:
                    self._update_inv(m)
                p_grad_mat = self._get_matrix_form_grad(m, classname)
                v<a id="change"> = </a>self._get_natural_grad(m, p_grad_mat, damping)
            else:
                v<a id="change"> = </a>self.get_zeros_like(m)
            &#47&#47 TODO: can we avoid the .clone() here?
            updates[m] = [x.clone() for x in v]
</code></pre><h3>After Change</h3><pre><code class='java'>
        self.sync_handles()

        if self.steps % self.TInv == 0:
            <a id="change">for m</a> in self.modules<a id="change">:
                </a>a_ranks<a id="change"> = </a>self.rank_iter.next(self.inv_block_count)
                g_ranks<a id="change"> = </a>self.rank_iter.next(self.inv_block_count)
                if hvd.rank() in a_ranks:
                    self._update_a_inv(m, damping, a_ranks)
                else:
                    self.m_aa_inv[m] = torch.zeros_like(self.m_aa[m])

                <a id="change">if hvd.rank() in g_ranks</a>:
                    self._update_g_inv(m, damping, g_ranks)
                else:
                    self.m_gg_inv[m]<a id="change"> = </a>torch.zeros_like(self.m_gg[m])
               
                if self.hvd_size &gt; 1:
                    handles.append(hvd.allreduce_async_(self.m_aa_inv[m], op=hvd.Sum))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/gpauloski/kfac_pytorch/commit/1389234e8535f66b7ce923158431d0fb6cbb3009#diff-840032dd873df62bd4f0bc4e8dc2517a4d3f20ff464523511bcb84514f486d19L146' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 100667557</div><div id='project'> Project Name: gpauloski/kfac_pytorch</div><div id='commit'> Commit Name: 1389234e8535f66b7ce923158431d0fb6cbb3009</div><div id='time'> Time: 2020-03-20</div><div id='author'> Author: gpauloski@yahoo.com</div><div id='file'> File Name: kfac/kfac.py</div><div id='m_class'> M Class Name: KFAC</div><div id='n_method'> N Class Name: KFAC</div><div id='m_method'> M Method Name: step(2)</div><div id='n_method'> N Method Name: step(2)</div><div id='m_parent_class'> M Parent Class: optim.Optimizer</div><div id='n_parent_class'> N Parent Class: optim.Optimizer</div><div id='m_file'> M File Name: kfac/kfac.py</div><div id='n_file'> N File Name: kfac/kfac.py</div><div id='m_start'> M Start Line: 150</div><div id='m_end'> M End Line: 170</div><div id='n_start'> N Start Line: 165</div><div id='n_end'> N End Line: 197</div><BR>