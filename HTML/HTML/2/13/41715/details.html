<html><h3>Pattern ID :41715
</h3><img src='117007480.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        normalize
    ])

    dataset<a id="change"> = datasets.__dict__[args.data]</a>
    partial_dataset = <a id="change">partial(</a>dataset<a id="change">)</a>
    train_source_dataset = dataset(root=args.root, task=args.source, download=True, transform=train_transform)
    train_source_loader = DataLoader(train_source_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    train_target_dataset = partial_dataset(root=args.root, task=args.target, download=True, transform=train_transform)</code></pre><h3>After Change</h3><pre><code class='java'>
    cudnn.benchmark = True

    &#47&#47 Data loading code
    train_transform = <a id="change">utils.get_train_transform(</a>args.train_resizing<a id="change">, random_horizontal_flip=True,
                                                random_color_jitter=False)</a>
    val_transform<a id="change"> = utils</a><a id="change">.get_val_transform(</a>args.val_resizing<a id="change">)</a>
    <a id="change">print("train_transform: "</a>, train_transform<a id="change">)</a>
    <a id="change">print("val_transform: "</a>, val_transform<a id="change">)</a>

    train_source_dataset, train_target_dataset, val_dataset, test_dataset, num_classes, args.class_names = \
        utils.get_dataset(args.data, args.root, args.source, args.target, train_transform, val_transform)
    train_source_loader = DataLoader(train_source_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)

    &#47&#47 create model
    print("=&gt; using pre-trained model &quot{}&quot".format(args.arch))
    backbone = utils.get_model(args.arch)
    pool_layer = nn.Identity() if args.no_pool else None
    if args.data == &quotImageNetCaltech&quot:
        classifier = Classifier(backbone, num_classes, head=backbone.copy_head(), pool_layer=pool_layer).to(device)
    else:
        classifier = ImageClassifier(backbone, num_classes, args.bottleneck_dim, pool_layer=pool_layer).to(device)

    domain_discri = DomainDiscriminator(in_feature=classifier.features_dim, hidden_size=1024).to(device)

    &#47&#47 define optimizer and lr scheduler
    optimizer = SGD(classifier.get_parameters() + domain_discri.get_parameters(),
                    args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True)
    lr_scheduler = LambdaLR(optimizer, lambda x:  args.lr * (1. + args.lr_gamma * float(x)) ** (-args.lr_decay))

    &#47&#47 define loss function
    domain_adv = DomainAdversarialLoss(domain_discri).to(device)

    &#47&#47 resume from the best checkpoint
    if args.phase != &quottrain&quot:
        checkpoint = torch.load(logger.get_checkpoint_path(&quotbest&quot), map_location=&quotcpu&quot)
        classifier.load_state_dict(checkpoint)

    &#47&#47 analysis the model
    if args.phase == &quotanalysis&quot:
        &#47&#47 extract features from both domains
        feature_extractor = nn.Sequential(classifier.backbone, classifier.bottleneck).to(device)
        source_feature = collect_feature(train_source_loader, feature_extractor, device)
        target_feature = collect_feature(train_target_loader, feature_extractor, device)
        &#47&#47 plot t-SNE
        tSNE_filename = osp.join(logger.visualize_directory, &quotTSNE.png&quot)
        tsne.visualize(source_feature, target_feature, tSNE_filename)
        print("Saving t-SNE to", tSNE_filename)
        &#47&#47 calculate A-distance, which is a measure for distribution discrepancy
        A_distance = a_distance.calculate(source_feature, target_feature, device)
        print("A-distance =", A_distance)
        return

    if args.phase == &quottest&quot:
        acc1 = utils.validate(test_loader, classifier, args, device)
        print(acc1)
        return

    &#47&#47 start training
    best_acc1 = 0.
    for epoch in range(args.epochs):
        &#47&#47 train for one epoch
        train(train_source_iter, train_target_iter, classifier, domain_adv, optimizer,
              lr_scheduler, epoch, args)

        &#47&#47 evaluate on validation set
        acc1 = utils.validate(val_loader, classifier, args, device)

        &#47&#47 remember best acc@1 and save checkpoint
        torch.save(classifier.state_dict(), logger.get_checkpoint_path(&quotlatest&quot))
        if acc1 &gt; best_acc1:
            shutil.copy(logger.get_checkpoint_path(&quotlatest&quot), logger.get_checkpoint_path(&quotbest&quot))
        best_acc1 = max(acc1, best_acc1)

    print("best_acc1 = {:3.1f}".format(best_acc1))

    &#47&#47 evaluate on test set
    classifier.load_state_dict(torch.load(logger.get_checkpoint_path(&quotbest&quot)))
    acc1<a id="change"> = utils</a><a id="change">.validate(</a>test_loader, classifier, args, device<a id="change">)</a>
    print("test_acc1 = {:3.1f}".format(acc1))

    logger.close()
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 10</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/thuml/transfer-learning-library/commit/1d780e2bcc7b08075283ebaf2160f7f2b09dabef#diff-29b94e9a2a9d9fe0095d2d0f1086eb0d625253c05c2026392e4ae4cc19cb2e84L50' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 117007480</div><div id='project'> Project Name: thuml/transfer-learning-library</div><div id='commit'> Commit Name: 1d780e2bcc7b08075283ebaf2160f7f2b09dabef</div><div id='time'> Time: 2021-08-11</div><div id='author'> Author: 13126830206@163.com</div><div id='file'> File Name: examples/domain_adaptation/partial/dann.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/domain_adaptation/partial/dann.py</div><div id='n_file'> N File Name: examples/domain_adaptation/partial/dann.py</div><div id='m_start'> M Start Line: 52</div><div id='m_end'> M End Line: 160</div><div id='n_start'> N Start Line: 50</div><div id='n_end'> N End Line: 132</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        normalize
    ])

    dataset<a id="change"> = datasets.__dict__[args.data]</a>
    partial_dataset = <a id="change">partial(</a>dataset<a id="change">)</a>
    train_source_dataset = dataset(root=args.root, task=args.source, download=True, transform=train_transform)
    train_source_loader = DataLoader(train_source_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    train_target_dataset = partial_dataset(root=args.root, task=args.target, download=True, transform=train_transform)</code></pre><h3>After Change</h3><pre><code class='java'>
    cudnn.benchmark = True

    &#47&#47 Data loading code
    train_transform = <a id="change">utils.get_train_transform(</a>args.train_resizing<a id="change">, random_horizontal_flip=True,
                                                random_color_jitter=False)</a>
    val_transform<a id="change"> = </a><a id="change">utils.get_val_transform(</a>args.val_resizing<a id="change">)</a>
    <a id="change">print("train_transform: "</a>, train_transform<a id="change">)</a>
    <a id="change">print("val_transform: "</a>, val_transform<a id="change">)</a>

    train_source_dataset, train_target_dataset, val_dataset, test_dataset, num_classes, args.class_names = \
        utils.get_dataset(args.data, args.root, args.source, args.target, train_transform, val_transform)
    train_source_loader = DataLoader(train_source_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)

    &#47&#47 create model
    print("=&gt; using pre-trained model &quot{}&quot".format(args.arch))
    backbone = utils.get_model(args.arch)
    pool_layer = nn.Identity() if args.no_pool else None
    if args.data == &quotImageNetCaltech&quot:
        classifier = Classifier(backbone, num_classes, head=backbone.copy_head(), pool_layer=pool_layer).to(device)
    else:
        classifier = ImageClassifier(backbone, num_classes, args.bottleneck_dim, pool_layer=pool_layer).to(device)

    &#47&#47 define domain classifier D, D_0
    D = DomainDiscriminator(in_feature=classifier.features_dim, hidden_size=1024, batch_norm=False).to(device)
    D_0 = DomainDiscriminator(in_feature=classifier.features_dim, hidden_size=1024, batch_norm=False).to(device)

    &#47&#47 define optimizer and lr scheduler
    optimizer = SGD(classifier.get_parameters() + D.get_parameters() + D_0.get_parameters(),
                    args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True)
    lr_scheduler = LambdaLR(optimizer, lambda x: args.lr * (1. + args.lr_gamma * float(x)) ** (-args.lr_decay))

    &#47&#47 define loss function
    domain_adv_D = DomainAdversarialLoss(D).to(device)
    domain_adv_D_0 = DomainAdversarialLoss(D_0).to(device)
    &#47&#47 define importance weight module
    importance_weight_module = ImportanceWeightModule(D, train_target_dataset.partial_classes_idx)

    &#47&#47 resume from the best checkpoint
    if args.phase != &quottrain&quot:
        checkpoint = torch.load(logger.get_checkpoint_path(&quotbest&quot), map_location=&quotcpu&quot)
        classifier.load_state_dict(checkpoint)

    &#47&#47 analysis the model
    if args.phase == &quotanalysis&quot:
        &#47&#47 extract features from both domains
        feature_extractor = nn.Sequential(classifier.backbone, classifier.bottleneck).to(device)
        source_feature = collect_feature(train_source_loader, feature_extractor, device)
        target_feature = collect_feature(train_target_loader, feature_extractor, device)
        &#47&#47 plot t-SNE
        tSNE_filename = osp.join(logger.visualize_directory, &quotTSNE.png&quot)
        tsne.visualize(source_feature, target_feature, tSNE_filename)
        print("Saving t-SNE to", tSNE_filename)
        &#47&#47 calculate A-distance, which is a measure for distribution discrepancy
        A_distance = a_distance.calculate(source_feature, target_feature, device)
        print("A-distance =", A_distance)
        return

    if args.phase == &quottest&quot:
        acc1 = utils.validate(test_loader, classifier, args, device)
        print(acc1)
        return

    &#47&#47 start training
    best_acc1 = 0.
    for epoch in range(args.epochs):
        &#47&#47 train for one epoch
        train(train_source_iter, train_target_iter, classifier, domain_adv_D, domain_adv_D_0,
              importance_weight_module, optimizer, lr_scheduler, epoch, args)

        &#47&#47 evaluate on validation set
        acc1 = utils.validate(val_loader, classifier, args, device)

        &#47&#47 remember best acc@1 and save checkpoint
        torch.save(classifier.state_dict(), logger.get_checkpoint_path(&quotlatest&quot))
        if acc1 &gt; best_acc1:
            shutil.copy(logger.get_checkpoint_path(&quotlatest&quot), logger.get_checkpoint_path(&quotbest&quot))
        best_acc1 = max(acc1, best_acc1)

    print("best_acc1 = {:3.1f}".format(best_acc1))

    &#47&#47 evaluate on test set
    classifier.load_state_dict(torch.load(logger.get_checkpoint_path(&quotbest&quot)))
    acc1<a id="change"> = </a><a id="change">utils.validate(</a>test_loader, classifier, args, device<a id="change">)</a>
    print("test_acc1 = {:3.1f}".format(acc1))

    logger.close()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thuml/transfer-learning-library/commit/1d780e2bcc7b08075283ebaf2160f7f2b09dabef#diff-b5084efd787f8b65d89cb95f67d7cdb4159cdd60ff54fbbd2e8c674ba320ca7eL37' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 117007477</div><div id='project'> Project Name: thuml/transfer-learning-library</div><div id='commit'> Commit Name: 1d780e2bcc7b08075283ebaf2160f7f2b09dabef</div><div id='time'> Time: 2021-08-11</div><div id='author'> Author: 13126830206@163.com</div><div id='file'> File Name: examples/domain_adaptation/partial/iwan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/domain_adaptation/partial/iwan.py</div><div id='n_file'> N File Name: examples/domain_adaptation/partial/iwan.py</div><div id='m_start'> M Start Line: 54</div><div id='m_end'> M End Line: 166</div><div id='n_start'> N Start Line: 52</div><div id='n_end'> N End Line: 139</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        normalize
    ])

    dataset<a id="change"> = datasets.__dict__[args.data]</a>
    partial_dataset = <a id="change">partial(</a>dataset<a id="change">)</a>
    train_source_dataset = dataset(root=args.root, task=args.source, download=True, transform=train_transform)
    train_source_loader = DataLoader(train_source_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    train_target_dataset = partial_dataset(root=args.root, task=args.target, download=True, transform=train_transform)</code></pre><h3>After Change</h3><pre><code class='java'>
    cudnn.benchmark = True

    &#47&#47 Data loading code
    train_transform = <a id="change">utils.get_train_transform(</a>args.train_resizing<a id="change">, random_horizontal_flip=True,
                                                random_color_jitter=False)</a>
    val_transform<a id="change"> = </a><a id="change">utils.get_val_transform(</a>args.val_resizing<a id="change">)</a>
    <a id="change">print("train_transform: "</a>, train_transform<a id="change">)</a>
    <a id="change">print("val_transform: "</a>, val_transform<a id="change">)</a>

    train_source_dataset, train_target_dataset, val_dataset, test_dataset, num_classes, args.class_names = \
        utils.get_dataset(args.data, args.root, args.source, args.target, train_transform, val_transform)
    train_source_loader = DataLoader(train_source_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)

    &#47&#47 create model
    print("=&gt; using pre-trained model &quot{}&quot".format(args.arch))
    backbone = utils.get_model(args.arch)
    pool_layer = nn.Identity() if args.no_pool else None
    if args.data == &quotImageNetCaltech&quot:
        classifier = Classifier(backbone, num_classes, pool_layer=pool_layer, head=backbone.copy_head()).to(device)
    else:
        classifier = ImageClassifier(backbone, num_classes, args.bottleneck_dim, pool_layer=pool_layer).to(device)
    domain_discri = DomainDiscriminator(in_feature=classifier.features_dim, hidden_size=1024).to(device)
    class_weight_module = AutomaticUpdateClassWeightModule(args.class_weight_update_steps, train_target_loader,
                                                           classifier, num_classes, device, args.temperature,
                                                           train_target_dataset.partial_classes_idx)
    &#47&#47 define optimizer and lr scheduler
    optimizer = SGD(classifier.get_parameters() + domain_discri.get_parameters(),
                    args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True)
    lr_scheduler = LambdaLR(optimizer, lambda x:  args.lr * (1. + args.lr_gamma * float(x)) ** (-args.lr_decay))

    &#47&#47 define loss function
    domain_adv = DomainAdversarialLoss(domain_discri).to(device)

    &#47&#47 resume from the best checkpoint
    if args.phase != &quottrain&quot:
        checkpoint = torch.load(logger.get_checkpoint_path(&quotbest&quot), map_location=&quotcpu&quot)
        classifier.load_state_dict(checkpoint)

    &#47&#47 analysis the model
    if args.phase == &quotanalysis&quot:
        &#47&#47 extract features from both domains
        feature_extractor = nn.Sequential(classifier.backbone, classifier.bottleneck).to(device)
        source_feature = collect_feature(train_source_loader, feature_extractor, device)
        target_feature = collect_feature(train_target_loader, feature_extractor, device)
        &#47&#47 plot t-SNE
        tSNE_filename = osp.join(logger.visualize_directory, &quotTSNE.png&quot)
        tsne.visualize(source_feature, target_feature, tSNE_filename)
        print("Saving t-SNE to", tSNE_filename)
        &#47&#47 calculate A-distance, which is a measure for distribution discrepancy
        A_distance = a_distance.calculate(source_feature, target_feature, device)
        print("A-distance =", A_distance)
        return

    if args.phase == &quottest&quot:
        acc1 = utils.validate(test_loader, classifier, args, device)
        print(acc1)
        return

    &#47&#47 start training
    best_acc1 = 0.
    for epoch in range(args.epochs):
        &#47&#47 train for one epoch
        train(train_source_iter, train_target_iter, classifier, domain_adv, class_weight_module,
              optimizer, lr_scheduler, epoch, args)

        &#47&#47 evaluate on validation set
        acc1 = utils.validate(val_loader, classifier, args, device)

        &#47&#47 remember best acc@1 and save checkpoint
        torch.save(classifier.state_dict(), logger.get_checkpoint_path(&quotlatest&quot))
        if acc1 &gt; best_acc1:
            shutil.copy(logger.get_checkpoint_path(&quotlatest&quot), logger.get_checkpoint_path(&quotbest&quot))
        best_acc1 = max(acc1, best_acc1)

    print("best_acc1 = {:3.1f}".format(best_acc1))

    &#47&#47 evaluate on test set
    classifier.load_state_dict(torch.load(logger.get_checkpoint_path(&quotbest&quot)))
    acc1<a id="change"> = </a><a id="change">utils.validate(</a>test_loader, classifier, args, device<a id="change">)</a>
    print("test_acc1 = {:3.1f}".format(acc1))

    logger.close()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thuml/transfer-learning-library/commit/1d780e2bcc7b08075283ebaf2160f7f2b09dabef#diff-281a34aa1a63e9e005c18e81c3969887242045b4345f90b1b0f97ce304f44579L36' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 117007476</div><div id='project'> Project Name: thuml/transfer-learning-library</div><div id='commit'> Commit Name: 1d780e2bcc7b08075283ebaf2160f7f2b09dabef</div><div id='time'> Time: 2021-08-11</div><div id='author'> Author: 13126830206@163.com</div><div id='file'> File Name: examples/domain_adaptation/partial/pada.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/domain_adaptation/partial/pada.py</div><div id='n_file'> N File Name: examples/domain_adaptation/partial/pada.py</div><div id='m_start'> M Start Line: 53</div><div id='m_end'> M End Line: 162</div><div id='n_start'> N Start Line: 51</div><div id='n_end'> N End Line: 134</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        normalize
    ])

    dataset<a id="change"> = datasets.__dict__[args.data]</a>
    partial_dataset = <a id="change">partial(</a>dataset<a id="change">)</a>
    train_source_dataset = dataset(root=args.root, task=args.source, download=True, transform=train_transform)
    train_source_loader = DataLoader(train_source_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    train_target_dataset = partial_dataset(root=args.root, task=args.target, download=True, transform=train_transform)</code></pre><h3>After Change</h3><pre><code class='java'>
    cudnn.benchmark = True

    &#47&#47 Data loading code
    train_transform = <a id="change">utils.get_train_transform(</a>args.train_resizing<a id="change">, random_horizontal_flip=True,
                                                random_color_jitter=False)</a>
    val_transform<a id="change"> = </a><a id="change">utils.get_val_transform(</a>args.val_resizing<a id="change">)</a>
    <a id="change">print("train_transform: "</a>, train_transform<a id="change">)</a>
    <a id="change">print("val_transform: "</a>, val_transform<a id="change">)</a>

    train_source_dataset, train_target_dataset, val_dataset, test_dataset, num_classes, args.class_names = \
        utils.get_dataset(args.data, args.root, args.source, args.target, train_transform, val_transform)
    train_source_loader = DataLoader(train_source_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)

    &#47&#47 create model
    print("=&gt; using pre-trained model &quot{}&quot".format(args.arch))
    backbone = utils.get_model(args.arch)
    pool_layer = nn.Identity() if args.no_pool else None
    backbone = models.__dict__[args.arch](pretrained=True)
    classifier = ImageClassifier(backbone, train_source_dataset.num_classes, args.num_blocks,
                                 bottleneck_dim=args.bottleneck_dim, dropout_p=args.dropout_p, pool_layer=pool_layer).to(device)
    adaptive_feature_norm = AdaptiveFeatureNorm(args.delta).to(device)

    &#47&#47 define optimizer
    &#47&#47 the learning rate is fixed according to origin paper
    optimizer = SGD(classifier.get_parameters(), args.lr, weight_decay=args.weight_decay)

    &#47&#47 resume from the best checkpoint
    if args.phase != &quottrain&quot:
        checkpoint = torch.load(logger.get_checkpoint_path(&quotbest&quot), map_location=&quotcpu&quot)
        classifier.load_state_dict(checkpoint)

    &#47&#47 analysis the model
    if args.phase == &quotanalysis&quot:
        &#47&#47 extract features from both domains
        feature_extractor = nn.Sequential(classifier.backbone, classifier.bottleneck).to(device)
        source_feature = collect_feature(train_source_loader, feature_extractor, device)
        target_feature = collect_feature(train_target_loader, feature_extractor, device)
        &#47&#47 plot t-SNE
        tSNE_filename = osp.join(logger.visualize_directory, &quotTSNE.png&quot)
        tsne.visualize(source_feature, target_feature, tSNE_filename)
        print("Saving t-SNE to", tSNE_filename)
        &#47&#47 calculate A-distance, which is a measure for distribution discrepancy
        A_distance = a_distance.calculate(source_feature, target_feature, device)
        print("A-distance =", A_distance)
        return

    if args.phase == &quottest&quot:
        acc1 = utils.validate(test_loader, classifier, args, device)
        print(acc1)
        return

    &#47&#47 start training
    best_acc1 = 0.
    for epoch in range(args.epochs):
        &#47&#47 train for one epoch
        train(train_source_iter, train_target_iter, classifier, adaptive_feature_norm, optimizer, epoch, args)

        &#47&#47 evaluate on validation set
        acc1 = utils.validate(val_loader, classifier, args, device)

        &#47&#47 remember best acc@1 and save checkpoint
        torch.save(classifier.state_dict(), logger.get_checkpoint_path(&quotlatest&quot))
        if acc1 &gt; best_acc1:
            shutil.copy(logger.get_checkpoint_path(&quotlatest&quot), logger.get_checkpoint_path(&quotbest&quot))
        best_acc1 = max(acc1, best_acc1)

    print("best_acc1 = {:3.1f}".format(best_acc1))

    &#47&#47 evaluate on test set
    classifier.load_state_dict(torch.load(logger.get_checkpoint_path(&quotbest&quot)))
    acc1<a id="change"> = </a><a id="change">utils.validate(</a>test_loader, classifier, args, device<a id="change">)</a>
    print("test_acc1 = {:3.1f}".format(acc1))

    logger.close()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thuml/transfer-learning-library/commit/1d780e2bcc7b08075283ebaf2160f7f2b09dabef#diff-545822ebc2897b26baf24a2a7cdfd14907922b30533c4ab6581e8d6959289e43L33' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 117007478</div><div id='project'> Project Name: thuml/transfer-learning-library</div><div id='commit'> Commit Name: 1d780e2bcc7b08075283ebaf2160f7f2b09dabef</div><div id='time'> Time: 2021-08-11</div><div id='author'> Author: 13126830206@163.com</div><div id='file'> File Name: examples/domain_adaptation/partial/afn.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/domain_adaptation/partial/afn.py</div><div id='n_file'> N File Name: examples/domain_adaptation/partial/afn.py</div><div id='m_start'> M Start Line: 50</div><div id='m_end'> M End Line: 149</div><div id='n_start'> N Start Line: 53</div><div id='n_end'> N End Line: 128</div><BR>