<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if self._scaler is not None:
            check.false(self._scaler.is_enabled(), "Do not mix APEX with PyTorch AMP")

        <a id="change">check.false(self._use_apex</a>, <a id="change">"Please only call configure_apex_amp once."</a><a id="change">)</a>
        if self.distributed.size &gt; 1:
            check.eq(
                num_losses,
                1,</code></pre><h3>After Change</h3><pre><code class='java'>
        if self._scaler is not None and self._scaler.is_enabled():
            raise det.errors.InvalidExperimentException("Do not mix APEX with PyTorch AMP.")

        <a id="change">if self._use_apex</a>:
            raise det.errors.InvalidExperimentException("Please only call configure_apex_amp once.")

        if self.distributed.size &gt; 1:
            if num_losses != 1:
                <a id="change">raise </a><a id="change">det.errors.InvalidExperimentException(
                    "When using distributed training, "
                    "Determined only supports configure_apex_amp with num_losses = 1."</a><a id="change">,
                )</a>
            <a id="change">if self._aggregation_frequency &gt; 1</a>:
                <a id="change">raise det.errors.InvalidExperimentException(
                    "context.configure_apex_amp is not supported with "
                    "distributed training and "
                    "aggregation frequency &gt; 1."</a><a id="change">,
                )</a>

        if not torch.cuda.is_available():
            raise det.errors.InvalidExperimentException(
                "context.configure_apex_amp is supported only on GPU slots.",</code></pre>