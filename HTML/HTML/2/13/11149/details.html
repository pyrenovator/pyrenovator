<html><h3>Pattern ID :11149
</h3><img src='38204582.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if self._scaler is not None:
            check.false(self._scaler.is_enabled(), "Do not mix APEX with PyTorch AMP")

        <a id="change">check.false(self._use_apex</a>, <a id="change">"Please only call configure_apex_amp once."</a><a id="change">)</a>
        if self.distributed.size &gt; 1:
            check.eq(
                num_losses,
                1,</code></pre><h3>After Change</h3><pre><code class='java'>
        if self._scaler is not None and self._scaler.is_enabled():
            raise det.errors.InvalidExperimentException("Do not mix APEX with PyTorch AMP.")

        <a id="change">if self._use_apex</a>:
            raise det.errors.InvalidExperimentException("Please only call configure_apex_amp once.")

        if self.distributed.size &gt; 1:
            if num_losses != 1:
                <a id="change">raise </a><a id="change">det.errors.InvalidExperimentException(
                    "When using distributed training, "
                    "Determined only supports configure_apex_amp with num_losses = 1."</a><a id="change">,
                )</a>
            <a id="change">if self._aggregation_frequency &gt; 1</a>:
                <a id="change">raise det.errors.InvalidExperimentException(
                    "context.configure_apex_amp is not supported with "
                    "distributed training and "
                    "aggregation frequency &gt; 1."</a><a id="change">,
                )</a>

        if not torch.cuda.is_available():
            raise det.errors.InvalidExperimentException(
                "context.configure_apex_amp is supported only on GPU slots.",</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/determined-ai/determined/commit/dd59244bf78d4c17ff5dfeeac1c47b759900f5bf#diff-d0efbe04f5f1ad69350b2a3257125b2bec5dfc90a822e95089384fdf0fbd25d8L462' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 38204582</div><div id='project'> Project Name: determined-ai/determined</div><div id='commit'> Commit Name: dd59244bf78d4c17ff5dfeeac1c47b759900f5bf</div><div id='time'> Time: 2022-09-22</div><div id='author'> Author: 103537968+drh-determined-ai@users.noreply.github.com</div><div id='file'> File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='m_class'> M Class Name: PyTorchTrialContext</div><div id='n_method'> N Class Name: PyTorchTrialContext</div><div id='m_method'> M Method Name: configure_apex_amp(15)</div><div id='n_method'> N Method Name: configure_apex_amp(15)</div><div id='m_parent_class'> M Parent Class: det.TrialContext,pytorch._PyTorchReducerContext</div><div id='n_parent_class'> N Parent Class: det.TrialContext,pytorch._PyTorchReducerContext</div><div id='m_file'> M File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='n_file'> N File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='m_start'> M Start Line: 462</div><div id='m_end'> M End Line: 488</div><div id='n_start'> N Start Line: 473</div><div id='n_end'> N End Line: 497</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        check.true(HAVE_AMP, "Failed to import torch.cuda.amp. PyTorch &gt;= 1.6 required.")

        <a id="change">check.false(self._use_apex</a>, <a id="change">"Do not mix APEX with PyTorch AMP."</a><a id="change">)</a>

        check.is_none(self._scaler, "Please only call wrap_scaler or use_amp once.")

        check.true(len(self.models) == 0, "Please call wrap_scaler before wrap_model.")</code></pre><h3>After Change</h3><pre><code class='java'>
                "Using context.wrap_scaler() requires PyTorch &gt;= 1.6.",
            )

        <a id="change">if self._use_apex</a>:
            <a id="change">raise </a><a id="change">det.errors.InvalidExperimentException("Do not mix APEX with PyTorch AMP."</a><a id="change">)</a>

        <a id="change">if self._scaler is not None</a>:
            <a id="change">raise det.errors.InvalidExperimentException(
                "Please only call wrap_scaler or use_amp once."</a><a id="change">,
            )</a>

        if self.models:
            raise det.errors.InvalidExperimentException(
                "Please call wrap_scaler before wrap_model.",</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/determined-ai/determined/commit/dd59244bf78d4c17ff5dfeeac1c47b759900f5bf#diff-d0efbe04f5f1ad69350b2a3257125b2bec5dfc90a822e95089384fdf0fbd25d8L354' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 38204579</div><div id='project'> Project Name: determined-ai/determined</div><div id='commit'> Commit Name: dd59244bf78d4c17ff5dfeeac1c47b759900f5bf</div><div id='time'> Time: 2022-09-22</div><div id='author'> Author: 103537968+drh-determined-ai@users.noreply.github.com</div><div id='file'> File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='m_class'> M Class Name: PyTorchTrialContext</div><div id='n_method'> N Class Name: PyTorchTrialContext</div><div id='m_method'> M Method Name: wrap_scaler(2)</div><div id='n_method'> N Method Name: wrap_scaler(2)</div><div id='m_parent_class'> M Parent Class: det.TrialContext,pytorch._PyTorchReducerContext</div><div id='n_parent_class'> N Parent Class: det.TrialContext,pytorch._PyTorchReducerContext</div><div id='m_file'> M File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='n_file'> N File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='m_start'> M Start Line: 371</div><div id='m_end'> M End Line: 382</div><div id='n_start'> N Start Line: 374</div><div id='n_end'> N End Line: 395</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        
        if self.env.managed_training:
            <a id="change">check.false(self._use_apex</a>, <a id="change">"Must call wrap_optimizer() before configure_apex_amp."</a><a id="change">)</a>
            check.gt_eq(
                backward_passes_per_step,
                1,
                "backward_passes_per_step for local gradient aggregation must be &gt;= 1",</code></pre><h3>After Change</h3><pre><code class='java'>

        
        if self.env.managed_training:
            <a id="change">if self._use_apex</a>:
                <a id="change">raise </a><a id="change">det.errors.InvalidExperimentException(
                    "Must call wrap_optimizer() before configure_apex_amp."</a><a id="change">,
                )</a>
            <a id="change">if backward_passes_per_step &lt; 1</a>:
                <a id="change">raise det.errors.InvalidExperimentException(
                    "backward_passes_per_step for local gradient aggregation must be &gt;= 1; "
                    f"got {backward_passes_per_step}."</a><a id="change">,
                )</a>

            if self.distributed.size &gt; 1 and self._distributed_backend.use_horovod():
                optimizer = hvd.DistributedOptimizer(
                    optimizer,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/determined-ai/determined/commit/dd59244bf78d4c17ff5dfeeac1c47b759900f5bf#diff-d0efbe04f5f1ad69350b2a3257125b2bec5dfc90a822e95089384fdf0fbd25d8L197' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 38204589</div><div id='project'> Project Name: determined-ai/determined</div><div id='commit'> Commit Name: dd59244bf78d4c17ff5dfeeac1c47b759900f5bf</div><div id='time'> Time: 2022-09-22</div><div id='author'> Author: 103537968+drh-determined-ai@users.noreply.github.com</div><div id='file'> File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='m_class'> M Class Name: PyTorchTrialContext</div><div id='n_method'> N Class Name: PyTorchTrialContext</div><div id='m_method'> M Method Name: wrap_optimizer(3)</div><div id='n_method'> N Method Name: wrap_optimizer(3)</div><div id='m_parent_class'> M Parent Class: det.TrialContext,pytorch._PyTorchReducerContext</div><div id='n_parent_class'> N Parent Class: det.TrialContext,pytorch._PyTorchReducerContext</div><div id='m_file'> M File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='n_file'> N File Name: harness/determined/pytorch/_pytorch_context.py</div><div id='m_start'> M Start Line: 228</div><div id='m_end'> M End Line: 233</div><div id='n_start'> N Start Line: 230</div><div id='n_end'> N End Line: 240</div><BR>