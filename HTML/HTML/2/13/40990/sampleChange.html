<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            output_ = output_.cuda()
            output_.spec.dist_spec.process_group = self.process_group
        output = output_.convert_to_dist_spec(distspec.shard(self.process_group, [0], [self.world_size]))
        <a id="change">return </a>output


class FusedDenseModules(nn.Module):</code></pre><h3>After Change</h3><pre><code class='java'>
        self.world_size = gpc.get_world_size(parallel_mode)

    def forward(self, sparse_features):
        keys = <a id="change">sparse_features.keys()</a>
        assert len(keys) == len(self.offsets), f"keys len: {len(keys)}, offsets len: {len(self.offsets)}"

        <a id="change">sparse_dict = sparse_features</a><a id="change">.to_dict()</a>
        flattened_sparse_features = torch.cat(
            [<a id="change">sparse_dict[key].values()</a> + offset for key, offset in zip(keys, self.offsets)])
        batch_offsets<a id="change"> = sparse_features</a><a id="change">.offsets()</a>

        batch_size<a id="change"> = </a>len(sparse_features.lengths()) // len(keys)
        flattened_sparse_features = self.embed(flattened_sparse_features, batch_offsets)

        if self.output_device == &quotcuda&quot and self.offsets.device.type == &quotcpu&quot:
            flattened_sparse_features = flattened_sparse_features.cuda()
            flattened_sparse_features.spec.dist_spec.process_group = self.group
        &#47&#47 [batch size * feature size, hidden size / world size]
        &#47&#47 it must convert to [batch size, feature size, hidden size / world size] before all to all
        flattened_sparse_features<a id="change"> = </a>flattened_sparse_features.view(batch_size, len(keys), -1)
        &#47&#47 print(f"before shape: {flattened_sparse_features.shape}, spec: {flattened_sparse_features.spec.dist_spec}")
        flattened_sparse_features = flattened_sparse_features.convert_to_dist_spec(
            distspec.shard(self.group, [0], [self.world_size]))
        &#47&#47 print(f"after shape: {flattened_sparse_features.shape}, spec: {flattened_sparse_features.spec.dist_spec}")
        <a id="change">return </a>flattened_sparse_features


class FusedDenseModules(nn.Module):</code></pre>