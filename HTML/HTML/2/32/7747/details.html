<html><h3>Pattern ID :7747
</h3><img src='27580836.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        prefix: str = "",
        keep_vars: bool = False,
    ) -&gt; Dict[str, Any]:
        <a id="change">if destination is None</a>:
            <a id="change">destination = </a><a id="change">OrderedDict()</a>
            &#47&#47 pyre-ignore [16]
            destination._metadata<a id="change"> = </a><a id="change">OrderedDict()</a>

        <a id="change">for </a>config, <a id="change">emb_module</a> in <a id="change">zip(
            </a>self._config.embedding_tables,
            self._emb_modules<a id="change">,
        )</a><a id="change">:
            key = </a>prefix<a id="change"> + f"{config.name}.weight"</a>
            param = emb_module.weight if keep_vars else emb_module.weight.data
            <a id="change">assert </a>config.local_rows == param.size(0)
            <a id="change">assert </a>config.local_cols == param.size(1)
            <a id="change">if config.global_metadata is not None</a>:
                &#47&#47 set additional field of sharded tensor based on local tensor properties
                config.global_metadata.tensor_properties.dtype<a id="change"> = </a>param.dtype
                config.global_metadata.tensor_properties.requires_grad<a id="change"> = </a>(
                    param.requires_grad
                )
                <a id="change">destination[
                    key
                ]</a><a id="change"> = ShardedTensor._init_from_local_shards_and_global_metadata(
                    local_shards=[Shard(param, config.local_metadata)],
                    sharded_tensor_metadata=config.global_metadata,
                    process_group=self._pg,
                )</a>
            else:
                <a id="change">destination[key]</a><a id="change"> = </a>param

        <a id="change">return destination</a>

    def named_parameters(
        self, prefix: str = "", recurse: bool = True
    ) -&gt; Iterator[Tuple[str, nn.Parameter]]:</code></pre><h3>After Change</h3><pre><code class='java'>
        prefix: str = "",
        keep_vars: bool = False,
    ) -&gt; Dict[str, Any]:
        params<a id="change"> = </a><a id="change">[
            emb_module.weight if keep_vars else emb_module.weight.data
            for emb_module in self._emb_modules
        ]</a>
        <a id="change">return </a><a id="change">_get_state_dict(
            </a>self._config.embedding_tables, params, self._pg, destination, prefix<a id="change">
        )</a>

    def named_parameters(
        self, prefix: str = "", recurse: bool = True
    ) -&gt; Iterator[Tuple[str, nn.Parameter]]:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 27</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/a4440f145ac9df2ba467e9f5d591b4773779d28f#diff-19b6f02a42a3b6b147a160975a984ac3c98ebffdc15b720887f337ede50257b7L375' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 27580836</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: a4440f145ac9df2ba467e9f5d591b4773779d28f</div><div id='time'> Time: 2022-01-06</div><div id='author'> Author: yingliufb@fb.com</div><div id='file'> File Name: torchrec/distributed/embedding_lookup.py</div><div id='m_class'> M Class Name: GroupedEmbedding</div><div id='n_method'> N Class Name: GroupedEmbedding</div><div id='m_method'> M Method Name: state_dict(4)</div><div id='n_method'> N Method Name: state_dict(4)</div><div id='m_parent_class'> M Parent Class: BaseEmbedding</div><div id='n_parent_class'> N Parent Class: BaseEmbedding</div><div id='m_file'> M File Name: torchrec/distributed/embedding_lookup.py</div><div id='n_file'> N File Name: torchrec/distributed/embedding_lookup.py</div><div id='m_start'> M Start Line: 375</div><div id='m_end'> M End Line: 404</div><div id='n_start'> N Start Line: 431</div><div id='n_end'> N End Line: 437</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        prefix: str = "",
        keep_vars: bool = False,
    ) -&gt; Dict[str, Any]:
        <a id="change">if destination is None</a>:
            <a id="change">destination = </a><a id="change">OrderedDict()</a>
            &#47&#47 pyre-ignore [16]
            destination._metadata<a id="change"> = </a><a id="change">OrderedDict()</a>

        <a id="change">for </a>config, <a id="change">emb_module</a> in <a id="change">zip(
            </a>self._config.embedding_tables,
            self._emb_modules<a id="change">,
        )</a><a id="change">:
            key = </a>prefix<a id="change"> + f"{config.name}.weight"</a>
            param = emb_module.weight if keep_vars else emb_module.weight.data
            <a id="change">assert </a>config.local_rows == param.size(0)
            <a id="change">assert </a>config.local_cols == param.size(1)
            <a id="change">if config.global_metadata is not None</a>:
                &#47&#47 set additional field of sharded tensor based on local tensor properties
                config.global_metadata.tensor_properties.dtype<a id="change"> = </a>param.dtype
                config.global_metadata.tensor_properties.requires_grad<a id="change"> = </a>(
                    param.requires_grad
                )
                <a id="change">destination[
                    key
                ]</a><a id="change"> = ShardedTensor._init_from_local_shards_and_global_metadata(
                    local_shards=[Shard(param, config.local_metadata)],
                    sharded_tensor_metadata=config.global_metadata,
                    process_group=self._pg,
                )</a>
            else:
                <a id="change">destination[key]</a><a id="change"> = </a>param

        <a id="change">return </a>destination

    def named_parameters(
        self, prefix: str = "", recurse: bool = True</code></pre><h3>After Change</h3><pre><code class='java'>
        prefix: str = "",
        keep_vars: bool = False,
    ) -&gt; Dict[str, Any]:
        params<a id="change"> = </a><a id="change">[
            emb_module.weight if keep_vars else emb_module.weight.data
            for emb_module in self._emb_modules
        ]</a>
        <a id="change">return </a><a id="change">_get_state_dict(
            </a>self._config.embedding_tables, params, self._pg, destination, prefix<a id="change">
        )</a>

    def named_parameters(
        self, prefix: str = "", recurse: bool = True
    ) -&gt; Iterator[Tuple[str, nn.Parameter]]:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/a4440f145ac9df2ba467e9f5d591b4773779d28f#diff-19b6f02a42a3b6b147a160975a984ac3c98ebffdc15b720887f337ede50257b7L369' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 27580837</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: a4440f145ac9df2ba467e9f5d591b4773779d28f</div><div id='time'> Time: 2022-01-06</div><div id='author'> Author: yingliufb@fb.com</div><div id='file'> File Name: torchrec/distributed/embedding_lookup.py</div><div id='m_class'> M Class Name: GroupedEmbedding</div><div id='n_method'> N Class Name: GroupedEmbedding</div><div id='m_method'> M Method Name: state_dict(4)</div><div id='n_method'> N Method Name: state_dict(4)</div><div id='m_parent_class'> M Parent Class: BaseEmbedding</div><div id='n_parent_class'> N Parent Class: BaseEmbedding</div><div id='m_file'> M File Name: torchrec/distributed/embedding_lookup.py</div><div id='n_file'> N File Name: torchrec/distributed/embedding_lookup.py</div><div id='m_start'> M Start Line: 375</div><div id='m_end'> M End Line: 404</div><div id='n_start'> N Start Line: 431</div><div id='n_end'> N End Line: 437</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        prefix: str = "",
        keep_vars: bool = False,
    ) -&gt; Dict[str, Any]:
        <a id="change">if destination is None</a>:
            <a id="change">destination = </a><a id="change">OrderedDict()</a>
            &#47&#47 pyre-ignore [16]
            destination._metadata<a id="change"> = </a><a id="change">OrderedDict()</a>

        <a id="change">for </a>config, <a id="change">emb_module</a> in <a id="change">zip(
            </a>self._config.embedding_tables,
            self._emb_modules<a id="change">,
        )</a><a id="change">:
            key = </a>prefix<a id="change"> + f"{config.name}.weight"</a>

            param = emb_module.weight if keep_vars else emb_module.weight.data
            <a id="change">assert </a>config.local_rows == param.size(0)
            <a id="change">assert </a>config.local_cols == param.size(1)
            <a id="change">if config.global_metadata is not None</a>:
                &#47&#47 set additional field of sharded tensor based on local tensor properties
                config.global_metadata.tensor_properties.dtype<a id="change"> = </a>param.dtype
                config.global_metadata.tensor_properties.requires_grad<a id="change"> = </a>(
                    param.requires_grad
                )
                <a id="change">destination[
                    key
                ]</a><a id="change"> = ShardedTensor._init_from_local_shards_and_global_metadata(
                    local_shards=[Shard(param, config.local_metadata)],
                    sharded_tensor_metadata=config.global_metadata,
                    process_group=self._pg,
                )</a>
            else:
                <a id="change">destination[key]</a><a id="change"> = </a>param
        <a id="change">return </a>destination

    def named_parameters(
        self, prefix: str = "", recurse: bool = True</code></pre><h3>After Change</h3><pre><code class='java'>
        prefix: str = "",
        keep_vars: bool = False,
    ) -&gt; Dict[str, Any]:
        params<a id="change"> = </a><a id="change">[
            emb_module.weight if keep_vars else emb_module.weight.data
            for emb_module in self._emb_modules
        ]</a>
        <a id="change">return </a><a id="change">_get_state_dict(
            </a>self._config.embedding_tables, params, self._pg, destination, prefix<a id="change">
        )</a>

    def named_parameters(
        self, prefix: str = "", recurse: bool = True
    ) -&gt; Iterator[Tuple[str, nn.Parameter]]:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/a4440f145ac9df2ba467e9f5d591b4773779d28f#diff-19b6f02a42a3b6b147a160975a984ac3c98ebffdc15b720887f337ede50257b7L909' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 27580835</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: a4440f145ac9df2ba467e9f5d591b4773779d28f</div><div id='time'> Time: 2022-01-06</div><div id='author'> Author: yingliufb@fb.com</div><div id='file'> File Name: torchrec/distributed/embedding_lookup.py</div><div id='m_class'> M Class Name: GroupedEmbeddingBag</div><div id='n_method'> N Class Name: GroupedEmbeddingBag</div><div id='m_method'> M Method Name: state_dict(4)</div><div id='n_method'> N Method Name: state_dict(4)</div><div id='m_parent_class'> M Parent Class: BaseEmbeddingBag</div><div id='n_parent_class'> N Parent Class: BaseEmbeddingBag</div><div id='m_file'> M File Name: torchrec/distributed/embedding_lookup.py</div><div id='n_file'> N File Name: torchrec/distributed/embedding_lookup.py</div><div id='m_start'> M Start Line: 915</div><div id='m_end'> M End Line: 944</div><div id='n_start'> N Start Line: 927</div><div id='n_end'> N End Line: 933</div><BR>