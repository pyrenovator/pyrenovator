<html><h3>Pattern ID :1455
</h3><img src='6752631.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    no_prox: bool,
    clip_global_grad_norm: Tensor,
):
    <a id="change">if clip_global_grad_norm&lt;1.0</a>:
        <a id="change">torch._foreach_mul_(</a>grads, <a id="change">clip_global_grad_norm.item()</a><a id="change">)</a>
    copy_grads = [g.clone() for g in grads]

    diff<a id="change"> = torch</a><a id="change">._foreach_sub(grads</a>, pre_grads<a id="change">)</a>
    &#47&#47 NOTE: line below while looking identical gives different result, due to float precision errors.
    &#47&#47 using mul+add produces identical results to single-tensor, using add+alpha doesn&quott
    &#47&#47 On cuda this difference doesn&quott matter due to its&quot own precision non-determinism
    &#47&#47 update = torch._foreach_add(grads, torch._foreach_mul(diff, beta2))
    update<a id="change"> = torch</a><a id="change">._foreach_add(grads</a>, diff<a id="change">, alpha=beta2)</a>

    torch._foreach_mul_(exp_avgs, beta1)
    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1) &#47&#47 m_t

    torch._foreach_mul_(exp_avg_diffs, beta2)
    torch._foreach_add_(exp_avg_diffs, diff, alpha=1 - beta2) &#47&#47 diff_t

    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs, update, update, value=1 - beta3) &#47&#47 n_t

    denom = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    <a id="change">update</a> = <a id="change">torch._foreach_div(</a>exp_avgs, bias_correction1<a id="change">)</a>
    &#47&#47 NOTE: same issue as above. beta2 * diff / bias_correction2 != diff * (beta2 / bias_correction2)
    &#47&#47 using faster version by default. 
    &#47&#47 torch._foreach_add_(update, torch._foreach_div(torch._foreach_mul(exp_avg_diffs, beta2), bias_correction2))
    <a id="change">torch._foreach_add_(</a>update, <a id="change">torch._foreach_mul(</a>exp_avg_diffs, beta2<a id="change"> / </a>bias_correction2<a id="change">)</a><a id="change">)</a>
    <a id="change">torch._foreach_div_(update</a>, <a id="change">denom</a><a id="change">)</a>

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        <a id="change">torch._foreach_add_(params</a>, <a id="change">update</a><a id="change">, alpha=-lr)</a>
    else:
        <a id="change">torch._foreach_add_(params</a>, <a id="change">update</a><a id="change">, alpha=-lr)</a>
        torch._foreach_div_(params, 1 + lr * weight_decay)
    return copy_grads</code></pre><h3>After Change</h3><pre><code class='java'>
    clip_global_grad_norm: Tensor,
):

    <a id="change">torch._foreach_mul_(</a>grads, clip_global_grad_norm<a id="change">)</a>

    &#47&#47 for memory saving, we use `neg_pre_grads`
    &#47&#47 to get some temp variable in a inplace way
    <a id="change">torch._foreach_add_(</a>neg_pre_grads, <a id="change">grads</a><a id="change">)</a>

    torch._foreach_mul_(exp_avgs, beta1)
    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)  &#47&#47 m_t

    torch._foreach_mul_(exp_avg_diffs, beta2)
    torch._foreach_add_(exp_avg_diffs, neg_pre_grads,
                        alpha=1 - beta2)  &#47&#47 diff_t

    <a id="change">torch._foreach_mul_(neg_pre_grads</a>, <a id="change">beta2</a><a id="change">)</a>
    <a id="change">torch._foreach_add_(neg_pre_grads</a>, <a id="change">grads</a><a id="change">)</a>
    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs,
                            neg_pre_grads,
                            neg_pre_grads,
                            value=1 - beta3)  &#47&#47 n_t

    denom = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    step_size_diff = <a id="change">lr</a><a id="change"> * beta2 / </a>bias_correction2
    step_size<a id="change"> = lr</a><a id="change"> / </a>bias_correction1

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        <a id="change">torch._foreach_addcdiv_(params</a>, <a id="change">exp_avgs</a>, <a id="change">denom</a><a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(params</a>,
                                <a id="change">exp_avg_diffs</a>,
                                <a id="change">denom</a><a id="change">,
                                value=-step_size_diff)</a>
    else:
        <a id="change">torch._foreach_addcdiv_(params</a>, <a id="change">exp_avgs</a>, <a id="change">denom</a><a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(params</a>,
                                <a id="change">exp_avg_diffs</a>,
                                <a id="change">denom</a><a id="change">,
                                value=-step_size_diff)</a>
        torch._foreach_div_(params, 1 + lr * weight_decay)
    <a id="change">torch._foreach_zero_(neg_pre_grads</a><a id="change">)</a>
    <a id="change">torch._foreach_add_(neg_pre_grads</a>, <a id="change">grads</a><a id="change">, alpha=-1.0)</a>
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 29</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/sail-sg/adan/commit/6aafc8b3e81540fe52da3d70ced2bc3c9f17624a#diff-61f7eb3450a10f3b466dcc44a8b30146ec63b38486822f81c29f75ac3088a06cL253' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6752631</div><div id='project'> Project Name: sail-sg/adan</div><div id='commit'> Commit Name: 6aafc8b3e81540fe52da3d70ced2bc3c9f17624a</div><div id='time'> Time: 2022-12-10</div><div id='author'> Author: xyxie@pku.edu.cn</div><div id='file'> File Name: CV/MAE/adan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _multi_tensor_adan(0)</div><div id='n_method'> N Method Name: _multi_tensor_adan(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: CV/MAE/adan.py</div><div id='n_file'> N File Name: CV/MAE/adan.py</div><div id='m_start'> M Start Line: 253</div><div id='m_end'> M End Line: 290</div><div id='n_start'> N Start Line: 280</div><div id='n_end'> N End Line: 323</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    no_prox: bool,
    clip_global_grad_norm: Tensor,
):
    <a id="change">if clip_global_grad_norm&lt;1.0</a>:
        <a id="change">torch._foreach_mul_(</a>grads, <a id="change">clip_global_grad_norm.item()</a><a id="change">)</a>
    copy_grads = [g.clone() for g in grads]

    diff<a id="change"> = </a><a id="change">torch._foreach_sub(</a>grads, pre_grads<a id="change">)</a>
    &#47&#47 NOTE: line below while looking identical gives different result, due to float precision errors.
    &#47&#47 using mul+add produces identical results to single-tensor, using add+alpha doesn&quott
    &#47&#47 On cuda this difference doesn&quott matter due to its&quot own precision non-determinism
    &#47&#47 update = torch._foreach_add(grads, torch._foreach_mul(diff, beta2))
    update<a id="change"> = </a><a id="change">torch._foreach_add(</a>grads, diff<a id="change">, alpha=beta2)</a>

    torch._foreach_mul_(exp_avgs, beta1)
    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1) &#47&#47 m_t

    torch._foreach_mul_(exp_avg_diffs, beta2)
    torch._foreach_add_(exp_avg_diffs, diff, alpha=1 - beta2) &#47&#47 diff_t

    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs, update, update, value=1 - beta3) &#47&#47 n_t

    <a id="change">denom</a> = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    <a id="change">update</a> = <a id="change">torch._foreach_div(</a>exp_avgs, bias_correction1<a id="change">)</a>
    &#47&#47 NOTE: same issue as above. beta2 * diff / bias_correction2 != diff * (beta2 / bias_correction2)
    &#47&#47 using faster version by default. 
    &#47&#47 torch._foreach_add_(update, torch._foreach_div(torch._foreach_mul(exp_avg_diffs, beta2), bias_correction2))
    <a id="change">torch._foreach_add_(</a>update, <a id="change">torch._foreach_mul(</a>exp_avg_diffs, beta2<a id="change"> / </a>bias_correction2<a id="change">)</a><a id="change">)</a>
    <a id="change">torch._foreach_div_(</a>update, denom<a id="change">)</a>

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        <a id="change">torch._foreach_add_(</a>params, update<a id="change">, alpha=-lr)</a>
    else:
        <a id="change">torch._foreach_add_(</a>params, update<a id="change">, alpha=-lr)</a>
        torch._foreach_div_(params, 1 + lr * weight_decay)
    return copy_grads</code></pre><h3>After Change</h3><pre><code class='java'>
    clip_global_grad_norm: Tensor,
):

    <a id="change">torch._foreach_mul_(</a>grads, clip_global_grad_norm<a id="change">)</a>

    &#47&#47 for memory saving, we use `neg_pre_grads`
    &#47&#47 to get some temp variable in a inplace way
    <a id="change">torch._foreach_add_(</a>neg_pre_grads, grads<a id="change">)</a>

    torch._foreach_mul_(exp_avgs, beta1)
    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)  &#47&#47 m_t

    torch._foreach_mul_(exp_avg_diffs, beta2)
    torch._foreach_add_(exp_avg_diffs, neg_pre_grads,
                        alpha=1 - beta2)  &#47&#47 diff_t

    <a id="change">torch._foreach_mul_(</a>neg_pre_grads, beta2<a id="change">)</a>
    <a id="change">torch._foreach_add_(</a>neg_pre_grads, grads<a id="change">)</a>
    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs,
                            neg_pre_grads,
                            neg_pre_grads,
                            value=1 - beta3)  &#47&#47 n_t

    <a id="change">denom</a> = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    step_size_diff = lr<a id="change"> * beta2 / </a>bias_correction2
    step_size<a id="change"> = </a>lr<a id="change"> / </a>bias_correction1

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        <a id="change">torch._foreach_addcdiv_(</a>params, exp_avgs, denom<a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(</a>params,
                                exp_avg_diffs,
                                denom<a id="change">,
                                value=-step_size_diff)</a>
    else:
        <a id="change">torch._foreach_addcdiv_(</a>params, exp_avgs, denom<a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(</a>params,
                                exp_avg_diffs,
                                denom<a id="change">,
                                value=-step_size_diff)</a>
        torch._foreach_div_(params, 1 + lr * weight_decay)
    <a id="change">torch._foreach_zero_(</a>neg_pre_grads<a id="change">)</a>
    <a id="change">torch._foreach_add_(</a>neg_pre_grads, grads<a id="change">, alpha=-1.0)</a>
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sail-sg/adan/commit/6aafc8b3e81540fe52da3d70ced2bc3c9f17624a#diff-61f7eb3450a10f3b466dcc44a8b30146ec63b38486822f81c29f75ac3088a06cL233' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6752630</div><div id='project'> Project Name: sail-sg/adan</div><div id='commit'> Commit Name: 6aafc8b3e81540fe52da3d70ced2bc3c9f17624a</div><div id='time'> Time: 2022-12-10</div><div id='author'> Author: xyxie@pku.edu.cn</div><div id='file'> File Name: CV/MAE/adan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _multi_tensor_adan(0)</div><div id='n_method'> N Method Name: _multi_tensor_adan(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: CV/MAE/adan.py</div><div id='n_file'> N File Name: CV/MAE/adan.py</div><div id='m_start'> M Start Line: 253</div><div id='m_end'> M End Line: 290</div><div id='n_start'> N Start Line: 280</div><div id='n_end'> N End Line: 323</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    no_prox: bool,
    clip_global_grad_norm: Tensor,
):
    <a id="change">if clip_global_grad_norm&lt;1.0</a>:
        <a id="change">torch._foreach_mul_(</a>grads, <a id="change">clip_global_grad_norm.item()</a><a id="change">)</a>
    copy_grads = [g.clone() for g in grads]

    diff<a id="change"> = </a><a id="change">torch._foreach_sub(</a>grads, pre_grads<a id="change">)</a>
    &#47&#47 NOTE: line below while looking identical gives different result, due to float precision errors.
    &#47&#47 using mul+add produces identical results to single-tensor, using add+alpha doesn&quott
    &#47&#47 On cuda this difference doesn&quott matter due to its&quot own precision non-determinism
    &#47&#47 update = torch._foreach_add(grads, torch._foreach_mul(diff, beta2))
    update<a id="change"> = </a><a id="change">torch._foreach_add(</a>grads, diff<a id="change">, alpha=beta2)</a>

    torch._foreach_mul_(exp_avgs, beta1)
    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1) &#47&#47 m_t

    torch._foreach_mul_(exp_avg_diffs, beta2)
    torch._foreach_add_(exp_avg_diffs, diff, alpha=1 - beta2) &#47&#47 diff_t

    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs, update, update, value=1 - beta3) &#47&#47 n_t

    <a id="change">denom</a> = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    <a id="change">update</a> = <a id="change">torch._foreach_div(</a>exp_avgs, bias_correction1<a id="change">)</a>
    &#47&#47 NOTE: same issue as above. beta2 * diff / bias_correction2 != diff * (beta2 / bias_correction2)
    &#47&#47 using faster version by default. 
    &#47&#47 torch._foreach_add_(update, torch._foreach_div(torch._foreach_mul(exp_avg_diffs, beta2), bias_correction2))
    <a id="change">torch._foreach_add_(</a>update, <a id="change">torch._foreach_mul(</a>exp_avg_diffs, beta2<a id="change"> / </a>bias_correction2<a id="change">)</a><a id="change">)</a>
    <a id="change">torch._foreach_div_(</a>update, denom<a id="change">)</a>

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        <a id="change">torch._foreach_add_(</a>params, update<a id="change">, alpha=-lr)</a>
    else:
        <a id="change">torch._foreach_add_(</a>params, update<a id="change">, alpha=-lr)</a>
        torch._foreach_div_(params, 1 + lr * weight_decay)
    return copy_grads</code></pre><h3>After Change</h3><pre><code class='java'>
    clip_global_grad_norm: Tensor,
):

    <a id="change">torch._foreach_mul_(</a>grads, clip_global_grad_norm<a id="change">)</a>

    &#47&#47 for memory saving, we use `neg_pre_grads`
    &#47&#47 to get some temp variable in a inplace way
    <a id="change">torch._foreach_add_(</a>neg_pre_grads, grads<a id="change">)</a>

    torch._foreach_mul_(exp_avgs, beta1)
    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)  &#47&#47 m_t

    torch._foreach_mul_(exp_avg_diffs, beta2)
    torch._foreach_add_(exp_avg_diffs, neg_pre_grads,
                        alpha=1 - beta2)  &#47&#47 diff_t

    <a id="change">torch._foreach_mul_(</a>neg_pre_grads, beta2<a id="change">)</a>
    <a id="change">torch._foreach_add_(</a>neg_pre_grads, grads<a id="change">)</a>
    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs,
                            neg_pre_grads,
                            neg_pre_grads,
                            value=1 - beta3)  &#47&#47 n_t

    <a id="change">denom</a> = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    step_size_diff = lr<a id="change"> * beta2 / </a>bias_correction2
    step_size<a id="change"> = </a>lr<a id="change"> / </a>bias_correction1

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        <a id="change">torch._foreach_addcdiv_(</a>params, exp_avgs, denom<a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(</a>params,
                                exp_avg_diffs,
                                denom<a id="change">,
                                value=-step_size_diff)</a>
    else:
        <a id="change">torch._foreach_addcdiv_(</a>params, exp_avgs, denom<a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(</a>params,
                                exp_avg_diffs,
                                denom<a id="change">,
                                value=-step_size_diff)</a>
        torch._foreach_div_(params, 1 + lr * weight_decay)
    <a id="change">torch._foreach_zero_(</a>neg_pre_grads<a id="change">)</a>
    <a id="change">torch._foreach_add_(</a>neg_pre_grads, grads<a id="change">, alpha=-1.0)</a>
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sail-sg/adan/commit/6aafc8b3e81540fe52da3d70ced2bc3c9f17624a#diff-0b1b3ccdbc86485785aa958a04606abf754be337e06ac1f9c1b9e5471aec76b3L233' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6752629</div><div id='project'> Project Name: sail-sg/adan</div><div id='commit'> Commit Name: 6aafc8b3e81540fe52da3d70ced2bc3c9f17624a</div><div id='time'> Time: 2022-12-10</div><div id='author'> Author: xyxie@pku.edu.cn</div><div id='file'> File Name: NLP/Transformer-XL/adan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _multi_tensor_adan(0)</div><div id='n_method'> N Method Name: _multi_tensor_adan(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: NLP/Transformer-XL/adan.py</div><div id='n_file'> N File Name: NLP/Transformer-XL/adan.py</div><div id='m_start'> M Start Line: 253</div><div id='m_end'> M End Line: 290</div><div id='n_start'> N Start Line: 280</div><div id='n_end'> N End Line: 323</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    no_prox: bool,
    clip_global_grad_norm: Tensor,
):
    <a id="change">if clip_global_grad_norm&lt;1.0</a>:
        <a id="change">torch._foreach_mul_(</a>grads, <a id="change">clip_global_grad_norm.item()</a><a id="change">)</a>
    copy_grads = [g.clone() for g in grads]

    diff<a id="change"> = </a><a id="change">torch._foreach_sub(</a>grads, pre_grads<a id="change">)</a>
    &#47&#47 NOTE: line below while looking identical gives different result, due to float precision errors.
    &#47&#47 using mul+add produces identical results to single-tensor, using add+alpha doesn&quott
    &#47&#47 On cuda this difference doesn&quott matter due to its&quot own precision non-determinism
    &#47&#47 update = torch._foreach_add(grads, torch._foreach_mul(diff, beta2))
    update<a id="change"> = </a><a id="change">torch._foreach_add(</a>grads, diff<a id="change">, alpha=beta2)</a>

    torch._foreach_mul_(exp_avgs, beta1)
    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1) &#47&#47 m_t

    torch._foreach_mul_(exp_avg_diffs, beta2)
    torch._foreach_add_(exp_avg_diffs, diff, alpha=1 - beta2) &#47&#47 diff_t

    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs, update, update, value=1 - beta3) &#47&#47 n_t

    <a id="change">denom</a> = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    <a id="change">update</a> = <a id="change">torch._foreach_div(</a>exp_avgs, bias_correction1<a id="change">)</a>
    &#47&#47 NOTE: same issue as above. beta2 * diff / bias_correction2 != diff * (beta2 / bias_correction2)
    &#47&#47 using faster version by default. 
    &#47&#47 torch._foreach_add_(update, torch._foreach_div(torch._foreach_mul(exp_avg_diffs, beta2), bias_correction2))
    <a id="change">torch._foreach_add_(</a>update, <a id="change">torch._foreach_mul(</a>exp_avg_diffs, beta2<a id="change"> / </a>bias_correction2<a id="change">)</a><a id="change">)</a>
    <a id="change">torch._foreach_div_(</a>update, denom<a id="change">)</a>

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        <a id="change">torch._foreach_add_(</a>params, update<a id="change">, alpha=-lr)</a>
    else:
        <a id="change">torch._foreach_add_(</a>params, update<a id="change">, alpha=-lr)</a>
        torch._foreach_div_(params, 1 + lr * weight_decay)
    return copy_grads</code></pre><h3>After Change</h3><pre><code class='java'>
    clip_global_grad_norm: Tensor,
):

    <a id="change">torch._foreach_mul_(</a>grads, clip_global_grad_norm<a id="change">)</a>

    &#47&#47 for memory saving, we use `neg_pre_grads`
    &#47&#47 to get some temp variable in a inplace way
    <a id="change">torch._foreach_add_(</a>neg_pre_grads, grads<a id="change">)</a>

    torch._foreach_mul_(exp_avgs, beta1)
    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)  &#47&#47 m_t

    torch._foreach_mul_(exp_avg_diffs, beta2)
    torch._foreach_add_(exp_avg_diffs, neg_pre_grads,
                        alpha=1 - beta2)  &#47&#47 diff_t

    <a id="change">torch._foreach_mul_(</a>neg_pre_grads, beta2<a id="change">)</a>
    <a id="change">torch._foreach_add_(</a>neg_pre_grads, grads<a id="change">)</a>
    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs,
                            neg_pre_grads,
                            neg_pre_grads,
                            value=1 - beta3)  &#47&#47 n_t

    <a id="change">denom</a> = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    step_size_diff = lr<a id="change"> * beta2 / </a>bias_correction2
    step_size<a id="change"> = </a>lr<a id="change"> / </a>bias_correction1

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        <a id="change">torch._foreach_addcdiv_(</a>params, exp_avgs, denom<a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(</a>params,
                                exp_avg_diffs,
                                denom<a id="change">,
                                value=-step_size_diff)</a>
    else:
        <a id="change">torch._foreach_addcdiv_(</a>params, exp_avgs, denom<a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(</a>params,
                                exp_avg_diffs,
                                denom<a id="change">,
                                value=-step_size_diff)</a>
        torch._foreach_div_(params, 1 + lr * weight_decay)
    <a id="change">torch._foreach_zero_(</a>neg_pre_grads<a id="change">)</a>
    <a id="change">torch._foreach_add_(</a>neg_pre_grads, grads<a id="change">, alpha=-1.0)</a>
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sail-sg/adan/commit/6aafc8b3e81540fe52da3d70ced2bc3c9f17624a#diff-67c755dae37728c9303895a1af500d18552b3dbedbe392933fa05ddb6bf2837cL233' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6752628</div><div id='project'> Project Name: sail-sg/adan</div><div id='commit'> Commit Name: 6aafc8b3e81540fe52da3d70ced2bc3c9f17624a</div><div id='time'> Time: 2022-12-10</div><div id='author'> Author: xyxie@pku.edu.cn</div><div id='file'> File Name: CV/timm/adan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _multi_tensor_adan(0)</div><div id='n_method'> N Method Name: _multi_tensor_adan(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: CV/timm/adan.py</div><div id='n_file'> N File Name: CV/timm/adan.py</div><div id='m_start'> M Start Line: 253</div><div id='m_end'> M End Line: 290</div><div id='n_start'> N Start Line: 280</div><div id='n_end'> N End Line: 323</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    no_prox: bool,
    clip_global_grad_norm: Tensor,
):
    <a id="change">if clip_global_grad_norm&lt;1.0</a>:
        <a id="change">torch._foreach_mul_(</a>grads, <a id="change">clip_global_grad_norm.item()</a><a id="change">)</a>
    copy_grads = [g.clone() for g in grads]

    diff<a id="change"> = </a><a id="change">torch._foreach_sub(</a>grads, pre_grads<a id="change">)</a>
    &#47&#47 NOTE: line below while looking identical gives different result, due to float precision errors.
    &#47&#47 using mul+add produces identical results to single-tensor, using add+alpha doesn&quott
    &#47&#47 On cuda this difference doesn&quott matter due to its&quot own precision non-determinism
    &#47&#47 update = torch._foreach_add(grads, torch._foreach_mul(diff, beta2))
    update<a id="change"> = </a><a id="change">torch._foreach_add(</a>grads, diff<a id="change">, alpha=beta2)</a>

    torch._foreach_mul_(exp_avgs, beta1)
    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1) &#47&#47 m_t

    torch._foreach_mul_(exp_avg_diffs, beta2)
    torch._foreach_add_(exp_avg_diffs, diff, alpha=1 - beta2) &#47&#47 diff_t

    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs, update, update, value=1 - beta3) &#47&#47 n_t

    <a id="change">denom</a> = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    <a id="change">update</a> = <a id="change">torch._foreach_div(</a>exp_avgs, bias_correction1<a id="change">)</a>
    &#47&#47 NOTE: same issue as above. beta2 * diff / bias_correction2 != diff * (beta2 / bias_correction2)
    &#47&#47 using faster version by default. 
    &#47&#47 torch._foreach_add_(update, torch._foreach_div(torch._foreach_mul(exp_avg_diffs, beta2), bias_correction2))
    <a id="change">torch._foreach_add_(</a>update, <a id="change">torch._foreach_mul(</a>exp_avg_diffs, beta2<a id="change"> / </a>bias_correction2<a id="change">)</a><a id="change">)</a>
    <a id="change">torch._foreach_div_(</a>update, denom<a id="change">)</a>

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        <a id="change">torch._foreach_add_(</a>params, update<a id="change">, alpha=-lr)</a>
    else:
        <a id="change">torch._foreach_add_(</a>params, update<a id="change">, alpha=-lr)</a>
        torch._foreach_div_(params, 1 + lr * weight_decay)
    return copy_grads</code></pre><h3>After Change</h3><pre><code class='java'>
    clip_global_grad_norm: Tensor,
):

    <a id="change">torch._foreach_mul_(</a>grads, clip_global_grad_norm<a id="change">)</a>

    &#47&#47 for memory saving, we use `neg_pre_grads`
    &#47&#47 to get some temp variable in a inplace way
    <a id="change">torch._foreach_add_(</a>neg_pre_grads, grads<a id="change">)</a>

    torch._foreach_mul_(exp_avgs, beta1)
    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)  &#47&#47 m_t

    torch._foreach_mul_(exp_avg_diffs, beta2)
    torch._foreach_add_(exp_avg_diffs, neg_pre_grads,
                        alpha=1 - beta2)  &#47&#47 diff_t

    <a id="change">torch._foreach_mul_(</a>neg_pre_grads, beta2<a id="change">)</a>
    <a id="change">torch._foreach_add_(</a>neg_pre_grads, grads<a id="change">)</a>
    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs,
                            neg_pre_grads,
                            neg_pre_grads,
                            value=1 - beta3)  &#47&#47 n_t

    <a id="change">denom</a> = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    step_size_diff = lr<a id="change"> * beta2 / </a>bias_correction2
    step_size<a id="change"> = </a>lr<a id="change"> / </a>bias_correction1

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        <a id="change">torch._foreach_addcdiv_(</a>params, exp_avgs, denom<a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(</a>params,
                                exp_avg_diffs,
                                denom<a id="change">,
                                value=-step_size_diff)</a>
    else:
        <a id="change">torch._foreach_addcdiv_(</a>params, exp_avgs, denom<a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(</a>params,
                                exp_avg_diffs,
                                denom<a id="change">,
                                value=-step_size_diff)</a>
        torch._foreach_div_(params, 1 + lr * weight_decay)
    <a id="change">torch._foreach_zero_(</a>neg_pre_grads<a id="change">)</a>
    <a id="change">torch._foreach_add_(</a>neg_pre_grads, grads<a id="change">, alpha=-1.0)</a>
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sail-sg/adan/commit/9805dec1acf0151eecd1634a018c36239b0ad603#diff-bccd0d63d4b29d7d5170fdb4f6a4e95ad9bd8137b9a7ea1feb61848f4cabaeacL233' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6752639</div><div id='project'> Project Name: sail-sg/adan</div><div id='commit'> Commit Name: 9805dec1acf0151eecd1634a018c36239b0ad603</div><div id='time'> Time: 2022-12-10</div><div id='author'> Author: xyxie@pku.edu.cn</div><div id='file'> File Name: adan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _multi_tensor_adan(0)</div><div id='n_method'> N Method Name: _multi_tensor_adan(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: adan.py</div><div id='n_file'> N File Name: adan.py</div><div id='m_start'> M Start Line: 253</div><div id='m_end'> M End Line: 290</div><div id='n_start'> N Start Line: 280</div><div id='n_end'> N End Line: 323</div><BR>