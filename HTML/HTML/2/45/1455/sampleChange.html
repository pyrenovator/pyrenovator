<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    no_prox: bool,
    clip_global_grad_norm: Tensor,
):
    <a id="change">if clip_global_grad_norm&lt;1.0</a>:
        <a id="change">torch._foreach_mul_(</a>grads, <a id="change">clip_global_grad_norm.item()</a><a id="change">)</a>
    copy_grads = [g.clone() for g in grads]

    diff<a id="change"> = torch</a><a id="change">._foreach_sub(grads</a>, pre_grads<a id="change">)</a>
    &#47&#47 NOTE: line below while looking identical gives different result, due to float precision errors.
    &#47&#47 using mul+add produces identical results to single-tensor, using add+alpha doesn&quott
    &#47&#47 On cuda this difference doesn&quott matter due to its&quot own precision non-determinism
    &#47&#47 update = torch._foreach_add(grads, torch._foreach_mul(diff, beta2))
    update<a id="change"> = torch</a><a id="change">._foreach_add(grads</a>, diff<a id="change">, alpha=beta2)</a>

    torch._foreach_mul_(exp_avgs, beta1)
    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1) &#47&#47 m_t

    torch._foreach_mul_(exp_avg_diffs, beta2)
    torch._foreach_add_(exp_avg_diffs, diff, alpha=1 - beta2) &#47&#47 diff_t

    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs, update, update, value=1 - beta3) &#47&#47 n_t

    denom = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    <a id="change">update</a> = <a id="change">torch._foreach_div(</a>exp_avgs, bias_correction1<a id="change">)</a>
    &#47&#47 NOTE: same issue as above. beta2 * diff / bias_correction2 != diff * (beta2 / bias_correction2)
    &#47&#47 using faster version by default. 
    &#47&#47 torch._foreach_add_(update, torch._foreach_div(torch._foreach_mul(exp_avg_diffs, beta2), bias_correction2))
    <a id="change">torch._foreach_add_(</a>update, <a id="change">torch._foreach_mul(</a>exp_avg_diffs, beta2<a id="change"> / </a>bias_correction2<a id="change">)</a><a id="change">)</a>
    <a id="change">torch._foreach_div_(update</a>, <a id="change">denom</a><a id="change">)</a>

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        <a id="change">torch._foreach_add_(params</a>, <a id="change">update</a><a id="change">, alpha=-lr)</a>
    else:
        <a id="change">torch._foreach_add_(params</a>, <a id="change">update</a><a id="change">, alpha=-lr)</a>
        torch._foreach_div_(params, 1 + lr * weight_decay)
    return copy_grads</code></pre><h3>After Change</h3><pre><code class='java'>
    clip_global_grad_norm: Tensor,
):

    <a id="change">torch._foreach_mul_(</a>grads, clip_global_grad_norm<a id="change">)</a>

    &#47&#47 for memory saving, we use `neg_pre_grads`
    &#47&#47 to get some temp variable in a inplace way
    <a id="change">torch._foreach_add_(</a>neg_pre_grads, <a id="change">grads</a><a id="change">)</a>

    torch._foreach_mul_(exp_avgs, beta1)
    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)  &#47&#47 m_t

    torch._foreach_mul_(exp_avg_diffs, beta2)
    torch._foreach_add_(exp_avg_diffs, neg_pre_grads,
                        alpha=1 - beta2)  &#47&#47 diff_t

    <a id="change">torch._foreach_mul_(neg_pre_grads</a>, <a id="change">beta2</a><a id="change">)</a>
    <a id="change">torch._foreach_add_(neg_pre_grads</a>, <a id="change">grads</a><a id="change">)</a>
    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs,
                            neg_pre_grads,
                            neg_pre_grads,
                            value=1 - beta3)  &#47&#47 n_t

    denom = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    step_size_diff = <a id="change">lr</a><a id="change"> * beta2 / </a>bias_correction2
    step_size<a id="change"> = lr</a><a id="change"> / </a>bias_correction1

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        <a id="change">torch._foreach_addcdiv_(params</a>, <a id="change">exp_avgs</a>, <a id="change">denom</a><a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(params</a>,
                                <a id="change">exp_avg_diffs</a>,
                                <a id="change">denom</a><a id="change">,
                                value=-step_size_diff)</a>
    else:
        <a id="change">torch._foreach_addcdiv_(params</a>, <a id="change">exp_avgs</a>, <a id="change">denom</a><a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(params</a>,
                                <a id="change">exp_avg_diffs</a>,
                                <a id="change">denom</a><a id="change">,
                                value=-step_size_diff)</a>
        torch._foreach_div_(params, 1 + lr * weight_decay)
    <a id="change">torch._foreach_zero_(neg_pre_grads</a><a id="change">)</a>
    <a id="change">torch._foreach_add_(neg_pre_grads</a>, <a id="change">grads</a><a id="change">, alpha=-1.0)</a>
</code></pre>