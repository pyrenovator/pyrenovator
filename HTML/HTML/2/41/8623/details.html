<html><h3>Pattern ID :8623
</h3><img src='29994307.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def forward(self, input_ids, labels=None, past=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,
                ):
        <a id="change">transformer_outputs</a> = <a id="change">self.transformer(</a>input_ids,
                                               self.w_wte<a id="change">,
                                               past=past,
                                               attention_mask=attention_mask,
                                               token_type_ids=token_type_ids,
                                               position_ids=position_ids,
                                               head_mask=head_mask)</a>

        hidden_states<a id="change"> = transformer_outputs</a><a id="change">[0]</a>
        &#47&#47 hidden_states should be torch.Size([1, 1024, 768]) after reshape
        &#47&#47 hidden_states=hidden_states.view(-1,self.n_positions,self.n_embed)
        <a id="change">lm_logits</a> = self.stateless_lm_head(self.w_wte, hidden_states)
        outputs<a id="change"> = </a>(<a id="change">lm_logits</a>,)<a id="change"> + transformer_outputs[1:]</a>
        <a id="change">if labels is not None</a>:
            &#47&#47 Shift so that tokens &lt; n predict n
            <a id="change">shift_logits = lm_logits[..., :-1, :]</a><a id="change">.contiguous()</a>
            shift_labels<a id="change"> = labels</a><a id="change">[..., 1:].contiguous()</a>
            &#47&#47 Flatten the tokens
            &#47&#47 loss_fct = CrossEntropyLoss(ignore_index=-100)

            def loss_fct(logits, labels):
                return nn.functional.cross_entropy(logits, labels, ignore_index=-100)
            loss<a id="change"> = </a>loss_fct(<a id="change">shift_logits.view(-1</a>, <a id="change">shift_logits.size(-1</a><a id="change">)</a><a id="change">)</a>,
                            <a id="change">shift_labels.view(-1</a><a id="change">)</a>)
            
            &#47&#47 HACK: changed to output just the loss, somehow this improves partitioning results,
            &#47&#47 need to understand why.
            outputs = (loss<a id="change"></a>,)
            &#47&#47 outputs = (loss,) + outputs

        &#47&#47 (loss), lm_logits, presents, (all hidden_states), (attentions)
        <a id="change">return </a>outputs


@add_start_docstrings(The GPT2 Model transformer with a language modeling and a multiple-choice classification</code></pre><h3>After Change</h3><pre><code class='java'>
                                   self.transformer.wte)

    def forward(self, input_ids, labels=None):
        hidden_states = <a id="change">self.transformer(</a>input_ids,
                                         self.w_wte<a id="change">)</a>

        &#47&#47 hidden_states should be torch.Size([1, 1024, 768]) after reshape
        &#47&#47 hidden_states=hidden_states.view(-1,self.n_positions,self.n_embed)
        <a id="change">lm_logits</a> = self.stateless_lm_head(self.w_wte, hidden_states)
        output = self.compute_output(lm_logits,
                                     labels=labels)

        &#47&#47 loss or logits
        <a id="change">return </a>output


@add_start_docstrings(The GPT2 Model transformer with a language modeling and a multiple-choice classification</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 30</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/7ac977abea2c3ebc7af0cc51d0b4bba4653d6c9f#diff-4ef13e2fb41ecd7b39d8c340cb5027aa3b7922505b04f6cce9e77b6d0a581297L215' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 29994307</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 7ac977abea2c3ebc7af0cc51d0b4bba4653d6c9f</div><div id='time'> Time: 2020-07-07</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: models/normal/NLP_models/modeling_gpt2_tied_weights.py</div><div id='m_class'> M Class Name: GPT2LMHeadModel</div><div id='n_method'> N Class Name: GPT2LMHeadModel</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(8)</div><div id='m_parent_class'> M Parent Class: GPT2PreTrainedModel</div><div id='n_parent_class'> N Parent Class: GPT2PreTrainedModel</div><div id='m_file'> M File Name: models/normal/NLP_models/modeling_gpt2_tied_weights.py</div><div id='n_file'> N File Name: models/normal/NLP_models/modeling_gpt2_tied_weights.py</div><div id='m_start'> M Start Line: 842</div><div id='m_end'> M End Line: 875</div><div id='n_start'> N Start Line: 215</div><div id='n_end'> N End Line: 226</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def forward(self, input_ids, labels=None, past=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,
                ):
        <a id="change">transformer_outputs</a> = <a id="change">self.transformer(</a>input_ids,
                                               self.w_wte<a id="change">,
                                               past=past,
                                               attention_mask=attention_mask,
                                               token_type_ids=token_type_ids,
                                               position_ids=position_ids,
                                               head_mask=head_mask)</a>

        hidden_states<a id="change"> = </a><a id="change">transformer_outputs[0]</a>
        &#47&#47 hidden_states should be torch.Size([1, 1024, 768]) after reshape
        &#47&#47 hidden_states=hidden_states.view(-1,self.n_positions,self.n_embed)
        <a id="change">lm_logits</a> = self.stateless_lm_head(self.w_wte, hidden_states)
        outputs<a id="change"> = </a>(lm_logits<a id="change"></a>,)<a id="change"> + transformer_outputs[1:]</a>
        <a id="change">if labels is not None</a>:
            &#47&#47 Shift so that tokens &lt; n predict n
            <a id="change">shift_logits = </a><a id="change">lm_logits[..., :-1, :].contiguous()</a>
            shift_labels<a id="change"> = </a><a id="change">labels[..., 1:].contiguous()</a>
            &#47&#47 Flatten the tokens
            &#47&#47 loss_fct = CrossEntropyLoss(ignore_index=-100)

            def loss_fct(logits, labels):
                return nn.functional.cross_entropy(logits, labels, ignore_index=-100)
            loss<a id="change"> = </a>loss_fct(<a id="change">shift_logits.view(-1</a>, <a id="change">shift_logits.size(-1</a><a id="change">)</a><a id="change">)</a>,
                            <a id="change">shift_labels.view(-1</a><a id="change">)</a>)
            
            &#47&#47 HACK: changed to output just the loss, somehow this improves partitioning results,
            &#47&#47 need to understand why.
            outputs = (loss<a id="change"></a>,)
            &#47&#47 outputs = (loss,) + outputs

        &#47&#47 (loss), lm_logits, presents, (all hidden_states), (attentions)
        <a id="change">return </a>outputs


@add_start_docstrings(The GPT2 Model transformer with a language modeling and a multiple-choice classification</code></pre><h3>After Change</h3><pre><code class='java'>
                                   self.transformer.wte)

    def forward(self, input_ids, labels=None):
        hidden_states = <a id="change">self.transformer(</a>input_ids,
                                         self.w_wte<a id="change">)</a>

        &#47&#47 hidden_states should be torch.Size([1, 1024, 768]) after reshape
        &#47&#47 hidden_states=hidden_states.view(-1,self.n_positions,self.n_embed)
        <a id="change">lm_logits</a> = self.stateless_lm_head(self.w_wte, hidden_states)
        output = self.compute_output(lm_logits,
                                     labels=labels)

        &#47&#47 loss or logits
        <a id="change">return </a>output


@add_start_docstrings(The GPT2 Model transformer with a language modeling and a multiple-choice classification</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/7ac977abea2c3ebc7af0cc51d0b4bba4653d6c9f#diff-4ef13e2fb41ecd7b39d8c340cb5027aa3b7922505b04f6cce9e77b6d0a581297L842' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 29994306</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 7ac977abea2c3ebc7af0cc51d0b4bba4653d6c9f</div><div id='time'> Time: 2020-07-07</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: models/normal/NLP_models/modeling_gpt2_tied_weights.py</div><div id='m_class'> M Class Name: GPT2LMHeadModel</div><div id='n_method'> N Class Name: GPT2LMHeadModel</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(8)</div><div id='m_parent_class'> M Parent Class: GPT2PreTrainedModel</div><div id='n_parent_class'> N Parent Class: GPT2PreTrainedModel</div><div id='m_file'> M File Name: models/normal/NLP_models/modeling_gpt2_tied_weights.py</div><div id='n_file'> N File Name: models/normal/NLP_models/modeling_gpt2_tied_weights.py</div><div id='m_start'> M Start Line: 842</div><div id='m_end'> M End Line: 875</div><div id='n_start'> N Start Line: 215</div><div id='n_end'> N End Line: 226</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        outputs = model(input_ids, labels=input_ids)
        loss, logits = outputs[:2]
        
        <a id="change">transformer_outputs</a> = <a id="change">self.transformer(</a>input_ids<a id="change">,
                                               past=past,
                                               attention_mask=attention_mask,
                                               token_type_ids=token_type_ids,
                                               position_ids=position_ids,
                                               head_mask=head_mask,
                                               inputs_embeds=inputs_embeds,
                                               )</a>

        hidden_states<a id="change"> = </a><a id="change">transformer_outputs[0]</a>

        <a id="change">lm_logits</a> = self.lm_head(hidden_states)

        outputs<a id="change"> = </a>(lm_logits<a id="change"></a>,)<a id="change"> + transformer_outputs[1:]</a>

        <a id="change">if labels is not None</a>:
            &#47&#47 Shift so that tokens &lt; n predict n
            <a id="change">shift_logits = </a><a id="change">lm_logits[..., :-1, :].contiguous()</a>
            shift_labels<a id="change"> = </a><a id="change">labels[..., 1:].contiguous()</a>
            &#47&#47 Flatten the tokens
            loss<a id="change"> = </a>self.lm_loss(<a id="change">shift_logits.view(-1</a>, <a id="change">shift_logits.size(-1</a><a id="change">)</a><a id="change">)</a>,
                                <a id="change">shift_labels.view(-1</a><a id="change">)</a>)
            outputs = (loss<a id="change"></a>,)  &#47&#47 + outputs

        &#47&#47 (loss), lm_logits, presents, (all hidden_states), (attentions)
        <a id="change">return </a>outputs
</code></pre><h3>After Change</h3><pre><code class='java'>
        outputs = model(input_ids, labels=input_ids)
        loss, logits = outputs[:2]
        
        hidden_states = <a id="change">self.transformer(</a>input_ids<a id="change">)</a>

        <a id="change">lm_logits</a> = self.lm_head(hidden_states)

        <a id="change">return </a>self.lm_output(lm_logits, labels=labels)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/172ead8dbdce72bc3a4cde891ce5b18055d11565#diff-104364ea8995d0c5f407fdc0e00aa585d68d84bc02aeff04e056348c70926906L737' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 29994309</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 172ead8dbdce72bc3a4cde891ce5b18055d11565</div><div id='time'> Time: 2020-06-02</div><div id='author'> Author: alondej@gmail.com</div><div id='file'> File Name: models/normal/NLP_models/modeling_ctrl.py</div><div id='m_class'> M Class Name: CTRLLMHeadModel</div><div id='n_method'> N Class Name: CTRLLMHeadModel</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(9)</div><div id='m_parent_class'> M Parent Class: CTRLPreTrainedModel</div><div id='n_parent_class'> N Parent Class: CTRLPreTrainedModel</div><div id='m_file'> M File Name: models/normal/NLP_models/modeling_ctrl.py</div><div id='n_file'> N File Name: models/normal/NLP_models/modeling_ctrl.py</div><div id='m_start'> M Start Line: 738</div><div id='m_end'> M End Line: 808</div><div id='n_start'> N Start Line: 625</div><div id='n_end'> N End Line: 668</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def forward(self, input_ids, labels=None, past=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,
                ):
        <a id="change">transformer_outputs</a> = <a id="change">self.transformer(</a>input_ids<a id="change">,
                                               past=past,
                                               attention_mask=attention_mask,
                                               token_type_ids=token_type_ids,
                                               position_ids=position_ids,
                                               head_mask=head_mask)</a>

        hidden_states<a id="change"> = </a><a id="change">transformer_outputs[0]</a>
        &#47&#47 hidden_states should be torch.Size([1, 1024, 768]) after reshape
        &#47&#47 hidden_states=hidden_states.view(-1,self.n_positions,self.n_embed)
        <a id="change">lm_logits</a> = self.lm_head(hidden_states)
        outputs<a id="change"> = </a>(lm_logits<a id="change"></a>,)<a id="change"> + transformer_outputs[1:]</a>
        <a id="change">if labels is not None</a>:
            &#47&#47 Shift so that tokens &lt; n predict n
            <a id="change">shift_logits = </a><a id="change">lm_logits[..., :-1, :].contiguous()</a>
            shift_labels<a id="change"> = </a><a id="change">labels[..., 1:].contiguous()</a>
            &#47&#47 Flatten the tokens
            &#47&#47 loss_fct = CrossEntropyLoss(ignore_index=-100)
            def loss_fct(logits, labels): return nn.functional.cross_entropy(
                logits, labels, ignore_index=-100)
            loss<a id="change"> = </a>loss_fct(<a id="change">shift_logits.view(-1</a>, <a id="change">shift_logits.size(-1</a><a id="change">)</a><a id="change">)</a>,
                            <a id="change">shift_labels.view(-1</a><a id="change">)</a>)
            
            &#47&#47 HACK: changed to output just the loss, somehow this improves partitioning results,
            &#47&#47 need to understand why.
            outputs = (loss<a id="change"></a>,)
            &#47&#47 outputs = (loss,) + outputs

        &#47&#47 (loss), lm_logits, presents, (all hidden_states), (attentions)
        <a id="change">return </a>outputs


@add_start_docstrings(The GPT2 Model transformer with a language modeling and a multiple-choice classification</code></pre><h3>After Change</h3><pre><code class='java'>
                                   self.transformer.wte)

    def forward(self, input_ids, labels=None):
        hidden_states = <a id="change">self.transformer(</a>input_ids<a id="change">)</a>

        <a id="change">lm_logits</a> = self.lm_head(hidden_states)
        &#47&#47 hidden_states should be torch.Size([1, 1024, 768]) after reshape
        &#47&#47 hidden_states=hidden_states.view(-1,self.n_positions,self.n_embed)
        output = self.compute_output(lm_logits,
                                     labels=labels)

        &#47&#47 loss or logits
        <a id="change">return </a>output


@add_start_docstrings(The GPT2 Model transformer with a language modeling and a multiple-choice classification</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/7ac977abea2c3ebc7af0cc51d0b4bba4653d6c9f#diff-666c8f5528a54b5399a46ef60eb53ee26cbb4e383b82d191cc09c11b6c186bd3L804' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 29994311</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 7ac977abea2c3ebc7af0cc51d0b4bba4653d6c9f</div><div id='time'> Time: 2020-07-07</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: models/normal/NLP_models/modeling_gpt2.py</div><div id='m_class'> M Class Name: GPT2LMHeadModel</div><div id='n_method'> N Class Name: GPT2LMHeadModel</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(8)</div><div id='m_parent_class'> M Parent Class: GPT2PreTrainedModel</div><div id='n_parent_class'> N Parent Class: GPT2PreTrainedModel</div><div id='m_file'> M File Name: models/normal/NLP_models/modeling_gpt2.py</div><div id='n_file'> N File Name: models/normal/NLP_models/modeling_gpt2.py</div><div id='m_start'> M Start Line: 804</div><div id='m_end'> M End Line: 835</div><div id='n_start'> N Start Line: 459</div><div id='n_end'> N End Line: 469</div><BR>