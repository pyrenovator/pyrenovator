<html><h3>Pattern ID :38039
</h3><img src='109075072.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            self.train(self.n_epochs, batch_size=self.batch_size)

            &#47&#47 Evaluate agent
            <a id="change">if 0 &lt; eval_freq &lt;= timesteps_since_eval and eval_env is not None</a>:
                timesteps_since_eval<a id="change"> %= </a>eval_freq
                <a id="change">sync_envs_normalization(</a>self.env, <a id="change">eval_env</a><a id="change">)</a>

                mean_reward<a id="change">, _ = </a><a id="change">evaluate_policy(</a>self, <a id="change">eval_env</a>, n_eval_episodes<a id="change">)</a>
                if self.tb_writer is not None:
                    self.tb_writer.add_scalar(&quotEval/reward&quot, mean_reward, self.num_timesteps)

                <a id="change">evaluations.append(</a>mean_reward<a id="change">)</a>
                <a id="change">if self.verbose &gt; 0</a>:
                    <a id="change">print(</a><a id="change">"Eval num_timesteps={}, mean_reward={:.2f}".format(</a>self.num_timesteps, evaluations[-1]<a id="change">))</a>
                    <a id="change">print("FPS: {:.2f}".format(</a>self.num_timesteps<a id="change"> / </a>(<a id="change">time.time()</a><a id="change"> - </a>self.start_time)<a id="change">)</a><a id="change">)</a>

        return self

    def get_opt_parameters(self):</code></pre><h3>After Change</h3><pre><code class='java'>
            self.train(self.n_epochs, batch_size=self.batch_size)

            &#47&#47 Evaluate the agent
            timesteps_since_eval<a id="change"> = </a><a id="change">self._eval_policy(</a>eval_freq, eval_env, n_eval_episodes,
                                                     timesteps_since_eval<a id="change">, deterministic=True)</a>
            &#47&#47 For tensorboard integration
            &#47&#47 if self.tb_writer is not None:
            &#47&#47     self.tb_writer.add_scalar(&quotEval/reward&quot, mean_reward, self.num_timesteps)
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 21</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/dlr-rm/stable-baselines3/commit/8831eff163bc061727e16b7f728162776dc5ba27#diff-f8bbda9f9732c12b0ea5e52bd22588b6acc3977b6fa5bfa6fd8ef3a8a3021f97L276' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 109075072</div><div id='project'> Project Name: dlr-rm/stable-baselines3</div><div id='commit'> Commit Name: 8831eff163bc061727e16b7f728162776dc5ba27</div><div id='time'> Time: 2020-01-07</div><div id='author'> Author: antonin.raffin@dlr.de</div><div id='file'> File Name: torchy_baselines/ppo/ppo.py</div><div id='m_class'> M Class Name: PPO</div><div id='n_method'> N Class Name: PPO</div><div id='m_method'> M Method Name: learn(9)</div><div id='n_method'> N Method Name: learn(9)</div><div id='m_parent_class'> M Parent Class: BaseRLModel</div><div id='n_parent_class'> N Parent Class: BaseRLModel</div><div id='m_file'> M File Name: torchy_baselines/ppo/ppo.py</div><div id='n_file'> N File Name: torchy_baselines/ppo/ppo.py</div><div id='m_start'> M Start Line: 276</div><div id='m_end'> M End Line: 320</div><div id='n_start'> N Start Line: 305</div><div id='n_end'> N End Line: 306</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                self.train(gradient_steps, batch_size=self.batch_size, policy_delay=self.policy_delay)

            &#47&#47 Evaluate episode
            <a id="change">if 0 &lt; eval_freq &lt;= timesteps_since_eval and eval_env is not None</a>:
                timesteps_since_eval<a id="change"> %= </a>eval_freq
                <a id="change">sync_envs_normalization(</a>self.env, eval_env<a id="change">)</a>
                mean_reward<a id="change">, _ = </a><a id="change">evaluate_policy(</a>self, eval_env, n_eval_episodes<a id="change">)</a>
                <a id="change">evaluations.append(</a>mean_reward<a id="change">)</a>
                <a id="change">if self.verbose &gt; 0</a>:
                    <a id="change">print(</a><a id="change">"Eval num_timesteps={}, mean_reward={:.2f}".format(</a>self.num_timesteps, evaluations[-1]<a id="change">))</a>
                    <a id="change">print("FPS: {:.2f}".format(</a>self.num_timesteps<a id="change"> / </a>(<a id="change">time.time()</a><a id="change"> - </a>self.start_time)<a id="change">)</a><a id="change">)</a>

        return self

    def get_opt_parameters(self):</code></pre><h3>After Change</h3><pre><code class='java'>
                self.train(gradient_steps, batch_size=self.batch_size, policy_delay=self.policy_delay)

            &#47&#47 Evaluate the agent
            timesteps_since_eval<a id="change"> = </a><a id="change">self._eval_policy(</a>eval_freq, eval_env, n_eval_episodes,
                                                     timesteps_since_eval<a id="change">, deterministic=True)</a>

        return self

    def get_opt_parameters(self):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/dlr-rm/stable-baselines3/commit/8831eff163bc061727e16b7f728162776dc5ba27#diff-5c42912597f7d995bbb7bf4d154d4e6a0171371036a631278ad5d1fa6e549b2cL251' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 109075073</div><div id='project'> Project Name: dlr-rm/stable-baselines3</div><div id='commit'> Commit Name: 8831eff163bc061727e16b7f728162776dc5ba27</div><div id='time'> Time: 2020-01-07</div><div id='author'> Author: antonin.raffin@dlr.de</div><div id='file'> File Name: torchy_baselines/td3/td3.py</div><div id='m_class'> M Class Name: TD3</div><div id='n_method'> N Class Name: TD3</div><div id='m_method'> M Method Name: learn(9)</div><div id='n_method'> N Method Name: learn(9)</div><div id='m_parent_class'> M Parent Class: BaseRLModel</div><div id='n_parent_class'> N Parent Class: BaseRLModel</div><div id='m_file'> M File Name: torchy_baselines/td3/td3.py</div><div id='n_file'> N File Name: torchy_baselines/td3/td3.py</div><div id='m_start'> M Start Line: 294</div><div id='m_end'> M End Line: 303</div><div id='n_start'> N Start Line: 291</div><div id='n_end'> N End Line: 292</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            self.train(self.n_epochs, batch_size=self.batch_size)

            &#47&#47 Evaluate agent
            <a id="change">if 0 &lt; eval_freq &lt;= timesteps_since_eval and eval_env is not None</a>:
                timesteps_since_eval<a id="change"> %= </a>eval_freq
                <a id="change">sync_envs_normalization(</a>self.env, eval_env<a id="change">)</a>

                mean_reward<a id="change">, _ = </a><a id="change">evaluate_policy(</a>self, eval_env, n_eval_episodes<a id="change">)</a>
                if self.tb_writer is not None:
                    self.tb_writer.add_scalar(&quotEval/reward&quot, mean_reward, self.num_timesteps)

                <a id="change">evaluations.append(</a>mean_reward<a id="change">)</a>
                <a id="change">if self.verbose &gt; 0</a>:
                    <a id="change">print(</a><a id="change">"Eval num_timesteps={}, mean_reward={:.2f}".format(</a>self.num_timesteps, evaluations[-1]<a id="change">))</a>
                    <a id="change">print("FPS: {:.2f}".format(</a>self.num_timesteps<a id="change"> / </a>(<a id="change">time.time()</a><a id="change"> - </a>self.start_time)<a id="change">)</a><a id="change">)</a>

        return self

    def get_opt_parameters(self):</code></pre><h3>After Change</h3><pre><code class='java'>
            self.train(self.n_epochs, batch_size=self.batch_size)

            &#47&#47 Evaluate the agent
            timesteps_since_eval<a id="change"> = </a><a id="change">self._eval_policy(</a>eval_freq, eval_env, n_eval_episodes,
                                                     timesteps_since_eval<a id="change">, deterministic=True)</a>
            &#47&#47 For tensorboard integration
            &#47&#47 if self.tb_writer is not None:
            &#47&#47     self.tb_writer.add_scalar(&quotEval/reward&quot, mean_reward, self.num_timesteps)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/dlr-rm/stable-baselines3/commit/8831eff163bc061727e16b7f728162776dc5ba27#diff-f8bbda9f9732c12b0ea5e52bd22588b6acc3977b6fa5bfa6fd8ef3a8a3021f97L270' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 109075074</div><div id='project'> Project Name: dlr-rm/stable-baselines3</div><div id='commit'> Commit Name: 8831eff163bc061727e16b7f728162776dc5ba27</div><div id='time'> Time: 2020-01-07</div><div id='author'> Author: antonin.raffin@dlr.de</div><div id='file'> File Name: torchy_baselines/ppo/ppo.py</div><div id='m_class'> M Class Name: PPO</div><div id='n_method'> N Class Name: PPO</div><div id='m_method'> M Method Name: learn(9)</div><div id='n_method'> N Method Name: learn(9)</div><div id='m_parent_class'> M Parent Class: BaseRLModel</div><div id='n_parent_class'> N Parent Class: BaseRLModel</div><div id='m_file'> M File Name: torchy_baselines/ppo/ppo.py</div><div id='n_file'> N File Name: torchy_baselines/ppo/ppo.py</div><div id='m_start'> M Start Line: 276</div><div id='m_end'> M End Line: 320</div><div id='n_start'> N Start Line: 305</div><div id='n_end'> N End Line: 306</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                self.train(gradient_steps, batch_size=self.batch_size)

            &#47&#47 Evaluate episode
            <a id="change">if 0 &lt; eval_freq &lt;= timesteps_since_eval and eval_env is not None</a>:
                timesteps_since_eval<a id="change"> %= </a>eval_freq
                <a id="change">sync_envs_normalization(</a>self.env, eval_env<a id="change">)</a>
                mean_reward<a id="change">, std_reward = </a><a id="change">evaluate_policy(</a>self, eval_env, n_eval_episodes<a id="change">)</a>
                <a id="change">evaluations.append(</a>mean_reward<a id="change">)</a>
                <a id="change">if self.verbose &gt; 0</a>:
                    <a id="change">print(</a><a id="change">"Eval num_timesteps={}, mean_reward={:.2f}, std_reward={:.2f}".format(</a>self.num_timesteps, mean_reward, std_reward<a id="change">))</a>
                    <a id="change">print("FPS: {:.2f}".format(</a>self.num_timesteps<a id="change"> / </a>(<a id="change">time.time()</a><a id="change"> - </a>self.start_time)<a id="change">)</a><a id="change">)</a>

        return self

    def get_opt_parameters(self):</code></pre><h3>After Change</h3><pre><code class='java'>

                self.train(gradient_steps, batch_size=self.batch_size)

            timesteps_since_eval<a id="change"> = </a><a id="change">self._eval_policy(</a>eval_freq, eval_env, n_eval_episodes,
                                                     timesteps_since_eval<a id="change">, deterministic=True)</a>

        return self

    def get_opt_parameters(self):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/dlr-rm/stable-baselines3/commit/8831eff163bc061727e16b7f728162776dc5ba27#diff-ace7082ddb02987587afd18b50d1bebc77d97a1c15fdef1fe5d602ffe5993fd2L256' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 109075075</div><div id='project'> Project Name: dlr-rm/stable-baselines3</div><div id='commit'> Commit Name: 8831eff163bc061727e16b7f728162776dc5ba27</div><div id='time'> Time: 2020-01-07</div><div id='author'> Author: antonin.raffin@dlr.de</div><div id='file'> File Name: torchy_baselines/sac/sac.py</div><div id='m_class'> M Class Name: SAC</div><div id='n_method'> N Class Name: SAC</div><div id='m_method'> M Method Name: learn(9)</div><div id='n_method'> N Method Name: learn(9)</div><div id='m_parent_class'> M Parent Class: BaseRLModel</div><div id='n_parent_class'> N Parent Class: BaseRLModel</div><div id='m_file'> M File Name: torchy_baselines/sac/sac.py</div><div id='n_file'> N File Name: torchy_baselines/sac/sac.py</div><div id='m_start'> M Start Line: 291</div><div id='m_end'> M End Line: 300</div><div id='n_start'> N Start Line: 288</div><div id='n_end'> N End Line: 289</div><BR>