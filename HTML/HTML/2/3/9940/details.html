<html><h3>Pattern ID :9940
</h3><img src='35586399.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 As the VecEnv resets automatically, new_obs is already the
        &#47&#47 first observation of the next episode
        if done and <a id="change">infos[0].get("terminal_observation"</a><a id="change">)</a> is not None:
            next_obs = infos[0]["terminal_observation"]
            &#47&#47 VecNormalize normalizes the terminal observation
            if self._vec_normalize_env is not None:</code></pre><h3>After Change</h3><pre><code class='java'>
            self._last_original_obs, new_obs_, reward_ = self._last_obs, new_obs, reward

        &#47&#47 Avoid modification by reference
        next_obs = <a id="change">deepcopy(</a>new_obs_<a id="change">)</a>
        &#47&#47 As the VecEnv resets automatically, new_obs is already the
        &#47&#47 first observation of the next episode
        for i, done in enumerate(dones):
            if done and infos[i].get("terminal_observation") is not None:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 2</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/dlr-rm/stable-baselines3/commit/507ed1762e62bd6c4e85ea572ba166b69116b1ac#diff-7809dfd549054549abed96b27661447862894a94e6873c67f8f96f4b6842debdL462' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 35586399</div><div id='project'> Project Name: dlr-rm/stable-baselines3</div><div id='commit'> Commit Name: 507ed1762e62bd6c4e85ea572ba166b69116b1ac</div><div id='time'> Time: 2021-12-01</div><div id='author'> Author: antonin.raffin@ensta.org</div><div id='file'> File Name: stable_baselines3/common/off_policy_algorithm.py</div><div id='m_class'> M Class Name: OffPolicyAlgorithm</div><div id='n_method'> N Class Name: OffPolicyAlgorithm</div><div id='m_method'> M Method Name: _store_transition(7)</div><div id='n_method'> N Method Name: _store_transition(7)</div><div id='m_parent_class'> M Parent Class: BaseAlgorithm</div><div id='n_parent_class'> N Parent Class: BaseAlgorithm</div><div id='m_file'> M File Name: stable_baselines3/common/off_policy_algorithm.py</div><div id='n_file'> N File Name: stable_baselines3/common/off_policy_algorithm.py</div><div id='m_start'> M Start Line: 462</div><div id='m_end'> M End Line: 498</div><div id='n_start'> N Start Line: 467</div><div id='n_end'> N End Line: 514</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        distillation_loss = ((1.0 - self._hardness) * loss) + (
            self._hardness * teacher_loss
        )
        global_step = <a id="change">kwargs.get("global_step"</a><a id="change">)</a>
        global_step = epoch * steps_per_epoch if global_step is None else global_step
        _log_losses(self.loggers, global_step, loss, teacher_loss, distillation_loss)
        return distillation_loss
</code></pre><h3>After Change</h3><pre><code class='java'>
            else {key: student_inputs[key] for key in self._teacher_input_keys}
        )
        &#47&#47 copy to keep from updating student&quots inputs
        teacher_inputs = <a id="change">deepcopy(</a>teacher_inputs<a id="change">)</a>

        if self._teacher == "self":
            _LOGGER.info("Copying current models state for self distillation")
            self._teacher = deepcopy(module)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/neuralmagic/sparseml/commit/00add9fd9b677d966f02dd7c32060e50140dc14f#diff-37a1d70c58e00289a182955989f1d34a7b14be0cae1ced43409e44cb36926904L218' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 35586398</div><div id='project'> Project Name: neuralmagic/sparseml</div><div id='commit'> Commit Name: 00add9fd9b677d966f02dd7c32060e50140dc14f</div><div id='time'> Time: 2022-02-01</div><div id='author'> Author: mark@neuralmagic.com</div><div id='file'> File Name: src/sparseml/pytorch/optim/modifier_distillation.py</div><div id='m_class'> M Class Name: DistillationModifier</div><div id='n_method'> N Class Name: DistillationModifier</div><div id='m_method'> M Method Name: loss_update(8)</div><div id='n_method'> N Method Name: loss_update(8)</div><div id='m_parent_class'> M Parent Class: ScheduledUpdateModifier</div><div id='n_parent_class'> N Parent Class: ScheduledModifier</div><div id='m_file'> M File Name: src/sparseml/pytorch/optim/modifier_distillation.py</div><div id='n_file'> N File Name: src/sparseml/pytorch/optim/modifier_distillation.py</div><div id='m_start'> M Start Line: 244</div><div id='m_end'> M End Line: 290</div><div id='n_start'> N Start Line: 260</div><div id='n_end'> N End Line: 327</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
		save_freq=-1,
		save_best_only=False,
	)
	callbacks = [checkpoint_manager, LinearLRScheduler(<a id="change">params.get("learning_rate"</a>, 5e-5<a id="change">)</a>, 1e-7, n_iterations)]
	if show_training:
		callbacks.append(TrainingHistoryVisualizationCallback("./temp/"))
	regularization = RegularizationList([</code></pre><h3>After Change</h3><pre><code class='java'>
		logging.info(f"Checkpoint folder: {checkpoint_folder}")
	save_params(params, os.path.join(checkpoint_folder, "params.pkl"))
	
	encoder_params = <a id="change">deepcopy(</a>params<a id="change">)</a>
	if encoder_data_folder is not None:
		encoder_params["data_folder"] = encoder_data_folder
	auto_encoder_training_output = train_auto_encoder(**encoder_params, verbose=verbose)
	reconstruction_fig = visualize_reconstruction(</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/neurotorch/neurotorch/commit/e1935377b39da9d0b74f9741bfb2c942a38c9437#diff-aff5f78b9555bd5040a4af58cb039071f72e13288ee8f728cdf4de462c192c8eL110' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 35586400</div><div id='project'> Project Name: neurotorch/neurotorch</div><div id='commit'> Commit Name: e1935377b39da9d0b74f9741bfb2c942a38c9437</div><div id='time'> Time: 2022-08-22</div><div id='author'> Author: 50332514+JeremieGince@users.noreply.github.com</div><div id='file'> File Name: applications/time_series_forecasting_spiking/results_generation.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train_with_params(8)</div><div id='n_method'> N Method Name: train_with_params(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: applications/time_series_forecasting_spiking/results_generation.py</div><div id='n_file'> N File Name: applications/time_series_forecasting_spiking/results_generation.py</div><div id='m_start'> M Start Line: 128</div><div id='m_end'> M End Line: 212</div><div id='n_start'> N Start Line: 121</div><div id='n_end'> N End Line: 233</div><BR>