<html><h3>Pattern ID :32440
</h3><img src='94610099.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                            forward_params[&quottext_encodings&quot] = text_emb
                        else:
                            &#47&#47 Then we need to pass the text instead
                            tokenized_texts = <a id="change">tokenize(</a>txt<a id="change">, truncate=True)</a>
                            assert tokenized_texts.shape[0] == len(img), f"The number of texts ({tokenized_texts.shape[0]}) should be the same as the number of images ({len(img)})"
                            forward_params[&quottext&quot] = tokenized_texts
                    loss = trainer.forward(img, **forward_params, unet_number=unet, _device=inference_device)
                    trainer.update(unet_number=unet)</code></pre><h3>After Change</h3><pre><code class='java'>
                        else:
                            &#47&#47 Then we need to pass the text instead
                            assert clip is not None
                            tokenized_texts = <a id="change">tokenize(txt, truncate=True).to(</a>inference_device<a id="change">)</a>
                            assert tokenized_texts.shape[0] == len(img), f"The number of texts ({tokenized_texts.shape[0]}) should be the same as the number of images ({len(img)})"
                            text_embed, text_encodings = clip.embed_text(tokenized_texts)
                            forward_params[&quottext_encodings&quot] = text_encodings
                    loss = trainer.forward(img, **forward_params, unet_number=unet, _device=inference_device)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/dalle2-pytorch/commit/cbaadb693116b214cc29d0569ed0c0a5b9178ab7#diff-1960fa4032117c7620576aea3bf67f017dd87a2077278af4c2f17abf549114b0L272' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 94610099</div><div id='project'> Project Name: lucidrains/dalle2-pytorch</div><div id='commit'> Commit Name: cbaadb693116b214cc29d0569ed0c0a5b9178ab7</div><div id='time'> Time: 2022-08-20</div><div id='author'> Author: aidan.dempster@gmail.com</div><div id='file'> File Name: train_decoder.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train(16)</div><div id='n_method'> N Method Name: train(15)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: train_decoder.py</div><div id='n_file'> N File Name: train_decoder.py</div><div id='m_start'> M Start Line: 374</div><div id='m_end'> M End Line: 473</div><div id='n_start'> N Start Line: 272</div><div id='n_end'> N End Line: 526</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                        else:
                            &#47&#47 Then we need to pass the text instead
                            assert clip is not None
                            tokenized_texts = <a id="change">tokenize(</a>txt<a id="change">, truncate=True)</a>
                            assert tokenized_texts.shape[0] == len(img), f"The number of texts ({tokenized_texts.shape[0]}) should be the same as the number of images ({len(img)})"
                            text_embed, text_encodings = clip.embed_text(tokenized_texts)
                            forward_params[&quottext_encodings&quot] = text_encodings
                    loss = trainer.forward(img.float(), **forward_params, unet_number=unet, _device=inference_device)</code></pre><h3>After Change</h3><pre><code class='java'>
                        else:
                            &#47&#47 Then we need to pass the text instead
                            assert clip is not None
                            tokenized_texts = <a id="change">tokenize(txt, truncate=True).to(device=inference_device)</a>
                            assert tokenized_texts.shape[0] == len(img), f"The number of texts ({tokenized_texts.shape[0]}) should be the same as the number of images ({len(img)})"
                            text_embed, text_encodings = clip.embed_text(tokenized_texts)
                            forward_params[&quottext_encodings&quot] = text_encodings
                    loss = trainer.forward(img.float(), **forward_params, unet_number=unet, _device=inference_device)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/dalle2-pytorch/commit/86e2d5ba84fa02843cb70d61877cf55a49908b2c#diff-1960fa4032117c7620576aea3bf67f017dd87a2077278af4c2f17abf549114b0L266' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 94610098</div><div id='project'> Project Name: lucidrains/dalle2-pytorch</div><div id='commit'> Commit Name: 86e2d5ba84fa02843cb70d61877cf55a49908b2c</div><div id='time'> Time: 2022-09-15</div><div id='author'> Author: 51308183+nousr@users.noreply.github.com</div><div id='file'> File Name: train_decoder.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train(16)</div><div id='n_method'> N Method Name: train(16)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: train_decoder.py</div><div id='n_file'> N File Name: train_decoder.py</div><div id='m_start'> M Start Line: 483</div><div id='m_end'> M End Line: 483</div><div id='n_start'> N Start Line: 483</div><div id='n_end'> N End Line: 483</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if text_embeddings[0] is None:
            &#47&#47 Generate text embeddings from text
            assert clip is not None, "clip is None, but text_embeddings is None"
            tokenized_texts = <a id="change">tokenize(</a>txts<a id="change">, truncate=True)</a>
            text_embed, text_encodings = clip.embed_text(tokenized_texts)
            sample_params["text_encodings"] = text_encodings
        else:
            &#47&#47 Then we are using precomputed text embeddings</code></pre><h3>After Change</h3><pre><code class='java'>
        if text_embeddings[0] is None:
            &#47&#47 Generate text embeddings from text
            assert clip is not None, "clip is None, but text_embeddings is None"
            tokenized_texts = <a id="change">tokenize(txts, truncate=True).to(device=device)</a>
            text_embed, text_encodings = clip.embed_text(tokenized_texts)
            sample_params["text_encodings"] = text_encodings
        else:
            &#47&#47 Then we are using precomputed text embeddings</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/dalle2-pytorch/commit/86e2d5ba84fa02843cb70d61877cf55a49908b2c#diff-1960fa4032117c7620576aea3bf67f017dd87a2077278af4c2f17abf549114b0L137' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 94610097</div><div id='project'> Project Name: lucidrains/dalle2-pytorch</div><div id='commit'> Commit Name: 86e2d5ba84fa02843cb70d61877cf55a49908b2c</div><div id='time'> Time: 2022-09-15</div><div id='author'> Author: 51308183+nousr@users.noreply.github.com</div><div id='file'> File Name: train_decoder.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: generate_samples(10)</div><div id='n_method'> N Method Name: generate_samples(10)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: train_decoder.py</div><div id='n_file'> N File Name: train_decoder.py</div><div id='m_start'> M Start Line: 159</div><div id='m_end'> M End Line: 159</div><div id='n_start'> N Start Line: 159</div><div id='n_end'> N End Line: 159</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            return x
        else:
            if isinstance(text[0], str):
                text = <a id="change">tokenize(</a>text<a id="change">)</a>
            if clip4clip:
                x = self.token_embedding(text).type(self.dtype)  &#47&#47 [batch_size, n_ctx, d_model]

                pos_emd = self.positional_embedding[:x.size(1), :].type(self.dtype)</code></pre><h3>After Change</h3><pre><code class='java'>
            return x
        else:
            if isinstance(text[0], str):
                text = <a id="change">tokenize(text).to(</a>device<a id="change">)</a>
            else:
                text = text.to(device)
            if clip4clip:
                x = self.token_embedding(text).type(self.dtype)  &#47&#47 [batch_size, n_ctx, d_model]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/towhee-io/towhee/commit/b2c5fb5affbaa22dfbbf0e029a28680b3a4ee30e#diff-0f28b7be7bc1b9cc3364c279bffc21db65ba230d77060d6a971a8f8270aa8fc6L504' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 94610100</div><div id='project'> Project Name: towhee-io/towhee</div><div id='commit'> Commit Name: b2c5fb5affbaa22dfbbf0e029a28680b3a4ee30e</div><div id='time'> Time: 2022-08-09</div><div id='author'> Author: chen.zhang@zilliz.com</div><div id='file'> File Name: towhee/models/clip/clip.py</div><div id='m_class'> M Class Name: CLIP</div><div id='n_method'> N Class Name: CLIP</div><div id='m_method'> M Method Name: encode_text(6)</div><div id='n_method'> N Method Name: encode_text(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: towhee/models/clip/clip.py</div><div id='n_file'> N File Name: towhee/models/clip/clip.py</div><div id='m_start'> M Start Line: 523</div><div id='m_end'> M End Line: 523</div><div id='n_start'> N Start Line: 504</div><div id='n_end'> N End Line: 527</div><BR>