<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        action = action.tanh()

        return action<a id="change">, std</a>


if __name__ == &quot__main__&quot:
    use_cuda = torch.cuda.is_available()</code></pre><h3>After Change</h3><pre><code class='java'>
            action = action_base.tanh()

            &#47&#47 According to "Soft Actor-Critic" (Haarnoja et. al) Appendix C
            action_bound_compensation = torch.log(1.<a id="change"> - </a>action.tanh().pow(2) + 1e-6)
            action_bound_compensation = action_bound_compensation.sum(dim=-1, keepdim=True)
            <a id="change">log_prob.sub_(</a>action_bound_compensation<a id="change">)</a>

        return action, log_prob

</code></pre>