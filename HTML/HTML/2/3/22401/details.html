<html><h3>Pattern ID :22401
</h3><img src='70922170.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def inference(self, resized_rgb_image):
        &#47&#47 Here should inference on the image, output a list of objects, each obj is a dict with two keys "id" and "bbox" and "score"
        &#47&#47 return [{"id": 0, "bbox": [x1, y1, x2, y2], "score":s%}, {...}, {...}, ...]
        return <a id="change">self.net.inference(</a>resized_rgb_image<a id="change">)</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
            output: List of objects, each obj is a dict with two keys "id" and "bbox" and "score"
            e.g. [{"id": 0, "bbox": [x1, y1, x2, y2], "score":s%}, {...}, {...}, ...]
        
        output<a id="change"> = </a><a id="change">self.net.inference(</a>resized_rgb_image<a id="change">)</a>
        return output
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/neuralet/smart-social-distancing/commit/cd16bb1ce196b55f8965d5cf76ed12602913afcb#diff-473cf3fa20402b5dcda468a9c21f1a55406d3695f7430bd618df2b4d54d84313L23' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 70922170</div><div id='project'> Project Name: neuralet/smart-social-distancing</div><div id='commit'> Commit Name: cd16bb1ce196b55f8965d5cf76ed12602913afcb</div><div id='time'> Time: 2020-04-12</div><div id='author'> Author: mehraliangit@gmail.com</div><div id='file'> File Name: libs/detectors/edgetpu/detector.py</div><div id='m_class'> M Class Name: Detector</div><div id='n_method'> N Class Name: Detector</div><div id='m_method'> M Method Name: inference(2)</div><div id='n_method'> N Method Name: inference(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: libs/detectors/edgetpu/detector.py</div><div id='n_file'> N File Name: libs/detectors/edgetpu/detector.py</div><div id='m_start'> M Start Line: 23</div><div id='m_end'> M End Line: 23</div><div id='n_start'> N Start Line: 32</div><div id='n_end'> N End Line: 33</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                    wav_tensor, sample_rate = torchaudio.load(path)
                    mel_tensor = wav2mel(wav_tensor, sample_rate)
                    cached_speaker_embedding = dvector.embed_utterance(mel_tensor)
                    cached_duration = dc(<a id="change">acoustic_model.inference(text_tensor=text.squeeze(0).to(device),
                                                                  speech_tensor=melspec.to(device),
                                                                  use_teacher_forcing=True,
                                                                  speaker_embeddings=cached_speaker_embedding.to(device))</a>[2],
                                         vis=os.path.join(cache_dir, "durations_visualization", path.split("/")[-1].rstrip(".wav") + ".png"))[0].cpu()
                cached_energy = energy_calc(input=norm_wave.unsqueeze(0),
                                            input_lengths=norm_wave_length,</code></pre><h3>After Change</h3><pre><code class='java'>
                    wav_tensor, sample_rate = torchaudio.load(path)
                    mel_tensor = wav2mel(wav_tensor, sample_rate)
                    cached_speaker_embedding = dvector.embed_utterance(mel_tensor)
                    attention_map = <a id="change">acoustic_model.inference(text_tensor=text.squeeze(0).to(device),
                                                             speech_tensor=melspec.to(device),
                                                             use_teacher_forcing=True,
                                                             speaker_embeddings=cached_speaker_embedding.to(device))</a>[2]
                    focus_rate = self._calculate_focus_rate(attention_map)
                    cached_duration<a id="change"> = </a>dc(attention_map,
                                         vis=os.path.join(cache_dir, "durations_visualization",
                                                          str(int(focus_rate * 1000)) + path.split("/")[-1].rstrip(".wav") + ".png"))[0].cpu()
                cached_energy = energy_calc(input=norm_wave.unsqueeze(0),</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/d976a59df996c408eced318b4f425227c436a6da#diff-528f1d1f9b65f826c6eec25f3934452fe1a81965d76a0070a71db9ef9ef36aa0L96' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 70922168</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: d976a59df996c408eced318b4f425227c436a6da</div><div id='time'> Time: 2021-09-05</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: TrainingInterfaces/Text_to_Spectrogram/FastSpeech2/FastSpeechDataset.py</div><div id='m_class'> M Class Name: FastSpeechDataset</div><div id='n_method'> N Class Name: FastSpeechDataset</div><div id='m_method'> M Method Name: cache_builder_process(11)</div><div id='n_method'> N Method Name: cache_builder_process(11)</div><div id='m_parent_class'> M Parent Class: Dataset</div><div id='n_parent_class'> N Parent Class: Dataset</div><div id='m_file'> M File Name: TrainingInterfaces/Text_to_Spectrogram/FastSpeech2/FastSpeechDataset.py</div><div id='n_file'> N File Name: TrainingInterfaces/Text_to_Spectrogram/FastSpeech2/FastSpeechDataset.py</div><div id='m_start'> M Start Line: 147</div><div id='m_end'> M End Line: 151</div><div id='n_start'> N Start Line: 147</div><div id='n_end'> N End Line: 154</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def inference(self, resized_rgb_image):
        &#47&#47 Here should inference on the image, output a list of objects, each obj is a dict with two keys "id" and "bbox" and "score" 
        &#47&#47 return [{"id": 0, "bbox": [x1, y1, x2, y2], "score":s%}, {...}, {...}, ...]
        return <a id="change">self.net.inference(</a>resized_rgb_image<a id="change">)</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
            output: List of objects, each obj is a dict with two keys "id" and "bbox" and "score"
            e.g. [{"id": 0, "bbox": [x1, y1, x2, y2], "score":s%}, {...}, {...}, ...]
        
        output<a id="change"> = </a><a id="change">self.net.inference(</a>resized_rgb_image<a id="change">)</a>
        return output
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/neuralet/smart-social-distancing/commit/67a0b9b5af10b039d73d71b50faa81f5a92e8f24#diff-037a408db6e7d552f809bee8efcb05a050a45c99c9f0bf3e0438ea0b277aa4aeL21' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 70922162</div><div id='project'> Project Name: neuralet/smart-social-distancing</div><div id='commit'> Commit Name: 67a0b9b5af10b039d73d71b50faa81f5a92e8f24</div><div id='time'> Time: 2020-04-12</div><div id='author'> Author: mehraliangit@gmail.com</div><div id='file'> File Name: libs/detectors/jetson/detector.py</div><div id='m_class'> M Class Name: Detector</div><div id='n_method'> N Class Name: Detector</div><div id='m_method'> M Method Name: inference(2)</div><div id='n_method'> N Method Name: inference(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: libs/detectors/jetson/detector.py</div><div id='n_file'> N File Name: libs/detectors/jetson/detector.py</div><div id='m_start'> M Start Line: 24</div><div id='m_end'> M End Line: 24</div><div id='n_start'> N Start Line: 32</div><div id='n_end'> N End Line: 33</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                cached_speech_len = torch.LongTensor([len(cached_speech)])
                if not speaker_embedding:
                    os.path.join(cache_dir, "durations_visualization")
                    cached_duration = dc(<a id="change">acoustic_model.inference(text_tensor=text.squeeze(0).to(device),
                                                                  speech_tensor=melspec.to(device),
                                                                  use_teacher_forcing=True,
                                                                  speaker_embeddings=None)</a>[2],
                                         vis=os.path.join(cache_dir, "durations_visualization", path.split("/")[-1].rstrip(".wav") + ".png"))[0].cpu()
                else:
                    wav_tensor, sample_rate = torchaudio.load(path)</code></pre><h3>After Change</h3><pre><code class='java'>
                cached_speech_len = torch.LongTensor([len(cached_speech)])
                if not speaker_embedding:
                    os.path.join(cache_dir, "durations_visualization")
                    attention_map = <a id="change">acoustic_model.inference(text_tensor=text.squeeze(0).to(device),
                                                             speech_tensor=melspec.to(device),
                                                             use_teacher_forcing=True,
                                                             speaker_embeddings=None)</a>[2]
                    focus_rate<a id="change"> = </a>self._calculate_focus_rate(attention_map)
                    cached_duration = dc(attention_map,
                                         vis=os.path.join(cache_dir, "durations_visualization",
                                                          str(int(focus_rate * 1000)) + "_" + path.split("/")[-1].rstrip(".wav") + ".png"))[0].cpu()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/ab75b11e270921ed26f285aef0265568c3612ebe#diff-528f1d1f9b65f826c6eec25f3934452fe1a81965d76a0070a71db9ef9ef36aa0L96' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 70922164</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: ab75b11e270921ed26f285aef0265568c3612ebe</div><div id='time'> Time: 2021-09-05</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: TrainingInterfaces/Text_to_Spectrogram/FastSpeech2/FastSpeechDataset.py</div><div id='m_class'> M Class Name: FastSpeechDataset</div><div id='n_method'> N Class Name: FastSpeechDataset</div><div id='m_method'> M Method Name: cache_builder_process(11)</div><div id='n_method'> N Method Name: cache_builder_process(11)</div><div id='m_parent_class'> M Parent Class: Dataset</div><div id='n_parent_class'> N Parent Class: Dataset</div><div id='m_file'> M File Name: TrainingInterfaces/Text_to_Spectrogram/FastSpeech2/FastSpeechDataset.py</div><div id='n_file'> N File Name: TrainingInterfaces/Text_to_Spectrogram/FastSpeech2/FastSpeechDataset.py</div><div id='m_start'> M Start Line: 138</div><div id='m_end'> M End Line: 142</div><div id='n_start'> N Start Line: 138</div><div id='n_end'> N End Line: 145</div><BR>