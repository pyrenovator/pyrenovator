<html><h3>Pattern ID :11224
</h3><img src='38323944.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.model = self.model.to(device)
        self.model.enable_attention_slicing()
        &#47&#47 TODO: gpu_id
        if kwargs.get(&quotcpu_offload&quot, False) and <a id="change">torch.cuda.is_available()</a>:
            self.model.enable_sequential_cpu_offload(gpu_id=0)

    def forward(self, image, mask, config: Config):</code></pre><h3>After Change</h3><pre><code class='java'>
            self.model.enable_xformers_memory_efficient_attention()

        &#47&#47 TODO: gpu_id
        if <a id="change">kwargs.get(&quotcpu_offload&quot, False) and use_gpu</a>:
            self.model.image_encoder<a id="change"> = </a>self.model.image_encoder.to(device)
            self.model.enable_sequential_cpu_offload(gpu_id=0)
        else:
            self.model = self.model.to(device)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/sanster/lama-cleaner/commit/148e97e8da489d7ffa0e1b2b5b245351e36debe7#diff-af6f0c3f40e17e26f5c5b5ed3c98c5978833ffc2cb54b6198819ef8af3f37f16L22' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 38323944</div><div id='project'> Project Name: sanster/lama-cleaner</div><div id='commit'> Commit Name: 148e97e8da489d7ffa0e1b2b5b245351e36debe7</div><div id='time'> Time: 2023-01-18</div><div id='author'> Author: cwq1913@gmail.com</div><div id='file'> File Name: lama_cleaner/model/paint_by_example.py</div><div id='m_class'> M Class Name: PaintByExample</div><div id='n_method'> N Class Name: PaintByExample</div><div id='m_method'> M Method Name: init_model(2)</div><div id='n_method'> N Method Name: init_model(2)</div><div id='m_parent_class'> M Parent Class: InpaintModel</div><div id='n_parent_class'> N Parent Class: InpaintModel</div><div id='m_file'> M File Name: lama_cleaner/model/paint_by_example.py</div><div id='n_file'> N File Name: lama_cleaner/model/paint_by_example.py</div><div id='m_start'> M Start Line: 30</div><div id='m_end'> M End Line: 36</div><div id='n_start'> N Start Line: 22</div><div id='n_end'> N End Line: 50</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        os.makedirs(tfb_log_dir)
    tfb_writer = SummaryWriter(tfb_log_dir)

    device = torch.device("cuda" if <a id="change">torch.cuda.is_available()</a> else "cpu")
    logger.info(device)
    dbnet = DBTextModel().to(device)
</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 load best cp
    if cfg.model.finetune_cp_path:
        cp_path = os.path.join(cfg.meta.root_dir, cfg.model.finetune_cp_path)
        if <a id="change">os.path.exists(cp_path) and cp_path.endswith(&quot.pth&quot)</a>:
            lr_optim<a id="change"> = </a>cfg.optimizer.lr_finetune
            logger.info("Loading best checkpoint: {}".format(cp_path))
            dbnet.load_state_dict(torch.load(cp_path, map_location=device))
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/huyhoang17/db_text_minimal/commit/b10f6da1476ae9e05a6f6a41a8838fb448645f50#diff-a454ee593451c002ad5a1322d7dc3d2574f234a8290c006a07243760eb453bbdL59' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 38323945</div><div id='project'> Project Name: huyhoang17/db_text_minimal</div><div id='commit'> Commit Name: b10f6da1476ae9e05a6f6a41a8838fb448645f50</div><div id='time'> Time: 2020-06-11</div><div id='author'> Author: hoangphan0710@gmail.com</div><div id='file'> File Name: src/train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/train.py</div><div id='n_file'> N File Name: src/train.py</div><div id='m_start'> M Start Line: 78</div><div id='m_end'> M End Line: 262</div><div id='n_start'> N Start Line: 78</div><div id='n_end'> N End Line: 245</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        assert flow.cuda.is_available() and flow.cuda.device_count() &gt;= len(device.replace(&quot,&quot, &quot&quot)), \
            f"Invalid CUDA &quot--device {device}&quot requested, use &quot--device cpu&quot or pass valid CUDA device(s)"

    if not (cpu or mps) and <a id="change">flow.cuda.is_available()</a>:  &#47&#47 prefer GPU if available
        devices = device.split(&quot,&quot) if device else &quot0&quot  &#47&#47 range(flow.cuda.device_count())  &#47&#47 i.e. 0,1,6,7
        n = len(devices)  &#47&#47 device count
        if n &gt; 1 and batch_size &gt; 0:  &#47&#47 check batch_size is divisible by device_count</code></pre><h3>After Change</h3><pre><code class='java'>
        os.environ[&quotCUDA_VISIBLE_DEVICES&quot] = device  &#47&#47 set environment variable
        assert with_cuda(), f&quotCUDA unavailable, invalid device {device} requested&quot  &#47&#47 check availability

    cuda = <a id="change">not cpu and with_cuda()</a>
    if cuda:
        devices = device.split(&quot,&quot) if device else &quot0&quot  &#47&#47 range(torch.cuda.device_count())  &#47&#47 i.e. 0,1,6,7
        n = len(devices)  &#47&#47 device count
        if n &gt; 1 and batch_size &gt; 0:  &#47&#47 check batch_size is divisible by device_count
            assert batch_size % n == 0, f&quotbatch-size {batch_size} not multiple of GPU count {n}&quot
        space = &quot &quot * (len(s) + 1)
        for i, d in enumerate(devices):
            s<a id="change"> += </a>f"{&quot&quot if i == 0 else space}CUDA:{d}\n"  &#47&#47 bytes to MB
    else:
        s += &quotCPU\n&quot
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/oneflow-inc/one-yolov5/commit/4f0088a5e5c32f67560514fcff66884e515f7138#diff-887acdc86b965c7bd7350cbe9458a749d11766e0a2551c353cf8a1f56caa7a75L63' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 38323950</div><div id='project'> Project Name: oneflow-inc/one-yolov5</div><div id='commit'> Commit Name: 4f0088a5e5c32f67560514fcff66884e515f7138</div><div id='time'> Time: 2022-07-31</div><div id='author'> Author: ccsuwen@gmail.com</div><div id='file'> File Name: utils/flow_utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: select_device(3)</div><div id='n_method'> N Method Name: select_device(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: utils/flow_utils.py</div><div id='n_file'> N File Name: utils/flow_utils.py</div><div id='m_start'> M Start Line: 65</div><div id='m_end'> M End Line: 96</div><div id='n_start'> N Start Line: 42</div><div id='n_end'> N End Line: 66</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
def init_data_model_parallel(group_count=1, backend=&quotnccl&quot):
  import torch.distributed as dist
  try:
    if <a id="change">dist.is_available()</a>:
      dist.init_process_group(backend=backend)
    glob_world_size, glob_world_rank = dist.get_world_size(), dist.get_rank()
    is_distributed = True</code></pre><h3>After Change</h3><pre><code class='java'>
def init_data_model_parallel(group_count=None, backend=&quotnccl&quot):
  import torch.distributed as dist
  try:
    if <a id="change">(&quotLOCAL_RANK&quot not in os.environ) and (&quotOMPI_COMM_WORLD_SIZE&quot in os.environ)</a>:
        dist.init_process_group(backend=backend,
            init_method=&quottcp://%s:%s&quot % (os.environ[&quotMASTER_ADDR&quot], os.environ.get(&quotMASTER_PORT&quot, &quot23456&quot)),
            rank=int(os.environ[&quotOMPI_COMM_WORLD_RANK&quot]), world_size=int(os.environ[&quotOMPI_COMM_WORLD_SIZE&quot]))
        dist_local_rank<a id="change"> = </a>int(os.environ[&quotOMPI_COMM_WORLD_LOCAL_RANK&quot])
    else:
        dist.init_process_group(backend=backend)
        dist_local_rank = int(os.environ.get(&quotLOCAL_RANK&quot, 0))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/tutel/commit/db843401d56e412a8193f39f99f9215acce2d228#diff-b3d172210f01a1dd8ad68aba4f83ddca474bfd0183d0c8e73636585ac814334cL23' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 38323943</div><div id='project'> Project Name: microsoft/tutel</div><div id='commit'> Commit Name: db843401d56e412a8193f39f99f9215acce2d228</div><div id='time'> Time: 2021-12-23</div><div id='author'> Author: weicu@microsoft.com</div><div id='file'> File Name: tutel/system_init.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: init_data_model_parallel(2)</div><div id='n_method'> N Method Name: init_data_model_parallel(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: tutel/system_init.py</div><div id='n_file'> N File Name: tutel/system_init.py</div><div id='m_start'> M Start Line: 26</div><div id='m_end'> M End Line: 45</div><div id='n_start'> N Start Line: 26</div><div id='n_end'> N End Line: 45</div><BR>