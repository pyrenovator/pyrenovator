<html><h3>Pattern ID :27891
</h3><img src='82836742.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    if len(cfg.device.gpu_ids) &gt; 1:
        print(&quotrank = &quot, local_rank)
        num_gpus = torch.cuda.device_count()
        <a id="change">torch.cuda.set_device(</a>local_rank % num_gpus<a id="change">)</a>
        dist.init_process_group(backend=&quotnccl&quot)
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=cfg.device.batchsize_per_gpu,
                                                       num_workers=cfg.device.workers_per_gpu, pin_memory=True,</code></pre><h3>After Change</h3><pre><code class='java'>
            warnings.warn(&quotWarning! Old .pth checkpoint is deprecated. &quot
                          &quotConvert the checkpoint with tools/convert_old_checkpoint.py &quot)
            ckpt = convert_old_model(ckpt)
        task.load_state_dict(<a id="change">ckpt[&quotstate_dict&quot]</a>)

    model_resume_path = os.path.join(cfg.save_dir, &quotmodel_last.ckpt&quot) if &quotresume&quot in cfg.schedule else None
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 2</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/rangilyu/nanodet/commit/6c7dd670c9bc0311a26bfe0ebb775726902db779#diff-7fc3906bb14652119559420a7b84aac995bfa36f9a7b0f00e215b9f0b6301d9bL43' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 82836742</div><div id='project'> Project Name: rangilyu/nanodet</div><div id='commit'> Commit Name: 6c7dd670c9bc0311a26bfe0ebb775726902db779</div><div id='time'> Time: 2021-04-11</div><div id='author'> Author: lyuchqi@gmail.com</div><div id='file'> File Name: tools/train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: tools/train.py</div><div id='n_file'> N File Name: tools/train.py</div><div id='m_start'> M Start Line: 43</div><div id='m_end'> M End Line: 87</div><div id='n_start'> N Start Line: 51</div><div id='n_end'> N End Line: 91</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Tell PyTorch to use the GPU.     
        print(&quotThere are %d GPU(s) available.&quot %torch.cuda.device_count())
        device_id = get_freer_gpu()
        <a id="change">torch.cuda.set_device(</a>device_id<a id="change">)</a>
        print(&quotWe will use the GPU: %s [%d]&quot %(torch.cuda.get_device_name(device_id), device_id))
        device = torch.device("cuda") 
    else: 
        print(&quotNo GPU available, using the CPU instead.&quot) </code></pre><h3>After Change</h3><pre><code class='java'>
        device_id = get_freer_gpu()
        &#47&#47torch.cuda.set_device(device_id)
        os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
        <a id="change">os.environ["CUDA_VISIBLE_DEVICES"]</a> = str(device_id)
        print(&quotWe will use the GPU: %s [%d]&quot %(torch.cuda.get_device_name(device_id), device_id))
        device = torch.device("cuda") 
    else: </code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lipairui/textgo/commit/451c57056624ecf6ac767c86b7bf00ea997f5ae3#diff-29c2f73c616fc05bbd3ecaa7b8fb5c6d26f033d925eda50ddaa740c3f948afadL43' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 82836741</div><div id='project'> Project Name: lipairui/textgo</div><div id='commit'> Commit Name: 451c57056624ecf6ac767c86b7bf00ea997f5ae3</div><div id='time'> Time: 2020-09-06</div><div id='author'> Author: perryprli@tencent.com</div><div id='file'> File Name: textgo/classifier/utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: get_device(0)</div><div id='n_method'> N Method Name: get_device(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: textgo/classifier/utils.py</div><div id='n_file'> N File Name: textgo/classifier/utils.py</div><div id='m_start'> M Start Line: 48</div><div id='m_end'> M End Line: 49</div><div id='n_start'> N Start Line: 48</div><div id='n_end'> N End Line: 51</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 If the OS is Windows or macOS, use gloo instead of nccl
    dist.init_process_group(backend=backend)
    &#47&#47 set distributed device
    <a id="change">torch.cuda.set_device(</a>local_rank<a id="change">)</a>
    device = torch.device("cuda:{}".format(local_rank))
    if verbose:
        print(f"[init] == local rank: {local_rank}, global rank: {rank} ==")
    return rank, local_rank, device</code></pre><h3>After Change</h3><pre><code class='java'>
    
    rank = int(os.environ["RANK"])
    local_rank = int(os.environ["LOCAL_RANK"])
    word_size = int(<a id="change">os.environ["WORLD_SIZE"]</a>)
    &#47&#47 If the OS is Windows or macOS, use gloo instead of nccl
    dist.init_process_group(backend=backend)
    &#47&#47 set distributed device</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/hongxinxiang/pytorch-multi-gpu-training-tutorial/commit/bc1ff7fe27e5d91d66e4efea68d0d1140e6d1e8f#diff-531f2698417945496a671eb45abd0acdabbd74b2e04a833b00404ca52b4a224eL19' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 82836740</div><div id='project'> Project Name: hongxinxiang/pytorch-multi-gpu-training-tutorial</div><div id='commit'> Commit Name: bc1ff7fe27e5d91d66e4efea68d0d1140e6d1e8f</div><div id='time'> Time: 2022-07-19</div><div id='author'> Author: xianghx21@gmail.com</div><div id='file'> File Name: single-machine-and-multi-GPU-DistributedDataParallel-launch.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: setup_DDP(2)</div><div id='n_method'> N Method Name: setup_DDP(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: single-machine-and-multi-GPU-DistributedDataParallel-launch.py</div><div id='n_file'> N File Name: single-machine-and-multi-GPU-DistributedDataParallel-launch.py</div><div id='m_start'> M Start Line: 35</div><div id='m_end'> M End Line: 39</div><div id='n_start'> N Start Line: 36</div><div id='n_end'> N End Line: 43</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        world_size=dist_params[&quotword_size&quot]
    )

    <a id="change">torch.cuda.set_device(</a>local_rank<a id="change">)</a>

    args, kwargs = args
    func(*args, **kwargs)
</code></pre><h3>After Change</h3><pre><code class='java'>
        )
    )

    set_device_type(<a id="change">dist_params[&quotdevice_type&quot]</a>)

    torch.distributed.init_process_group(
        backend=dist_params[&quotdist_backend&quot],</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/cnstark/easytorch/commit/983b725c12a7032481d039d6190764816e0e1ddb#diff-aa41d1555bd99c7520eda551decf5039db59a79fcdc16d26df7f38af36ec6c53L10' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 82836739</div><div id='project'> Project Name: cnstark/easytorch</div><div id='commit'> Commit Name: 983b725c12a7032481d039d6190764816e0e1ddb</div><div id='time'> Time: 2022-10-25</div><div id='author'> Author: 45590791+cnstark@users.noreply.github.com</div><div id='file'> File Name: easytorch/launcher/dist_wrap.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: dist_func(3)</div><div id='n_method'> N Method Name: dist_func(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: easytorch/launcher/dist_wrap.py</div><div id='n_file'> N File Name: easytorch/launcher/dist_wrap.py</div><div id='m_start'> M Start Line: 37</div><div id='m_end'> M End Line: 37</div><div id='n_start'> N Start Line: 31</div><div id='n_end'> N End Line: 40</div><BR>