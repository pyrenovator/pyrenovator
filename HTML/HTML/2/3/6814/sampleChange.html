<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            &#47&#47    logger.info(f"Total Norm beginning {total_norm}")
            for g, p in zip(gradients, params):
                if is_model_parallel_parameter(p) or (self.model_parallel_rank == 0):
                    param_norm = <a id="change">g.data.double().norm(</a>2<a id="change">)</a>
                    total_norm += param_norm.item()**2
            &#47&#47 Sum across all model parallel GPUs.
            total_norm_cuda = torch.cuda.FloatTensor([float(total_norm)])
</code></pre><h3>After Change</h3><pre><code class='java'>
        else:
            &#47&#47 if dist.get_rank() == 0:
            &#47&#47    logger.info(f"Total Norm beginning {total_norm}")
            grad_norms = <a id="change">[]</a>
            for g, p in zip(gradients, params):
                if is_model_parallel_parameter(p) or (self.model_parallel_rank == 0):
                    grad_norms.append(g.cuda(non_blocking=True).double().norm(2))

            &#47&#47 Sum across all model parallel GPUs.
            total_norm_cuda = torch.sum(torch.pow(torch.stack(grad_norms), 2))

            torch.distributed.all_reduce(total_norm_cuda,
                                         op=torch.distributed.ReduceOp.SUM,
                                         group=self.dp_process_group)

            self._model_parallel_all_reduce(tensor=total_norm_cuda,
                                            op=torch.distributed.ReduceOp.SUM)

            total_norm = <a id="change">total_norm_cuda.item()</a>**(1. / norm_type)

        if total_norm == float(
                &quotinf&quot) or total_norm == -float(&quotinf&quot) or total_norm != total_norm:</code></pre>