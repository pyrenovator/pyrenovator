<html><h3>Pattern ID :9414
</h3><img src='33674144.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        entry_count = entry_count.expand_as(cumulative_sum)  &#47&#47 [1, T] =&gt; [B, T]

        cum_mean = cumulative_sum / entry_count  &#47&#47 B, T
        cum_var = (cumulative_pow_sum - 2 * cum_mean * cumulative_sum) / entry_count + <a id="change">cum_mean.pow(</a>2<a id="change">)</a>  &#47&#47 B, T
        cum_std = (cum_var + eps).sqrt()  &#47&#47 B, T

        cum_mean = cum_mean.reshape(batch_size * n_channels, 1, n_frames)
        cum_std = cum_std.reshape(batch_size * n_channels, 1, n_frames)

        x<a id="change"> = </a>(input - cum_mean) / cum_std

        if n_dim == 4:
            x = x.reshape(batch_size, n_channels, n_freqs, n_frames)</code></pre><h3>After Change</h3><pre><code class='java'>

        normed = (input - cumulative_mean) / cumulative_std

        return <a id="change">normed.reshape(</a>batch_size, num_channels, num_freqs, num_frames<a id="change">)</a>

    def norm_wrapper(self, norm_type: str):
        if norm_type == "offline_laplace_norm":
            norm = self.offline_laplace_norm</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/haoxiangsnr/fullsubnet/commit/321920989a4aa255158770213e814befc8f94f27#diff-87e6f26f790592e0e88f3e3e6e0565e3c91814fa9ef24279702e757caa37fff2L225' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33674144</div><div id='project'> Project Name: haoxiangsnr/fullsubnet</div><div id='commit'> Commit Name: 321920989a4aa255158770213e814befc8f94f27</div><div id='time'> Time: 2021-02-08</div><div id='author'> Author: haoxiangsnr@gmail.com</div><div id='file'> File Name: audio_zen/model/base_model.py</div><div id='m_class'> M Class Name: BaseModel</div><div id='n_method'> N Class Name: BaseModel</div><div id='m_method'> M Method Name: cumulative_layer_norm(1)</div><div id='n_method'> N Method Name: cumulative_layer_norm(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: audio_zen/model/base_model.py</div><div id='n_file'> N File Name: audio_zen/model/base_model.py</div><div id='m_start'> M Start Line: 225</div><div id='m_end'> M End Line: 261</div><div id='n_start'> N Start Line: 276</div><div id='n_end'> N End Line: 304</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 compute x^k
        exponentials = torch.zeros([x.shape[0], x.shape[1], self.order + 1])
        for o in range(self.order + 1):
            exponentials[:, :, o] = <a id="change">torch.pow(</a>x, o<a id="change">)</a>

        &#47&#47 multiply by coefficients
        exponentials = torch.multiply(exponentials, self.coeffs)

        &#47&#47 Collapse dimensions
        exponentials<a id="change"> = </a>torch.sum(exponentials, dim=2)

        &#47&#47 sum all values for each dim
        return torch.sum(exponentials, dim=1)</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 Tensorflow&quots exponentiation gives float64 values if x are float32
            &#47&#47 and the exponent are integer
            ks = anp.array(range(self.order + 1), dtype=x.dtype, like=x)
            exponentials = <a id="change">x.reshape(</a>x.shape + (1,)<a id="change">)</a> ** ks
            assert exponentials.dtype == x.dtype
            if exponentials.dtype != self.coeffs.dtype:
                &#47&#47 Tensorflow does not automatically cast float32 to complex128,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/esa/torchquad/commit/a3a1438960cc83afb6844c8279415e131be2ca42#diff-fe1b22e294ae47b16639a2139938eeec68d387dfa25d702da595cb2e75ef1bafL76' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33674146</div><div id='project'> Project Name: esa/torchquad</div><div id='commit'> Commit Name: a3a1438960cc83afb6844c8279415e131be2ca42</div><div id='time'> Time: 2022-03-14</div><div id='author'> Author: ga84muv@mytum.de</div><div id='file'> File Name: torchquad/tests/integration_test_functions.py</div><div id='m_class'> M Class Name: Polynomial</div><div id='n_method'> N Class Name: Polynomial</div><div id='m_method'> M Method Name: _poly(2)</div><div id='n_method'> N Method Name: _poly(2)</div><div id='m_parent_class'> M Parent Class: IntegrationTestFunction</div><div id='n_parent_class'> N Parent Class: IntegrationTestFunction</div><div id='m_file'> M File Name: torchquad/tests/integration_test_functions.py</div><div id='n_file'> N File Name: torchquad/tests/integration_test_functions.py</div><div id='m_start'> M Start Line: 78</div><div id='m_end'> M End Line: 89</div><div id='n_start'> N Start Line: 123</div><div id='n_end'> N End Line: 147</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
      batch_mean, batch_var = self.running_mean, self.running_var
      &#47&#47 NOTE: this can be precomputed for static inference. if you manually update running_var, you have to reset this
      if not hasattr(self, "batch_invstd") or not self.batch_invstd:
        self.batch_invstd = <a id="change">batch_var.add(self.eps).pow(</a>-0.5<a id="change">)</a>
      batch_invstd<a id="change"> = </a>self.batch_invstd

    return x.batchnorm(self.weight, self.bias, batch_mean, batch_invstd)
</code></pre><h3>After Change</h3><pre><code class='java'>
    else:
      batch_mean = self.running_mean
      &#47&#47 NOTE: this can be precomputed for static inference. we expand it here so it fuses
      batch_invstd = <a id="change">self.running_var.reshape(</a>1, -1, 1, 1<a id="change">)</a>.expand(x.shape).add(self.eps).rsqrt()

    return x.batchnorm(self.weight, self.bias, batch_mean, batch_invstd)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/geohot/tinygrad/commit/b12b60af2063ff148f0bde8cee8985bd0cf39cf5#diff-7ac848bc13575d7110063958cae9cfce8bc428a18b16f0ee2f9291a4ca0c3bb3L14' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33674159</div><div id='project'> Project Name: geohot/tinygrad</div><div id='commit'> Commit Name: b12b60af2063ff148f0bde8cee8985bd0cf39cf5</div><div id='time'> Time: 2023-03-22</div><div id='author'> Author: 72895+geohot@users.noreply.github.com</div><div id='file'> File Name: tinygrad/nn/__init__.py</div><div id='m_class'> M Class Name: BatchNorm2d</div><div id='n_method'> N Class Name: BatchNorm2d</div><div id='m_method'> M Method Name: __call__(2)</div><div id='n_method'> N Method Name: __call__(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: tinygrad/nn/__init__.py</div><div id='n_file'> N File Name: tinygrad/nn/__init__.py</div><div id='m_start'> M Start Line: 24</div><div id='m_end'> M End Line: 36</div><div id='n_start'> N Start Line: 31</div><div id='n_end'> N End Line: 33</div><BR>