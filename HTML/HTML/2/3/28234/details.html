<html><h3>Pattern ID :28234
</h3><img src='83402063.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        query_idx = list(map(lambda x: find_pattern(x[0], x[1]), zip(search_queries, input_ids.tolist())))

        if isinstance(layer, list):
            representations = list(<a id="change">map(</a>lambda x: x[torch.arange(num_inputs)[:, None], query_idx].mean(1), hidden_states<a id="change">)</a>)
        else:
            if layer != &quotall&quot:
                if layer is None:</code></pre><h3>After Change</h3><pre><code class='java'>
            elif layer &gt; self.layers:
                raise ValueError(f"Number of layers specified ({layer}) exceed layers in model ({self.layers})!")
            &#47&#47 representations = hidden_states[torch.arange(num_inputs)[:, None], query_idx].mean(1)
            representations = <a id="change">torch.stack(</a>[hs.squeeze()[idx[0]:idx[1]].mean(0) for hs, idx in zip(hidden_states.split(<a id="change">[</a>1<a id="change"></a>] * num_inputs), query_idx)]<a id="change">)</a>
        
        return representations

    def extract_paired_representations(self, sentence_words: Union[Tuple[str], List[Tuple[str]]], layer:int = None) -&gt; Tuple:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/kanishkamisra/minicons/commit/b9f76e2dfc167677a5e1a4877b8bc76799a01974#diff-8c1f2fc97e5f7216e0680ae998509ceb2ab120468507039f6c95d403310ffc43L109' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 83402063</div><div id='project'> Project Name: kanishkamisra/minicons</div><div id='commit'> Commit Name: b9f76e2dfc167677a5e1a4877b8bc76799a01974</div><div id='time'> Time: 2021-08-21</div><div id='author'> Author: menogetusername@gmail.com</div><div id='file'> File Name: minicons/cwe.py</div><div id='m_class'> M Class Name: CWE</div><div id='n_method'> N Class Name: CWE</div><div id='m_method'> M Method Name: extract_representation(3)</div><div id='n_method'> N Method Name: extract_representation(3)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: minicons/cwe.py</div><div id='n_file'> N File Name: minicons/cwe.py</div><div id='m_start'> M Start Line: 109</div><div id='m_end'> M End Line: 132</div><div id='n_start'> N Start Line: 110</div><div id='n_end'> N End Line: 132</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            else:
                return None

        new_state = tuple(<a id="change">map(</a>reorder_state, cached_state<a id="change">)</a>)
        utils.set_incremental_state(self, incremental_state, &quotcached_state&quot, new_state)

    def max_positions(self):</code></pre><h3>After Change</h3><pre><code class='java'>
        if incremental_state is None or len(incremental_state) == 0:
            return
        prev_hiddens, prev_cells, input_feed = self.get_cached_state(incremental_state)
        cached_state = (prev_hiddens, prev_cells, <a id="change">[</a>input_feed<a id="change"></a>])
        new_state = [self.reorder_state(state, new_order) for state in cached_state]
        prev_hiddens_tensor = <a id="change">torch.stack(</a>new_state[0]<a id="change">)</a>
        prev_cells_tensor = torch.stack(new_state[1])
        cached_state_new = torch.jit.annotate(
            Dict[str, Optional[Tensor]],
            {"prev_hiddens": prev_hiddens_tensor, "prev_cells": prev_cells_tensor, "input_feed": new_state[2][0]})</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/kssteven418/i-bert/commit/57526c63433c0b1c997fc91c0881867532567266#diff-6ce7c0d4f0fb1f8f34940039a8bb42a6d902a8894dd18517bee43dac3fc09971L498' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 83402064</div><div id='project'> Project Name: kssteven418/i-bert</div><div id='commit'> Commit Name: 57526c63433c0b1c997fc91c0881867532567266</div><div id='time'> Time: 2020-04-16</div><div id='author'> Author: xfrui@fb.com</div><div id='file'> File Name: fairseq/models/lstm.py</div><div id='m_class'> M Class Name: LSTMDecoder</div><div id='n_method'> N Class Name: LSTMDecoder</div><div id='m_method'> M Method Name: reorder_incremental_state(3)</div><div id='n_method'> N Method Name: reorder_incremental_state(3)</div><div id='m_parent_class'> M Parent Class: FairseqIncrementalDecoder</div><div id='n_parent_class'> N Parent Class: FairseqIncrementalDecoder</div><div id='m_file'> M File Name: fairseq/models/lstm.py</div><div id='n_file'> N File Name: fairseq/models/lstm.py</div><div id='m_start'> M Start Line: 498</div><div id='m_end'> M End Line: 513</div><div id='n_start'> N Start Line: 538</div><div id='n_end'> N End Line: 550</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.blocks = nn.ModuleList([ReversibleBlock(f, g) for (f, g) in blocks])

    def forward(self, x, arg_route = (True, True), **kwargs):
        f_args, g_args = <a id="change">map(</a>lambda route: kwargs if route else {}, arg_route<a id="change">)</a>
        block_kwargs = {&quotf_args&quot: f_args, &quotg_args&quot: g_args}
        return _ReversibleFunction.apply(x, self.blocks, block_kwargs)
</code></pre><h3>After Change</h3><pre><code class='java'>
        self.blocks = nn.ModuleList([ReversibleBlock(f=f, g=g) for f, g in blocks])

    def forward(self, x, **kwargs):
        x = torch.cat(<a id="change">[</a>x, x<a id="change"></a>], dim=-1)

        blocks = self.blocks
        args = route_args(self.args_route, kwargs, len(blocks))
        args = list(map(lambda x: {&quotf_args&quot: x[0], &quotg_args&quot: x[1]}, args))

        layers_and_args = list(zip(blocks, args))

        if self.training and self.layer_dropout &gt; 0:
            layers_and_args = layer_drop(layers_and_args, self.layer_dropout)
            blocks, args = map(lambda ind: list(map(itemgetter(ind), layers_and_args)), (0, 1))

        out =  _ReversibleFunction.apply(x, blocks, args)
        return <a id="change">torch.stack(</a>out.chunk(2, dim=-1)<a id="change">)</a>.sum(dim=0)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/linear-attention-transformer/commit/fa23ce09a98a63d26116e3935ad5902cf705255d#diff-29d3c048298bdaa9a9670921a4026430e5b09b55bc1da20d65da18bd5c85f575L118' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 83402066</div><div id='project'> Project Name: lucidrains/linear-attention-transformer</div><div id='commit'> Commit Name: fa23ce09a98a63d26116e3935ad5902cf705255d</div><div id='time'> Time: 2020-06-04</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: linear_attention_transformer/reversible.py</div><div id='m_class'> M Class Name: ReversibleSequence</div><div id='n_method'> N Class Name: ReversibleSequence</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: linear_attention_transformer/reversible.py</div><div id='n_file'> N File Name: linear_attention_transformer/reversible.py</div><div id='m_start'> M Start Line: 118</div><div id='m_end'> M End Line: 121</div><div id='n_start'> N Start Line: 161</div><div id='n_end'> N End Line: 174</div><BR>