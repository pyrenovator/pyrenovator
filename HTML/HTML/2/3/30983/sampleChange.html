<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        }
        state.update(client_state)

        logger.info(<a id="change">&quotSaving model checkpoint: {}&quot.format(</a>save_path<a id="change">)</a>)
        torch.save(state, save_path)

    def _save_zero_checkpoint(self, save_path, tag):</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 A hack to save the checkpointing directory. Pipeline parallelism overrides
        &#47&#47 module_state_dict() and uses this path to save the model. module_state_dict()
        &#47&#47 then instead just returns self._curr_save_path.
        self._curr_save_path = <a id="change">os.path.dirname(</a>save_path<a id="change">)</a>

        state = {
            &quotmodule&quot:
            self.module_state_dict(),</code></pre>