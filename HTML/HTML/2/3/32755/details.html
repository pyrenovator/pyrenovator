<html><h3>Pattern ID :32755
</h3><img src='95145203.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        super().__init__()
        emb_dim = default(emb_dim, dim)
        self.token_emb = nn.Embedding(num_tokens, emb_dim)
        self.pos_emb = <a id="change">nn.Embedding(</a>max_seq_len, emb_dim<a id="change">)</a>
        self.to_model_dim = identity if emb_dim == dim else nn.Linear(emb_dim, dim)

        self.reformer = Reformer(dim, depth, max_seq_len, heads = heads, bucket_size = bucket_size, n_hashes = n_hashes, ff_chunks = ff_chunks, attn_chunks = attn_chunks, causal = causal, weight_tie = weight_tie, lsh_dropout = lsh_dropout, random_rotations_per_head = random_rotations_per_head, twin_attention = twin_attention, use_scale_norm = use_scale_norm, use_full_attn = use_full_attn, full_attn_thres = full_attn_thres, num_mem_kv = num_mem_kv)
        self.to_logits = identity if return_embeddings else nn.Linear(dim, num_tokens)</code></pre><h3>After Change</h3><pre><code class='java'>
        super().__init__()
        emb_dim = default(emb_dim, dim)
        self.token_emb = nn.Embedding(num_tokens, emb_dim)
        self.pos_emb = FixedPositionEmbedding(emb_dim)<a id="change"> if </a>fixed_position_emb<a id="change"> else </a><a id="change">nn.Embedding(</a>max_seq_len, emb_dim<a id="change">)</a>
        self.to_model_dim = identity if emb_dim == dim else nn.Linear(emb_dim, dim)

        self.reformer = Reformer(dim, depth, max_seq_len, heads = heads, bucket_size = bucket_size, n_hashes = n_hashes, ff_chunks = ff_chunks, attn_chunks = attn_chunks, causal = causal, weight_tie = weight_tie, lsh_dropout = lsh_dropout, random_rotations_per_head = random_rotations_per_head, twin_attention = twin_attention, use_scale_norm = use_scale_norm, use_full_attn = use_full_attn, full_attn_thres = full_attn_thres, num_mem_kv = num_mem_kv)
        self.to_logits = identity if return_embeddings else nn.Linear(dim, num_tokens)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/reformer-pytorch/commit/89a0d83b01971b3fc7ff28e57b931af62f0289af#diff-7e740488ff2ce47996cfdd5356981ed3e6ac2bb38739a71b400275ac91d1c3bfL476' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95145203</div><div id='project'> Project Name: lucidrains/reformer-pytorch</div><div id='commit'> Commit Name: 89a0d83b01971b3fc7ff28e57b931af62f0289af</div><div id='time'> Time: 2020-01-29</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: reformer_pytorch/reformer_pytorch.py</div><div id='m_class'> M Class Name: ReformerLM</div><div id='n_method'> N Class Name: ReformerLM</div><div id='m_method'> M Method Name: __init__(22)</div><div id='n_method'> N Method Name: __init__(21)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: reformer_pytorch/reformer_pytorch.py</div><div id='n_file'> N File Name: reformer_pytorch/reformer_pytorch.py</div><div id='m_start'> M Start Line: 476</div><div id='m_end'> M End Line: 478</div><div id='n_start'> N Start Line: 485</div><div id='n_end'> N End Line: 489</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    ):
        super().__init__()
        self.token_emb = nn.Embedding(num_tokens, dim)
        self.pos_emb = <a id="change">nn.Embedding(</a>max_seq_len, dim<a id="change">)</a>

        self.layers = nn.ModuleList([])

        for _ in range(depth):</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 positional embeddings

        self.abs_pos_emb = <a id="change">nn.Embedding(</a>max_seq_len, dim<a id="change">) if </a>absolute_pos_emb<a id="change"> else </a>None

        layer_pos_emb = None
        if not absolute_pos_emb:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/fast-transformer-pytorch/commit/1072a19a1eef44234a5637d5e916050173300b28#diff-baf9a81f13acc33e891fd7b4e107bcf15ac301e0a4b12d120a33f224ec154333L107' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95145206</div><div id='project'> Project Name: lucidrains/fast-transformer-pytorch</div><div id='commit'> Commit Name: 1072a19a1eef44234a5637d5e916050173300b28</div><div id='time'> Time: 2021-08-25</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: fast_transformer_pytorch/fast_transformer_pytorch.py</div><div id='m_class'> M Class Name: FastTransformer</div><div id='n_method'> N Class Name: FastTransformer</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: fast_transformer_pytorch/fast_transformer_pytorch.py</div><div id='n_file'> N File Name: fast_transformer_pytorch/fast_transformer_pytorch.py</div><div id='m_start'> M Start Line: 120</div><div id='m_end'> M End Line: 120</div><div id='n_start'> N Start Line: 165</div><div id='n_end'> N End Line: 174</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        **kwargs
    ):
        super().__init__()
        self.time_embeddings = <a id="change">nn.Embedding(</a>num_timesteps, dim<a id="change">)</a>  &#47&#47 also offer a continuous version of timestep embeddings, with a 2 layer MLP
        self.learned_query = nn.Parameter(torch.randn(dim))
        self.causal_transformer = CausalTransformer(dim = dim, **kwargs)
</code></pre><h3>After Change</h3><pre><code class='java'>
        **kwargs
    ):
        super().__init__()
        self.time_embeddings = <a id="change">nn.Embedding(</a>num_timesteps, dim<a id="change">) if </a>exists(num_timesteps)<a id="change"> else </a>nn.Sequential(Rearrange(&quotb -&gt; b 1&quot), MLP(1, dim)) &#47&#47 also offer a continuous version of timestep embeddings, with a 2 layer MLP
        self.learned_query = nn.Parameter(torch.randn(dim))
        self.causal_transformer = CausalTransformer(dim = dim, **kwargs)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/dalle2-pytorch/commit/7fb3f695d5afe76a4e71a6626d60ecfe1be5f0b7#diff-038ecede954c29266888cb88e37cc06f61ba2433f8b5142c7b2b2cefde5ed0edL235' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95145205</div><div id='project'> Project Name: lucidrains/dalle2-pytorch</div><div id='commit'> Commit Name: 7fb3f695d5afe76a4e71a6626d60ecfe1be5f0b7</div><div id='time'> Time: 2022-04-14</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: dalle2_pytorch/dalle2_pytorch.py</div><div id='m_class'> M Class Name: DiffusionPriorNetwork</div><div id='n_method'> N Class Name: DiffusionPriorNetwork</div><div id='m_method'> M Method Name: __init__(3)</div><div id='n_method'> N Method Name: __init__(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dalle2_pytorch/dalle2_pytorch.py</div><div id='n_file'> N File Name: dalle2_pytorch/dalle2_pytorch.py</div><div id='m_start'> M Start Line: 242</div><div id='m_end'> M End Line: 242</div><div id='n_start'> N Start Line: 282</div><div id='n_end'> N End Line: 282</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        image_seq_len = (vae.image_size // (2 ** vae.num_layers)) ** 2

        self.text_emb = nn.Embedding(num_text_tokens, dim)
        self.image_emb = <a id="change">nn.Embedding(</a>num_image_tokens, dim<a id="change">)</a>

        self.text_pos_emb = nn.Embedding(text_seq_len, dim)
        self.image_pos_emb = nn.Embedding(image_seq_len, dim)
</code></pre><h3>After Change</h3><pre><code class='java'>
        image_seq_len = (vae.image_size // (2 ** vae.num_layers)) ** 2

        self.text_emb = nn.Embedding(num_text_tokens, dim)
        self.image_emb = <a id="change">nn.Embedding(</a>num_image_tokens, dim<a id="change">) if </a>not tie_image_embedding<a id="change"> else </a>vae.codebook  &#47&#47 leave an area of uncertainty up to user to figure out

        self.text_pos_emb = nn.Embedding(text_seq_len, dim)
        self.image_pos_emb = nn.Embedding(image_seq_len, dim)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/dalle-pytorch/commit/66bf8cd88cfae953d1982e29b81510de52cae53c#diff-7fee3463af42e34e03d754e24fa9b24e3cde98aae198d2cb45f200d6da301016L210' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95145204</div><div id='project'> Project Name: lucidrains/dalle-pytorch</div><div id='commit'> Commit Name: 66bf8cd88cfae953d1982e29b81510de52cae53c</div><div id='time'> Time: 2021-01-07</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: dalle_pytorch/dalle_pytorch.py</div><div id='m_class'> M Class Name: DALLE</div><div id='n_method'> N Class Name: DALLE</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dalle_pytorch/dalle_pytorch.py</div><div id='n_file'> N File Name: dalle_pytorch/dalle_pytorch.py</div><div id='m_start'> M Start Line: 223</div><div id='m_end'> M End Line: 227</div><div id='n_start'> N Start Line: 224</div><div id='n_end'> N End Line: 228</div><BR>