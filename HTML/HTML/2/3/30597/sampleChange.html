<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        loss = (self.entropy_coeff * log_probs - values).mean()

        optimizer.zero_grad()
        <a id="change">loss.backward()</a>
        if self.max_grad &gt; 0:
            T.nn.utils.clip_grad_norm_(actor.parameters(), self.max_grad)
        optimizer.step()
</code></pre><h3>After Change</h3><pre><code class='java'>
        :param observations:
        
        actor_parameters = actor.parameters()
        optimizer<a id="change"> = </a>self.optimizer_class(actor_parameters, lr=self.lr)
        &#47&#47 make sure critic isn&quott updated!
        if critic2 is not None:
            critic_variables = critic1.parameters() + critic2.parameters()
        else:
            critic_variables = critic1.parameters()
        for var in critic_variables:
            var.requires_grad = False

        distributions = actor.get_action_distribution(observations)
        &#47&#47 use the reparametrization trick for backpropagation
        &#47&#47 https://gregorygundersen.com/blog/2018/04/29/reparameterization/
        actions = distributions.rsample()
        if self.squashed_output:
            actions = T.tanh(actions)
        log_probs = distributions.log_prob(actions)
        entropy = distributions.entropy().mean()

        values1 = critic1(observations, actions)
        if critic2 is not None:
            values2 = critic2(observations, actions)
            values = T.min(values1, values2)
        else:
            values = values1

        loss = (self.entropy_coeff * log_probs - values).mean()

        <a id="change">self.run_optimizer(</a>optimizer, loss, actor_parameters<a id="change">)</a>

        &#47&#47 reset critic parameters
        for var in critic_variables:
            var.requires_grad = True</code></pre>