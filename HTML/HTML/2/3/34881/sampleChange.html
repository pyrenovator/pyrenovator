<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        db = torch.zeros((heads, src_seq, tgt_seq), device = device, dtype = dtype) if attn_bias.requires_grad else torch.zeros((heads, 0, 0), device = device, dtype = dtype)
        do_scaled = torch.zeros_like(l)

        <a id="change">backward(</a>do, o, l, q, k, v, dq, dk, dv, db, do_scaled, mask, attn_bias, scale, causal, q_block_size, k_block_size<a id="change">)</a>

        db = db if attn_bias.requires_grad else None

        return dq, dk, dv, None, db, None, None, None, None</code></pre><h3>After Change</h3><pre><code class='java'>

        db = db if attn_bias.requires_grad else None

        <a id="change">return </a>dq<a id="change">, dk, dv, None, db, None, None, None, None, None, None</a>

&#47&#47 wrapper function

def flash_cosine_sim_attention(</code></pre>