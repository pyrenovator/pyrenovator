<html><h3>Pattern ID :34881
</h3><img src='99952929.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        db = torch.zeros((heads, src_seq, tgt_seq), device = device, dtype = dtype) if attn_bias.requires_grad else torch.zeros((heads, 0, 0), device = device, dtype = dtype)
        do_scaled = torch.zeros_like(l)

        <a id="change">backward(</a>do, o, l, q, k, v, dq, dk, dv, db, do_scaled, mask, attn_bias, scale, causal, q_block_size, k_block_size<a id="change">)</a>

        db = db if attn_bias.requires_grad else None

        return dq, dk, dv, None, db, None, None, None, None</code></pre><h3>After Change</h3><pre><code class='java'>

        db = db if attn_bias.requires_grad else None

        <a id="change">return </a>dq<a id="change">, dk, dv, None, db, None, None, None, None, None, None</a>

&#47&#47 wrapper function

def flash_cosine_sim_attention(</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/flash-cosine-sim-attention/commit/1dec3cb2ccf7f31f8c28a130899d3885cd3abcc7#diff-9682bb6c6f80a5ac1b8cf3f60ac58f5e21edb3235f4a36b9caed7ba4823b0931L110' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 99952929</div><div id='project'> Project Name: lucidrains/flash-cosine-sim-attention</div><div id='commit'> Commit Name: 1dec3cb2ccf7f31f8c28a130899d3885cd3abcc7</div><div id='time'> Time: 2022-09-04</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='m_class'> M Class Name: FlashCosineSimAttention</div><div id='n_method'> N Class Name: FlashCosineSimAttention</div><div id='m_method'> M Method Name: backward(2)</div><div id='n_method'> N Method Name: backward(2)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='n_file'> N File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='m_start'> M Start Line: 110</div><div id='m_end'> M End Line: 124</div><div id='n_start'> N Start Line: 114</div><div id='n_end'> N End Line: 130</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    @staticmethod
    def backward(ctx, do):
        o, l, q, k, v = ctx.saved_tensors
        dq, dk, dv = <a id="change">backward(</a>do, o, l, q, k, v<a id="change">)</a>
        return dq, dk, dv

&#47&#47 wrapper function
</code></pre><h3>After Change</h3><pre><code class='java'>
        scale = ctx.scale

        dq, dk, dv = backward(do, o, l, q, k, v, scale)
        <a id="change">return </a>dq<a id="change">, dk, dv, None</a>

&#47&#47 wrapper function

def flash_cosine_sim_attention(q, k, v, scale = 8):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/flash-cosine-sim-attention/commit/ad07af6f17255ebe36d46a7d1053e0c03b5a6aea#diff-9682bb6c6f80a5ac1b8cf3f60ac58f5e21edb3235f4a36b9caed7ba4823b0931L30' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 99952928</div><div id='project'> Project Name: lucidrains/flash-cosine-sim-attention</div><div id='commit'> Commit Name: ad07af6f17255ebe36d46a7d1053e0c03b5a6aea</div><div id='time'> Time: 2022-08-13</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='m_class'> M Class Name: FlashCosineSimAttention</div><div id='n_method'> N Class Name: FlashCosineSimAttention</div><div id='m_method'> M Method Name: backward(2)</div><div id='n_method'> N Method Name: backward(2)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='n_file'> N File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='m_start'> M Start Line: 32</div><div id='m_end'> M End Line: 33</div><div id='n_start'> N Start Line: 35</div><div id='n_end'> N End Line: 38</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        db = torch.zeros((heads, src_seq, tgt_seq), device = device, dtype = dtype) if attn_bias.requires_grad else torch.zeros((heads, 0, 0), device = device, dtype = dtype)
        do_scaled = torch.zeros_like(l)

        <a id="change">backward(</a>do, o, l, q, k, v, dq, dk, dv, db, do_scaled, mask, attn_bias, scale, causal, q_block_size, k_block_size, backward_tile_size<a id="change">)</a>

        db = db if attn_bias.requires_grad else None

        return dq, dk, dv, None, db, None, None, None, None, None, None</code></pre><h3>After Change</h3><pre><code class='java'>

        db = db if attn_bias.requires_grad else None

        <a id="change">return </a>dq<a id="change">, dk, dv, None, db, None, None, None, None, None, None, None, None, None, None</a>

&#47&#47 wrapper function

def flash_cosine_sim_attention(</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/flash-cosine-sim-attention/commit/4eee865e17a85bb19074dc4f9f9be84d113f0f07#diff-9682bb6c6f80a5ac1b8cf3f60ac58f5e21edb3235f4a36b9caed7ba4823b0931L109' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 99952930</div><div id='project'> Project Name: lucidrains/flash-cosine-sim-attention</div><div id='commit'> Commit Name: 4eee865e17a85bb19074dc4f9f9be84d113f0f07</div><div id='time'> Time: 2022-09-07</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='m_class'> M Class Name: FlashCosineSimAttention</div><div id='n_method'> N Class Name: FlashCosineSimAttention</div><div id='m_method'> M Method Name: backward(2)</div><div id='n_method'> N Method Name: backward(2)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='n_file'> N File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='m_start'> M Start Line: 114</div><div id='m_end'> M End Line: 130</div><div id='n_start'> N Start Line: 140</div><div id='n_end'> N End Line: 170</div><BR>