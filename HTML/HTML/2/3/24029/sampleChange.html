<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if torch.cuda.is_available():
            dist.all_reduce(torch.zeros(1).cuda())
        else:
            <a id="change">dist.all_reduce(</a>torch.zeros(1)<a id="change">)</a>

        if is_master(args):
            logging.getLogger().setLevel(logging.INFO)
        else:</code></pre><h3>After Change</h3><pre><code class='java'>
    if args.distributed_world_size == 1:
        raise ValueError(&quotCannot initialize distributed with distributed_world_size=1&quot)

    <a id="change">if </a>not getattr(args, &quottpu&quot, False):
        if torch.distributed.is_initialized():
            warnings.warn(&quotDistributed is already initialized, cannot initialize twice!&quot)
        else:
            logger.info(&quotdistributed init (rank {}): {}&quot.format(
                args.distributed_rank, args.distributed_init_method,
            ))
            dist.init_process_group(
                backend=args.distributed_backend,
                init_method=args.distributed_init_method,
                world_size=args.distributed_world_size,
                rank=args.distributed_rank,
            )
            logger.info(&quotinitialized host {} as rank {}&quot.format(
                socket.gethostname(), args.distributed_rank,
            ))

            &#47&#47 perform a dummy all-reduce to initialize the NCCL communicator
            if torch.cuda.is_available():
                dist.all_reduce(torch.zeros(1).cuda())

        args.distributed_rank = torch.distributed.get_rank()
    else:
        import torch_xla.core.xla_model as xm
        assert xm.xrt_world_size() == args.distributed_world_size
        args.device_id = xm.get_local_ordinal()
        args.distributed_rank<a id="change"> = </a>xm.get_ordinal()
        xm.rendezvous(&quotdistributed_init&quot)  &#47&#47 wait for all workers
        xm.mark_step()
</code></pre>