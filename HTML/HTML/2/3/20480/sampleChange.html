<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 zero stage 1 mode
        if not self.partition_gradients:
            required_version = pkg_version.parse("0.3.17")
            ckpt_version = <a id="change">state_dict_list[0]</a>.get("ds_version", False)
            error_str = f"ZeRO stage 1 changed in {required_version} and is not backwards compatible " \
                "with older stage 1 checkpoints. If you&quotd like to load an old ZeRO-1 checkpoint " \
                "please use an older version of DeepSpeed (&lt;= 0.5.8) and set &quotlegacy_stage1&quot: true in your zero config json."</code></pre><h3>After Change</h3><pre><code class='java'>
        

        &#47&#47 I think it should actually be ok to reload the optimizer before the model.
        dp_rank = <a id="change">dist.get_rank(group=self.dp_process_group)</a>
        current_rank_sd = state_dict_list[dp_rank]
        self.loss_scaler = current_rank_sd[&quotloss_scaler&quot]
        self.dynamic_loss_scale = current_rank_sd[&quotdynamic_loss_scale&quot]
        self.overflow = current_rank_sd[&quotoverflow&quot]</code></pre>