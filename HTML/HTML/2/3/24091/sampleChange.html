<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 Need to average total_norm across different GPUs due to the presence of moe params
    pg = groups.get_data_parallel_group()
    scaled_norm = total_norm * 1.0 / float(dist.get_world_size(group=pg))
    scaled_norm_tensor = <a id="change">torch.tensor(</a>scaled_norm<a id="change">,
                                      device=self.fp32_groups_flat[i].device,
                                      dtype=torch.float)</a>
    dist.all_reduce(scaled_norm_tensor, group=pg)
    total_norm = scaled_norm_tensor.item()

    clip_coef = max_norm / (total_norm + 1e-6)</code></pre><h3>After Change</h3><pre><code class='java'>
                    param_norm = p.grad.data.norm(norm_type)
                    total_norm += param_norm.item()**norm_type
            else:
                param_norm = <a id="change">p.grad.data.float()</a>.norm(norm_type)
                total_norm += param_norm.item()<a id="change">**</a>norm_type

        &#47&#47 Sum across all model parallel GPUs.
        total_norm_cuda = torch.cuda.FloatTensor([float(total_norm)])</code></pre>