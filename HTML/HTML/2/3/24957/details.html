<html><h3>Pattern ID :24957
</h3><img src='76766039.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            )
            loss.backward()
            optimizer.step()
            <a id="change">optimizer.zero_grad()</a>

        return loss.detach()

    def evaluate_batch(self, batch, stage):</code></pre><h3>After Change</h3><pre><code class='java'>
        self.g_optimizer.zero_grad()

        predictions = self.compute_forward(inputs, sb.Stage.TRAIN)
        d_loss<a id="change"> = </a>self.compute_objectives(
            predictions, inputs, sb.Stage.TRAIN, "discriminator"
        )
        d_loss.backward()
        self.d_optimizer.step()
        self.d_optimizer.zero_grad()

        <a id="change">return </a>g_loss.detach() + d_loss.detach()

    def evaluate_batch(self, batch, stage):
        inputs = batch[0]</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/speechbrain/speechbrain/commit/de699fa49ebc3f5b1cca4061a41ddf44e8770c66#diff-3c0b8827ddfccfad16b32e2bf9ec374784d77df413a473dd95d09500436f12b0L45' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 76766039</div><div id='project'> Project Name: speechbrain/speechbrain</div><div id='commit'> Commit Name: de699fa49ebc3f5b1cca4061a41ddf44e8770c66</div><div id='time'> Time: 2020-09-24</div><div id='author'> Author: plantinga.peter@protonmail.com</div><div id='file'> File Name: recipes/minimal_examples/neural_networks/enhance_GAN/example_enhance_gan_experiment.py</div><div id='m_class'> M Class Name: EnhanceGanBrain</div><div id='n_method'> N Class Name: EnhanceGanBrain</div><div id='m_method'> M Method Name: fit_batch(2)</div><div id='n_method'> N Method Name: fit_batch(2)</div><div id='m_parent_class'> M Parent Class: sb.Brain</div><div id='n_parent_class'> N Parent Class: sb.Brain</div><div id='m_file'> M File Name: recipes/minimal_examples/neural_networks/enhance_GAN/example_enhance_gan_experiment.py</div><div id='n_file'> N File Name: recipes/minimal_examples/neural_networks/enhance_GAN/example_enhance_gan_experiment.py</div><div id='m_start'> M Start Line: 48</div><div id='m_end'> M End Line: 57</div><div id='n_start'> N Start Line: 45</div><div id='n_end'> N End Line: 63</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                loss = loss.sum(dim=0)
                valid_loss += loss.item()
                
                <a id="change">self.optimizer.zero_grad()</a>
                
                if idx &lt; 5:
                    for _mixture_resampled, _estimated_sources in zip(mixture_resampled, estimated_sources):
                        print(_mixture_resampled.size(), _estimated_sources.size())</code></pre><h3>After Change</h3><pre><code class='java'>
        
        valid_loss = 0
        valid_main_loss = 0
        valid_reconstruction_loss<a id="change"> = </a>0
        valid_similarity_loss = 0
        valid_dissimilarity_loss = 0
        
        n_valid = len(self.valid_loader.dataset)
        
        with torch.no_grad():
            for idx, (mixture, sources, titles) in enumerate(self.valid_loader):
                if self.use_cuda:
                    mixture = mixture.cuda()
                    sources = sources.cuda()

                print(mixture.size(), sources.size())
                batch_size, n_sources, _, T = sources.size()
                mixture, sources = mixture.view(batch_size, T), sources.view(batch_size * n_sources, T)

                mixture_resampled, sources_resampled = [], []

                for idx in range(self.stage):
                    _mixture, _sources = self.resamplers[idx](mixture), self.resamplers[idx](sources)
                    _mixture, _sources = _mixture.view(batch_size, 1, -1), _sources.view(batch_size * n_sources, 1, -1)
                    mixture_resampled.append(_mixture)
                    sources_resampled.append(_sources)
            
                &#47&#47 Forward
                estimated_sources, latent_estimated = self.model.extract_latent(mixture_resampled, masking=True, max_stage=self.stage)
                reconstructed, _ = self.model.extract_latent(mixture_resampled, masking=False, max_stage=self.stage)
                _, latent_target = self.model.extract_latent(sources_resampled, masking=False, max_stage=self.stage)

                &#47&#47 Main loss
                main_loss = 0
                for _estimated_sources, _sources in zip(estimated_sources, sources_resampled):
                    _sources = _sources.view(batch_size, n_sources, *_estimated_sources.size()[-2:])
                    _loss = self.criterion.metrics[&quotmain&quot](_estimated_sources, _sources, batch_mean=False)
                    main_loss = main_loss + _loss

                &#47&#47 Reconstruction loss
                reconstruction_loss = 0
                for _reconstructed, _mixture in zip(reconstructed, mixture_resampled):
                    _loss = self.criterion.metrics[&quotreconstruction&quot](_reconstructed, _mixture, batch_mean=False)
                    reconstruction_loss = reconstruction_loss + _loss
                
                &#47&#47 Similarity and dissimilarity loss
                similarity_loss, dissimilarity_loss = 0, 0
                for _latent_estimated, _latent_target in zip(latent_estimated, latent_target):
                    _latent_target = _latent_target.view(batch_size, n_sources, *_latent_target.size()[-2:])

                    _loss = self.criterion.metrics[&quotsimilarity&quot](_latent_estimated, _latent_target, batch_mean=False)
                    similarity_loss = similarity_loss + _loss

                    _loss = self.criterion.metrics[&quotdissimilarity&quot](_latent_estimated, batch_mean=False)
                    dissimilarity_loss = dissimilarity_loss + _loss
            
                loss = main_loss + self.criterion.weights[&quotreconstruction&quot] * reconstruction_loss + self.criterion.weights[&quotsimilarity&quot] * similarity_loss + self.criterion.weights[&quotdissimilarity&quot] * dissimilarity_loss
                loss = loss.sum(dim=0)
                valid_loss += loss.item()

                valid_loss += loss.item()
                valid_main_loss += main_loss.item()
                valid_reconstruction_loss += reconstruction_loss.item()
                valid_similarity_loss += similarity_loss.item()
                valid_dissimilarity_loss += dissimilarity_loss.item()
                
                if idx &lt; 5:
                    for _mixture_resampled, _estimated_sources in zip(mixture_resampled, estimated_sources):
                        print(_mixture_resampled.size(), _estimated_sources.size())
                        raise NotImplementedError
                        _mixture_resampled = _mixture_resampled[0].squeeze(dim=0).cpu()
                        _estimated_sources = _estimated_sources[0].cpu()
                        
                        save_dir = os.path.join(self.sample_dir, titles[0])
                        os.makedirs(save_dir, exist_ok=True)
                        save_path = os.path.join(save_dir, "mixture.wav")
                        torchaudio.save(save_path, _mixture_resampled, sample_rate=self.sr, bits_per_sample=BITS_PER_SAMPLE_MUSDB18)
                        
                        for source_idx, _estimated_source in enumerate(_estimated_sources):
                            target = self.valid_loader.dataset.target[source_idx]
                            save_path = os.path.join(save_dir, "epoch{}-{}.wav".format(epoch + 1, target))
                            torchaudio.save(save_path, _estimated_source, sample_rate=self.sr, bits_per_sample=BITS_PER_SAMPLE_MUSDB18)
            
        valid_loss /= n_valid
        valid_main_loss /= n_valid
        valid_reconstruction_loss /= n_valid
        valid_similarity_loss /= n_valid
        valid_dissimilarity_loss /= n_valid
        
        <a id="change">return </a>valid_loss, valid_main_loss, valid_reconstruction_loss, valid_similarity_loss, valid_dissimilarity_loss</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tky823/dnn-based_source_separation/commit/5abf28af1871d3f13e68fb3a3fd9aa859488461f#diff-25176983d92b56aecbfd6b65dc7f3865f5be21e18b4d65613535fabec6e9550fL234' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 76766043</div><div id='project'> Project Name: tky823/dnn-based_source_separation</div><div id='commit'> Commit Name: 5abf28af1871d3f13e68fb3a3fd9aa859488461f</div><div id='time'> Time: 2021-08-13</div><div id='author'> Author: 40362510+tky823@users.noreply.github.com</div><div id='file'> File Name: egs/musdb18/meta-tasnet/src/adhoc_driver.py</div><div id='m_class'> M Class Name: Trainer</div><div id='n_method'> N Class Name: Trainer</div><div id='m_method'> M Method Name: run_one_epoch_eval(2)</div><div id='n_method'> N Method Name: run_one_epoch_eval(2)</div><div id='m_parent_class'> M Parent Class: TrainerBase</div><div id='n_parent_class'> N Parent Class: TrainerBase</div><div id='m_file'> M File Name: egs/musdb18/meta-tasnet/src/adhoc_driver.py</div><div id='n_file'> N File Name: egs/musdb18/meta-tasnet/src/adhoc_driver.py</div><div id='m_start'> M Start Line: 240</div><div id='m_end'> M End Line: 315</div><div id='n_start'> N Start Line: 240</div><div id='n_end'> N End Line: 328</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        target_v_value = (new_min_curr_state_q_value - new_curr_state_log_pi).detach()
        v_loss = F.mse_loss(curr_state_v_value, target_v_value)
        v_loss_value = v_loss.detach().cpu().numpy()
        <a id="change">self.v_optimizer.zero_grad()</a>
        v_loss.backward()
        self.v_optimizer.step()
        
        &#47&#47compute q loss</code></pre><h3>After Change</h3><pre><code class='java'>
        new_curr_state_q1_value = self.q1_network(state_batch, new_curr_state_action)
        new_curr_state_q2_value = self.q2_network(state_batch, new_curr_state_action)

        next_state_q1_value<a id="change"> = </a>self.target_q1_network(next_state_batch, next_state_action)
        next_state_q2_value = self.target_q2_network(next_state_batch, next_state_action)
        next_state_min_q = torch.min(next_state_q1_value, next_state_q2_value)
        target_q = (next_state_min_q - self.alpha * next_state_log_pi)
        target_q = reward_batch + self.gamma * (1. - done_batch) * target_q

        new_min_curr_state_q_value = torch.min(new_curr_state_q1_value, new_curr_state_q2_value)

        &#47&#47compute q loss
        q1_loss = F.mse_loss(curr_state_q1_value, target_q.detach())
        q2_loss = F.mse_loss(curr_state_q2_value, target_q.detach())
        q1_loss_value = q1_loss.detach().cpu().numpy()
        q2_loss_value = q2_loss.detach().cpu().numpy()
        self.q1_optimizer.zero_grad()
        q1_loss.backward()
        self.q1_optimizer.step()
        self.q2_optimizer.zero_grad()
        q2_loss.backward()
        self.q2_optimizer.step()

        &#47&#47compute policy loss
        policy_loss = ((self.alpha * new_curr_state_log_pi) - new_min_curr_state_q_value).mean()
        policy_loss_value = policy_loss.detach().cpu().numpy()
        self.policy_optimizer.zero_grad()
        policy_loss.backward()
        self.policy_optimizer.step()

        &#47&#47compute entropy loss
        if self.automatic_entropy_tuning:
            alpha_loss = -(self.log_alpha * (new_curr_state_log_pi + self.target_entropy).detach()).mean()
            alpha_loss_value = alpha_loss.detach().cpu().numpy()
            self.alpha_optim.zero_grad()
            alpha_loss.backward()
            self.alpha_optim.step()

            self.alpha = self.log_alpha.exp()
            alpha_value = self.alpha.detach().cpu().numpy()
        else:
            alpha_loss = torch.tensor(0.).to(util.device)
            alpha_value = self.alpha.detach().cpu().numpy()
        self.tot_update_count += 1
        <a id="change">return </a>q1_loss_value, q2_loss_value, policy_loss_value, alpha_loss_value, alpha_value

    def try_update_target_network(self):
        if self.tot_update_count % self.update_target_network_interval == 0:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/x35f/unstable_baselines/commit/0fc82ae6328814fe2dad0c8e0ae1b172d3e5f981#diff-5db644cd5bd54e3d73a4e86c3b2e245fd8f43751389307d15efbb99043bc1b0fL73' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 76766045</div><div id='project'> Project Name: x35f/unstable_baselines</div><div id='commit'> Commit Name: 0fc82ae6328814fe2dad0c8e0ae1b172d3e5f981</div><div id='time'> Time: 2021-03-12</div><div id='author'> Author: ym8411012@126.com</div><div id='file'> File Name: sac/models.py</div><div id='m_class'> M Class Name: SACAgent</div><div id='n_method'> N Class Name: SACAgent</div><div id='m_method'> M Method Name: update(2)</div><div id='n_method'> N Method Name: update(2)</div><div id='m_parent_class'> M Parent Class: BaseAgent,torch.nn.Module</div><div id='n_parent_class'> N Parent Class: BaseAgent,torch.nn.Module</div><div id='m_file'> M File Name: sac/models.py</div><div id='n_file'> N File Name: sac/models.py</div><div id='m_start'> M Start Line: 75</div><div id='m_end'> M End Line: 129</div><div id='n_start'> N Start Line: 83</div><div id='n_end'> N End Line: 129</div><BR>