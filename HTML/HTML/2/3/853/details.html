<html><h3>Pattern ID :853
</h3><img src='4196290.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        avg_loss = (total_loss / total_samples)

        tracker.log(<a id="change">{</a>f&quot{phase} {loss_type}&quot: avg_loss<a id="change">}</a>)

def report_cosine_sims(diffusion_prior, dataloader, text_conditioned, device):
    diffusion_prior.eval()</code></pre><h3>After Change</h3><pre><code class='java'>
        stats = {f"{phase}/{loss_type}": avg_loss}
        trainer.print(stats)

        <a id="change">if </a><a id="change">exists(</a>tracker<a id="change">)</a>:
            tracker.log(stats, step=trainer.step.item() + 1)

</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/dalle2-pytorch/commit/fe19b508ca57dfddd2743e955970b0392d177e88#diff-283456789c9565d0a8ad235fc41b44188f310b3043c3bcdeb6678f1958a57b51L32' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4196290</div><div id='project'> Project Name: lucidrains/dalle2-pytorch</div><div id='commit'> Commit Name: fe19b508ca57dfddd2743e955970b0392d177e88</div><div id='time'> Time: 2022-06-19</div><div id='author'> Author: 51308183+nousr@users.noreply.github.com</div><div id='file'> File Name: train_diffusion_prior.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: eval_model(7)</div><div id='n_method'> N Method Name: eval_model(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: train_diffusion_prior.py</div><div id='n_file'> N File Name: train_diffusion_prior.py</div><div id='m_start'> M Start Line: 32</div><div id='m_end'> M End Line: 58</div><div id='n_start'> N Start Line: 67</div><div id='n_end'> N End Line: 112</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

            probs = (logits / max(temperature, 1e-3)).softmax(dim = -1)

            packed_probs, packed_shape = pack(<a id="change">[</a>probs<a id="change"></a>], &quot* c&quot)

            sampled_ids = torch.multinomial(packed_probs, 1)
</code></pre><h3>After Change</h3><pre><code class='java'>

            seq = torch.where(mask, sampled_ids, seq)

            <a id="change">if </a><a id="change">exists(</a>self.token_critic<a id="change">)</a>:
                scores = self.token_critic(seq)
                scores = rearrange(scores, &quotb n 1 -&gt; b n&quot)
            else:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/x-transformers/commit/2714629a282adde9beffb13c8726c13168dcda0c#diff-ce5cabaf7d7735b211def6ec240eb93ca21923dfea7587132f74a6a990764c39L119' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4196291</div><div id='project'> Project Name: lucidrains/x-transformers</div><div id='commit'> Commit Name: 2714629a282adde9beffb13c8726c13168dcda0c</div><div id='time'> Time: 2023-01-19</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: x_transformers/nonautoregressive_wrapper.py</div><div id='m_class'> M Class Name: NonAutoregressiveWrapper</div><div id='n_method'> N Class Name: NonAutoregressiveWrapper</div><div id='m_method'> M Method Name: generate(4)</div><div id='n_method'> N Method Name: generate(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: x_transformers/nonautoregressive_wrapper.py</div><div id='n_file'> N File Name: x_transformers/nonautoregressive_wrapper.py</div><div id='m_start'> M Start Line: 171</div><div id='m_end'> M End Line: 184</div><div id='n_start'> N Start Line: 195</div><div id='n_end'> N End Line: 207</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        attn = softmax(dots, dim=-1)

        self.save_attn(<a id="change">{</a>"attn": attn, "v": v<a id="change">}</a>)

        out = torch.einsum("b h i j, b h j d -&gt; b h i d", attn, v)
        out = rearrange(out, "b h n d -&gt; b n (h d)")</code></pre><h3>After Change</h3><pre><code class='java'>
            k_top, v_top = cache[cache_key]
            k = torch.cat([k_top, k], dim=-2)
            v = torch.cat([v_top, v], dim=-2)
        <a id="change">if </a><a id="change">exists(</a>cache<a id="change">)</a>:
            cache[cache_key] = k, v

        dots = torch.einsum("b h i d, b h j d -&gt; b h i j", q, k)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/bohuanglab/protein-localization-transformer/commit/e66767bf380cd9963aca64135339e98395c46a5c#diff-fd74e98395cadc39a61fe651c749e84a598bf8537ecba20913a7401eda5f10e5L65' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4196292</div><div id='project'> Project Name: bohuanglab/protein-localization-transformer</div><div id='commit'> Commit Name: e66767bf380cd9963aca64135339e98395c46a5c</div><div id='time'> Time: 2022-05-23</div><div id='author'> Author: 42391631+EmaadKhwaja@users.noreply.github.com</div><div id='file'> File Name: celle/attention.py</div><div id='m_class'> M Class Name: Attention</div><div id='n_method'> N Class Name: Attention</div><div id='m_method'> M Method Name: forward(6)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: celle/attention.py</div><div id='n_file'> N File Name: celle/attention.py</div><div id='m_start'> M Start Line: 73</div><div id='m_end'> M End Line: 92</div><div id='n_start'> N Start Line: 73</div><div id='n_end'> N End Line: 116</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.score_consensus_attn = score_consensus_attn

        assert downsample_factor &lt;= max_block_size, &quotfinal downsample factor should be less than the maximum block size&quot
        self.block_sizes = <a id="change">[</a>*range(1, max_block_size + 1)<a id="change"></a>]
        self.block_pad_multiple = lcm(*self.block_sizes)
        self.downsample_factor = downsample_factor
</code></pre><h3>After Change</h3><pre><code class='java'>
        assert exists(max_block_size) ^ exists(blocks), &quoteither max_block_size or blocks are given on initialization&quot
        self.token_emb = nn.Embedding(num_tokens, dim)

        <a id="change">if </a><a id="change">exists(</a>blocks<a id="change">)</a>:
            assert isinstance(blocks, tuple), &quotblocks must be a tuple of block sizes&quot
            self.blocks = tuple(map(lambda el: el if isinstance(el, tuple) else (el, 0), blocks))
            assert all([(offset &lt; block_size) for block_size, offset in self.blocks]), &quotoffset must be always smaller than the block size&quot</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/charformer-pytorch/commit/93752db1258e73c73a11bbffdf66eab87d3c3f07#diff-b53e4eafe33df5c05215d9d91887926f93106c2e94bd37d4b90472b69243bb2bL65' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 4196294</div><div id='project'> Project Name: lucidrains/charformer-pytorch</div><div id='commit'> Commit Name: 93752db1258e73c73a11bbffdf66eab87d3c3f07</div><div id='time'> Time: 2021-07-14</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: charformer_pytorch/charformer_pytorch.py</div><div id='m_class'> M Class Name: GBST</div><div id='n_method'> N Class Name: GBST</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: charformer_pytorch/charformer_pytorch.py</div><div id='n_file'> N File Name: charformer_pytorch/charformer_pytorch.py</div><div id='m_start'> M Start Line: 92</div><div id='m_end'> M End Line: 93</div><div id='n_start'> N Start Line: 76</div><div id='n_end'> N End Line: 104</div><BR>