<html><h3>Pattern ID :34226
</h3><img src='98106361.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                pred = &quot&quot.join(dataloader.tokenizer.decode(decoder.generate(torch.LongTensor([args.bos_token]).to(
                    device), args.max_seq_len, eos_token=args.eos_token, context=encoded[:1].detach())[:-1]).split(&quot &quot)).replace(&quotĠ&quot, &quot &quot).strip()
                s = seq[&quotinput_ids&quot][0]
                truth = &quot&quot.join(<a id="change">dataloader.tokenizer.decode(</a>s[1:list(s).index(args.eos_token)]<a id="change">)</a>.split(&quot &quot)).replace(&quotĠ&quot, &quot &quot).strip()
                if args.wandb:
                    table = wandb.Table(columns=["Truth", "Prediction"])
                    table.add_data(truth, pred)</code></pre><h3>After Change</h3><pre><code class='java'>
                wandb.log({&quottrain/loss&quot: loss.item()})
            if (i+1) % args.sample_freq == 0:
                num_samples = 4
                dec = decoder.generate(<a id="change">torch.LongTensor([args.bos_token]*len(encoded[:num_samples]))[:, None]</a>.to(device), args.max_seq_len,
                                       eos_token=args.pad_token, context=encoded.detach()[:num_samples])
                pred = token2str(dec[:num_samples], dataloader.tokenizer)
                truth = token2str(seq[&quotinput_ids&quot], dataloader.tokenizer)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lukas-blecher/latex-ocr/commit/ba220245e394aea309e5e158ba8d6958522a1f07#diff-ed183d67207df065a11e1289f19d34cc2abbc5448dea952683cfe9728c342b95L21' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 98106361</div><div id='project'> Project Name: lukas-blecher/latex-ocr</div><div id='commit'> Commit Name: ba220245e394aea309e5e158ba8d6958522a1f07</div><div id='time'> Time: 2021-02-02</div><div id='author'> Author: luk.blecher@gmx.de</div><div id='file'> File Name: train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train(1)</div><div id='n_method'> N Method Name: train(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: train.py</div><div id='n_file'> N File Name: train.py</div><div id='m_start'> M Start Line: 21</div><div id='m_end'> M End Line: 54</div><div id='n_start'> N Start Line: 21</div><div id='n_end'> N End Line: 56</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                    length_penalty=1.0,
                    early_stopping=True
                )
                preds = [<a id="change">self.tokenizer.decode(</a>g<a id="change">, skip_special_tokens=True,
                                               clean_up_tokenization_spaces=True)</a> for g in generated_ids]
                target = [self.tokenizer.decode(t, skip_special_tokens=True,
                                                clean_up_tokenization_spaces=True) for t in y]
                if i % 50 == 0:</code></pre><h3>After Change</h3><pre><code class='java'>
                &#47&#47  Labels for computing the sequence classification/regression loss. 
                &#47&#47 Indices should be in [-100, 0, ..., config.vocab_size - 1]. 
                &#47&#47 All labels set to -100 are ignored (masked), the loss is only computed for labels in [0, ..., config.vocab_size]
                lm_labels = <a id="change">y[:, 1:]</a>.clone().detach()
                lm_labels[y[:, 1:] == self.tokenizer.pad_token_id] = -100
                
                target_mask = batch[&quottarget_ids_y&quot].to(self.device, dtype=torch.long)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/kimdanny/user-simulation-t5/commit/62ee6ec06a3adbf87c420fc449c8e74696741d7a#diff-8cd3d6986df510f1977863fc7fd7528c97027a89c236ad27ff87e07a3d834de4L149' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 98106362</div><div id='project'> Project Name: kimdanny/user-simulation-t5</div><div id='commit'> Commit Name: 62ee6ec06a3adbf87c420fc449c8e74696741d7a</div><div id='time'> Time: 2022-02-13</div><div id='author'> Author: dannykim153@gmail.com</div><div id='file'> File Name: t5_mtl/train.py</div><div id='m_class'> M Class Name: T5Trainer</div><div id='n_method'> N Class Name: T5Trainer</div><div id='m_method'> M Method Name: validate(2)</div><div id='n_method'> N Method Name: validate(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: t5_mtl/train.py</div><div id='n_file'> N File Name: t5_mtl/train.py</div><div id='m_start'> M Start Line: 151</div><div id='m_end'> M End Line: 178</div><div id='n_start'> N Start Line: 203</div><div id='n_end'> N End Line: 229</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            )
            if stop:
                &#47&#47 Decode all tokens
                output_text = <a id="change">self.tokenizer.decode(
                    </a>all_input_ids.squeeze(-1)<a id="change">, skip_special_tokens=True,
                    cleanup_tokenization_spaces=False
                )</a>
                &#47&#47 Slice with input_length to remove padding
                token_ids = all_input_ids[-new_input_length:]
                tokens = self.tokenizer.batch_decode(token_ids)
                &#47&#47 Add NaN for the first prompt token</code></pre><h3>After Change</h3><pre><code class='java'>
            if stop:
                &#47&#47 Decode generated tokens
                generated_text = self.decode(
                    <a id="change">all_input_ids[-stopping_criteria.current_tokens :, 0]</a>
                )
                output_text = request.inputs + generated_text
                &#47&#47 Slice with input_length to remove padding
                token_ids = all_input_ids[-new_input_length:]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/huggingface/text-generation-inference/commit/15511edc01a0725d374840f0e77d085eb5821483#diff-ddf51fed54a3f6f10409487d931b35506acd36999839574d37b0a5cfb452ed95L268' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 98106365</div><div id='project'> Project Name: huggingface/text-generation-inference</div><div id='commit'> Commit Name: 15511edc01a0725d374840f0e77d085eb5821483</div><div id='time'> Time: 2023-01-20</div><div id='author'> Author: olivier@huggingface.co</div><div id='file'> File Name: server/text_generation/models/causal_lm.py</div><div id='m_class'> M Class Name: CausalLM</div><div id='n_method'> N Class Name: CausalLM</div><div id='m_method'> M Method Name: generate_token(2)</div><div id='n_method'> N Method Name: generate_token(2)</div><div id='m_parent_class'> M Parent Class: Model</div><div id='n_parent_class'> N Parent Class: Model</div><div id='m_file'> M File Name: server/text_generation/models/causal_lm.py</div><div id='n_file'> N File Name: server/text_generation/models/causal_lm.py</div><div id='m_start'> M Start Line: 322</div><div id='m_end'> M End Line: 345</div><div id='n_start'> N Start Line: 331</div><div id='n_end'> N End Line: 354</div><BR>