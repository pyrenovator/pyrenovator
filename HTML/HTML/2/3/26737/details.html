<html><h3>Pattern ID :26737
</h3><img src='79922643.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    trainset = ImageTextDataset(X, y, transform)
    trainloader = torch.utils.data.DataLoader(trainset, shuffle = True, batch_size = args.batch_size)

    resnet50 = <a id="change">models.resnet50(pretrained = True).to(device).eval()</a>
    bert = BertModel.from_pretrained(&quotbert-base-uncased&quot).to(device).eval()
    model = Towers(len(np.unique(y))).to(device)
    opt = RMSprop(model.parameters(), lr = args.lr, momentum = 0.9)
</code></pre><h3>After Change</h3><pre><code class='java'>
            imgs = imgs.to(device)
            labels = labels.to(device)

            img_embeddings = <a id="change">resnet50(imgs).to(device).squeeze(2).squeeze(2</a><a id="change">)</a>
            text_embeddings = torch.stack([get_encoding(text, bert, device) for text in texts]).to(device)
            
            opt.zero_grad()
            outputs, imgs_f, texts_f = model(img_embeddings, text_embeddings)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 2</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/amanjain1397/huse/commit/5937721f1f9b59fcfb5f1ce0e3a34797a83e9302#diff-b10564ab7d2c520cdd0243874879fb0a782862c3c902ab535faabe57d5a505e1L76' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 79922643</div><div id='project'> Project Name: amanjain1397/huse</div><div id='commit'> Commit Name: 5937721f1f9b59fcfb5f1ce0e3a34797a83e9302</div><div id='time'> Time: 2020-06-11</div><div id='author'> Author: amanjain1397@gmail.com</div><div id='file'> File Name: main.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train(1)</div><div id='n_method'> N Method Name: train(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: main.py</div><div id='n_file'> N File Name: main.py</div><div id='m_start'> M Start Line: 76</div><div id='m_end'> M End Line: 109</div><div id='n_start'> N Start Line: 76</div><div id='n_end'> N End Line: 117</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        _, sr = sf.read(path_list[0])
        if speaker_embedding:
            wav2mel = torch.jit.load("Models/SpeakerEmbedding/wav2mel.pt")
            dvector = <a id="change">torch.jit.load("Models/SpeakerEmbedding/dvector-step250000.pt").eval()</a>
        ap = AudioPreprocessor(input_sr=sr, output_sr=16000, melspec_buckets=80, hop_length=256, n_fft=1024, cut_silence=cut_silences)
        for path in tqdm(path_list):
            transcript = self.path_to_transcript_dict[path]
            wave, sr = sf.read(path)</code></pre><h3>After Change</h3><pre><code class='java'>
                cached_speech = ap.audio_to_mel_spec_tensor(audio=norm_wave, normalize=False).transpose(0, 1).cpu().numpy()
                cached_speech_len = torch.LongTensor([len(cached_speech)]).numpy()
                if speaker_embedding:
                    cached_speaker_embedding = <a id="change">speaker_embedding_function.encode_batch(norm_wave).squeeze(0).squeeze(0</a><a id="change">)</a>.detach().cpu().numpy()
                    process_internal_dataset_chunk.append([cached_text,
                                                           cached_text_len,
                                                           cached_speech,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/4705928bf2e8184d5b2bb1ef9dcc7213e398026d#diff-a49dac0d7d67877dcf8511175f18c0155a9aba98acf2e0ee7bfd3a2ed69fb144L78' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 79922642</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 4705928bf2e8184d5b2bb1ef9dcc7213e398026d</div><div id='time'> Time: 2021-09-13</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: TrainingInterfaces/Text_to_Spectrogram/Tacotron2/TacotronDataset.py</div><div id='m_class'> M Class Name: TacotronDataset</div><div id='n_method'> N Class Name: TacotronDataset</div><div id='m_method'> M Method Name: cache_builder_process(7)</div><div id='n_method'> N Method Name: cache_builder_process(7)</div><div id='m_parent_class'> M Parent Class: Dataset</div><div id='n_parent_class'> N Parent Class: Dataset</div><div id='m_file'> M File Name: TrainingInterfaces/Text_to_Spectrogram/Tacotron2/TacotronDataset.py</div><div id='n_file'> N File Name: TrainingInterfaces/Text_to_Spectrogram/Tacotron2/TacotronDataset.py</div><div id='m_start'> M Start Line: 83</div><div id='m_end'> M End Line: 98</div><div id='n_start'> N Start Line: 83</div><div id='n_end'> N End Line: 96</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
  valset = ImageTextDataset(X, y, transform)
  valloader = torch.utils.data.DataLoader(valset, shuffle = False, batch_size = 1)

  resnet50 = <a id="change">models.resnet50(pretrained = True).to(device).eval()</a>
  bert = BertModel.from_pretrained(&quotbert-base-uncased&quot).to(device).eval()
  
  model = Towers().to(device).eval()
  model.load_state_dict(torch.load(args.model))</code></pre><h3>After Change</h3><pre><code class='java'>
        img = img.to(device)
        label = label.to(device)

        img_embeddings = <a id="change">resnet50(img).to(device).squeeze(2).squeeze(2</a><a id="change">)</a>
        text_embeddings = torch.stack([get_encoding(text, bert, device) for text in text]).to(device)

        pred, img_f, text_f = model(img_embeddings, text_embeddings)
      </code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/amanjain1397/huse/commit/5937721f1f9b59fcfb5f1ce0e3a34797a83e9302#diff-b10564ab7d2c520cdd0243874879fb0a782862c3c902ab535faabe57d5a505e1L150' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 79922640</div><div id='project'> Project Name: amanjain1397/huse</div><div id='commit'> Commit Name: 5937721f1f9b59fcfb5f1ce0e3a34797a83e9302</div><div id='time'> Time: 2020-06-11</div><div id='author'> Author: amanjain1397@gmail.com</div><div id='file'> File Name: main.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: evaluate(1)</div><div id='n_method'> N Method Name: evaluate(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: main.py</div><div id='n_file'> N File Name: main.py</div><div id='m_start'> M Start Line: 151</div><div id='m_end'> M End Line: 180</div><div id='n_start'> N Start Line: 159</div><div id='n_end'> N End Line: 190</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        _, sr = sf.read(path_list[0])
        if speaker_embedding:
            wav2mel = torch.jit.load("Models/SpeakerEmbedding/wav2mel.pt")
            dvector = <a id="change">torch.jit.load("Models/SpeakerEmbedding/dvector-step250000.pt").eval()</a>
        ap = AudioPreprocessor(input_sr=sr,
                               output_sr=16000,
                               melspec_buckets=80,
                               hop_length=256,</code></pre><h3>After Change</h3><pre><code class='java'>
                    if np.count_nonzero(cached_duration.numpy() == 0) &gt; 4:
                        continue
                else:
                    cached_speaker_embedding = <a id="change">speaker_embedding_function.encode_batch(norm_wave).squeeze(0).squeeze(0</a><a id="change">)</a>
                    attention_map = acoustic_model.inference(text_tensor=text.squeeze(0).to(device),
                                                             speech_tensor=melspec.to(device),
                                                             use_teacher_forcing=True,
                                                             speaker_embeddings=cached_speaker_embedding.to(device),</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/4705928bf2e8184d5b2bb1ef9dcc7213e398026d#diff-528f1d1f9b65f826c6eec25f3934452fe1a81965d76a0070a71db9ef9ef36aa0L100' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 79922644</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 4705928bf2e8184d5b2bb1ef9dcc7213e398026d</div><div id='time'> Time: 2021-09-13</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: TrainingInterfaces/Text_to_Spectrogram/FastSpeech2/FastSpeechDataset.py</div><div id='m_class'> M Class Name: FastSpeechDataset</div><div id='n_method'> N Class Name: FastSpeechDataset</div><div id='m_method'> M Method Name: cache_builder_process(11)</div><div id='n_method'> N Method Name: cache_builder_process(11)</div><div id='m_parent_class'> M Parent Class: Dataset</div><div id='n_parent_class'> N Parent Class: Dataset</div><div id='m_file'> M File Name: TrainingInterfaces/Text_to_Spectrogram/FastSpeech2/FastSpeechDataset.py</div><div id='n_file'> N File Name: TrainingInterfaces/Text_to_Spectrogram/FastSpeech2/FastSpeechDataset.py</div><div id='m_start'> M Start Line: 115</div><div id='m_end'> M End Line: 156</div><div id='n_start'> N Start Line: 115</div><div id='n_end'> N End Line: 153</div><BR>