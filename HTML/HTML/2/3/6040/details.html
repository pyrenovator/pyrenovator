<html><h3>Pattern ID :6040
</h3><img src='21024160.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    if should_shift:
        shift_height, shift_width = int(window_height * shift_size), int(window_width * shift_size)
        inputs = <a id="change">tf.roll(</a>inputs<a id="change">, shift=(shift_height * -1, shift_width * -1), axis=[1, 2])</a>
        mask = make_window_attention_mask(inputs.shape[1], inputs.shape[2], window_height, window_width, shift_height, shift_width)
    else:
        mask = None
</code></pre><h3>After Change</h3><pre><code class='java'>
    if should_shift:
        &#47&#47 nn = tf.roll(nn, shift=(shift_height, shift_width), axis=[1, 2])
        nn = tf.concat([nn[:, -shift_height:], nn[:, :-shift_height]], axis=1)
        nn = tf.concat([nn[:, :, -shift_width:], <a id="change">nn[:, :, :-shift_width]</a>], axis=2)
    return nn

</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/bcd22fc3dc9889d71afdf773b78b74d3211754be#diff-24b1f385fb327a5f8149f07df27a49f3739c9d2de68a962cf49726311a2add9bL129' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 21024160</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: bcd22fc3dc9889d71afdf773b78b74d3211754be</div><div id='time'> Time: 2022-04-06</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/swin_transformer_v2/swin_transformer_v2.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: shifted_window_attention(5)</div><div id='n_method'> N Method Name: shifted_window_attention(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/swin_transformer_v2/swin_transformer_v2.py</div><div id='n_file'> N File Name: keras_cv_attention_models/swin_transformer_v2/swin_transformer_v2.py</div><div id='m_start'> M Start Line: 130</div><div id='m_end'> M End Line: 157</div><div id='n_start'> N Start Line: 129</div><div id='n_end'> N End Line: 163</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            bboxes = augmentations["bboxes"]

        if len(bboxes) &gt; 0:
            bboxes = <a id="change">torch.tensor(bboxes).roll(dims=1, shifts=1)</a>
            &#47&#47yolo_xywh = coco_to_yolo_tensors(bboxes[..., 1:5], w0=tg_width, h0=tg_height)
            &#47&#47bboxes[..., 1:] = yolo_xywh
            out_bboxes = torch.zeros((bboxes.shape[0], 6))
            out_bboxes[..., 1:] = bboxes</code></pre><h3>After Change</h3><pre><code class='java'>
            sh, sw = img.shape[0:2]
            img, ratio, pad = letterbox(img, (tg_height, tg_width), auto=False, scaleup=False)
            if labels.size:  &#47&#47 normalized xywh to pixel xyxy format
                <a id="change">labels[:, 1:]</a> = xywhn2xyxy(labels[:, 1:], ratio[0] * sw, ratio[1] * sh, padw=pad[0], padh=pad[1])
            nl = len(labels)
            if nl:
                labels[:, 1:5] = xyxy2xywhn(labels[:, 1:5], w=img.shape[1], h=img.shape[0], clip=True, eps=1E-3)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/alessandromondin/yolov5m/commit/67592bd2ca15b093b59ddd4a11287df9c55f48d0#diff-3c75bbc5e19f6e327e6c16638ad2413dc0d3200a9db147adecb096c4f05994faL82' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 21024165</div><div id='project'> Project Name: alessandromondin/yolov5m</div><div id='commit'> Commit Name: 67592bd2ca15b093b59ddd4a11287df9c55f48d0</div><div id='time'> Time: 2022-11-21</div><div id='author'> Author: alessandromondin00@gmail.com</div><div id='file'> File Name: dataset_ultra.py</div><div id='m_class'> M Class Name: MS_COCO_2017</div><div id='n_method'> N Class Name: MS_COCO_2017</div><div id='m_method'> M Method Name: __getitem__(2)</div><div id='n_method'> N Method Name: __getitem__(2)</div><div id='m_parent_class'> M Parent Class: Dataset</div><div id='n_parent_class'> N Parent Class: Dataset</div><div id='m_file'> M File Name: dataset_ultra.py</div><div id='n_file'> N File Name: dataset_ultra.py</div><div id='m_start'> M Start Line: 85</div><div id='m_end'> M End Line: 122</div><div id='n_start'> N Start Line: 87</div><div id='n_end'> N End Line: 133</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    all_lengths = torch.linspace(near, far - resolution, point_num).to(target_device)
    lengths = all_lengths + torch.rand((ray_num, point_num)).to(target_device) * resolution
    &#47&#47 sampled coords is (row_id, col_id)
    ray_raw = (<a id="change">(sampled_coords[..., :-1] * torch.Tensor([-1., 1.]).to(target_device)).roll(shifts = 1, dims = 1)</a> + torch.Tensor([-w / 2, h / 2]).to(target_device)) / focal
    ray_raw = torch.sum(torch.cat([ray_raw, -torch.ones(ray_raw.shape[0], 1, dtype = torch.float32).to(target_device)], dim = -1).unsqueeze(-2) * cam_tfs[..., :-1], dim = -1)
    &#47&#47 return shape (ray_num, point_num, 3), (ray_num, point_num), rgb(ray_num, rgb), cams(ray_num, ray_dir, ray_t)
    pts = cam_tfs[:, :, -1].unsqueeze(-2) + lengths[:, :, None] * ray_raw[:, None, :]</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 return shape (ray_num, point_num, 3), (ray_num, point_num), rgb(ray_num, rgb), cams(ray_num, ray_dir, ray_t)
    pts = cam_tf[:, -1] + ray_raw[:, None, :] * lengths[:, :, None]
    &#47&#47 ray_raw is of shape (ray_num, 3)
    return torch.cat((pts, ray_raw.unsqueeze(-2).repeat(1, point_num, 1)), dim = -1), lengths, output_rgb, torch.cat((<a id="change">cam_tf[:, -1]</a>.unsqueeze(0).repeat(ray_raw.shape[0], 1), ray_raw), dim = -1)

def fov2Focal(fov:float, img_width:float) -&gt; float:
    return .5 * img_width / np.tan(.5 * fov)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/enigmatisms/nerf/commit/926e53d582b75b6d41aa6ffb38432e21706adc6f#diff-02847a575530271cafd549fcced1759b0b5f0561c2c608cac9054f1954992a7bL72' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 21024173</div><div id='project'> Project Name: enigmatisms/nerf</div><div id='commit'> Commit Name: 926e53d582b75b6d41aa6ffb38432e21706adc6f</div><div id='time'> Time: 2022-04-14</div><div id='author'> Author: 984041003@qq.com</div><div id='file'> File Name: py/utils.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: validSampler(10)</div><div id='n_method'> N Method Name: validSampler(10)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: py/utils.py</div><div id='n_file'> N File Name: py/utils.py</div><div id='m_start'> M Start Line: 73</div><div id='m_end'> M End Line: 89</div><div id='n_start'> N Start Line: 109</div><div id='n_end'> N End Line: 118</div><BR>