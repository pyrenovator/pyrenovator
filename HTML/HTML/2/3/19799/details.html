<html><h3>Pattern ID :19799
</h3><img src='64506890.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    def update(self, iter=None):
        cur_nimg = self.batch_size * self.d_updates_per_step * iter
        <a id="change">if </a>self.ema_rampup is not None:
            ema_nimg<a id="change"> = </a>min(self.ema_nimg, cur_nimg * self.ema_rampup)
        ema_beta = 0.5 ** (self.batch_size / max(ema_nimg, 1e-8))
        for p_ema, p in zip(self.target.parameters(), self.source.parameters()):
            p_ema.copy_(p.lerp(p_ema, ema_beta))</code></pre><h3>After Change</h3><pre><code class='java'>

        with torch.no_grad():
            for key in self.source_dict:
                <a id="change">self.target_dict[key].data.copy_(</a>self.target_dict[key].data*decay + \
                                                 self.source_dict[key].data*(1. - decay)<a id="change">)</a>


class Ema_stylegan(object):
    def __init__(self, source, target, ema_kimg, ema_rampup, effective_batch_size, d_updates_per_step):</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/postech-cvlab/pytorch-studiogan/commit/1d01577888dcf78a5742f1a3b4f472c24543705f#diff-2436d321347d697a9892a647be2fabeb2c289447ce8fc97cb749778105cbf7c5L44' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 64506890</div><div id='project'> Project Name: postech-cvlab/pytorch-studiogan</div><div id='commit'> Commit Name: 1d01577888dcf78a5742f1a3b4f472c24543705f</div><div id='time'> Time: 2021-09-25</div><div id='author'> Author: 60963841+alex4727@users.noreply.github.com</div><div id='file'> File Name: src/utils/ema.py</div><div id='m_class'> M Class Name: Ema</div><div id='n_method'> N Class Name: Ema</div><div id='m_method'> M Method Name: update(2)</div><div id='n_method'> N Method Name: update(2)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: src/utils/ema.py</div><div id='n_file'> N File Name: src/utils/ema.py</div><div id='m_start'> M Start Line: 44</div><div id='m_end'> M End Line: 51</div><div id='n_start'> N Start Line: 46</div><div id='n_end'> N End Line: 56</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                    params, offsets = partition_param_map[part]
                    found = False
                    for p_idx, _p in enumerate(params):
                        <a id="change">if </a>p.__hash__() == _p.__hash__():
                            found<a id="change"> = </a>True
                            if offsets[p_idx][0] is not None:
                                my_part = part.narrow(0,
                                                      offsets[p_idx][0],</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 copy back reduced gradients but only those needed for this local rank
            for param, updated_grad in zip(self.fp16_groups[i], _unflatten_dense_tensors(flat_all_grads, self.fp16_groups[i])):
                if param in my_params:
                    <a id="change">param.grad.copy_(</a>updated_grad<a id="change">)</a>

    def step(self, closure=None):
        &#47&#47 First compute norm for all group so we know if there is overflow
        self.overflow = self.overflow_checker.check()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/deepspeed/commit/55ed105771d08fbffc0cb6d8cd56a2e61206ad1d#diff-458bf13440cbc0013d248079431500c00e3907e0a747c77fa0c8dda6b8b25f88L530' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 64506876</div><div id='project'> Project Name: microsoft/deepspeed</div><div id='commit'> Commit Name: 55ed105771d08fbffc0cb6d8cd56a2e61206ad1d</div><div id='time'> Time: 2020-09-15</div><div id='author'> Author: jerasley@microsoft.com</div><div id='file'> File Name: deepspeed/runtime/zero/stage1.py</div><div id='m_class'> M Class Name: FP16_DeepSpeedZeroOptimizer_Stage1</div><div id='n_method'> N Class Name: FP16_DeepSpeedZeroOptimizer_Stage1</div><div id='m_method'> M Method Name: reduce_scatter_gradients(4)</div><div id='n_method'> N Method Name: reduce_scatter_gradients(4)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: deepspeed/runtime/zero/stage1.py</div><div id='n_file'> N File Name: deepspeed/runtime/zero/stage1.py</div><div id='m_start'> M Start Line: 539</div><div id='m_end'> M End Line: 613</div><div id='n_start'> N Start Line: 536</div><div id='n_end'> N End Line: 605</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                    elif step_size &gt; 0:
                        p.data.add_(exp_avg, alpha=-step_size * group[&quotlr&quot])
                
                <a id="change">if </a>half_precision:
                    p.data<a id="change"> = </a>p.data.half()
                    p.grad = p.grad.half() 

        return loss</code></pre><h3>After Change</h3><pre><code class='java'>
                        p_fp32.add_(exp_avg, alpha=-step_size * group[&quotlr&quot])
                
                if p.dtype in {torch.float16, torch.bfloat16}:
                    <a id="change">p.copy_(</a>p_fp32<a id="change">)</a>

        return loss
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/feng-lab/pytorch-image-models/commit/a426511c95e131389237e4ed2696f5967bc66130#diff-ff45ce4f09a1fa670169ee361c351290b8ae7962fb54adf22d5c4f44a3a9b872L88' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 64506883</div><div id='project'> Project Name: feng-lab/pytorch-image-models</div><div id='commit'> Commit Name: a426511c95e131389237e4ed2696f5967bc66130</div><div id='time'> Time: 2021-08-18</div><div id='author'> Author: rwightman@gmail.com</div><div id='file'> File Name: timm/optim/adabelief.py</div><div id='m_class'> M Class Name: AdaBelief</div><div id='n_method'> N Class Name: AdaBelief</div><div id='m_method'> M Method Name: step(2)</div><div id='n_method'> N Method Name: step(2)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: timm/optim/adabelief.py</div><div id='n_file'> N File Name: timm/optim/adabelief.py</div><div id='m_start'> M Start Line: 96</div><div id='m_end'> M End Line: 203</div><div id='n_start'> N Start Line: 98</div><div id='n_end'> N End Line: 201</div><BR>