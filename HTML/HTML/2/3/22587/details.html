<html><h3>Pattern ID :22587
</h3><img src='71382689.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
  grad_weight = grad_weight.reshape(C.bs, grad_weight.shape[1] // C.bs, *grad_weight.shape[2:]).sum(dim=0)
  grad_weight = grad_weight.view(C.groups, C.cin, C.rcout, *grad_weight.shape[1:]).transpose(2, 1)
  &#47&#47 narrow removes excess for strided
  dw[:] = <a id="change">grad_weight.contiguous().view(C.groups*C.rcout, C.cin, *grad_weight.shape[3:]).narrow(
            </a>2, <a id="change">0</a>, dw.shape[2]<a id="change">)</a>.narrow(3, 0, dw.shape[3])
  return dw

def processing_op(op,x,w,ret,stride,groups):</code></pre><h3>After Change</h3><pre><code class='java'>
  x = x.reshape(1, C.bs * C.groups * C.cin, C.iy, C.ix)
  grad_weight = torch.conv2d(x, grad_output, dilation=stride, groups=C.bs*C.groups*C.cin)
  grad_weight = grad_weight.reshape(C.bs, C.groups, C.cin, C.rcout, *grad_weight.shape[2:]).sum(dim=0).transpose(2, 1)
  dw[:] = <a id="change">grad_weight.reshape(C.groups*C.rcout, C.cin, *grad_weight.shape[3:])[:, :, :dw.shape[2], :dw.shape[3]]</a>
  return dw

def processing_op(op,x,w,ret,stride,groups):
  if op == ProcessingOps.CONV:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 2</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/geohot/tinygrad/commit/861323c121a120e2290fc290e35efe26408bdae4#diff-60843a1cab58c1331c2852d045fc62b210ce37a102b5e7c490fc576606ba02ebL31' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 71382689</div><div id='project'> Project Name: geohot/tinygrad</div><div id='commit'> Commit Name: 861323c121a120e2290fc290e35efe26408bdae4</div><div id='time'> Time: 2022-06-11</div><div id='author'> Author: geohot@gmail.com</div><div id='file'> File Name: tinygrad/llops/ops_torch.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: convdw(5)</div><div id='n_method'> N Method Name: convdw(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: tinygrad/llops/ops_torch.py</div><div id='n_file'> N File Name: tinygrad/llops/ops_torch.py</div><div id='m_start'> M Start Line: 31</div><div id='m_end'> M End Line: 41</div><div id='n_start'> N Start Line: 31</div><div id='n_end'> N End Line: 37</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                        if p.__hash__() == _p.__hash__():
                            found = True
                            if offsets[p_idx][0] is not None:
                                my_part = <a id="change">part.narrow(0</a>,
                                                      offsets[p_idx][0],
                                                      offsets[p_idx][1]<a id="change">)</a>
                                parts.append(my_part)
                    assert found
                if p is not None:
                    updated_grad = _unflatten_dense_tensors(torch.cat(parts), [p])</code></pre><h3>After Change</h3><pre><code class='java'>
            flat_all_grads = torch.cat(flat_comm_grads)

            &#47&#47 copy back reduced gradients but only those needed for this local rank
            for param, updated_grad in zip(self.fp16_groups[i], _unflatten_dense_tensors(flat_all_grads, <a id="change">self.fp16_groups[i]</a>)):
                if param in my_params:
                    param.grad.copy_(updated_grad)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/deepspeed/commit/55ed105771d08fbffc0cb6d8cd56a2e61206ad1d#diff-458bf13440cbc0013d248079431500c00e3907e0a747c77fa0c8dda6b8b25f88L530' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 71382688</div><div id='project'> Project Name: microsoft/deepspeed</div><div id='commit'> Commit Name: 55ed105771d08fbffc0cb6d8cd56a2e61206ad1d</div><div id='time'> Time: 2020-09-15</div><div id='author'> Author: jerasley@microsoft.com</div><div id='file'> File Name: deepspeed/runtime/zero/stage1.py</div><div id='m_class'> M Class Name: FP16_DeepSpeedZeroOptimizer_Stage1</div><div id='n_method'> N Class Name: FP16_DeepSpeedZeroOptimizer_Stage1</div><div id='m_method'> M Method Name: reduce_scatter_gradients(4)</div><div id='n_method'> N Method Name: reduce_scatter_gradients(4)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: deepspeed/runtime/zero/stage1.py</div><div id='n_file'> N File Name: deepspeed/runtime/zero/stage1.py</div><div id='m_start'> M Start Line: 539</div><div id='m_end'> M End Line: 613</div><div id='n_start'> N Start Line: 536</div><div id='n_end'> N End Line: 605</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            for i, sub_group in enumerate(self.fp16_groups):
                self.averaged_gradients[i] = [
                    torch.zeros_like(param.ds_tensor) if param.grad is None else
                    <a id="change">param.grad.data.narrow(0</a>,
                                           0,
                                           param.ds_tensor.numel()<a id="change">)</a>
                    for param in sub_group
                ]
                &#47&#47 self.averaged_gradients[i] = self.get_flat_partition(
                &#47&#47     self.fp16_groups[i],</code></pre><h3>After Change</h3><pre><code class='java'>
        if not self.offload_optimizer:
            for i, sub_group in enumerate(self.fp16_groups):
                self.averaged_gradients[i] = [
                    <a id="change">self.__param_id_to_grad_partition[param.ds_id]</a>
                    if param.requires_grad else torch.zeros_like(param.ds_tensor)
                    for param in sub_group
                ]
                &#47&#47 self.averaged_gradients[i] = self.get_flat_partition(</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/deepspeed/commit/4912e0ad7efcaf97389ae944259aa0e9f331038a#diff-1ad5daa1b31aa5573616024068d646f0c38e88d4d3a71d3d0e4bc352ea232178L1722' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 71382687</div><div id='project'> Project Name: microsoft/deepspeed</div><div id='commit'> Commit Name: 4912e0ad7efcaf97389ae944259aa0e9f331038a</div><div id='time'> Time: 2022-01-20</div><div id='author'> Author: 31414860+jfc4050@users.noreply.github.com</div><div id='file'> File Name: deepspeed/runtime/zero/stage3.py</div><div id='m_class'> M Class Name: DeepSpeedZeroOptimizer_Stage3</div><div id='n_method'> N Class Name: DeepSpeedZeroOptimizer_Stage3</div><div id='m_method'> M Method Name: independent_gradient_partition_epilogue(1)</div><div id='n_method'> N Method Name: independent_gradient_partition_epilogue(1)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: deepspeed/runtime/zero/stage3.py</div><div id='n_file'> N File Name: deepspeed/runtime/zero/stage3.py</div><div id='m_start'> M Start Line: 1727</div><div id='m_end'> M End Line: 1757</div><div id='n_start'> N Start Line: 1723</div><div id='n_end'> N End Line: 1750</div><BR>