<html><h3>Pattern ID :1371
</h3><img src='6510755.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 [batch, num_heads, hh * ww, hh * ww]
    attention_scores = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([query, key])
    attention_scores = MultiHeadRelativePositionalEmbedding(name=name and name + "pos_emb")(attention_scores)
    attention_scores = <a id="change">tf.nn.softmax(</a>attention_scores<a id="change">, axis=-1)</a>

    if attn_dropout &gt; 0:
        attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
    &#47&#47 value = [batch, num_heads, hh * ww, key_dim]</code></pre><h3>After Change</h3><pre><code class='java'>
    attention_scores = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([query, key])
    attention_scores = MultiHeadRelativePositionalEmbedding(name=name and name + "pos_emb")(attention_scores)
    &#47&#47 attention_scores = tf.nn.softmax(attention_scores, axis=-1, name=name and name + "_attention_scores")
    attention_scores = <a id="change">keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(</a>attention_scores<a id="change">)</a>

    if attn_dropout &gt; 0:
        attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
    &#47&#47 value = [batch, num_heads, hh * ww, key_dim]</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 9</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/081eea59f8d5c76d516ee894e8ef66cb1bb0970c#diff-02508ae7c53082aa4dbb63a5c5fd259b84bc5bdbf0b1971ed6cb2060ef9b9779L113' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6510755</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 081eea59f8d5c76d516ee894e8ef66cb1bb0970c</div><div id='time'> Time: 2021-11-24</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/beit/beit.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: attention_block(8)</div><div id='n_method'> N Method Name: attention_block(8)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/beit/beit.py</div><div id='n_file'> N File Name: keras_cv_attention_models/beit/beit.py</div><div id='m_start'> M Start Line: 113</div><div id='m_end'> M End Line: 114</div><div id='n_start'> N Start Line: 113</div><div id='n_end'> N End Line: 115</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 print(f"&gt;&gt;&gt;&gt; {qkv.shape = }, {qq.shape = }, {kk.shape = }, {vv.shape = }")

    &#47&#47 Factorized attention.
    kk = <a id="change">tf.nn.softmax(</a>kk<a id="change">, axis=2)</a>  &#47&#47 On `blocks` dimension
    &#47&#47 attn = tf.matmul(kk, vv, transpose_a=True)  &#47&#47 &quotb h n k, b h n v -&gt; b h k v&quot, [batch, num_heads, key_dim, key_dim]
    &#47&#47 factor_att = tf.matmul(qq, attn)    &#47&#47 &quotb h n k, b h k v -&gt; b h n v&quot, [batch, num_heads, blocks, key_dim]
    attn = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1], transpose_a=True))([kk, vv])</code></pre><h3>After Change</h3><pre><code class='java'>

    &#47&#47 Factorized attention.
    &#47&#47 kk = tf.nn.softmax(kk, axis=2)  &#47&#47 On `blocks` dimension
    kk = <a id="change">keras.layers.Softmax(axis=2, name=name and name + "attention_scores")(</a>kk<a id="change">)</a>  &#47&#47 On `blocks` dimension
    &#47&#47 attn = tf.matmul(kk, vv, transpose_a=True)  &#47&#47 &quotb h n k, b h n v -&gt; b h k v&quot, [batch, num_heads, key_dim, key_dim]
    &#47&#47 factor_att = tf.matmul(qq, attn)    &#47&#47 &quotb h n k, b h k v -&gt; b h n v&quot, [batch, num_heads, blocks, key_dim]
    attn = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1], transpose_a=True))([kk, vv])</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/6e229eae788d3611b11e3b5b2980678e8fc83266#diff-d77c1598360af0cc3aabc2698058ce15f5c2c73e1d4d820c94c37380c4e0f51eL130' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6510754</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 6e229eae788d3611b11e3b5b2980678e8fc83266</div><div id='time'> Time: 2021-11-24</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/coat/coat.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: factor_attention_conv_relative_positional_encoding(5)</div><div id='n_method'> N Method Name: factor_attention_conv_relative_positional_encoding(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/coat/coat.py</div><div id='n_file'> N File Name: keras_cv_attention_models/coat/coat.py</div><div id='m_start'> M Start Line: 141</div><div id='m_end'> M End Line: 141</div><div id='n_start'> N Start Line: 142</div><div id='n_end'> N End Line: 142</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    pos_emb = RelativePositionalEmbedding(use_absolute_pos=not relative, name=name and name + "pos_emb")(pos_query)
    pos_emb = tf.reshape(pos_emb, [-1, *attention_scores.shape[1:]])
    attention_scores = keras.layers.Add()([attention_scores, pos_emb])
    attention_scores = <a id="change">tf.nn.softmax(</a>attention_scores<a id="change">, axis=-1)</a>

    if attn_dropout &gt; 0:
        attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
    &#47&#47 value = [batch, num_heads, hh * ww, vv_dim]</code></pre><h3>After Change</h3><pre><code class='java'>
    pos_emb = tf.reshape(pos_emb, [-1, *attention_scores.shape[1:]])
    attention_scores = keras.layers.Add()([attention_scores, pos_emb])
    &#47&#47 attention_scores = tf.nn.softmax(attention_scores, axis=-1)
    attention_scores = <a id="change">keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(</a>attention_scores<a id="change">)</a>

    if attn_dropout &gt; 0:
        attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
    &#47&#47 value = [batch, num_heads, hh * ww, vv_dim]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/6e229eae788d3611b11e3b5b2980678e8fc83266#diff-4163380308eb60d4185b09d8359be777af3cee4f5ffc9acbf77fc54a76f12e5bL123' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6510753</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 6e229eae788d3611b11e3b5b2980678e8fc83266</div><div id='time'> Time: 2021-11-24</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/botnet/botnet.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: mhsa_with_relative_position_embedding(9)</div><div id='n_method'> N Method Name: mhsa_with_relative_position_embedding(9)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/botnet/botnet.py</div><div id='n_file'> N File Name: keras_cv_attention_models/botnet/botnet.py</div><div id='m_start'> M Start Line: 156</div><div id='m_end'> M End Line: 157</div><div id='n_start'> N Start Line: 156</div><div id='n_end'> N End Line: 158</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    pos = pos[..., -key_hh:, -key_ww:]
    pos = tf.reshape(pos, [-1, *attention_scores.shape[1:]])
    attention_scores = keras.layers.Add()([attention_scores, pos])
    attention_scores = <a id="change">tf.nn.softmax(</a>attention_scores<a id="change">, axis=-1)</a>

    if attn_dropout &gt; 0:
        attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
    &#47&#47 value = [batch, num_heads, key_hh * key_ww, key_dim]</code></pre><h3>After Change</h3><pre><code class='java'>
    pos = tf.reshape(pos, [-1, *attention_scores.shape[1:]])
    attention_scores = keras.layers.Add()([attention_scores, pos])
    &#47&#47 attention_scores = tf.nn.softmax(attention_scores, axis=-1)
    attention_scores = <a id="change">keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(</a>attention_scores<a id="change">)</a>

    if attn_dropout &gt; 0:
        attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
    &#47&#47 value = [batch, num_heads, key_hh * key_ww, key_dim]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/6e229eae788d3611b11e3b5b2980678e8fc83266#diff-f52477ea03b2a8f18695211ce6217a2ea1e5dd5afaa7adfd75a231c5a04692bcL14' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6510752</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 6e229eae788d3611b11e3b5b2980678e8fc83266</div><div id='time'> Time: 2021-11-24</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/cmt/cmt.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: light_multi_head_self_attention(9)</div><div id='n_method'> N Method Name: light_multi_head_self_attention(9)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/cmt/cmt.py</div><div id='n_file'> N File Name: keras_cv_attention_models/cmt/cmt.py</div><div id='m_start'> M Start Line: 54</div><div id='m_end'> M End Line: 55</div><div id='n_start'> N Start Line: 54</div><div id='n_end'> N End Line: 56</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 print(f"&gt;&gt;&gt;&gt; {pos.shape = }, {attention_scores.shape = }")
    pos = tf.reshape(pos, [-1, *attention_scores.shape[1:]])
    attention_scores = keras.layers.Add()([attention_scores, pos])
    attention_scores = <a id="change">tf.nn.softmax(</a>attention_scores<a id="change">, axis=-1)</a>

    if attn_dropout &gt; 0:
        attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
</code></pre><h3>After Change</h3><pre><code class='java'>
    pos = tf.reshape(pos, [-1, *attention_scores.shape[1:]])
    attention_scores = keras.layers.Add()([attention_scores, pos])
    &#47&#47 attention_scores = tf.nn.softmax(attention_scores, axis=-1)
    attention_scores = <a id="change">keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(</a>attention_scores<a id="change">)</a>

    if attn_dropout &gt; 0:
        attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/6e229eae788d3611b11e3b5b2980678e8fc83266#diff-5a0036f02e35afe08c2170dc9b0e1131865080e554a879c04557e7a873330e74L17' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6510759</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 6e229eae788d3611b11e3b5b2980678e8fc83266</div><div id='time'> Time: 2021-11-24</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/halonet/halonet.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: halo_attention(11)</div><div id='n_method'> N Method Name: halo_attention(11)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/halonet/halonet.py</div><div id='n_file'> N File Name: keras_cv_attention_models/halonet/halonet.py</div><div id='m_start'> M Start Line: 77</div><div id='m_end'> M End Line: 78</div><div id='n_start'> N Start Line: 77</div><div id='n_end'> N End Line: 79</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    attn = keras.layers.AveragePooling2D(pool_size=kernel_size, strides=kernel_size)(inputs)
    attn = keras.layers.Dense(kernel_size ** 4 * num_heads, use_bias=True, name=name + "attn")(attn) / qk_scale
    attn = tf.reshape(attn, [-1, hh, ww, num_heads, kernel_size * kernel_size, kernel_size * kernel_size])  &#47&#47 [1, 14, 14, 6, 4, 4]
    attn = <a id="change">tf.nn.softmax(</a>attn<a id="change">, axis=-1)</a>
    if attn_dropout &gt; 0:
        attn = keras.layers.Dropout(attn_dropout)(attn)

    out = tf.matmul(attn, vv)  &#47&#47 [1, 14, 14, 6, 4, 32]</code></pre><h3>After Change</h3><pre><code class='java'>
    attn = keras.layers.Dense(kernel_size ** 4 * num_heads, use_bias=True, name=name + "attn")(attn) / qk_scale
    attn = tf.reshape(attn, [-1, hh, ww, num_heads, kernel_size * kernel_size, kernel_size * kernel_size])  &#47&#47 [1, 14, 14, 6, 4, 4]
    &#47&#47 attn = tf.nn.softmax(attn, axis=-1)
    attn = <a id="change">keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(</a>attn<a id="change">)</a>
    if attn_dropout &gt; 0:
        attn = keras.layers.Dropout(attn_dropout)(attn)

    out = tf.matmul(attn, vv)  &#47&#47 [1, 14, 14, 6, 4, 32]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/d9f3514af2dab63a76a19ae7bd998a730a9cfd07#diff-70a89c021ce6222b18ca0d8fb02d602b2b15343d3af62a18df1b7034f8955a29L73' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6510758</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: d9f3514af2dab63a76a19ae7bd998a730a9cfd07</div><div id='time'> Time: 2021-11-26</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/volo/volo.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: outlook_attention_simple(6)</div><div id='n_method'> N Method Name: outlook_attention_simple(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/volo/volo.py</div><div id='n_file'> N File Name: keras_cv_attention_models/volo/volo.py</div><div id='m_start'> M Start Line: 94</div><div id='m_end'> M End Line: 95</div><div id='n_start'> N Start Line: 94</div><div id='n_end'> N End Line: 96</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 [batch, num_heads, hh * ww, hh * ww]
    attention_scores = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([query, key]) * qk_scale
    attention_scores = MultiHeadRelativePositionalEmbedding(with_cls_token=False, name=name and name + "pos_emb")(attention_scores)
    attention_scores = <a id="change">tf.nn.softmax(</a>attention_scores<a id="change">, axis=-1)</a>

    if attn_dropout &gt; 0:
        attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
    &#47&#47 value = [batch, num_heads, hh * ww, vv_dim]</code></pre><h3>After Change</h3><pre><code class='java'>
    attention_scores = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([query, key]) * qk_scale
    attention_scores = MultiHeadRelativePositionalEmbedding(with_cls_token=False, name=name and name + "pos_emb")(attention_scores)
    &#47&#47 attention_scores = tf.nn.softmax(attention_scores, axis=-1)
    attention_scores = <a id="change">keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(</a>attention_scores<a id="change">)</a>

    if attn_dropout &gt; 0:
        attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
    &#47&#47 value = [batch, num_heads, hh * ww, vv_dim]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/6e229eae788d3611b11e3b5b2980678e8fc83266#diff-93ecfaca577d52a2659237cd2194089f24ed46627c659ab6c323a696749405deL16' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6510757</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 6e229eae788d3611b11e3b5b2980678e8fc83266</div><div id='time'> Time: 2021-11-24</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/coatnet/coatnet.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: mhsa_with_multi_head_relative_position_embedding(9)</div><div id='n_method'> N Method Name: mhsa_with_multi_head_relative_position_embedding(9)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/coatnet/coatnet.py</div><div id='n_file'> N File Name: keras_cv_attention_models/coatnet/coatnet.py</div><div id='m_start'> M Start Line: 45</div><div id='m_end'> M End Line: 46</div><div id='n_start'> N Start Line: 45</div><div id='n_end'> N End Line: 47</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    attn = keras.layers.Dense(kernel_size ** 4 * num_heads, name=name + "attn")(attn) / qk_scale
    &#47&#47 [1, 14, 14, 6, 9, 9]
    attn = tf.reshape(attn, (-1, hh, ww, num_heads, kernel_size * kernel_size, kernel_size * kernel_size))
    attention_weights = <a id="change">tf.nn.softmax(</a>attn<a id="change">, axis=-1)</a>
    if attn_dropout &gt; 0:
        attention_weights = keras.layers.Dropout(attn_dropout)(attention_weights)

     unfold </code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 [1, 14, 14, 6, 9, 9]
    attn = tf.reshape(attn, (-1, hh, ww, num_heads, kernel_size * kernel_size, kernel_size * kernel_size))
    &#47&#47 attention_weights = tf.nn.softmax(attn, axis=-1)
    attention_weights = <a id="change">keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(</a>attn<a id="change">)</a>
    if attn_dropout &gt; 0:
        attention_weights = keras.layers.Dropout(attn_dropout)(attention_weights)

     unfold </code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/081eea59f8d5c76d516ee894e8ef66cb1bb0970c#diff-70a89c021ce6222b18ca0d8fb02d602b2b15343d3af62a18df1b7034f8955a29L19' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6510756</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 081eea59f8d5c76d516ee894e8ef66cb1bb0970c</div><div id='time'> Time: 2021-11-24</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/volo/volo.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: outlook_attention(9)</div><div id='n_method'> N Method Name: outlook_attention(9)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/volo/volo.py</div><div id='n_file'> N File Name: keras_cv_attention_models/volo/volo.py</div><div id='m_start'> M Start Line: 34</div><div id='m_end'> M End Line: 35</div><div id='n_start'> N Start Line: 34</div><div id='n_end'> N End Line: 36</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 attn = keras.layers.Dense(filters * randix, use_bias=True, name=name and name + "attn_se_dense_2")(attn)
    attn = conv2d_no_bias(attn, filters * randix, 1, use_bias=True, name=name and name + "attn_se_2_")
    attn = tf.reshape(attn, [-1, 1, 1, filters, randix])
    attn = <a id="change">tf.nn.softmax(</a>attn<a id="change">, axis=-1)</a>

    &#47&#47 value and output
    value = keras.layers.Concatenate(axis=-1)([tf.expand_dims(embed_out, -1), tf.expand_dims(key, -1)])
    output = keras.layers.Multiply()([value, attn])</code></pre><h3>After Change</h3><pre><code class='java'>
    attn = conv2d_no_bias(attn, filters * randix, 1, use_bias=True, name=name and name + "attn_se_2_")
    attn = tf.reshape(attn, [-1, 1, 1, filters, randix])
    &#47&#47 attn = tf.nn.softmax(attn, axis=-1)
    attn = <a id="change">keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(</a>attn<a id="change">)</a>

    &#47&#47 value and output
    value = keras.layers.Concatenate(axis=-1)([tf.expand_dims(embed_out, -1), tf.expand_dims(key, -1)])
    output = keras.layers.Multiply()([value, attn])</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leondgarse/keras_cv_attention_models/commit/6e229eae788d3611b11e3b5b2980678e8fc83266#diff-79b1a70b09d57566dea2028f6e7c969015fb1f8c501290405121445e11e276b0L25' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 6510751</div><div id='project'> Project Name: leondgarse/keras_cv_attention_models</div><div id='commit'> Commit Name: 6e229eae788d3611b11e3b5b2980678e8fc83266</div><div id='time'> Time: 2021-11-24</div><div id='author'> Author: leondgarse@gmail.com</div><div id='file'> File Name: keras_cv_attention_models/cotnet/cotnet.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: cot_attention(6)</div><div id='n_method'> N Method Name: cot_attention(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: keras_cv_attention_models/cotnet/cotnet.py</div><div id='n_file'> N File Name: keras_cv_attention_models/cotnet/cotnet.py</div><div id='m_start'> M Start Line: 85</div><div id='m_end'> M End Line: 86</div><div id='n_start'> N Start Line: 85</div><div id='n_end'> N End Line: 87</div><BR>