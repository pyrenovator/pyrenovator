<html><h3>Pattern ID :34080
</h3><img src='97441980.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            for name, input_size in self.input_sizes.items():
                &#47&#47 select slice of embedding belonging to a single input
                variable_embedding = embedding[..., start : (start + input_size)]
                variable_embedding_mean.append(<a id="change">variable_embedding.abs().mean(</a>-1<a id="change">)</a>)
                var_outputs.append(self.single_variable_grns[name](variable_embedding))
                start += input_size
            var_outputs = torch.stack(var_outputs, axis=-1)</code></pre><h3>After Change</h3><pre><code class='java'>
            if outputs.ndim == 3:  &#47&#47 -&gt; batch size, time, hidden size, n_variables
                sparse_weights = torch.ones(outputs.size(0), outputs.size(1), 1, 1, device=outputs.device)  &#47&#47
            else:  &#47&#47 ndim == 2 -&gt; batch size, hidden size, n_variables
                sparse_weights<a id="change"> = </a>torch.ones(<a id="change">outputs.size(</a>0<a id="change">)</a>, 1, 1, device=outputs.device)
        return outputs, sparse_weights

</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/jdb78/pytorch-forecasting/commit/4b0e3005131eafedd8fe0d16081984f90c284a62#diff-9a8b783572f864adbf7d350ca9150d50513b8e1d871585f96c7310e6336d1c87L310' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 97441980</div><div id='project'> Project Name: jdb78/pytorch-forecasting</div><div id='commit'> Commit Name: 4b0e3005131eafedd8fe0d16081984f90c284a62</div><div id='time'> Time: 2020-08-12</div><div id='author'> Author: beitner.jan@bcg.com</div><div id='file'> File Name: pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py</div><div id='m_class'> M Class Name: VariableSelectionNetwork</div><div id='n_method'> N Class Name: VariableSelectionNetwork</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py</div><div id='n_file'> N File Name: pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py</div><div id='m_start'> M Start Line: 310</div><div id='m_end'> M End Line: 335</div><div id='n_start'> N Start Line: 351</div><div id='n_end'> N End Line: 382</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  &#47&#47 This is equal to comparing with thresolds

    return <a id="change">thresholded.mean()</a>  &#47&#47 Or thresholded.mean() if you are interested in average across the batch


def mask2onehot(masks, num_classes=19, ignore_class=255):</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 But if you are passing output from UNet or something it will most probably
    &#47&#47 be with the BATCH x 1 x H x W shape
    &#47&#47 code is borrowed from https://www.kaggle.com/iezepov/fast-iou-scoring-metric-in-pytorch-and-numpy
    num_classes<a id="change"> = </a><a id="change">outputs.size()</a>[1]
    with torch.no_grad():
        labels[labels==ignore_index] = num_classes
        outputs = torch.argmax(outputs, dim=1)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lehduong/knowledge-distillation-by-replacing-cheap-conv/commit/551fab7b0e17e665f98d3a59fe0cc4fe5def3c79#diff-9ff632ee77415c737020125a2c83b31d8a7a74d02b3fe5a9579c8f40b48c1356L9' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 97441978</div><div id='project'> Project Name: lehduong/knowledge-distillation-by-replacing-cheap-conv</div><div id='commit'> Commit Name: 551fab7b0e17e665f98d3a59fe0cc4fe5def3c79</div><div id='time'> Time: 2020-02-02</div><div id='author'> Author: 1612372@hcmut.edu.vn</div><div id='file'> File Name: models/metric.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: iou(3)</div><div id='n_method'> N Method Name: iou(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: models/metric.py</div><div id='n_file'> N File Name: models/metric.py</div><div id='m_start'> M Start Line: 15</div><div id='m_end'> M End Line: 24</div><div id='n_start'> N Start Line: 11</div><div id='n_end'> N End Line: 18</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 attention is batch x time x heads x time_to_attend
        &#47&#47 average over batches, heads + only keep prediction attention and attention on observed timesteps
        if attention_prediction_horizon is None:  &#47&#47 average over all horizons
            attention = <a id="change">out["attention"][..., : self.hparams.encode_length].mean(
                dim=average_dims + [2]
            )</a>  &#47&#47 todo: how to handle zero attention due to shorter encode length?
            attention = attention / attention.sum(-1).unsqueeze(-1)  &#47&#47 renormalize
            attention = attention.mean(0)  &#47&#47 average attention over all predictions
        else:</code></pre><h3>After Change</h3><pre><code class='java'>
        for i in range(len(attention)):  &#47&#47 very inefficient but does the trick
            if 0 &lt; out["encode_lengths"][i] &lt; attention.size(1):
                attention[i, -out["encode_lengths"][i] :] = attention[i, : out["encode_lengths"][i]].clone()
                attention[i, : <a id="change">attention.size(</a>1<a id="change">)</a> - out["encode_lengths"][i]] = 0.0

        if average_batches:  &#47&#47 if to average over batches
            static_variables = static_variables.mean(dim=0)
            encoder_variables = encoder_variables.mean(dim=0)
            decoder_variables = decoder_variables.mean(dim=0)
            attention = attention.mean(dim=0)
            attention = attention / attention.sum(-1).unsqueeze(-1)  &#47&#47 renormalize

            attention = torch.zeros(self.hparams.max_encode_length).scatter(
                dim=0,
                index=torch.arange(self.hparams.max_encode_length - attention.size(0), self.hparams.max_encode_length),
                src=attention,
            )
        else:
            attention = attention / attention.sum(-1).unsqueeze(-1)  &#47&#47 renormalize
            attention<a id="change"> = </a>torch.zeros(attention.size(0), self.hparams.max_encode_length).scatter(
                dim=1,
                index=torch.arange(
                    self.hparams.max_encode_length - attention.size(0), self.hparams.max_encode_length</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/jdb78/pytorch-forecasting/commit/79cfec0818dbe78d8773534e6ce8f5fd578c3c3a#diff-326fa71fa47eb2d5a00ea5fa14cac5f65c967b6856192e5c0e687cbce8b581e3L551' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 97441990</div><div id='project'> Project Name: jdb78/pytorch-forecasting</div><div id='commit'> Commit Name: 79cfec0818dbe78d8773534e6ce8f5fd578c3c3a</div><div id='time'> Time: 2020-06-22</div><div id='author'> Author: beitner.jan@bcg.com</div><div id='file'> File Name: temporal_fusion_transformer_pytorch/model/__init__.py</div><div id='m_class'> M Class Name: TemporalFusionTransformer</div><div id='n_method'> N Class Name: TemporalFusionTransformer</div><div id='m_method'> M Method Name: interpret_output(4)</div><div id='n_method'> N Method Name: interpret_output(4)</div><div id='m_parent_class'> M Parent Class: pl.LightningModule</div><div id='n_parent_class'> N Parent Class: pl.LightningModule</div><div id='m_file'> M File Name: temporal_fusion_transformer_pytorch/model/__init__.py</div><div id='n_file'> N File Name: temporal_fusion_transformer_pytorch/model/__init__.py</div><div id='m_start'> M Start Line: 555</div><div id='m_end'> M End Line: 605</div><div id='n_start'> N Start Line: 553</div><div id='n_end'> N End Line: 610</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        x = torch.cat((x, static_x), dim=1)

        &#47&#47 take the mean of all time steps (rows) with additional row for static features; output size = (batch_size, 64)
        x = <a id="change">torch.mean(</a>x, 1<a id="change">)</a>

        &#47&#47 pass through fully-connected part to lower dimension to 2 (binary classification)
        return self.feed_forward(x)
</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 x = torch.mean(x, 1)

        &#47&#47 take the masked mean of all time steps (rows) with additional row for static features; output size = (batch_size, 64)
        mask = torch.cat((mask, torch.ones((<a id="change">x.size()</a>[0], 1), dtype=torch.bool)), dim=1).unsqueeze(2).long()
        time_length = torch.FloatTensor(time_length).unsqueeze(1)
        x<a id="change"> = </a>torch.sum(x * (1 - mask), dim=1) / (time_length + 1)    &#47&#47 masked aggregation

        &#47&#47 pass through fully-connected part to lower dimension to 2 (binary classification)
        return self.feed_forward(x)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/mims-harvard/raindrop/commit/c78a0c22f831e2f0ee3f125343ad6e4a2894d680#diff-9130f919aee7ec822bf1d82f703744c78d2ed7fa68b5691eab535c724d94934aL137' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 97441989</div><div id='project'> Project Name: mims-harvard/raindrop</div><div id='commit'> Commit Name: c78a0c22f831e2f0ee3f125343ad6e4a2894d680</div><div id='time'> Time: 2021-08-19</div><div id='author'> Author: mz4730@student.uni-lj.si</div><div id='file'> File Name: code/baselines/Transformer_baseline.py</div><div id='m_class'> M Class Name: Transformer_P12</div><div id='n_method'> N Class Name: Transformer_P12</div><div id='m_method'> M Method Name: forward(4)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: code/baselines/Transformer_baseline.py</div><div id='n_file'> N File Name: code/baselines/Transformer_baseline.py</div><div id='m_start'> M Start Line: 147</div><div id='m_end'> M End Line: 163</div><div id='n_start'> N Start Line: 146</div><div id='n_end'> N End Line: 167</div><BR>