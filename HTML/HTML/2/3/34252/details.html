<html><h3>Pattern ID :34252
</h3><img src='98201946.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    self.logger_fl.info(log_dict_nc(self.out_dict))
                    self.writer.add_scalar(&quotTrain/loss&quot, loss, step)
                    self.writer.add_scalar(&quotTrain/ntp_acc&quot, ntp_acc, step)
                    <a id="change">self.writer.add_scalar(&quotTrain/learning_rate&quot</a>, lr, step<a id="change">)</a>
                else:
                    self.logger.info(log_dict(logs))
                    self.logger_fl.info(log_dict_nc(logs))
</code></pre><h3>After Change</h3><pre><code class='java'>
                        should_log = True
                    self._log(self.out_dict, mode=&quottrain&quot, to_console=should_log)
                elif &quoteval_loss&quot in logs:  &#47&#47 `Trainer.is_in_train` is not False so can&quott use
                    loss, ntp_acc = logs[&quoteval_loss&quot]<a id="change">, logs[&quoteval_ntp_acc&quot]</a>
                    &#47&#47 Log eval on an epoch-level
                    &#47&#47 Evaluation finished during training; TODO: didn&quott verify other positive cases
                    n_ep = state.epoch
                    assert n_ep.is_integer()</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 2</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/stefanheng/symbolic-music-generation/commit/6f4e76c4da49558977ce2dfd8bddca8531c2894f#diff-3ed359f79d5eaf00ee67f5c9b85cc9e57797840c915865dda929f227d15dc7aeL229' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 98201946</div><div id='project'> Project Name: stefanheng/symbolic-music-generation</div><div id='commit'> Commit Name: 6f4e76c4da49558977ce2dfd8bddca8531c2894f</div><div id='time'> Time: 2022-04-10</div><div id='author'> Author: 43276957+SpongeBobBang@users.noreply.github.com</div><div id='file'> File Name: musicnlp/util/train.py</div><div id='m_class'> M Class Name: ColoredPrinterCallbackForClm</div><div id='n_method'> N Class Name: ColoredPrinterCallbackForClm</div><div id='m_method'> M Method Name: on_log(5)</div><div id='n_method'> N Method Name: on_log(5)</div><div id='m_parent_class'> M Parent Class: ColoredPrinterCallback</div><div id='n_parent_class'> N Parent Class: ColoredPrinterCallback</div><div id='m_file'> M File Name: musicnlp/util/train.py</div><div id='n_file'> N File Name: musicnlp/util/train.py</div><div id='m_start'> M Start Line: 229</div><div id='m_end'> M End Line: 252</div><div id='n_start'> N Start Line: 277</div><div id='n_end'> N End Line: 306</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                    total_loss.backward()
                if self.rank == 0:
                    scalar_total_loss = total_loss.cpu().item() * self.t_config.gradient_accumulation_steps
                    <a id="change">self.tb_writer.add_scalar(&quotscalar/total_loss&quot</a>, scalar_total_loss, writer_step<a id="change">)</a>
                writer_step += 1
            if max_grad_norm &gt; 0:
                if self.t_config.fp16:
                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), max_grad_norm)</code></pre><h3>After Change</h3><pre><code class='java'>
                if batch_postprocessors is not None:
                    batch = batch_postprocessors[taskname](batch)
                batch_taskname = (batch, taskname)
                total_loss<a id="change">, losses_dict</a> = self.train_on_batch(batch_taskname, args)

                self.write_loss(total_loss,writer_step,losses_dict)
                writer_step += 1</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/airaria/textbrewer/commit/51c4701a52a04de61cbcd6b1e48d7f1b3f4159e1#diff-a0df517a14c387489a167be94325fce9e4eceac1f693d479ceaa7822801fdf1aL36' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 98201944</div><div id='project'> Project Name: airaria/textbrewer</div><div id='commit'> Commit Name: 51c4701a52a04de61cbcd6b1e48d7f1b3f4159e1</div><div id='time'> Time: 2020-11-08</div><div id='author'> Author: yangziqing@163.com</div><div id='file'> File Name: src/textbrewer/distiller_multitask.py</div><div id='m_class'> M Class Name: MultiTaskDistiller</div><div id='n_method'> N Class Name: MultiTaskDistiller</div><div id='m_method'> M Method Name: train(11)</div><div id='n_method'> N Method Name: train(11)</div><div id='m_parent_class'> M Parent Class: GeneralDistiller</div><div id='n_parent_class'> N Parent Class: GeneralDistiller</div><div id='m_file'> M File Name: src/textbrewer/distiller_multitask.py</div><div id='n_file'> N File Name: src/textbrewer/distiller_multitask.py</div><div id='m_start'> M Start Line: 81</div><div id='m_end'> M End Line: 103</div><div id='n_start'> N Start Line: 81</div><div id='n_end'> N End Line: 96</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            logger.debug(&quotEpoch {}&quot.format(epoch))
            lr = scheduler.get_lr()[0]
            logger.debug("Learning rate = {:.3e}".format(lr))
            <a id="change">writer.add_scalar(&quotlearning_rate&quot</a>, lr, n_iter<a id="change">)</a>

            n_iter = train(
                model=model,
                data=train_data,</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 Initialize scaler and scaler training labels by subtracting mean and dividing standard deviation (regression only)
    if args.dataset_type == &quotregression&quot:
        logger.debug(&quotFitting scaler&quot)
        train_smiles<a id="change">, train_labels</a> = zip(*train_data)
        scaler = StandardScaler().fit(train_labels)
        train_data = list(zip(train_smiles, scaler.transform(train_labels).tolist()))
    else:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/aamini/chemprop/commit/64f98d60d13bc4bd7131ea4453b03163503cce0c#diff-ed183d67207df065a11e1289f19d34cc2abbc5448dea952683cfe9728c342b95L26' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 98201941</div><div id='project'> Project Name: aamini/chemprop</div><div id='commit'> Commit Name: 64f98d60d13bc4bd7131ea4453b03163503cce0c</div><div id='time'> Time: 2018-10-02</div><div id='author'> Author: swansonk.14@gmail.com</div><div id='file'> File Name: train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: run_training(1)</div><div id='n_method'> N Method Name: run_training(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: train.py</div><div id='n_file'> N File Name: train.py</div><div id='m_start'> M Start Line: 59</div><div id='m_end'> M End Line: 117</div><div id='n_start'> N Start Line: 58</div><div id='n_end'> N End Line: 102</div><BR>