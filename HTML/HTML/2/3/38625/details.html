<html><h3>Pattern ID :38625
</h3><img src='110508640.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def __init__(self):
        super().__init__()

        self._moe_layer = <a id="change">tutel_moe.moe_layer(
            gate_type = {&quottype&quot: &quottop&quot, &quotk&quot: top_value, &quotfp32_gate&quot: args.fp32_gate},
            experts = {&quottype&quot: &quotffn&quot, &quotcount_per_node&quot: num_local_experts, &quothidden_size_per_expert&quot: hidden_size, &quotactivation_fn&quot: lambda x: F.relu(x)},
            model_dim = model_dim,
            scan_expert_func = lambda name, param: setattr(param, &quotskip_allreduce&quot, True),
            seeds = (1, dist_rank + 1, 1),
            a2a_ffn_overlap_degree = a2a_ffn_overlap_degree,
            use_model_parallel = args.use_model_parallel,
        ).to(</a>device<a id="change">)</a>

        &#47&#47 Summary of different parameter types: gate, local_experts
        local_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotlocal_experts&quot)])
        shared_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotgate&quot)])</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self):
        super().__init__()

        self._moe_layer = <a id="change">tutel_moe.moe_layer(
            gate_type = {&quottype&quot: &quottop&quot, &quotk&quot: top_value, &quotfp32_gate&quot: args.fp32_gate},
            experts = {&quottype&quot: &quotffn&quot, &quotcount_per_node&quot: num_local_experts, &quothidden_size_per_expert&quot: hidden_size, &quotactivation_fn&quot: lambda x: F.relu(x)},
            model_dim = model_dim,
            scan_expert_func = lambda name, param: setattr(param, &quotskip_allreduce&quot, True),
            seeds = (1, dist_rank + 1, 1),
            a2a_ffn_overlap_degree = a2a_ffn_overlap_degree,
            parallel_type = args.parallel_type,
        )</a>

        &#47&#47 Summary of different parameter types: gate, local_experts
        local_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotlocal_experts&quot)])
        shared_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotgate&quot)])</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 5</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/microsoft/tutel/commit/712bf2e41dd4a62abf1ba06fd94932e252577c2a#diff-8f9f7edfd20f6b67991ee3d2c6a60f2c6b98d6221917f631b985add500f2650fL68' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 110508640</div><div id='project'> Project Name: microsoft/tutel</div><div id='commit'> Commit Name: 712bf2e41dd4a62abf1ba06fd94932e252577c2a</div><div id='time'> Time: 2022-02-25</div><div id='author'> Author: ghostplant@qq.com</div><div id='file'> File Name: tutel/examples/helloworld.py</div><div id='m_class'> M Class Name: ExampleModel</div><div id='n_method'> N Class Name: ExampleModel</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: tutel/examples/helloworld.py</div><div id='n_file'> N File Name: tutel/examples/helloworld.py</div><div id='m_start'> M Start Line: 68</div><div id='m_end'> M End Line: 76</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 76</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        super().__init__()
        self._ddp_params_and_buffers_to_ignore = list()

        self._moe_layer = <a id="change">tutel_moe.moe_layer(
            gate_type = {&quottype&quot: &quottop&quot, &quotk&quot: top_value, &quotfp32_gate&quot: args.fp32_gate},
            experts = {&quottype&quot: &quotffn&quot, &quotcount_per_node&quot: num_local_experts, &quothidden_size_per_expert&quot: hidden_size, &quotactivation_fn&quot: lambda x: F.relu(x)},
            model_dim = model_dim,
            scan_expert_func = lambda name, param: setattr(param, &quotskip_allreduce&quot, True),
            seeds = (1, dist_rank + 1, 1),
            a2a_ffn_overlap_degree = a2a_ffn_overlap_degree,
        ).to(</a>device<a id="change">)</a>

        &#47&#47 Summary of different parameter types: gate, local_experts
        local_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotlocal_experts&quot)])
        shared_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotgate&quot)])</code></pre><h3>After Change</h3><pre><code class='java'>
        super().__init__()
        self._ddp_params_and_buffers_to_ignore = list()

        self._moe_layer = <a id="change">tutel_moe.moe_layer(
            gate_type = {&quottype&quot: &quottop&quot, &quotk&quot: top_value, &quotfp32_gate&quot: args.fp32_gate},
            experts = {&quottype&quot: &quotffn&quot, &quotcount_per_node&quot: num_local_experts, &quothidden_size_per_expert&quot: hidden_size, &quotactivation_fn&quot: lambda x: F.relu(x)},
            model_dim = model_dim,
            scan_expert_func = lambda name, param: setattr(param, &quotskip_allreduce&quot, True),
            seeds = (1, dist_rank + 1, 1),
            a2a_ffn_overlap_degree = a2a_ffn_overlap_degree,
        )</a>

        &#47&#47 Summary of different parameter types: gate, local_experts
        local_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotlocal_experts&quot)])
        shared_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotgate&quot)])</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/tutel/commit/712bf2e41dd4a62abf1ba06fd94932e252577c2a#diff-4361d2c4184c57ae688516020992d8f59ea253a5a5e1aaeba82bb17c54d2a38eL64' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 110508641</div><div id='project'> Project Name: microsoft/tutel</div><div id='commit'> Commit Name: 712bf2e41dd4a62abf1ba06fd94932e252577c2a</div><div id='time'> Time: 2022-02-25</div><div id='author'> Author: ghostplant@qq.com</div><div id='file'> File Name: tutel/examples/helloworld_ddp.py</div><div id='m_class'> M Class Name: ExampleModel</div><div id='n_method'> N Class Name: ExampleModel</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: tutel/examples/helloworld_ddp.py</div><div id='n_file'> N File Name: tutel/examples/helloworld_ddp.py</div><div id='m_start'> M Start Line: 68</div><div id='m_end'> M End Line: 75</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 75</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def __init__(self):
        super().__init__()

        self._moe_layer = <a id="change">tutel_moe.moe_layer(
            gate_type = {&quottype&quot: &quottop&quot, &quotk&quot: 1},
            experts = {&quottype&quot: &quotffn&quot, &quotcount_per_node&quot: -parallel_env.global_size, &quothidden_size_per_expert&quot: hidden_size * num_local_experts * parallel_env.global_size, &quotactivation_fn&quot: lambda x: F.relu(x)},
            model_dim = model_dim,
            scan_expert_func = lambda name, param: setattr(param, &quotskip_allreduce&quot, True),
            seeds = (1, dist_rank + 1, 1),
        ).to(</a>device<a id="change">)</a>

        &#47&#47 Summary of different parameter types: gate, local_experts
        local_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotlocal_experts&quot)])
        shared_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotgate&quot)])</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self):
        super().__init__()

        self._moe_layer = <a id="change">tutel_moe.moe_layer(
            gate_type = {&quottype&quot: &quottop&quot, &quotk&quot: 1},
            experts = {&quottype&quot: &quotffn&quot, &quotcount_per_node&quot: -parallel_env.global_size, &quothidden_size_per_expert&quot: hidden_size * num_local_experts * parallel_env.global_size, &quotactivation_fn&quot: lambda x: F.relu(x)},
            model_dim = model_dim,
            scan_expert_func = lambda name, param: setattr(param, &quotskip_allreduce&quot, True),
            seeds = (1, dist_rank + 1, 1),
        )</a>

        &#47&#47 Summary of different parameter types: gate, local_experts
        local_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotlocal_experts&quot)])
        shared_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotgate&quot)])</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/tutel/commit/712bf2e41dd4a62abf1ba06fd94932e252577c2a#diff-19c584b063e69914915d840e19382c6c06f094395ac58c48a06085de9feb5975L58' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 110508638</div><div id='project'> Project Name: microsoft/tutel</div><div id='commit'> Commit Name: 712bf2e41dd4a62abf1ba06fd94932e252577c2a</div><div id='time'> Time: 2022-02-25</div><div id='author'> Author: ghostplant@qq.com</div><div id='file'> File Name: tutel/examples/helloworld_megatron.py</div><div id='m_class'> M Class Name: ExampleModel</div><div id='n_method'> N Class Name: ExampleModel</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: tutel/examples/helloworld_megatron.py</div><div id='n_file'> N File Name: tutel/examples/helloworld_megatron.py</div><div id='m_start'> M Start Line: 61</div><div id='m_end'> M End Line: 67</div><div id='n_start'> N Start Line: 61</div><div id='n_end'> N End Line: 67</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def __init__(self):
        super().__init__()

        self._moe_layer = <a id="change">tutel_moe.moe_layer(
            gate_type = {&quottype&quot: &quottop&quot, &quotk&quot: 2},
            experts = {&quottype&quot: &quotffn&quot, &quotcount_per_node&quot: num_local_experts, &quothidden_size_per_expert&quot: hidden_size, &quotactivation_fn&quot: lambda x: F.relu(x)},
            model_dim = model_dim,
            scan_expert_func = lambda name, param: setattr(param, &quotskip_allreduce&quot, True),
            seeds = (1, dist_rank + 1, 1),
        ).to(</a>device<a id="change">)</a>

        &#47&#47 Summary of different parameter types: gate, local_experts
        local_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotlocal_experts&quot)])
        shared_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotgate&quot)])</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self):
        super().__init__()

        self._moe_layer = <a id="change">tutel_moe.moe_layer(
            gate_type = {&quottype&quot: &quottop&quot, &quotk&quot: 2},
            experts = {&quottype&quot: &quotffn&quot, &quotcount_per_node&quot: num_local_experts, &quothidden_size_per_expert&quot: hidden_size, &quotactivation_fn&quot: lambda x: F.relu(x)},
            model_dim = model_dim,
            scan_expert_func = lambda name, param: setattr(param, &quotskip_allreduce&quot, True),
            seeds = (1, dist_rank + 1, 1),
        )</a>

        &#47&#47 Summary of different parameter types: gate, local_experts
        local_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotlocal_experts&quot)])
        shared_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotgate&quot)])</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/tutel/commit/712bf2e41dd4a62abf1ba06fd94932e252577c2a#diff-54dd84032a50c8b68b746befdecbd83d7d55181b2df6429b95d56c288450bd36L60' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 110508639</div><div id='project'> Project Name: microsoft/tutel</div><div id='commit'> Commit Name: 712bf2e41dd4a62abf1ba06fd94932e252577c2a</div><div id='time'> Time: 2022-02-25</div><div id='author'> Author: ghostplant@qq.com</div><div id='file'> File Name: tutel/examples/helloworld_sharded_experts.py</div><div id='m_class'> M Class Name: ExampleModel</div><div id='n_method'> N Class Name: ExampleModel</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: tutel/examples/helloworld_sharded_experts.py</div><div id='n_file'> N File Name: tutel/examples/helloworld_sharded_experts.py</div><div id='m_start'> M Start Line: 63</div><div id='m_end'> M End Line: 69</div><div id='n_start'> N Start Line: 63</div><div id='n_end'> N End Line: 69</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def __init__(self):
        super().__init__()

        self._moe_layer = <a id="change">tutel_moe.moe_layer(
            gate_type = {&quottype&quot: &quottop&quot, &quotk&quot: top_value, &quotfp32_gate&quot: args.fp32_gate},
            experts = {&quottype&quot: &quotffn&quot, &quotcount_per_node&quot: num_local_experts, &quothidden_size_per_expert&quot: hidden_size, &quotactivation_fn&quot: lambda x: F.relu(x)},
            model_dim = model_dim,
            scan_expert_func = lambda name, param: setattr(param, &quotskip_allreduce&quot, True),
            seeds = (1, dist_rank + 1, 1),
            a2a_ffn_overlap_degree = a2a_ffn_overlap_degree,
        ).to(</a>device<a id="change">)</a>

        &#47&#47 Summary of different parameter types: gate, local_experts
        local_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotlocal_experts&quot)])
        shared_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotgate&quot)])</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self):
        super().__init__()

        self._moe_layer = <a id="change">tutel_moe.moe_layer(
            gate_type = {&quottype&quot: &quottop&quot, &quotk&quot: top_value, &quotfp32_gate&quot: args.fp32_gate},
            experts = {&quottype&quot: &quotffn&quot, &quotcount_per_node&quot: num_local_experts, &quothidden_size_per_expert&quot: hidden_size, &quotactivation_fn&quot: lambda x: F.relu(x)},
            model_dim = model_dim,
            scan_expert_func = lambda name, param: setattr(param, &quotskip_allreduce&quot, True),
            seeds = (1, dist_rank + 1, 1),
            a2a_ffn_overlap_degree = a2a_ffn_overlap_degree,
        )</a>

        &#47&#47 Summary of different parameter types: gate, local_experts
        local_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotlocal_experts&quot)])
        shared_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotgate&quot)])</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/tutel/commit/712bf2e41dd4a62abf1ba06fd94932e252577c2a#diff-c3ae3c5594ed4b53ab9786bbfddc4dc8511f97b6f01b06f927070ef16c448e80L64' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 110508637</div><div id='project'> Project Name: microsoft/tutel</div><div id='commit'> Commit Name: 712bf2e41dd4a62abf1ba06fd94932e252577c2a</div><div id='time'> Time: 2022-02-25</div><div id='author'> Author: ghostplant@qq.com</div><div id='file'> File Name: tutel/examples/helloworld_amp.py</div><div id='m_class'> M Class Name: ExampleModel</div><div id='n_method'> N Class Name: ExampleModel</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: tutel/examples/helloworld_amp.py</div><div id='n_file'> N File Name: tutel/examples/helloworld_amp.py</div><div id='m_start'> M Start Line: 67</div><div id='m_end'> M End Line: 74</div><div id='n_start'> N Start Line: 67</div><div id='n_end'> N End Line: 74</div><BR>