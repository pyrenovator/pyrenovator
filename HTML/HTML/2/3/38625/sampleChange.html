<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def __init__(self):
        super().__init__()

        self._moe_layer = <a id="change">tutel_moe.moe_layer(
            gate_type = {&quottype&quot: &quottop&quot, &quotk&quot: top_value, &quotfp32_gate&quot: args.fp32_gate},
            experts = {&quottype&quot: &quotffn&quot, &quotcount_per_node&quot: num_local_experts, &quothidden_size_per_expert&quot: hidden_size, &quotactivation_fn&quot: lambda x: F.relu(x)},
            model_dim = model_dim,
            scan_expert_func = lambda name, param: setattr(param, &quotskip_allreduce&quot, True),
            seeds = (1, dist_rank + 1, 1),
            a2a_ffn_overlap_degree = a2a_ffn_overlap_degree,
            use_model_parallel = args.use_model_parallel,
        ).to(</a>device<a id="change">)</a>

        &#47&#47 Summary of different parameter types: gate, local_experts
        local_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotlocal_experts&quot)])
        shared_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotgate&quot)])</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self):
        super().__init__()

        self._moe_layer = <a id="change">tutel_moe.moe_layer(
            gate_type = {&quottype&quot: &quottop&quot, &quotk&quot: top_value, &quotfp32_gate&quot: args.fp32_gate},
            experts = {&quottype&quot: &quotffn&quot, &quotcount_per_node&quot: num_local_experts, &quothidden_size_per_expert&quot: hidden_size, &quotactivation_fn&quot: lambda x: F.relu(x)},
            model_dim = model_dim,
            scan_expert_func = lambda name, param: setattr(param, &quotskip_allreduce&quot, True),
            seeds = (1, dist_rank + 1, 1),
            a2a_ffn_overlap_degree = a2a_ffn_overlap_degree,
            parallel_type = args.parallel_type,
        )</a>

        &#47&#47 Summary of different parameter types: gate, local_experts
        local_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotlocal_experts&quot)])
        shared_count = sum([torch.numel(param) for name, param in self._moe_layer.get_parameter_iterator(param_type=&quotgate&quot)])</code></pre>