<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            attn_mask.append(encodings_dict[&quotattention_mask&quot])
        max_length = max([len(idx) for idx in text_idx])
        text_idx = [idx + [self.padding_token_idx] * (max_length - len(idx)) for idx in text_idx]
        attn_mask = <a id="change">[mask + [0] * (max_length - len(mask)) for mask in attn_mask]</a>
        text_idx = torch.LongTensor(text_idx).to(self.device)
        attn_mask = torch.LongTensor(attn_mask).to(self.device)

        input_text = text_idx[:, :-1]</code></pre><h3>After Change</h3><pre><code class='java'>
        attn_masks = []
        for text in text_sequence:
            sentence = &quot &quot.join([self.sos_token] + text + [self.eos_token])
            encoding_dict = <a id="change">self.tokenizer(</a>sentence<a id="change">,
                                           max_length=self.max_seq_length,
                                           padding="max_length",
                                           truncation=True,
                                           return_tensors="pt")</a>
            input_ids.append(encoding_dict[&quotinput_ids&quot])
            attn_masks.append(encoding_dict[&quotattention_mask&quot])
        input_ids = torch.cat(input_ids, dim=0).to(self.device)
        attn_masks = torch.cat(attn_masks, dim=0).to(self.device)</code></pre>