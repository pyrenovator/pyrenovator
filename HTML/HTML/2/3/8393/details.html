<html><h3>Pattern ID :8393
</h3><img src='29279823.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        k_img, v_img = map(lambda t: F.unfold(t, kernel_size, padding = padding, dilation = dilation), (k_img, v_img))
        k_img, v_img = map(lambda t: rearrange(t, &quotb (d j) i -&gt; b i j d&quot, j = kernel_size ** 2), (k_img, v_img))

        k_text, v_text = <a id="change">map(</a>lambda t: repeat(t, &quotb j d -&gt; b i j d&quot, i = img_seq_len), (k_text, v_text)<a id="change">)</a>

        &#47&#47 let image attend to all of text

        k_img = torch.cat((k_text, k_img), dim = 2)</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 aggregate

        attn_image_to_text, attn_image = <a id="change">attn</a>[..., <a id="change">:</a>text_len], attn[..., text_len:]

        out_image_to_image = einsum(&quotb i j, b i j d -&gt; b i d&quot, attn_image, v_img)
        out_image_to_text = einsum(&quotb i j, b j d -&gt; b i d&quot, attn_image_to_text, v_text)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/dalle-pytorch/commit/f14a313431e9072bef9a8219ea3d99d7683ada06#diff-58807b9968341c05944b3adf3de1bfcd9d33660121a78f3ef013fd0e9bf16fdfL94' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 29279823</div><div id='project'> Project Name: lucidrains/dalle-pytorch</div><div id='commit'> Commit Name: f14a313431e9072bef9a8219ea3d99d7683ada06</div><div id='time'> Time: 2021-04-22</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: dalle_pytorch/attention.py</div><div id='m_class'> M Class Name: SparseConvCausalAttention</div><div id='n_method'> N Class Name: SparseConvCausalAttention</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dalle_pytorch/attention.py</div><div id='n_file'> N File Name: dalle_pytorch/attention.py</div><div id='m_start'> M Start Line: 116</div><div id='m_end'> M End Line: 168</div><div id='n_start'> N Start Line: 94</div><div id='n_end'> N End Line: 173</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    
    &#47&#47 Split up batched outputs, then post-process each example.
    unbatched_outputs = utils.unbatch_preds(detached_outputs)
    return <a id="change">map(</a>self._postprocess, unbatched_outputs<a id="change">)</a>

  def input_spec(self):
    return {
        "context": lit_types.TextSegment(),</code></pre><h3>After Change</h3><pre><code class='java'>

        answer_end_index = outputs.end_logits.argmax()

        predict_answer_tokens = <a id="change">inputs</a>.input_ids[0, <a id="change">answer_start_index</a> : answer_end_index + 1]

        prediction.append(self.tokenizer.decode(predict_answer_tokens))
    print(&quotPredictions for the data is----&gt;/n&quot)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pair-code/lit/commit/c2fd433b7d7ee04e60dd117c3decc8e22ac42287#diff-e66e2ed65042046b3905d9bc4b44fa2c3bb19cb8c07da5a9540002725714fab0L181' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 29279826</div><div id='project'> Project Name: pair-code/lit</div><div id='commit'> Commit Name: c2fd433b7d7ee04e60dd117c3decc8e22ac42287</div><div id='time'> Time: 2022-06-15</div><div id='author'> Author: 31214277+aryan1107@users.noreply.github.com</div><div id='file'> File Name: lit_nlp/examples/models/tydi.py</div><div id='m_class'> M Class Name: TydiModel</div><div id='n_method'> N Class Name: TydiModel</div><div id='m_method'> M Method Name: predict_minibatch(2)</div><div id='n_method'> N Method Name: predict_minibatch(2)</div><div id='m_parent_class'> M Parent Class: lit_model.Model</div><div id='n_parent_class'> N Parent Class: lit_model.Model</div><div id='m_file'> M File Name: lit_nlp/examples/models/tydi.py</div><div id='n_file'> N File Name: lit_nlp/examples/models/tydi.py</div><div id='m_start'> M Start Line: 186</div><div id='m_end'> M End Line: 203</div><div id='n_start'> N Start Line: 134</div><div id='n_end'> N End Line: 156</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 prepare text key / values for the image tokens to attend to

        k_text, v_text = <a id="change">map(</a>lambda t: repeat(t, &quotb n d -&gt; (b ax) n d&quot, ax = img_size), (k_text, v_text)<a id="change">)</a>
        k_img = torch.cat((k_text, k_img), dim = 1)
        v_img = torch.cat((v_text, v_img), dim = 1)

        &#47&#47 similarity</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 aggregate

        attn_image_to_text, attn_image_to_image = <a id="change">attn</a>[..., <a id="change">:</a>text_len], attn[..., text_len:]

        out_image_to_image = einsum(&quotb x i j, b x j d -&gt; b x i d&quot, attn_image_to_image, v_img)
        out_image_to_text = einsum(&quotb x i j, b j d -&gt; b x i d&quot, attn_image_to_text, v_text)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/dalle-pytorch/commit/100a110d6fa98b8559a780ae71d9ef1f1c7789c7#diff-58807b9968341c05944b3adf3de1bfcd9d33660121a78f3ef013fd0e9bf16fdfL204' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 29279827</div><div id='project'> Project Name: lucidrains/dalle-pytorch</div><div id='commit'> Commit Name: 100a110d6fa98b8559a780ae71d9ef1f1c7789c7</div><div id='time'> Time: 2021-04-22</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: dalle_pytorch/attention.py</div><div id='m_class'> M Class Name: SparseAxialCausalAttention</div><div id='n_method'> N Class Name: SparseAxialCausalAttention</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dalle_pytorch/attention.py</div><div id='n_file'> N File Name: dalle_pytorch/attention.py</div><div id='m_start'> M Start Line: 230</div><div id='m_end'> M End Line: 279</div><div id='n_start'> N Start Line: 208</div><div id='n_end'> N End Line: 281</div><BR>