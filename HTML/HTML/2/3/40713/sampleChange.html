<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        
        &#47&#47&#47&#47--------------------------------------------------------------------.
        &#47&#47&#47&#47&#47&#47 Load static tensor into GPU (and expand over the time dimension) 
        <a id="change">if </a>da_static is not None:
            &#47&#47 - Apply scaler 
            if self.scaler is not None:
                da_static = self.scaler.transform(da_static, variable_dim=&quotfeature&quot).compute()
            &#47&#47 - Expand by only creating a new view on the existing tensor (not allocating new memory) 
            dim_time<a id="change"> = </a>0   &#47&#47 static has: [node, features]
            new_dim_size = [-1 for i in range(len(da_static.shape) + 1)]
            new_dim_size[dim_time] = len(input_k)
            self.torch_static = torch.tensor(da_static.values, dtype=torch_dtype, device=device).unsqueeze(dim_time).expand(new_dim_size).unsqueeze(0) </code></pre><h3>After Change</h3><pre><code class='java'>
                da_static = da_static.transpose(*required_static_order)    
            &#47&#47&#47&#47----------------------------------------------------------------.
            &#47&#47 If da_static still lazy, load data 
            da_static = <a id="change">da_static.compute()</a>
            &#47&#47&#47&#47----------------------------------------------------------------.
            &#47&#47&#47&#47 Add batch and time dimension and then expand along time dimension 
            &#47&#47 - Define ways to unsqueeze the static tensor 
            unsqueeze_time_dim = dim_info[&quottime&quot] - 1 &#47&#47 (without batch dim ...)</code></pre>