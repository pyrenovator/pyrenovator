<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                pbar.set_postfix(ShellLoss = &quot{:.3f}&quot.format(val_loss.item()), 
                                 KernelLoss = &quot{:.3f}&quot.format(loss.item()))
                if val_loss.item() &gt; 1 or loss.item() &gt; 1 or val_loss.item() &lt; 0 or loss.item() &lt; 0:
                    <a id="change">pdb.set_trace()</a>
            
        return
    
    </code></pre><h3>After Change</h3><pre><code class='java'>
        val_epoch = self.val_generator.epoch()
        n_steps = self.train_generator.steps_per_epoch
        sum_loss = 0
        sum_val_loss<a id="change"> = </a>0
        with tqdm(train_epoch, total = n_steps,
                  desc = &quotSearching | Epoch {} | Training&quot.format(self.epoch)) as pbar:
            for step, (x, y_truth) in enumerate(pbar):
                x = torch.as_tensor(x, device=self.device, dtype=torch.float)
                y_truth = torch.as_tensor(y_truth, device=self.device, dtype=torch.float)
                try:
                    val_x, val_y_truth = next(val_epoch)
                except StopIteration:
                    val_epoch = self.val_generator.epoch()
                    val_x, val_y_truth = next(val_epoch)
                val_x = torch.as_tensor(val_x, device=self.device, dtype=torch.float)
                val_y_truth = torch.as_tensor(val_y_truth, device=self.device, dtype=torch.float)

                &#47&#47 optim_shell
                self.optim_shell.zero_grad()
                val_y_pred = self.model(val_x)
                val_loss = self.loss(val_y_pred, val_y_truth)
                sum_val_loss += val_loss.item()
                val_loss.backward()
                self.optim_shell.step()
                
                &#47&#47 optim_kernel
                self.optim_kernel.zero_grad()
                y_pred = self.model(x)
                loss = self.loss(y_pred, y_truth)
                sum_loss += loss.item()
                loss.backward()
                nn.utils.clip_grad_norm_(self.model.kernel.parameters(),
                                         self.config[&quotsearch&quot][&quotgrad_clip&quot])
                self.optim_kernel.step()
                
                &#47&#47 postfix for progress bar
                postfix = OrderedDict()
                postfix[&quotLoss(optim_shell)&quot] = round(sum_val_loss/(step+1), 3)
                postfix[&quotLoss(optim_kernel)&quot] = round(sum_loss/(step+1), 3)
                pbar.set_postfix(postfix)
                
        <a id="change">return </a>round(sum_val_loss/n_steps, 3), round(sum_loss/n_steps, 3)
    
    
    def validate(self):</code></pre>