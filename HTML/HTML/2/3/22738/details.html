<html><h3>Pattern ID :22738
</h3><img src='72197576.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                if log_interval is not None and self._episode_num % log_interval == 0:
                    self._dump_logs()

        mean_reward = <a id="change">np.mean(</a>episode_rewards<a id="change">)</a> if <a id="change">num_collected_episodes &gt; 0</a> else 0.0

        callback.on_rollout_end()
</code></pre><h3>After Change</h3><pre><code class='java'>
                    self._episode_num += 1

                    if action_noise is not None:
                        kwargs = dict(indices=[idx])<a id="change"> if </a>env.num_envs &gt; 1<a id="change"> else </a>{}
                        action_noise.reset(**kwargs)

                    &#47&#47 Log training infos</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/dlr-rm/stable-baselines3/commit/507ed1762e62bd6c4e85ea572ba166b69116b1ac#diff-7809dfd549054549abed96b27661447862894a94e6873c67f8f96f4b6842debdL544' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 72197576</div><div id='project'> Project Name: dlr-rm/stable-baselines3</div><div id='commit'> Commit Name: 507ed1762e62bd6c4e85ea572ba166b69116b1ac</div><div id='time'> Time: 2021-12-01</div><div id='author'> Author: antonin.raffin@ensta.org</div><div id='file'> File Name: stable_baselines3/common/off_policy_algorithm.py</div><div id='m_class'> M Class Name: OffPolicyAlgorithm</div><div id='n_method'> N Class Name: OffPolicyAlgorithm</div><div id='m_method'> M Method Name: collect_rollouts(8)</div><div id='n_method'> N Method Name: collect_rollouts(8)</div><div id='m_parent_class'> M Parent Class: BaseAlgorithm</div><div id='n_parent_class'> N Parent Class: BaseAlgorithm</div><div id='m_file'> M File Name: stable_baselines3/common/off_policy_algorithm.py</div><div id='n_file'> N File Name: stable_baselines3/common/off_policy_algorithm.py</div><div id='m_start'> M Start Line: 544</div><div id='m_end'> M End Line: 619</div><div id='n_start'> N Start Line: 565</div><div id='n_end'> N End Line: 628</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        x = self.temporal_transformer(x)

        x = <a id="change">x.mean(dim = 1)</a> if <a id="change">self.pool == &quotmean&quot</a> else x[:, 0]

        x = self.to_latent(x)
        return self.mlp_head(x)</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 excise out the spatial cls tokens or average pool for temporal attention

        x = x[:, :, 0]<a id="change"> if </a>not self.global_average_pool<a id="change"> else </a>reduce(x, &quotb f n d -&gt; b f d&quot, &quotmean&quot)

        &#47&#47 append temporal CLS tokens
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/vit-pytorch/commit/6ec8fdaa6dc114b3789b80635a64b126756c02c8#diff-5a6a5461c9325f66f90d85a868b229d6aa9c98ac38f278a4b3ae4f117e238ef8L135' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 72197575</div><div id='project'> Project Name: lucidrains/vit-pytorch</div><div id='commit'> Commit Name: 6ec8fdaa6dc114b3789b80635a64b126756c02c8</div><div id='time'> Time: 2022-10-24</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: vit_pytorch/vivit.py</div><div id='m_class'> M Class Name: ViT</div><div id='n_method'> N Class Name: ViT</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: vit_pytorch/vivit.py</div><div id='n_file'> N File Name: vit_pytorch/vivit.py</div><div id='m_start'> M Start Line: 139</div><div id='m_end'> M End Line: 166</div><div id='n_start'> N Start Line: 144</div><div id='n_end'> N End Line: 180</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        assert phase in [&quotGmain&quot, &quotGreg&quot, &quotGboth&quot, &quotDmain&quot, &quotDreg&quot, &quotDboth&quot]
        do_Gmain = (phase in [&quotGmain&quot, &quotGboth&quot])
        do_Dmain = (phase in [&quotDmain&quot, &quotDboth&quot])
        do_Gpl   = <a id="change">(phase in [&quotGreg&quot, &quotGboth&quot])</a> and (self.pl_weight != 0)
        do_Dr1   = (phase in [&quotDreg&quot, &quotDboth&quot]) and (self.r1_gamma != 0)

        loss_numpy = {}

        &#47&#47 Gmain: Maximize logits for generated images.
        if do_Gmain:
            gen_img, _gen_ws = self.run_G(gen_z, gen_c, sync=(sync and not do_Gpl)) &#47&#47 May get synced by Gpl.
            &#47&#47 d_gen_ws_dgen_z = torch.autograd.grad(outputs=[_gen_ws.sum()], inputs=[gen_z], create_graph=True, only_inputs=True)[0]
            &#47&#47 aaaaaaaaaa0 = dic2[phase + &quotd_gen_ws_dgen_z&quot]
            &#47&#47 aaaaaaaaaa1 = d_gen_ws_dgen_z.cpu().detach().numpy()
            &#47&#47 ddd = np.mean((dic2[phase + &quotd_gen_ws_dgen_z&quot] - d_gen_ws_dgen_z.cpu().detach().numpy()) ** 2)
            &#47&#47 print(&quotddd=%.6f&quot % ddd)
            &#47&#47 aaaaaaaaa1 = dic2[phase + &quotgen_img&quot]
            &#47&#47 aaaaaaaaa2 = gen_img.cpu().detach().numpy()
            &#47&#47 ddd = np.mean((dic2[phase + &quotgen_img&quot] - gen_img.cpu().detach().numpy()) ** 2)
            &#47&#47 print(&quotddd=%.6f&quot % ddd)
            &#47&#47 ddd = np.mean((dic2[phase + &quot_gen_ws&quot] - _gen_ws.cpu().detach().numpy()) ** 2)
            &#47&#47 print(&quotddd=%.6f&quot % ddd)

            gen_logits = self.run_D(gen_img, gen_c, sync=False)
            &#47&#47 ddd = np.mean((dic2[phase + &quotgen_logits&quot] - gen_logits.cpu().detach().numpy()) ** 2)
            &#47&#47 print(&quotddd=%.6f&quot % ddd)

            loss_Gmain = torch.nn.functional.softplus(-gen_logits)  &#47&#47 -log(sigmoid(gen_logits))
            loss_Gmain = loss_Gmain.mean()
            loss_numpy[&quotloss_Gmain&quot] = loss_Gmain.cpu().detach().numpy()

            loss_G = loss_Gmain
            loss_G = loss_G * float(gain)
            loss_G.backward()  &#47&#47 咩酱：gain即上文提到的这个阶段的训练间隔。

        &#47&#47 Gpl: Apply path length regularization.
        if do_Gpl:
            &#47&#47 print(&quot----------------- do_Gpl -----------------&quot)
            batch_size = gen_z.shape[0] // self.pl_batch_shrink
            batch_size = max(batch_size, 1)
            &#47&#47 with misc.ddp_sync(self.G_flownet, sync):
            &#47&#47     flow = self.G_flownet(torch.cat((cloth[:batch_size], aff_pose[:batch_size]), dim=1))
            &#47&#47 warp_cloth = F.grid_sample(cloth[:batch_size, :3, :, :], flow)

            gen_c_ = None
            if gen_c is not None:
                gen_c_ = gen_c[:batch_size]

            gen_img, gen_ws = self.run_G(gen_z[:batch_size], gen_c_, sync=sync)
            &#47&#47 ddd = np.mean((dic2[phase + &quotgen_img&quot] - gen_img.cpu().detach().numpy()) ** 2)
            &#47&#47 print(&quotddd=%.6f&quot % ddd)
            &#47&#47 ddd = np.mean((dic2[phase + &quotgen_ws&quot] - gen_ws.cpu().detach().numpy()) ** 2)
            &#47&#47 print(&quotddd=%.6f&quot % ddd)
            pl_noise = torch.randn_like(gen_img) / np.sqrt(gen_img.shape[2] * gen_img.shape[3])
            &#47&#47 pl_noise = torch.ones_like(gen_img) / np.sqrt(gen_img.shape[2] * gen_img.shape[3])
            pl_grads = torch.autograd.grad(outputs=[(gen_img * pl_noise).sum()], inputs=[gen_ws], create_graph=True, only_inputs=True)[0]

            pl_lengths = pl_grads.square().sum(2).mean(1).sqrt()
            &#47&#47 ddd = np.mean((dic2[phase + &quotpl_grads&quot] - pl_grads.cpu().detach().numpy()) ** 2)
            &#47&#47 print(&quotddd=%.6f&quot % ddd)
            &#47&#47 ddd = np.mean((dic2[phase + &quotpl_lengths&quot] - pl_lengths.cpu().detach().numpy()) ** 2)
            &#47&#47 print(&quotddd=%.6f&quot % ddd)
            if self.pl_mean is None:
                self.pl_mean = torch.zeros([1, ], dtype=torch.float32, device=pl_lengths.device)
            pl_mean = self.pl_mean.lerp(pl_lengths.mean(), self.pl_decay)
            self.pl_mean.copy_(pl_mean.detach())

            pl_penalty = (pl_lengths - pl_mean).square()
            loss_Gpl = pl_penalty * self.pl_weight

            loss_Gpl = <a id="change">(gen_img[:, 0, 0, 0] * 0 + loss_Gpl).mean()</a> * float(gain)
            loss_numpy[&quotloss_Gpl&quot] = loss_Gpl.cpu().detach().numpy()
            loss_Gpl.backward()  &#47&#47 咩酱：gain即上文提到的这个阶段的训练间隔。
</code></pre><h3>After Change</h3><pre><code class='java'>
            phase = {&quotGreg&quot: &quotnone&quot, &quotGboth&quot: &quotGmain&quot}.get(phase, phase)
        if self.r1_gamma == 0:
            phase = {&quotDreg&quot: &quotnone&quot, &quotDboth&quot: &quotDmain&quot}.get(phase, phase)
        blur_sigma = max(1 - cur_nimg / (self.blur_fade_kimg * 1e3), 0) * self.blur_init_sigma<a id="change"> if </a>self.blur_fade_kimg &gt; 0<a id="change"> else </a>0

        loss_numpy = {}
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/miemie2013/miemiegan/commit/c5e7ecffe23cf2fc2613ee06ec8fd94c8d7230ac#diff-4a5f5fcbb93c9b5e973214dc967c292a612eba7ff7a63f6841cac2829b7b8afdL138' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 72197574</div><div id='project'> Project Name: miemie2013/miemiegan</div><div id='commit'> Commit Name: c5e7ecffe23cf2fc2613ee06ec8fd94c8d7230ac</div><div id='time'> Time: 2022-03-24</div><div id='author'> Author: 53960695+miemie2013@users.noreply.github.com</div><div id='file'> File Name: mmgan/models/architectures/styleganv3_model.py</div><div id='m_class'> M Class Name: StyleGANv3Model</div><div id='n_method'> N Class Name: StyleGANv3Model</div><div id='m_method'> M Method Name: accumulate_gradients(9)</div><div id='n_method'> N Method Name: accumulate_gradients(9)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: mmgan/models/architectures/styleganv3_model.py</div><div id='n_file'> N File Name: mmgan/models/architectures/styleganv3_model.py</div><div id='m_start'> M Start Line: 140</div><div id='m_end'> M End Line: 251</div><div id='n_start'> N Start Line: 140</div><div id='n_end'> N End Line: 251</div><BR>