<html><h3>Pattern ID :24584
</h3><img src='76302952.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        params: List[Dict[str, Any]] = []
        memo: Set[torch.nn.parameter.Parameter] = set()
        for module in model.modules():
            for key, value in <a id="change">module.named_parameters(recurse=False)</a>:
                if not value.requires_grad:
                    continue
                &#47&#47 Avoid duplicating parameters
                if value in memo:
                    continue
                memo.add(value)
                lr = cfg.SOLVER.OPTIMIZER.BASE_LR
                weight_decay = cfg.SOLVER.OPTIMIZER.WEIGHT_DECAY
                if isinstance(module, NORM_MODULE_TYPES):
                    weight_decay = cfg.SOLVER.OPTIMIZER.WEIGHT_DECAY_NORM
                elif <a id="change"></a>key == "bias":
                    &#47&#47 NOTE: unlike Detectron v1, we now default BIAS_LR_FACTOR to 1.0
                    &#47&#47 and WEIGHT_DECAY_BIAS to WEIGHT_DECAY so that bias optimizer
                    &#47&#47 hyperparameters are by default exactly the same as for regular</code></pre><h3>After Change</h3><pre><code class='java'>
    @staticmethod
    def build(model, cfg):
        optimizer = optim.SGD(
            <a id="change">model.parameters()</a>,
            lr=cfg.SOLVER.OPTIMIZER.BASE_LR,
            weight_decay=cfg.SOLVER.OPTIMIZER.WEIGHT_DECAY,
            momentum=cfg.SOLVER.OPTIMIZER.MOMENTUM,</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 3</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/megvii-basedetection/cvpods/commit/df4be33a7792139213b4010095bb07589aec2096#diff-bd436aa5966770221692c914643fe6e7bce1b0c612bc977148f64e53094025bcL42' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 76302952</div><div id='project'> Project Name: megvii-basedetection/cvpods</div><div id='commit'> Commit Name: df4be33a7792139213b4010095bb07589aec2096</div><div id='time'> Time: 2021-02-01</div><div id='author'> Author: zhubenjin@megvii.com</div><div id='file'> File Name: cvpods/solver/optimizer_builder.py</div><div id='m_class'> M Class Name: SGDBuilder</div><div id='n_method'> N Class Name: SGDBuilder</div><div id='m_method'> M Method Name: build(2)</div><div id='n_method'> N Method Name: build(2)</div><div id='m_parent_class'> M Parent Class: OptimizerBuilder</div><div id='n_parent_class'> N Parent Class: OptimizerBuilder</div><div id='m_file'> M File Name: cvpods/solver/optimizer_builder.py</div><div id='n_file'> N File Name: cvpods/solver/optimizer_builder.py</div><div id='m_start'> M Start Line: 42</div><div id='m_end'> M End Line: 66</div><div id='n_start'> N Start Line: 43</div><div id='n_end'> N End Line: 43</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                              collate_fn=collate_and_pad,
                              persistent_workers=True)
    step_counter = 0
    optimizer = torch.optim.AdamW([p for name, p in <a id="change">net.named_parameters()</a> if <a id="change">&quotpost_flow&quot not in name</a>], lr=lr,
                                  betas=(0.9, 0.98), eps=1e-9)
    optimizer_postflow = torch.optim.RAdam(net.post_flow.parameters(), lr=lr)
    scheduler = WarmupScheduler(optimizer, peak_lr=lr, warmup_steps=warmup_steps,</code></pre><h3>After Change</h3><pre><code class='java'>
                              collate_fn=collate_and_pad,
                              persistent_workers=True)
    step_counter = 0
    optimizer = torch.optim.AdamW(<a id="change">net.parameters()</a>, lr=lr, betas=(0.9, 0.98), eps=1e-9)
    scheduler = WarmupScheduler(optimizer, peak_lr=lr, warmup_steps=warmup_steps,
                                max_steps=phase_1_steps + phase_2_steps)
    grad_scaler = GradScaler()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/01470ffe73349f191328d8491a53aef1296e8675#diff-241ea0a4da614fa4b0dbd748ea10aae1c34265d57f46001c62ec2994245ee0f7L34' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 76302955</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 01470ffe73349f191328d8491a53aef1296e8675</div><div id='time'> Time: 2023-02-06</div><div id='author'> Author: lux.florian@gmail.com</div><div id='file'> File Name: TrainingInterfaces/Text_to_Spectrogram/PortaSpeech/portaspeech_train_loop.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train_loop(15)</div><div id='n_method'> N Method Name: train_loop(16)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: TrainingInterfaces/Text_to_Spectrogram/PortaSpeech/portaspeech_train_loop.py</div><div id='n_file'> N File Name: TrainingInterfaces/Text_to_Spectrogram/PortaSpeech/portaspeech_train_loop.py</div><div id='m_start'> M Start Line: 48</div><div id='m_end'> M End Line: 225</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 208</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        with torch.no_grad():
            for name, module in model.named_modules():
                if isinstance(module, nn.Conv2d):
                    for n, p in <a id="change">module.named_parameters()</a>:
                        <a id="change">if </a>n == "weight_orig":
                            p.grad.mul_(module.weight_mask)
                        if n == "bias":
                            p.grad.mul_(module.weight_mask[:, 0, 0, 0])</code></pre><h3>After Change</h3><pre><code class='java'>
                            p.grad.mul_(module.weight_mask[:, 0, 0, 0])
        
        &#47&#47 what is the chance of an unpruned weight to be exactly 0?
        for param in <a id="change">model.parameters()</a>:
            param.grad.data.mul_(torch.abs(param.data) &gt; 0)
        
        scaler.step(optimizer)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/eidoslab/simplify/commit/e3cef5125238684e5bce46c85ed1ed762abd377e#diff-41474220006d3f313aca51ff34722bbf4cb675023f4106ffd78a318ae920faccL40' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 76302948</div><div id='project'> Project Name: eidoslab/simplify</div><div id='commit'> Commit Name: e3cef5125238684e5bce46c85ed1ed762abd377e</div><div id='time'> Time: 2021-07-01</div><div id='author'> Author: carlo.alberto.barbano@outlook.com</div><div id='file'> File Name: training/train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: training/train.py</div><div id='n_file'> N File Name: training/train.py</div><div id='m_start'> M Start Line: 49</div><div id='m_end'> M End Line: 129</div><div id='n_start'> N Start Line: 49</div><div id='n_end'> N End Line: 147</div><BR>