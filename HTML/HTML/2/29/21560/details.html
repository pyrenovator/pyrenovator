<html><h3>Pattern ID :21560
</h3><img src='68848746.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            print("=&gt; no checkpoint found at &quot{}&quot".format(args.resume))

    if args.distributed:
        model<a id="change">, optimizer</a> = bagua.bagua_init(
            model,
            optimizer,
            distributed_algorithm=args.algorithm,</code></pre><h3>After Change</h3><pre><code class='java'>
        weight_decay=args.weight_decay,
    )

    <a id="change">if args.algorithm == "gradient_allreduce"</a>:
        from bagua.torch_api.algorithms import gradient_allreduce
        algorithm<a id="change"> = </a><a id="change">gradient_allreduce.GradientAllReduceAlgorithm()</a>
    elif <a id="change">args.algorithm == "decentralized"</a>:
        from bagua.torch_api.algorithms import decentralized
        algorithm<a id="change"> = </a><a id="change">decentralized.DecentralizedAlgorithm()</a>
    elif <a id="change">args.algorithm == "bytegrad"</a>:
        from bagua.torch_api.algorithms import bytegrad
        algorithm<a id="change"> = </a><a id="change">bytegrad.ByteGradAlgorithm()</a>
    elif <a id="change">args.algorithm == "onebit_adam"</a>:
        from bagua.torch_api.algorithms import onebit_adam
        <a id="change">optimizer = onebit_adam</a><a id="change">.OnebitAdamOptimizer(model.parameters()</a><a id="change">)</a>
        algorithm<a id="change"> = onebit_adam</a><a id="change">.OnebitAdamAlgorithm(optimizer</a>, <a id="change">10</a><a id="change">)</a>
    else:
        <a id="change">raise </a>NotImplementedError

    scaler = torch.cuda.amp.GradScaler(enabled=args.amp)

    &#47&#47 optionally resume from a checkpoint
    if args.resume:
        if os.path.isfile(args.resume):
            print("=&gt; loading checkpoint &quot{}&quot".format(args.resume))
            &#47&#47 Map model to be loaded to specified single gpu.
            loc = "cuda:{}".format(bagua.get_local_rank())
            checkpoint = torch.load(args.resume, map_location=loc)
            args.start_epoch = checkpoint["epoch"]
            best_acc1 = checkpoint["best_acc1"]
            if bagua.get_local_rank() is not None:
                &#47&#47 best_acc1 may be from a checkpoint from a different GPU
                best_acc1 = best_acc1.to(bagua.get_local_rank())
            model.load_state_dict(checkpoint["state_dict"])
            optimizer.load_state_dict(checkpoint["optimizer"])
            print(
                "=&gt; loaded checkpoint &quot{}&quot (epoch {})".format(
                    args.resume, checkpoint["epoch"]
                )
            )
        else:
            print("=&gt; no checkpoint found at &quot{}&quot".format(args.resume))

    if args.distributed:
        model = model.with_bagua(
            <a id="change">[optimizer</a>],
            algorithm,
        )
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 22</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/baguasys/bagua/commit/3e13646866d0fccae01e35d242919ad6be6a765c#diff-d0e6cb752d12a4cd0a93d1057594977d5aa00a489959775c71047b5d2fa693d2L198' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 68848746</div><div id='project'> Project Name: baguasys/bagua</div><div id='commit'> Commit Name: 3e13646866d0fccae01e35d242919ad6be6a765c</div><div id='time'> Time: 2021-09-15</div><div id='author'> Author: admin@mail.xrlian.com</div><div id='file'> File Name: examples/imagenet/main.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main_worker(1)</div><div id='n_method'> N Method Name: main_worker(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/imagenet/main.py</div><div id='n_file'> N File Name: examples/imagenet/main.py</div><div id='m_start'> M Start Line: 198</div><div id='m_end'> M End Line: 237</div><div id='n_start'> N Start Line: 198</div><div id='n_end'> N End Line: 253</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            print("=&gt; no checkpoint found at &quot{}&quot".format(args.resume))

    if args.distributed:
        model<a id="change">, optimizer</a> = bagua.bagua_init(
            model,
            optimizer,
            distributed_algorithm=args.algorithm,</code></pre><h3>After Change</h3><pre><code class='java'>
        weight_decay=args.weight_decay,
    )

    <a id="change">if args.algorithm == "gradient_allreduce"</a>:
        from bagua.torch_api.algorithms import gradient_allreduce
        algorithm<a id="change"> = </a><a id="change">gradient_allreduce.GradientAllReduceAlgorithm()</a>
    elif <a id="change">args.algorithm == "decentralized"</a>:
        from bagua.torch_api.algorithms import decentralized
        algorithm<a id="change"> = </a><a id="change">decentralized.DecentralizedAlgorithm()</a>
    elif <a id="change">args.algorithm == "bytegrad"</a>:
        from bagua.torch_api.algorithms import bytegrad
        algorithm<a id="change"> = </a><a id="change">bytegrad.ByteGradAlgorithm()</a>
    elif <a id="change">args.algorithm == "onebit_adam"</a>:
        from bagua.torch_api.algorithms import onebit_adam
        <a id="change">optimizer = </a><a id="change">onebit_adam.OnebitAdamOptimizer(model.parameters()</a><a id="change">)</a>
        algorithm<a id="change"> = </a><a id="change">onebit_adam.OnebitAdamAlgorithm(</a>optimizer, <a id="change">10</a><a id="change">)</a>
    else:
        <a id="change">raise </a>NotImplementedError

    scaler = torch.cuda.amp.GradScaler(enabled=args.amp)

    &#47&#47 optionally resume from a checkpoint
    if args.resume:
        if os.path.isfile(args.resume):
            print("=&gt; loading checkpoint &quot{}&quot".format(args.resume))
            &#47&#47 Map model to be loaded to specified single gpu.
            loc = "cuda:{}".format(bagua.get_local_rank())
            checkpoint = torch.load(args.resume, map_location=loc)
            args.start_epoch = checkpoint["epoch"]
            best_acc1 = checkpoint["best_acc1"]
            if bagua.get_local_rank() is not None:
                &#47&#47 best_acc1 may be from a checkpoint from a different GPU
                best_acc1 = best_acc1.to(bagua.get_local_rank())
            model.load_state_dict(checkpoint["state_dict"])
            optimizer.load_state_dict(checkpoint["optimizer"])
            print(
                "=&gt; loaded checkpoint &quot{}&quot (epoch {})".format(
                    args.resume, checkpoint["epoch"]
                )
            )
        else:
            print("=&gt; no checkpoint found at &quot{}&quot".format(args.resume))

    if args.distributed:
        model = model.with_bagua(
            <a id="change">[</a>optimizer<a id="change"></a>],
            algorithm,
        )
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/baguasys/bagua/commit/3e13646866d0fccae01e35d242919ad6be6a765c#diff-d0e6cb752d12a4cd0a93d1057594977d5aa00a489959775c71047b5d2fa693d2L187' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 68848747</div><div id='project'> Project Name: baguasys/bagua</div><div id='commit'> Commit Name: 3e13646866d0fccae01e35d242919ad6be6a765c</div><div id='time'> Time: 2021-09-15</div><div id='author'> Author: admin@mail.xrlian.com</div><div id='file'> File Name: examples/imagenet/main.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main_worker(1)</div><div id='n_method'> N Method Name: main_worker(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/imagenet/main.py</div><div id='n_file'> N File Name: examples/imagenet/main.py</div><div id='m_start'> M Start Line: 198</div><div id='m_end'> M End Line: 237</div><div id='n_start'> N Start Line: 198</div><div id='n_end'> N End Line: 253</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 Distributed training (should be after apex fp16 initialization)
    if args.distributed:
        model<a id="change">, optimizer</a> = bagua.bagua_init(model, optimizer, algorithm=args.algorithm)

    &#47&#47 Train!
    logger.info("***** Running training *****")</code></pre><h3>After Change</h3><pre><code class='java'>
        optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon
    )

    <a id="change">if args.algorithm == "gradient_allreduce"</a>:
        from bagua.torch_api.algorithms import gradient_allreduce
        algorithm<a id="change"> = </a><a id="change">gradient_allreduce.GradientAllReduceAlgorithm()</a>
    elif <a id="change">args.algorithm == "decentralized"</a>:
        from bagua.torch_api.algorithms import decentralized
        algorithm<a id="change"> = </a><a id="change">decentralized.DecentralizedAlgorithm()</a>
    elif <a id="change">args.algorithm == "bytegrad"</a>:
        from bagua.torch_api.algorithms import bytegrad
        algorithm<a id="change"> = </a><a id="change">bytegrad.ByteGradAlgorithm()</a>
    elif <a id="change">args.algorithm == "onebit_adam"</a>:
        from bagua.torch_api.algorithms import onebit_adam
        <a id="change">optimizer = </a><a id="change">onebit_adam.OnebitAdamOptimizer(model.parameters()</a><a id="change">)</a>
        algorithm<a id="change"> = </a><a id="change">onebit_adam.OnebitAdamAlgorithm(</a>optimizer, <a id="change">10</a><a id="change">)</a>
    else:
        <a id="change">raise </a>NotImplementedError

    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total
    )

    &#47&#47 Check if saved optimizer or scheduler states exist
    if os.path.isfile(
        os.path.join(args.model_name_or_path, "optimizer.pt")
    ) and os.path.isfile(os.path.join(args.model_name_or_path, "scheduler.pt")):
        &#47&#47 Load in optimizer and scheduler states
        optimizer.load_state_dict(
            torch.load(os.path.join(args.model_name_or_path, "optimizer.pt"))
        )
        scheduler.load_state_dict(
            torch.load(os.path.join(args.model_name_or_path, "scheduler.pt"))
        )

    if args.fp16:
        try:
            from apex import amp
        except ImportError:
            raise ImportError(
                "Please install apex from https://www.github.com/nvidia/apex to use fp16 training."
            )

        model, optimizer = amp.initialize(
            model, optimizer, opt_level=args.fp16_opt_level
        )

    &#47&#47 multi-gpu training (should be after apex fp16 initialization)
    if args.n_gpu &gt; 1:
        model = torch.nn.DataParallel(model)

    &#47&#47 Distributed training (should be after apex fp16 initialization)
    if args.distributed:
        model = model.with_bagua(<a id="change">[</a>optimizer<a id="change"></a>], algorithm)

    &#47&#47 Train!
    logger.info("***** Running training *****")</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/baguasys/bagua/commit/3e13646866d0fccae01e35d242919ad6be6a765c#diff-8083b0a699171a201b1412fba9c05dcabe1b8395306102340d05b2f7b7cc8b44L82' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 68848750</div><div id='project'> Project Name: baguasys/bagua</div><div id='commit'> Commit Name: 3e13646866d0fccae01e35d242919ad6be6a765c</div><div id='time'> Time: 2021-09-15</div><div id='author'> Author: admin@mail.xrlian.com</div><div id='file'> File Name: examples/squad/main.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train(4)</div><div id='n_method'> N Method Name: train(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/squad/main.py</div><div id='n_file'> N File Name: examples/squad/main.py</div><div id='m_start'> M Start Line: 133</div><div id='m_end'> M End Line: 170</div><div id='n_start'> N Start Line: 133</div><div id='n_end'> N End Line: 187</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        start_epoch = checkpoint["epoch"]

    model<a id="change">, optimizer</a> = bagua.bagua_init(
        model, optimizer, distributed_algorithm=args.algorithm
    )
</code></pre><h3>After Change</h3><pre><code class='java'>
    model = Net().cuda()
    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)

    <a id="change">if args.algorithm == "gradient_allreduce"</a>:
        from bagua.torch_api.algorithms import gradient_allreduce
        algorithm<a id="change"> = </a><a id="change">gradient_allreduce.GradientAllReduceAlgorithm()</a>
    elif <a id="change">args.algorithm == "decentralized"</a>:
        from bagua.torch_api.algorithms import decentralized
        algorithm<a id="change"> = </a><a id="change">decentralized.DecentralizedAlgorithm()</a>
    elif <a id="change">args.algorithm == "bytegrad"</a>:
        from bagua.torch_api.algorithms import bytegrad
        algorithm<a id="change"> = </a><a id="change">bytegrad.ByteGradAlgorithm()</a>
    elif <a id="change">args.algorithm == "onebit_adam"</a>:
        from bagua.torch_api.algorithms import onebit_adam
        <a id="change">optimizer = </a><a id="change">onebit_adam.OnebitAdamOptimizer(model.parameters()</a><a id="change">)</a>
        algorithm<a id="change"> = </a><a id="change">onebit_adam.OnebitAdamAlgorithm(</a>optimizer, <a id="change">10</a><a id="change">)</a>
    else:
        <a id="change">raise </a>NotImplementedError

    start_epoch = 1
    if os.path.exists(args.checkpoint_path):
        checkpoint = torch.load(args.checkpoint_path)
        model.load_state_dict(checkpoint["model_state_dict"])
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        start_epoch = checkpoint["epoch"]

    model = model.with_bagua(
        <a id="change">[</a>optimizer<a id="change"></a>], algorithm
    )

    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/baguasys/bagua/commit/3e13646866d0fccae01e35d242919ad6be6a765c#diff-491a3583ec464027b59d07307df0f8ad908f8e31860b1b8d80415d6fba6497ccL89' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 68848749</div><div id='project'> Project Name: baguasys/bagua</div><div id='commit'> Commit Name: 3e13646866d0fccae01e35d242919ad6be6a765c</div><div id='time'> Time: 2021-09-15</div><div id='author'> Author: admin@mail.xrlian.com</div><div id='file'> File Name: examples/elastic_training/main.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(0)</div><div id='n_method'> N Method Name: main(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/elastic_training/main.py</div><div id='n_file'> N File Name: examples/elastic_training/main.py</div><div id='m_start'> M Start Line: 208</div><div id='m_end'> M End Line: 220</div><div id='n_start'> N Start Line: 163</div><div id='n_end'> N End Line: 236</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    model = Net().cuda()
    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)

    model<a id="change">, optimizer</a> = bagua.bagua_init(
        model, optimizer, distributed_algorithm=args.algorithm
    )
</code></pre><h3>After Change</h3><pre><code class='java'>
    model = Net().cuda()
    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)

    <a id="change">if args.algorithm == "gradient_allreduce"</a>:
        from bagua.torch_api.algorithms import gradient_allreduce
        algorithm<a id="change"> = </a><a id="change">gradient_allreduce.GradientAllReduceAlgorithm()</a>
    elif <a id="change">args.algorithm == "decentralized"</a>:
        from bagua.torch_api.algorithms import decentralized
        algorithm<a id="change"> = </a><a id="change">decentralized.DecentralizedAlgorithm()</a>
    elif <a id="change">args.algorithm == "bytegrad"</a>:
        from bagua.torch_api.algorithms import bytegrad
        algorithm<a id="change"> = </a><a id="change">bytegrad.ByteGradAlgorithm()</a>
    elif <a id="change">args.algorithm == "onebit_adam"</a>:
        from bagua.torch_api.algorithms import onebit_adam
        <a id="change">optimizer = </a><a id="change">onebit_adam.OnebitAdamOptimizer(model.parameters()</a><a id="change">)</a>
        algorithm<a id="change"> = </a><a id="change">onebit_adam.OnebitAdamAlgorithm(</a>optimizer, <a id="change">10</a><a id="change">)</a>
    else:
        <a id="change">raise </a>NotImplementedError

    model = model.with_bagua(
        <a id="change">[</a>optimizer<a id="change"></a>], algorithm
    )

    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/baguasys/bagua/commit/3e13646866d0fccae01e35d242919ad6be6a765c#diff-c030f0e038ca82a947f2d9c4efc646fe2e74194d60008856474c8e4902d44ebaL88' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 68848754</div><div id='project'> Project Name: baguasys/bagua</div><div id='commit'> Commit Name: 3e13646866d0fccae01e35d242919ad6be6a765c</div><div id='time'> Time: 2021-09-15</div><div id='author'> Author: admin@mail.xrlian.com</div><div id='file'> File Name: examples/mnist/main.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(0)</div><div id='n_method'> N Method Name: main(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/mnist/main.py</div><div id='n_file'> N File Name: examples/mnist/main.py</div><div id='m_start'> M Start Line: 195</div><div id='m_end'> M End Line: 200</div><div id='n_start'> N Start Line: 149</div><div id='n_end'> N End Line: 216</div><BR>