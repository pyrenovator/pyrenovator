<html><h3>Pattern ID :18873
</h3><img src='61374153.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.assertIn("eval_precision", dataset.get_field_schema())
        self.assertIn("eval_recall", dataset.get_field_schema())

        <a id="change">dataset.delete_evaluation(</a>"eval"<a id="change">)</a>

        self.assertNotIn("eval", dataset.list_evaluations())
        self.assertNotIn("eval_accuracy", dataset.get_field_schema())
        self.assertNotIn("eval_precision", dataset.get_field_schema())</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 Test rename

        <a id="change">dataset.rename_evaluation("eval"</a>, <a id="change">"eval2"</a><a id="change">)</a>

        self.assertNotIn("eval", dataset.list_evaluations())
        self.assertNotIn("eval_accuracy", dataset.get_field_schema())
        self.assertNotIn("eval_precision", dataset.get_field_schema())
        self.assertNotIn("eval_recall", dataset.get_field_schema())

        <a id="change">self.assertIn("eval2"</a>, <a id="change">dataset.list_evaluations())</a>
        <a id="change">self.assertIn("eval2_accuracy"</a>, <a id="change">dataset.get_field_schema()</a><a id="change">)</a>
        <a id="change">self.assertIn("eval2_precision"</a>, dataset.get_field_schema()<a id="change">)</a>
        <a id="change">self.assertIn("eval2_recall"</a>, <a id="change">dataset.get_field_schema()</a><a id="change">)</a>

        &#47&#47 Test deletion

        <a id="change">dataset.delete_evaluation(</a>"eval2"<a id="change">)</a>

        <a id="change">self.assertNotIn("eval2"</a>, <a id="change">dataset.list_evaluations()</a><a id="change">)</a>
        <a id="change">self.assertNotIn("eval2_accuracy"</a>, dataset.get_field_schema()<a id="change">)</a>
        <a id="change">self.assertNotIn("eval2_precision"</a>, <a id="change">dataset.get_field_schema())</a>
        <a id="change">self.assertNotIn("eval2_recall"</a>, <a id="change">dataset.get_field_schema()</a><a id="change">)</a>

    @drop_datasets
    def test_evaluate_segmentations_rgb(self):
        dataset = self._make_segmentation_dataset()</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 17</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/voxel51/fiftyone/commit/401312876b34f0f48dd270255ec497673f0f8c9a#diff-b4dc8e475d33460e6cbbfeab1dc2b4b7cff019a7b3bd1d29d5401aa2e4cdedacL2276' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 61374153</div><div id='project'> Project Name: voxel51/fiftyone</div><div id='commit'> Commit Name: 401312876b34f0f48dd270255ec497673f0f8c9a</div><div id='time'> Time: 2023-03-02</div><div id='author'> Author: brimoor@umich.edu</div><div id='file'> File Name: tests/unittests/evaluation_tests.py</div><div id='m_class'> M Class Name: SegmentationTests</div><div id='n_method'> N Class Name: SegmentationTests</div><div id='m_method'> M Method Name: test_evaluate_segmentations_on_disk_simple(1)</div><div id='n_method'> N Method Name: test_evaluate_segmentations_on_disk_simple(1)</div><div id='m_parent_class'> M Parent Class: unittest.TestCase</div><div id='n_parent_class'> N Parent Class: unittest.TestCase</div><div id='m_file'> M File Name: tests/unittests/evaluation_tests.py</div><div id='n_file'> N File Name: tests/unittests/evaluation_tests.py</div><div id='m_start'> M Start Line: 2276</div><div id='m_end'> M End Line: 2276</div><div id='n_start'> N Start Line: 2446</div><div id='n_end'> N End Line: 2535</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.assertIn("eval_precision", dataset.get_field_schema())
        self.assertIn("eval_recall", dataset.get_field_schema())

        <a id="change">dataset.delete_evaluation(</a>"eval"<a id="change">)</a>

        self.assertNotIn("eval", dataset.list_evaluations())
        self.assertNotIn("eval_accuracy", dataset.get_field_schema())
        self.assertNotIn("eval_precision", dataset.get_field_schema())</code></pre><h3>After Change</h3><pre><code class='java'>

    @drop_datasets
    def test_evaluate_segmentations_on_disk_simple(self):
        <a id="change">dataset</a> = self._make_segmentation_dataset()

        &#47&#47 Convert to on-disk segmentations
        foul.export_segmentations(dataset, "ground_truth", self._new_dir())
        foul.export_segmentations(dataset, "predictions", self._new_dir())

        &#47&#47 Test empty view

        empty_view = dataset.limit(0)
        self.assertEqual(len(empty_view), 0)

        results = empty_view.evaluate_segmentations(
            "predictions",
            gt_field="ground_truth",
            eval_key="eval",
            method="simple",
        )

        self.assertIn("eval_accuracy", dataset.get_field_schema())
        self.assertIn("eval_precision", dataset.get_field_schema())
        self.assertIn("eval_recall", dataset.get_field_schema())

        empty_view.load_evaluation_view("eval")
        empty_view.get_evaluation_info("eval")

        results.report()
        results.print_report()

        metrics = results.metrics()
        self.assertEqual(metrics["support"], 0)

        actual = results.confusion_matrix()
        self.assertEqual(actual.shape, (0, 0))

        &#47&#47 Test evaluation (including missing data)

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")  &#47&#47 suppress missing masks warning

            results = dataset.evaluate_segmentations(
                "predictions",
                gt_field="ground_truth",
                eval_key="eval",
                method="simple",
                mask_targets={0: "background", 1: "cat", 2: "dog"},
            )

        dataset.load_evaluation_view("eval")
        dataset.get_evaluation_info("eval")

        results.report()
        results.print_report()

        metrics = results.metrics()
        self.assertEqual(metrics["support"], 4)

        &#47&#47 rows = GT, cols = predicted, labels = [background, cat, dog]
        actual = results.confusion_matrix()
        expected = np.array([[2, 1, 1], [1, 1, 0], [1, 0, 1]], dtype=int)

        self.assertEqual(actual.shape, expected.shape)
        self.assertTrue((actual == expected).all())

        self.assertIn("eval", dataset.list_evaluations())
        self.assertIn("eval_accuracy", dataset.get_field_schema())
        self.assertIn("eval_precision", dataset.get_field_schema())
        self.assertIn("eval_recall", dataset.get_field_schema())

        &#47&#47 Test rename

        <a id="change">dataset.rename_evaluation("eval"</a>, <a id="change">"eval2"</a><a id="change">)</a>

        self.assertNotIn("eval", dataset.list_evaluations())
        self.assertNotIn("eval_accuracy", dataset.get_field_schema())
        self.assertNotIn("eval_precision", dataset.get_field_schema())
        self.assertNotIn("eval_recall", dataset.get_field_schema())

        <a id="change">self.assertIn("eval2"</a>, <a id="change">dataset.list_evaluations())</a>
        <a id="change">self.assertIn("eval2_accuracy"</a>, dataset.get_field_schema()<a id="change">)</a>
        <a id="change">self.assertIn("eval2_precision"</a>, <a id="change">dataset.get_field_schema()</a><a id="change">)</a>
        <a id="change">self.assertIn("eval2_recall"</a>, <a id="change">dataset.get_field_schema()</a><a id="change">)</a>

        &#47&#47 Test deletion

        <a id="change">dataset.delete_evaluation(</a>"eval2"<a id="change">)</a>

        <a id="change">self.assertNotIn("eval2"</a>, <a id="change">dataset.list_evaluations()</a><a id="change">)</a>
        <a id="change">self.assertNotIn("eval2_accuracy"</a>, dataset.get_field_schema()<a id="change">)</a>
        <a id="change">self.assertNotIn("eval2_precision"</a>, <a id="change">dataset.get_field_schema())</a>
        <a id="change">self.assertNotIn("eval2_recall"</a>, <a id="change">dataset.get_field_schema()</a><a id="change">)</a>

    @drop_datasets
    def test_evaluate_segmentations_rgb(self):
        dataset = self._make_segmentation_dataset()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/voxel51/fiftyone/commit/401312876b34f0f48dd270255ec497673f0f8c9a#diff-b4dc8e475d33460e6cbbfeab1dc2b4b7cff019a7b3bd1d29d5401aa2e4cdedacL2203' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 61374154</div><div id='project'> Project Name: voxel51/fiftyone</div><div id='commit'> Commit Name: 401312876b34f0f48dd270255ec497673f0f8c9a</div><div id='time'> Time: 2023-03-02</div><div id='author'> Author: brimoor@umich.edu</div><div id='file'> File Name: tests/unittests/evaluation_tests.py</div><div id='m_class'> M Class Name: SegmentationTests</div><div id='n_method'> N Class Name: SegmentationTests</div><div id='m_method'> M Method Name: test_evaluate_segmentations_on_disk_simple(1)</div><div id='n_method'> N Method Name: test_evaluate_segmentations_on_disk_simple(1)</div><div id='m_parent_class'> M Parent Class: unittest.TestCase</div><div id='n_parent_class'> N Parent Class: unittest.TestCase</div><div id='m_file'> M File Name: tests/unittests/evaluation_tests.py</div><div id='n_file'> N File Name: tests/unittests/evaluation_tests.py</div><div id='m_start'> M Start Line: 2276</div><div id='m_end'> M End Line: 2276</div><div id='n_start'> N Start Line: 2446</div><div id='n_end'> N End Line: 2535</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.assertIn("eval_recall", dataset.get_field_schema())
        self.assertIn("eval_recall", dataset.get_frame_field_schema())

        <a id="change">dataset.delete_evaluation(</a>"eval"<a id="change">)</a>

        self.assertNotIn("eval", dataset.list_evaluations())
        self.assertNotIn("eval_accuracy", dataset.get_field_schema())
        self.assertNotIn("eval_accuracy", dataset.get_frame_field_schema())</code></pre><h3>After Change</h3><pre><code class='java'>

    @drop_datasets
    def test_evaluate_video_segmentations_simple(self):
        <a id="change">dataset</a> = self._make_video_segmentation_dataset()

        &#47&#47 Test empty view

        empty_view = dataset.limit(0)
        self.assertEqual(len(empty_view), 0)

        results = empty_view.evaluate_segmentations(
            "frames.predictions",
            gt_field="frames.ground_truth",
            eval_key="eval",
            method="simple",
        )

        self.assertIn("eval_accuracy", dataset.get_field_schema())
        self.assertIn("eval_accuracy", dataset.get_frame_field_schema())
        self.assertIn("eval_precision", dataset.get_field_schema())
        self.assertIn("eval_precision", dataset.get_frame_field_schema())
        self.assertIn("eval_recall", dataset.get_field_schema())
        self.assertIn("eval_recall", dataset.get_frame_field_schema())

        empty_view.load_evaluation_view("eval")
        empty_view.get_evaluation_info("eval")

        results.report()
        results.print_report()

        metrics = results.metrics()
        self.assertEqual(metrics["support"], 0)

        actual = results.confusion_matrix()
        self.assertEqual(actual.shape, (0, 0))

        &#47&#47 Test evaluation (including missing data)

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")  &#47&#47 suppress missing masks warning

            results = dataset.evaluate_segmentations(
                "frames.predictions",
                gt_field="frames.ground_truth",
                eval_key="eval",
                method="simple",
                mask_targets={0: "background", 1: "cat", 2: "dog"},
            )

        dataset.load_evaluation_view("eval")
        dataset.get_evaluation_info("eval")

        results.report()
        results.print_report()

        metrics = results.metrics()
        self.assertEqual(metrics["support"], 4)

        &#47&#47 rows = GT, cols = predicted, labels = [background, cat, dog]
        actual = results.confusion_matrix()
        expected = np.array([[2, 1, 1], [1, 1, 0], [1, 0, 1]], dtype=int)

        self.assertEqual(actual.shape, expected.shape)
        self.assertTrue((actual == expected).all())

        self.assertIn("eval", dataset.list_evaluations())
        self.assertIn("eval_accuracy", dataset.get_field_schema())
        self.assertIn("eval_accuracy", dataset.get_frame_field_schema())
        self.assertIn("eval_precision", dataset.get_field_schema())
        self.assertIn("eval_precision", dataset.get_frame_field_schema())
        self.assertIn("eval_recall", dataset.get_field_schema())
        self.assertIn("eval_recall", dataset.get_frame_field_schema())

        &#47&#47 Test rename

        <a id="change">dataset.rename_evaluation("eval"</a>, <a id="change">"eval2"</a><a id="change">)</a>

        self.assertNotIn("eval", dataset.list_evaluations())
        self.assertNotIn("eval_accuracy", dataset.get_field_schema())
        self.assertNotIn("eval_accuracy", dataset.get_frame_field_schema())
        self.assertNotIn("eval_precision", dataset.get_field_schema())
        self.assertNotIn("eval_precision", dataset.get_frame_field_schema())
        self.assertNotIn("eval_recall", dataset.get_field_schema())
        self.assertNotIn("eval_recall", dataset.get_frame_field_schema())

        <a id="change">self.assertIn("eval2"</a>, <a id="change">dataset.list_evaluations())</a>
        <a id="change">self.assertIn("eval2_accuracy"</a>, <a id="change">dataset.get_field_schema()</a><a id="change">)</a>
        self.assertIn("eval2_accuracy", dataset.get_frame_field_schema())
        <a id="change">self.assertIn("eval2_precision"</a>, <a id="change">dataset.get_field_schema()</a><a id="change">)</a>
        <a id="change">self.assertIn("eval2_precision"</a>, dataset.get_frame_field_schema()<a id="change">)</a>
        self.assertIn("eval2_recall", dataset.get_field_schema())
        self.assertIn("eval2_recall", dataset.get_frame_field_schema())

        &#47&#47 Test deletion

        <a id="change">dataset.delete_evaluation(</a>"eval2"<a id="change">)</a>

        <a id="change">self.assertNotIn("eval2"</a>, <a id="change">dataset.list_evaluations()</a><a id="change">)</a>
        <a id="change">self.assertNotIn("eval2_accuracy"</a>, <a id="change">dataset.get_field_schema()</a><a id="change">)</a>
        self.assertNotIn("eval2_accuracy", dataset.get_frame_field_schema())
        <a id="change">self.assertNotIn("eval2_precision"</a>, <a id="change">dataset.get_field_schema())</a>
        <a id="change">self.assertNotIn("eval2_precision"</a>, dataset.get_frame_field_schema()<a id="change">)</a>
        self.assertNotIn("eval2_recall", dataset.get_field_schema())
        self.assertNotIn("eval2_recall", dataset.get_frame_field_schema())

</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/voxel51/fiftyone/commit/401312876b34f0f48dd270255ec497673f0f8c9a#diff-b4dc8e475d33460e6cbbfeab1dc2b4b7cff019a7b3bd1d29d5401aa2e4cdedacL2409' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 61374125</div><div id='project'> Project Name: voxel51/fiftyone</div><div id='commit'> Commit Name: 401312876b34f0f48dd270255ec497673f0f8c9a</div><div id='time'> Time: 2023-03-02</div><div id='author'> Author: brimoor@umich.edu</div><div id='file'> File Name: tests/unittests/evaluation_tests.py</div><div id='m_class'> M Class Name: VideoSegmentationTests</div><div id='n_method'> N Class Name: VideoSegmentationTests</div><div id='m_method'> M Method Name: test_evaluate_video_segmentations_simple(1)</div><div id='n_method'> N Method Name: test_evaluate_video_segmentations_simple(1)</div><div id='m_parent_class'> M Parent Class: unittest.TestCase</div><div id='n_parent_class'> N Parent Class: unittest.TestCase</div><div id='m_file'> M File Name: tests/unittests/evaluation_tests.py</div><div id='n_file'> N File Name: tests/unittests/evaluation_tests.py</div><div id='m_start'> M Start Line: 2484</div><div id='m_end'> M End Line: 2484</div><div id='n_start'> N Start Line: 2676</div><div id='n_end'> N End Line: 2776</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.assertIn("eval_precision", dataset.get_field_schema())
        self.assertIn("eval_recall", dataset.get_field_schema())

        <a id="change">dataset.delete_evaluation(</a>"eval"<a id="change">)</a>

        self.assertNotIn("eval", dataset.list_evaluations())
        self.assertNotIn("eval_accuracy", dataset.get_field_schema())
        self.assertNotIn("eval_precision", dataset.get_field_schema())</code></pre><h3>After Change</h3><pre><code class='java'>

    @drop_datasets
    def test_evaluate_segmentations_simple(self):
        <a id="change">dataset</a> = self._make_segmentation_dataset()

        &#47&#47 Test empty view

        empty_view = dataset.limit(0)
        self.assertEqual(len(empty_view), 0)

        results = empty_view.evaluate_segmentations(
            "predictions",
            gt_field="ground_truth",
            eval_key="eval",
            method="simple",
        )

        self.assertIn("eval_accuracy", dataset.get_field_schema())
        self.assertIn("eval_precision", dataset.get_field_schema())
        self.assertIn("eval_recall", dataset.get_field_schema())

        empty_view.load_evaluation_view("eval")
        empty_view.get_evaluation_info("eval")

        results.report()
        results.print_report()

        metrics = results.metrics()
        self.assertEqual(metrics["support"], 0)

        actual = results.confusion_matrix()
        self.assertEqual(actual.shape, (0, 0))

        &#47&#47 Test evaluation (including missing data)

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")  &#47&#47 suppress missing masks warning

            results = dataset.evaluate_segmentations(
                "predictions",
                gt_field="ground_truth",
                eval_key="eval",
                method="simple",
                mask_targets={0: "background", 1: "cat", 2: "dog"},
            )

        dataset.load_evaluation_view("eval")
        dataset.get_evaluation_info("eval")

        results.report()
        results.print_report()

        metrics = results.metrics()
        self.assertEqual(metrics["support"], 4)

        &#47&#47 rows = GT, cols = predicted, labels = [background, cat, dog]
        actual = results.confusion_matrix()
        expected = np.array([[2, 1, 1], [1, 1, 0], [1, 0, 1]], dtype=int)

        self.assertEqual(actual.shape, expected.shape)
        self.assertTrue((actual == expected).all())

        self.assertIn("eval", dataset.list_evaluations())
        self.assertIn("eval_accuracy", dataset.get_field_schema())
        self.assertIn("eval_precision", dataset.get_field_schema())
        self.assertIn("eval_recall", dataset.get_field_schema())

        &#47&#47 Test rename

        <a id="change">dataset.rename_evaluation("eval"</a>, <a id="change">"eval2"</a><a id="change">)</a>

        self.assertNotIn("eval", dataset.list_evaluations())
        self.assertNotIn("eval_accuracy", dataset.get_field_schema())
        self.assertNotIn("eval_precision", dataset.get_field_schema())
        self.assertNotIn("eval_recall", dataset.get_field_schema())

        <a id="change">self.assertIn("eval2"</a>, <a id="change">dataset.list_evaluations())</a>
        <a id="change">self.assertIn("eval2_accuracy"</a>, <a id="change">dataset.get_field_schema()</a><a id="change">)</a>
        <a id="change">self.assertIn("eval2_precision"</a>, <a id="change">dataset.get_field_schema()</a><a id="change">)</a>
        <a id="change">self.assertIn("eval2_recall"</a>, dataset.get_field_schema()<a id="change">)</a>

        &#47&#47 Test deletion

        <a id="change">dataset.delete_evaluation(</a>"eval2"<a id="change">)</a>

        <a id="change">self.assertNotIn("eval2"</a>, <a id="change">dataset.list_evaluations()</a><a id="change">)</a>
        <a id="change">self.assertNotIn("eval2_accuracy"</a>, <a id="change">dataset.get_field_schema())</a>
        <a id="change">self.assertNotIn("eval2_precision"</a>, dataset.get_field_schema()<a id="change">)</a>
        <a id="change">self.assertNotIn("eval2_recall"</a>, <a id="change">dataset.get_field_schema()</a><a id="change">)</a>

    @drop_datasets
    def test_evaluate_segmentations_on_disk_simple(self):
        dataset = self._make_segmentation_dataset()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/voxel51/fiftyone/commit/401312876b34f0f48dd270255ec497673f0f8c9a#diff-b4dc8e475d33460e6cbbfeab1dc2b4b7cff019a7b3bd1d29d5401aa2e4cdedacL2126' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 61374113</div><div id='project'> Project Name: voxel51/fiftyone</div><div id='commit'> Commit Name: 401312876b34f0f48dd270255ec497673f0f8c9a</div><div id='time'> Time: 2023-03-02</div><div id='author'> Author: brimoor@umich.edu</div><div id='file'> File Name: tests/unittests/evaluation_tests.py</div><div id='m_class'> M Class Name: SegmentationTests</div><div id='n_method'> N Class Name: SegmentationTests</div><div id='m_method'> M Method Name: test_evaluate_segmentations_simple(1)</div><div id='n_method'> N Method Name: test_evaluate_segmentations_simple(1)</div><div id='m_parent_class'> M Parent Class: unittest.TestCase</div><div id='n_parent_class'> N Parent Class: unittest.TestCase</div><div id='m_file'> M File Name: tests/unittests/evaluation_tests.py</div><div id='n_file'> N File Name: tests/unittests/evaluation_tests.py</div><div id='m_start'> M Start Line: 2195</div><div id='m_end'> M End Line: 2195</div><div id='n_start'> N Start Line: 2357</div><div id='n_end'> N End Line: 2442</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.assertIn("eval_precision", dataset.get_field_schema())
        self.assertIn("eval_recall", dataset.get_field_schema())

        <a id="change">dataset.delete_evaluation(</a>"eval"<a id="change">)</a>

        self.assertNotIn("eval", dataset.list_evaluations())
        self.assertNotIn("eval_accuracy", dataset.get_field_schema())
        self.assertNotIn("eval_precision", dataset.get_field_schema())</code></pre><h3>After Change</h3><pre><code class='java'>

    @drop_datasets
    def test_evaluate_segmentations_rgb(self):
        <a id="change">dataset</a> = self._make_segmentation_dataset()

        &#47&#47 Use opposite case in `mask_targets` to test case-insensitivity
        targets_map = {0: "&#47&#47000000", 1: "&#47&#47FF6D04", 2: "&#47&#47499cef"}
        mask_targets = {
            "&#47&#47000000": "background",
            "&#47&#47ff6d04": "cat",
            "&#47&#47499CEF": "dog",
        }

        &#47&#47 Convert to RGB segmentations
        foul.transform_segmentations(dataset, "ground_truth", targets_map)
        foul.transform_segmentations(dataset, "predictions", targets_map)

        &#47&#47 Convert to on-disk segmentations
        foul.export_segmentations(dataset, "ground_truth", self._new_dir())
        foul.export_segmentations(dataset, "predictions", self._new_dir())

        &#47&#47 Test empty view

        empty_view = dataset.limit(0)
        self.assertEqual(len(empty_view), 0)

        results = empty_view.evaluate_segmentations(
            "predictions",
            gt_field="ground_truth",
            eval_key="eval",
            method="simple",
        )

        self.assertIn("eval_accuracy", dataset.get_field_schema())
        self.assertIn("eval_precision", dataset.get_field_schema())
        self.assertIn("eval_recall", dataset.get_field_schema())

        empty_view.load_evaluation_view("eval")
        empty_view.get_evaluation_info("eval")

        results.report()
        results.print_report()

        metrics = results.metrics()
        self.assertEqual(metrics["support"], 0)

        actual = results.confusion_matrix()
        self.assertEqual(actual.shape, (0, 0))

        &#47&#47 Test evaluation (including missing data)

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")  &#47&#47 suppress missing masks warning

            results = dataset.evaluate_segmentations(
                "predictions",
                gt_field="ground_truth",
                eval_key="eval",
                method="simple",
                mask_targets=mask_targets,
            )

        dataset.load_evaluation_view("eval")
        dataset.get_evaluation_info("eval")

        results.report()
        results.print_report()

        metrics = results.metrics()
        self.assertEqual(metrics["support"], 4)

        &#47&#47 rows = GT, cols = predicted, labels = [background, cat, dog]
        &#47&#47 Ordering is based on int representation of hex color strings
        actual = results.confusion_matrix()
        expected = np.array([[2, 1, 1], [1, 1, 0], [1, 0, 1]], dtype=int)

        self.assertEqual(actual.shape, expected.shape)
        self.assertTrue((actual == expected).all())

        self.assertIn("eval", dataset.list_evaluations())
        self.assertIn("eval_accuracy", dataset.get_field_schema())
        self.assertIn("eval_precision", dataset.get_field_schema())
        self.assertIn("eval_recall", dataset.get_field_schema())

        &#47&#47 Test rename

        <a id="change">dataset.rename_evaluation("eval"</a>, <a id="change">"eval2"</a><a id="change">)</a>

        self.assertNotIn("eval", dataset.list_evaluations())
        self.assertNotIn("eval_accuracy", dataset.get_field_schema())
        self.assertNotIn("eval_precision", dataset.get_field_schema())
        self.assertNotIn("eval_recall", dataset.get_field_schema())

        <a id="change">self.assertIn("eval2"</a>, <a id="change">dataset.list_evaluations())</a>
        <a id="change">self.assertIn("eval2_accuracy"</a>, <a id="change">dataset.get_field_schema()</a><a id="change">)</a>
        <a id="change">self.assertIn("eval2_precision"</a>, dataset.get_field_schema()<a id="change">)</a>
        <a id="change">self.assertIn("eval2_recall"</a>, <a id="change">dataset.get_field_schema()</a><a id="change">)</a>

        &#47&#47 Test deletion

        <a id="change">dataset.delete_evaluation(</a>"eval2"<a id="change">)</a>

        <a id="change">self.assertNotIn("eval2"</a>, <a id="change">dataset.list_evaluations()</a><a id="change">)</a>
        <a id="change">self.assertNotIn("eval2_accuracy"</a>, <a id="change">dataset.get_field_schema())</a>
        <a id="change">self.assertNotIn("eval2_precision"</a>, <a id="change">dataset.get_field_schema()</a><a id="change">)</a>
        <a id="change">self.assertNotIn("eval2_recall"</a>, dataset.get_field_schema()<a id="change">)</a>


class VideoSegmentationTests(unittest.TestCase):
    def _make_video_segmentation_dataset(self):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/voxel51/fiftyone/commit/401312876b34f0f48dd270255ec497673f0f8c9a#diff-b4dc8e475d33460e6cbbfeab1dc2b4b7cff019a7b3bd1d29d5401aa2e4cdedacL2284' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 61374101</div><div id='project'> Project Name: voxel51/fiftyone</div><div id='commit'> Commit Name: 401312876b34f0f48dd270255ec497673f0f8c9a</div><div id='time'> Time: 2023-03-02</div><div id='author'> Author: brimoor@umich.edu</div><div id='file'> File Name: tests/unittests/evaluation_tests.py</div><div id='m_class'> M Class Name: SegmentationTests</div><div id='n_method'> N Class Name: SegmentationTests</div><div id='m_method'> M Method Name: test_evaluate_segmentations_rgb(1)</div><div id='n_method'> N Method Name: test_evaluate_segmentations_rgb(1)</div><div id='m_parent_class'> M Parent Class: unittest.TestCase</div><div id='n_parent_class'> N Parent Class: unittest.TestCase</div><div id='m_file'> M File Name: tests/unittests/evaluation_tests.py</div><div id='n_file'> N File Name: tests/unittests/evaluation_tests.py</div><div id='m_start'> M Start Line: 2370</div><div id='m_end'> M End Line: 2370</div><div id='n_start'> N Start Line: 2539</div><div id='n_end'> N End Line: 2641</div><BR>