<html><h3>Pattern ID :18420
</h3><img src='60194838.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        model : the model contains  weights need to setting the constraints.

    
    <a id="change">for </a>name,w in  model.named_parameters()<a id="change">:
        </a>if &quotbias&quot not in name  and w.trainable:
            w_data = w.value().detach()
            <a id="change">w.assign(</a>w_data<a id="change">/ </a>(epsilon()<a id="change"> +</a>sqrt(reduce_sum(square(w_data),axis=axis,keepdims=True)))<a id="change">)</a>

def  min_max_norm(model,min_value=0.0, max_value=1.0, rate=3.0, axis=0):
    
    MinMaxNorm weight constraint.</code></pre><h3>After Change</h3><pre><code class='java'>

    

    <a id="change">for module</a> in <a id="change">model.children()</a><a id="change">:
        for </a>key, <a id="change">param</a> in module._parameters.items()<a id="change">:
            </a>if param is not None and <a id="change">param.trainable==True</a>:
                &#47&#47 Tensors stored in modules are graph leaves, and we don&quott want to
                &#47&#47 track autograd history of `param_applied`, so we have to use
                w_data = param.value().detach()
                <a id="change">reduce_axis = </a><a id="change">list(</a><a id="change">range(len(</a>w_data.shape<a id="change">)</a><a id="change">))</a>
                <a id="change">reduce_axis.remove(reduce_axis</a><a id="change">[-1])</a>
                <a id="change">param_applied =</a>w_data<a id="change">/ </a>(epsilon()<a id="change"> +</a>sqrt(reduce_sum(square(w_data),axis=reduce_axis,keepdims=True)))
                <a id="change">if isinstance(param</a>, tf.Variable<a id="change">)</a>:
                    <a id="change">module._parameters[key]</a><a id="change"> = Parameter(param_applied</a><a id="change">, trainable=param.trainable)</a>
                else:
                    param<a id="change"> = param_applied</a>

def  min_max_norm(model,min_value=0.0, max_value=1.0, rate=1.0, axis=0):
    
    MinMaxNorm weight constraint.</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 23</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/allanyiin/trident/commit/b2f780799b10d1d116e460e98accbd67f6f882ab#diff-bc94bbb66df4dc80907d09466a0e48df4870a4cc5d50707925c74e2239d7c1d4L54' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60194838</div><div id='project'> Project Name: allanyiin/trident</div><div id='commit'> Commit Name: b2f780799b10d1d116e460e98accbd67f6f882ab</div><div id='time'> Time: 2020-12-31</div><div id='author'> Author: allan@asiaminer.com.tw</div><div id='file'> File Name: trident/optims/tensorflow_constraints.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: unit_norm(2)</div><div id='n_method'> N Method Name: unit_norm(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: trident/optims/tensorflow_constraints.py</div><div id='n_file'> N File Name: trident/optims/tensorflow_constraints.py</div><div id='m_start'> M Start Line: 54</div><div id='m_end'> M End Line: 58</div><div id='n_start'> N Start Line: 76</div><div id='n_end'> N End Line: 89</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        axis (int):axis along which to calculate weight norms.

    
    <a id="change">for </a>name,<a id="change">w</a> in  model.named_parameters()<a id="change">:
        </a>if &quotbias&quot not in name  and w.trainable:
            w_data=w.value().detach()
            norms = sqrt(reduce_sum(square(w_data), axis=axis, keepdims=True))
            desired = clip(norms, 0, max_value)
            <a id="change">w.assign(</a>w_data<a id="change"> * </a>(desired<a id="change"> / </a>(epsilon() + norms))<a id="change">)</a>

def  non_neg_norm(model):
    
    Constrains the weights to be non-negative.</code></pre><h3>After Change</h3><pre><code class='java'>

    

    <a id="change">for module</a> in <a id="change">model.children()</a><a id="change">:
        for </a>key, <a id="change">param</a> in module._parameters.items()<a id="change">:
            </a>if param is not None  and <a id="change">param.trainable==True</a>  and key!="bias":
                &#47&#47 Tensors stored in modules are graph leaves, and we don&quott want to
                &#47&#47 track autograd history of `param_applied`, so we have to use
                w_data = param.value().detach()
                <a id="change">reduce_axis = </a><a id="change">list(</a><a id="change">range(len(</a>w_data.shape<a id="change">)</a><a id="change">))</a>
                <a id="change">reduce_axis.remove(</a><a id="change">reduce_axis[-1])</a>
                norms = sqrt(reduce_sum(square(w_data), axis=reduce_axis, keepdims=True))
                desired = clip(norms, 0, max_value)
                <a id="change">param_applied =</a>w_data<a id="change"> * </a>(desired<a id="change"> / </a>(epsilon() + norms))
                <a id="change">if isinstance(</a>param, tf.Variable<a id="change">)</a>:
                    <a id="change">module._parameters[key]</a><a id="change"> = Parameter(</a>param_applied<a id="change">, trainable=param.trainable)</a>
                else:
                    param<a id="change"> = </a>param_applied


def  non_neg_norm(model):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/allanyiin/trident/commit/b2f780799b10d1d116e460e98accbd67f6f882ab#diff-bc94bbb66df4dc80907d09466a0e48df4870a4cc5d50707925c74e2239d7c1d4L17' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60194839</div><div id='project'> Project Name: allanyiin/trident</div><div id='commit'> Commit Name: b2f780799b10d1d116e460e98accbd67f6f882ab</div><div id='time'> Time: 2020-12-31</div><div id='author'> Author: allan@asiaminer.com.tw</div><div id='file'> File Name: trident/optims/tensorflow_constraints.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: max_norm(3)</div><div id='n_method'> N Method Name: max_norm(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: trident/optims/tensorflow_constraints.py</div><div id='n_file'> N File Name: trident/optims/tensorflow_constraints.py</div><div id='m_start'> M Start Line: 27</div><div id='m_end'> M End Line: 33</div><div id='n_start'> N Start Line: 30</div><div id='n_end'> N End Line: 46</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        model : the model contains  weights need to setting the constraints.

    
    <a id="change">for </a>name,<a id="change">w</a> in  model.named_parameters()<a id="change">:
        </a>if &quotbias&quot not in name  and w.trainable:
            w_data = w.value().detach()
            <a id="change">w.assign(</a>w_data<a id="change">/ </a>(epsilon()<a id="change"> +</a>sqrt(reduce_sum(square(w_data),axis=axis,keepdims=True)))<a id="change">)</a>

def  min_max_norm(model,min_value=0.0, max_value=1.0, rate=3.0, axis=0):
    
    MinMaxNorm weight constraint.</code></pre><h3>After Change</h3><pre><code class='java'>

    

    <a id="change">for module</a> in <a id="change">model.children()</a><a id="change">:
        for </a>key, <a id="change">param</a> in module._parameters.items()<a id="change">:
            </a>if param is not None and <a id="change">param.trainable==True</a>:
                &#47&#47 Tensors stored in modules are graph leaves, and we don&quott want to
                &#47&#47 track autograd history of `param_applied`, so we have to use
                w_data = param.value().detach()
                <a id="change">reduce_axis = </a><a id="change">list(</a><a id="change">range(len(</a>w_data.shape<a id="change">)</a><a id="change">))</a>
                <a id="change">reduce_axis.remove(</a><a id="change">reduce_axis[-1])</a>
                <a id="change">param_applied =</a>w_data<a id="change">/ </a>(epsilon()<a id="change"> +</a>sqrt(reduce_sum(square(w_data),axis=reduce_axis,keepdims=True)))
                <a id="change">if isinstance(</a>param, tf.Variable<a id="change">)</a>:
                    <a id="change">module._parameters[key]</a><a id="change"> = Parameter(</a>param_applied<a id="change">, trainable=param.trainable)</a>
                else:
                    param<a id="change"> = </a>param_applied

def  min_max_norm(model,min_value=0.0, max_value=1.0, rate=1.0, axis=0):
    </code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/allanyiin/trident/commit/b2f780799b10d1d116e460e98accbd67f6f882ab#diff-bc94bbb66df4dc80907d09466a0e48df4870a4cc5d50707925c74e2239d7c1d4L46' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60194836</div><div id='project'> Project Name: allanyiin/trident</div><div id='commit'> Commit Name: b2f780799b10d1d116e460e98accbd67f6f882ab</div><div id='time'> Time: 2020-12-31</div><div id='author'> Author: allan@asiaminer.com.tw</div><div id='file'> File Name: trident/optims/tensorflow_constraints.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: unit_norm(2)</div><div id='n_method'> N Method Name: unit_norm(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: trident/optims/tensorflow_constraints.py</div><div id='n_file'> N File Name: trident/optims/tensorflow_constraints.py</div><div id='m_start'> M Start Line: 54</div><div id='m_end'> M End Line: 58</div><div id='n_start'> N Start Line: 76</div><div id='n_end'> N End Line: 89</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>



    <a id="change">for </a>name,<a id="change">w</a> in  model.named_parameters()<a id="change">:
        </a>if &quotbias&quot not in name  and w.trainable:
            w_data = w.value().detach()
            norms = sqrt(reduce_sum(square(w_data), axis=axis, keepdims=True))
            desired = (rate * clip(norms, min_value, max_value) + (1 - rate) * norms)
            <a id="change">w.assign(</a>w_data<a id="change"> * </a>(desired<a id="change"> / </a>(epsilon() + norms))<a id="change">)</a>



</code></pre><h3>After Change</h3><pre><code class='java'>
        axis (int): axis along which to calculate weight norms
    

    <a id="change">for module</a> in <a id="change">model.children()</a><a id="change">:
        for </a>key, <a id="change">param</a> in module._parameters.items()<a id="change">:
            </a>if param is not None  and <a id="change">param.trainable==True</a> and key!="bias":

                &#47&#47 Tensors stored in modules are graph leaves, and we don&quott want to
                &#47&#47 track autograd history of `param_applied`, so we have to use
                w_data = param.value().detach()
                <a id="change">reduce_axis=</a><a id="change">list(</a><a id="change">range(len(</a>w_data.shape<a id="change">)</a><a id="change">))</a>
                <a id="change">reduce_axis.remove(</a><a id="change">reduce_axis[-1])</a>
                norms = sqrt(reduce_sum(square(w_data), axis=reduce_axis, keepdims=True))
                desired = (rate * clip(norms, min_value, max_value) + (1 - rate) * norms)
                <a id="change">param_applied =</a>w_data<a id="change"> * </a>(desired<a id="change"> / </a>(epsilon() + norms))
                <a id="change">if isinstance(</a>param, tf.Variable<a id="change">)</a>:
                    <a id="change">module._parameters[key]</a><a id="change"> = Parameter(</a>param_applied<a id="change">, trainable=param.trainable)</a>
                else:
                    param<a id="change"> = </a>param_applied


</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/allanyiin/trident/commit/b2f780799b10d1d116e460e98accbd67f6f882ab#diff-bc94bbb66df4dc80907d09466a0e48df4870a4cc5d50707925c74e2239d7c1d4L59' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 60194837</div><div id='project'> Project Name: allanyiin/trident</div><div id='commit'> Commit Name: b2f780799b10d1d116e460e98accbd67f6f882ab</div><div id='time'> Time: 2020-12-31</div><div id='author'> Author: allan@asiaminer.com.tw</div><div id='file'> File Name: trident/optims/tensorflow_constraints.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: min_max_norm(5)</div><div id='n_method'> N Method Name: min_max_norm(5)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: trident/optims/tensorflow_constraints.py</div><div id='n_file'> N File Name: trident/optims/tensorflow_constraints.py</div><div id='m_start'> M Start Line: 74</div><div id='m_end'> M End Line: 85</div><div id='n_start'> N Start Line: 103</div><div id='n_end'> N End Line: 124</div><BR>