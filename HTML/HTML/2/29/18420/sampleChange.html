<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        model : the model contains  weights need to setting the constraints.

    
    <a id="change">for </a>name,w in  model.named_parameters()<a id="change">:
        </a>if &quotbias&quot not in name  and w.trainable:
            w_data = w.value().detach()
            <a id="change">w.assign(</a>w_data<a id="change">/ </a>(epsilon()<a id="change"> +</a>sqrt(reduce_sum(square(w_data),axis=axis,keepdims=True)))<a id="change">)</a>

def  min_max_norm(model,min_value=0.0, max_value=1.0, rate=3.0, axis=0):
    
    MinMaxNorm weight constraint.</code></pre><h3>After Change</h3><pre><code class='java'>

    

    <a id="change">for module</a> in <a id="change">model.children()</a><a id="change">:
        for </a>key, <a id="change">param</a> in module._parameters.items()<a id="change">:
            </a>if param is not None and <a id="change">param.trainable==True</a>:
                &#47&#47 Tensors stored in modules are graph leaves, and we don&quott want to
                &#47&#47 track autograd history of `param_applied`, so we have to use
                w_data = param.value().detach()
                <a id="change">reduce_axis = </a><a id="change">list(</a><a id="change">range(len(</a>w_data.shape<a id="change">)</a><a id="change">))</a>
                <a id="change">reduce_axis.remove(reduce_axis</a><a id="change">[-1])</a>
                <a id="change">param_applied =</a>w_data<a id="change">/ </a>(epsilon()<a id="change"> +</a>sqrt(reduce_sum(square(w_data),axis=reduce_axis,keepdims=True)))
                <a id="change">if isinstance(param</a>, tf.Variable<a id="change">)</a>:
                    <a id="change">module._parameters[key]</a><a id="change"> = Parameter(param_applied</a><a id="change">, trainable=param.trainable)</a>
                else:
                    param<a id="change"> = param_applied</a>

def  min_max_norm(model,min_value=0.0, max_value=1.0, rate=1.0, axis=0):
    
    MinMaxNorm weight constraint.</code></pre>