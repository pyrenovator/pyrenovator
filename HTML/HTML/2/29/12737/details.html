<html><h3>Pattern ID :12737
</h3><img src='43111105.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 update libai_cfg by kwargs
        for k, v in self.kwargs.items():
            <a id="change">self.libai_cfg[k] = v</a>


class T5LoaderLibai(ModelLoaderLiBai):
    def __init__(self, model, libai_cfg, pretrained_model_path, **kwargs):</code></pre><h3>After Change</h3><pre><code class='java'>
        with open(config_file, mode="r", encoding="utf-8") as f:
            cfg_dict = json.load(f)

        <a id="change">self._update_cfg("vocab_size"</a>, cfg_dict["vocab_size"]<a id="change">)</a>
        <a id="change">self._update_cfg("hidden_size"</a>, cfg_dict["d_model"]<a id="change">)</a>
        <a id="change">self._update_cfg("hidden_layers"</a>, cfg_dict["num_layers"]<a id="change">)</a>
        <a id="change">self._update_cfg("num_attention_heads"</a>, cfg_dict["num_heads"]<a id="change">)</a>
        <a id="change">self._update_cfg("intermediate_size"</a>, cfg_dict["d_ff"]<a id="change">)</a>
        self._update_cfg("hidden_dropout_prob", cfg_dict["dropout_rate"])
        <a id="change">self._update_cfg("attention_probs_dropout_prob"</a>, cfg_dict["dropout_rate"]<a id="change">)</a>
        self._update_cfg("max_position_embeddings", cfg_dict.get("n_positions", 512))
        <a id="change">self._update_cfg(
            "relative_attention_num_buckets"</a>, cfg_dict["relative_attention_num_buckets"]<a id="change">
        )</a>
        <a id="change">self._update_cfg("embedding_dropout_prob"</a>, cfg_dict["dropout_rate"]<a id="change">)</a>
        <a id="change">self._update_cfg("initializer_range"</a>, cfg_dict["initializer_factor"]<a id="change">)</a>
        <a id="change">self._update_cfg("layernorm_eps"</a>, cfg_dict["layer_norm_epsilon"]<a id="change">)</a>
        <a id="change">self._update_cfg("head_size"</a>, cfg_dict["d_kv"]<a id="change">)</a>

        &#47&#47 update libai_cfg by kwargs
        for k, <a id="change">v</a> in self.kwargs.items():
            <a id="change">self._update_cfg(</a>k, <a id="change">v</a><a id="change">)</a>

        <a id="change">self._update_cfg_log()</a>


class T5LoaderLibai(ModelLoaderLiBai):
    def __init__(self, model, libai_cfg, pretrained_model_path, **kwargs):</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 15</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/oneflow-inc/libai/commit/48ba327a2052edf573ae93616c3a180f6d18b4a3#diff-cc2bd90ae8c5efb295996ca11833ac662d5fc9d793278756761d9b5b30c5a965L267' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43111105</div><div id='project'> Project Name: oneflow-inc/libai</div><div id='commit'> Commit Name: 48ba327a2052edf573ae93616c3a180f6d18b4a3</div><div id='time'> Time: 2022-08-21</div><div id='author'> Author: 53039617+xiezipeng-ML@users.noreply.github.com</div><div id='file'> File Name: libai/models/utils/model_utils/t5_loader.py</div><div id='m_class'> M Class Name: T5LoaderHuggerFace</div><div id='n_method'> N Class Name: T5LoaderHuggerFace</div><div id='m_method'> M Method Name: _load_config_from_json(2)</div><div id='n_method'> N Method Name: _load_config_from_json(2)</div><div id='m_parent_class'> M Parent Class: ModelLoaderHuggerFace</div><div id='n_parent_class'> N Parent Class: ModelLoaderHuggerFace</div><div id='m_file'> M File Name: libai/models/utils/model_utils/t5_loader.py</div><div id='n_file'> N File Name: libai/models/utils/model_utils/t5_loader.py</div><div id='m_start'> M Start Line: 267</div><div id='m_end'> M End Line: 283</div><div id='n_start'> N Start Line: 268</div><div id='n_end'> N End Line: 288</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                self.libai_cfg[k] = v

        &#47&#47 update libai_cfg by kwargs
        for k, <a id="change">v</a> in self.kwargs.items():
            <a id="change">self.libai_cfg[k] = </a>v

        &#47&#47 use original BERT residual connection ordering
        self.libai_cfg.apply_residual_post_layernorm = True</code></pre><h3>After Change</h3><pre><code class='java'>
            cfg_dict = json.load(f)

        &#47&#47 update libai_cfg by config.json
        <a id="change">self._update_cfg("vocab_size"</a>, cfg_dict["vocab_size"]<a id="change">)</a>
        <a id="change">self._update_cfg("hidden_size"</a>, cfg_dict["hidden_size"]<a id="change">)</a>
        <a id="change">self._update_cfg("hidden_layers"</a>, cfg_dict["num_hidden_layers"]<a id="change">)</a>
        <a id="change">self._update_cfg("num_attention_heads"</a>, cfg_dict["num_attention_heads"]<a id="change">)</a>
        <a id="change">self._update_cfg("intermediate_size"</a>, cfg_dict["intermediate_size"]<a id="change">)</a>
        <a id="change">self._update_cfg("hidden_dropout_prob"</a>, cfg_dict["hidden_dropout_prob"]<a id="change">)</a>
        <a id="change">self._update_cfg("attention_probs_dropout_prob"</a>, cfg_dict["attention_probs_dropout_prob"]<a id="change">)</a>
        <a id="change">self._update_cfg("max_position_embeddings"</a>, cfg_dict["max_position_embeddings"]<a id="change">)</a>
        <a id="change">self._update_cfg("num_tokentypes"</a>, cfg_dict["type_vocab_size"]<a id="change">)</a>
        <a id="change">self._update_cfg("initializer_range"</a>, cfg_dict["initializer_range"]<a id="change">)</a>
        <a id="change">self._update_cfg("layernorm_eps"</a>, cfg_dict["layer_norm_eps"]<a id="change">)</a>

        &#47&#47 update libai_cfg by kwargs
        for k, <a id="change">v</a> in self.kwargs.items():
            <a id="change">self._update_cfg(</a>k, v<a id="change">)</a>

        &#47&#47 use original BERT residual connection ordering
        self.libai_cfg.apply_residual_post_layernorm = True

        <a id="change">self._update_cfg_log()</a>


class BertLoaderLiBai(ModelLoaderLiBai):
    def __init__(self, model, libai_cfg, pretrained_model_path, **kwargs):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/oneflow-inc/libai/commit/48ba327a2052edf573ae93616c3a180f6d18b4a3#diff-f1533aabc44a7bb611558582c7de5cd17677de66778fed3b41d502017c3469edL228' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43111104</div><div id='project'> Project Name: oneflow-inc/libai</div><div id='commit'> Commit Name: 48ba327a2052edf573ae93616c3a180f6d18b4a3</div><div id='time'> Time: 2022-08-21</div><div id='author'> Author: 53039617+xiezipeng-ML@users.noreply.github.com</div><div id='file'> File Name: libai/models/utils/model_utils/bert_loader.py</div><div id='m_class'> M Class Name: BertLoaderHuggerFace</div><div id='n_method'> N Class Name: BertLoaderHuggerFace</div><div id='m_method'> M Method Name: _load_config_from_json(2)</div><div id='n_method'> N Method Name: _load_config_from_json(2)</div><div id='m_parent_class'> M Parent Class: ModelLoaderHuggerFace</div><div id='n_parent_class'> N Parent Class: ModelLoaderHuggerFace</div><div id='m_file'> M File Name: libai/models/utils/model_utils/bert_loader.py</div><div id='n_file'> N File Name: libai/models/utils/model_utils/bert_loader.py</div><div id='m_start'> M Start Line: 238</div><div id='m_end'> M End Line: 250</div><div id='n_start'> N Start Line: 238</div><div id='n_end'> N End Line: 257</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.libai_cfg.ffn_hidden_size = cfg_dict.get("n_inner", 4 * self.libai_cfg["hidden_size"])

        &#47&#47 update libai_cfg by kwargs
        for k, <a id="change">v</a> in self.kwargs.items():
            <a id="change">self.libai_cfg[k] = </a>v


class GPT2LoaderLiBai(ModelLoaderLiBai):</code></pre><h3>After Change</h3><pre><code class='java'>
            cfg_dict = json.load(f)

        &#47&#47 update libai_cfg by config.json
        <a id="change">self._update_cfg("num_layers"</a>, cfg_dict["n_layer"]<a id="change">)</a>
        <a id="change">self._update_cfg("hidden_size"</a>, cfg_dict["n_embd"]<a id="change">)</a>
        <a id="change">self._update_cfg("num_attention_heads"</a>, cfg_dict["n_head"]<a id="change">)</a>
        <a id="change">self._update_cfg("max_seq_length"</a>, cfg_dict["n_positions"]<a id="change">)</a>
        <a id="change">self._update_cfg("embedding_dropout_prob"</a>, cfg_dict["embd_pdrop"]<a id="change">)</a>
        <a id="change">self._update_cfg("attention_dropout_prob"</a>, cfg_dict["attn_pdrop"]<a id="change">)</a>
        <a id="change">self._update_cfg("output_dropout_prob"</a>, cfg_dict["resid_pdrop"]<a id="change">)</a>
        <a id="change">self._update_cfg("layernorm_epsilon"</a>, cfg_dict["layer_norm_epsilon"]<a id="change">)</a>
        <a id="change">self._update_cfg("vocab_size"</a>, cfg_dict["vocab_size"]<a id="change">)</a>
        <a id="change">self._update_cfg("initializer_range"</a>, cfg_dict["initializer_range"]<a id="change">)</a>
        <a id="change">self._update_cfg(
            "ffn_hidden_size"</a>, cfg_dict.get("n_inner", 4 * self.libai_cfg["hidden_size"])<a id="change">
        )</a>

        &#47&#47 update libai_cfg by kwargs
        for k, <a id="change">v</a> in self.kwargs.items():
            <a id="change">self._update_cfg(</a>k, v<a id="change">)</a>

        <a id="change">self._update_cfg_log()</a>


class GPT2LoaderLiBai(ModelLoaderLiBai):
    def __init__(self, model, libai_cfg, pretrained_model_path, **kwargs):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/oneflow-inc/libai/commit/48ba327a2052edf573ae93616c3a180f6d18b4a3#diff-f7dbd6060457dd4db90f46a22d02c0d5e72e97e6b90567fac376f9d87556487bL137' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43111107</div><div id='project'> Project Name: oneflow-inc/libai</div><div id='commit'> Commit Name: 48ba327a2052edf573ae93616c3a180f6d18b4a3</div><div id='time'> Time: 2022-08-21</div><div id='author'> Author: 53039617+xiezipeng-ML@users.noreply.github.com</div><div id='file'> File Name: libai/models/utils/model_utils/gpt_loader.py</div><div id='m_class'> M Class Name: GPT2LoaderHuggerFace</div><div id='n_method'> N Class Name: GPT2LoaderHuggerFace</div><div id='m_method'> M Method Name: _load_config_from_json(2)</div><div id='n_method'> N Method Name: _load_config_from_json(2)</div><div id='m_parent_class'> M Parent Class: ModelLoaderHuggerFace</div><div id='n_parent_class'> N Parent Class: ModelLoaderHuggerFace</div><div id='m_file'> M File Name: libai/models/utils/model_utils/gpt_loader.py</div><div id='n_file'> N File Name: libai/models/utils/model_utils/gpt_loader.py</div><div id='m_start'> M Start Line: 147</div><div id='m_end'> M End Line: 161</div><div id='n_start'> N Start Line: 147</div><div id='n_end'> N End Line: 165</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.libai_cfg.head_size = cfg_dict["d_kv"]

        &#47&#47 update libai_cfg by kwargs
        for k, <a id="change">v</a> in self.kwargs.items():
            <a id="change">self.libai_cfg[k] = </a>v


class T5LoaderLibai(ModelLoaderLiBai):</code></pre><h3>After Change</h3><pre><code class='java'>
        with open(config_file, mode="r", encoding="utf-8") as f:
            cfg_dict = json.load(f)

        <a id="change">self._update_cfg("vocab_size"</a>, cfg_dict["vocab_size"]<a id="change">)</a>
        <a id="change">self._update_cfg("hidden_size"</a>, cfg_dict["d_model"]<a id="change">)</a>
        <a id="change">self._update_cfg("hidden_layers"</a>, cfg_dict["num_layers"]<a id="change">)</a>
        <a id="change">self._update_cfg("num_attention_heads"</a>, cfg_dict["num_heads"]<a id="change">)</a>
        <a id="change">self._update_cfg("intermediate_size"</a>, cfg_dict["d_ff"]<a id="change">)</a>
        self._update_cfg("hidden_dropout_prob", cfg_dict["dropout_rate"])
        <a id="change">self._update_cfg("attention_probs_dropout_prob"</a>, cfg_dict["dropout_rate"]<a id="change">)</a>
        <a id="change">self._update_cfg("max_position_embeddings"</a>, cfg_dict.get("n_positions", 512)<a id="change">)</a>
        <a id="change">self._update_cfg(
            "relative_attention_num_buckets"</a>, cfg_dict["relative_attention_num_buckets"]<a id="change">
        )</a>
        <a id="change">self._update_cfg("embedding_dropout_prob"</a>, cfg_dict["dropout_rate"]<a id="change">)</a>
        <a id="change">self._update_cfg("initializer_range"</a>, cfg_dict["initializer_factor"]<a id="change">)</a>
        self._update_cfg("layernorm_eps", cfg_dict["layer_norm_epsilon"])
        <a id="change">self._update_cfg("head_size"</a>, cfg_dict["d_kv"]<a id="change">)</a>

        &#47&#47 update libai_cfg by kwargs
        for k, <a id="change">v</a> in self.kwargs.items():
            <a id="change">self._update_cfg(</a>k, v<a id="change">)</a>

        <a id="change">self._update_cfg_log()</a>


class T5LoaderLibai(ModelLoaderLiBai):
    def __init__(self, model, libai_cfg, pretrained_model_path, **kwargs):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/oneflow-inc/libai/commit/48ba327a2052edf573ae93616c3a180f6d18b4a3#diff-cc2bd90ae8c5efb295996ca11833ac662d5fc9d793278756761d9b5b30c5a965L259' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43111106</div><div id='project'> Project Name: oneflow-inc/libai</div><div id='commit'> Commit Name: 48ba327a2052edf573ae93616c3a180f6d18b4a3</div><div id='time'> Time: 2022-08-21</div><div id='author'> Author: 53039617+xiezipeng-ML@users.noreply.github.com</div><div id='file'> File Name: libai/models/utils/model_utils/t5_loader.py</div><div id='m_class'> M Class Name: T5LoaderHuggerFace</div><div id='n_method'> N Class Name: T5LoaderHuggerFace</div><div id='m_method'> M Method Name: _load_config_from_json(2)</div><div id='n_method'> N Method Name: _load_config_from_json(2)</div><div id='m_parent_class'> M Parent Class: ModelLoaderHuggerFace</div><div id='n_parent_class'> N Parent Class: ModelLoaderHuggerFace</div><div id='m_file'> M File Name: libai/models/utils/model_utils/t5_loader.py</div><div id='n_file'> N File Name: libai/models/utils/model_utils/t5_loader.py</div><div id='m_start'> M Start Line: 267</div><div id='m_end'> M End Line: 283</div><div id='n_start'> N Start Line: 268</div><div id='n_end'> N End Line: 288</div><BR>