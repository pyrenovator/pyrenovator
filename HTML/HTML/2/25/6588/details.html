<html><h3>Pattern ID :6588
</h3><img src='22897884.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self._is_weighted = is_weighted
        self._embedding_bag_configs: List[EmbeddingBagConfig] = embedding_configs
        self.embedding_bags: nn.ModuleList = nn.ModuleList()
        <a id="change">self._embedding_names: List[str] = </a><a id="change">[]</a>
        self._lengths_per_embedding: List[int] = []
        <a id="change">shared_feature</a>: Dict[str, bool] = <a id="change">{}</a>
        table_names = set()
        for emb_config in self._embedding_bag_configs:
            if emb_config.name in table_names:
                raise ValueError(f"Duplicate table name {emb_config.name}")
            table_names.add(emb_config.name)
            emb_module = IntNBitTableBatchedEmbeddingBagsCodegen(
                embedding_specs=[
                    (
                        "",
                        emb_config.num_embeddings,
                        emb_config.embedding_dim,
                        data_type_to_sparse_type(emb_config.data_type),
                        EmbeddingLocation.HOST
                        if device.type == "cpu"
                        else EmbeddingLocation.DEVICE,
                    )
                ],
                pooling_mode=pooling_type_to_pooling_mode(emb_config.pooling),
                weight_lists=[table_name_to_quantized_weights[emb_config.name]],
                device=device,
            )
            self.embedding_bags.append(emb_module)
            if not emb_config.feature_names:
                emb_config.feature_names = [emb_config.name]
            <a id="change">for </a>feature_name in emb_config.feature_names<a id="change">:
                </a><a id="change">if feature_name not in shared_feature</a>:
                    <a id="change">shared_feature[feature_name] = </a>False
                else:
                    <a id="change">shared_feature[feature_name] = </a>True
                self._lengths_per_embedding.append(emb_config.embedding_dim)

        <a id="change">for </a>emb_config in self._embedding_bag_configs<a id="change">:
            </a><a id="change">for </a>feature_name in emb_config.feature_names<a id="change">:
                </a><a id="change">if shared_feature[feature_name]</a>:
                    <a id="change">self._embedding_names.append(feature_name</a><a id="change"> + "@" + </a>emb_config.name<a id="change">)</a>
                else:
                    <a id="change">self._embedding_names.append(feature_name</a><a id="change">)</a>

    def forward(
        self,
        features: KeyedJaggedTensor,</code></pre><h3>After Change</h3><pre><code class='java'>
                len(emb_config.feature_names) * [emb_config.embedding_dim]
            )

        self._embedding_names: List[str] = <a id="change">ebc_get_embedding_names(</a>embedding_configs<a id="change">)</a>

    def forward(
        self,
        features: KeyedJaggedTensor,</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 21</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/e5ba978877d387d5b5579f3c7781e57ad2822235#diff-30aabdcc064c9c3eee50a1dfb7ca08d561f2c524858be4eee22357c7e2befe70L160' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 22897884</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: e5ba978877d387d5b5579f3c7781e57ad2822235</div><div id='time'> Time: 2022-02-19</div><div id='author'> Author: xingl@fb.com</div><div id='file'> File Name: torchrec/quant/embedding_modules.py</div><div id='m_class'> M Class Name: EmbeddingBagCollection</div><div id='n_method'> N Class Name: EmbeddingBagCollection</div><div id='m_method'> M Method Name: __init__(5)</div><div id='n_method'> N Method Name: __init__(5)</div><div id='m_parent_class'> M Parent Class: EmbeddingBagCollectionInterface</div><div id='n_parent_class'> N Parent Class: EmbeddingBagCollectionInterface</div><div id='m_file'> M File Name: torchrec/quant/embedding_modules.py</div><div id='n_file'> N File Name: torchrec/quant/embedding_modules.py</div><div id='m_start'> M Start Line: 160</div><div id='m_end'> M End Line: 203</div><div id='n_start'> N Start Line: 163</div><div id='n_end'> N End Line: 190</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self._is_weighted = is_weighted
        self._embedding_bag_configs: List[EmbeddingBagConfig] = embedding_configs
        self.embedding_bags: nn.ModuleList = nn.ModuleList()
        <a id="change">self._embedding_names: List[str] = </a><a id="change">[]</a>
        self._lengths_per_embedding: List[int] = []
        <a id="change">shared_feature</a>: Dict[str, bool] = <a id="change">{}</a>
        table_names = set()
        for emb_config in self._embedding_bag_configs:
            if emb_config.name in table_names:
                raise ValueError(f"Duplicate table name {emb_config.name}")
            table_names.add(emb_config.name)
            emb_module = IntNBitTableBatchedEmbeddingBagsCodegen(
                embedding_specs=[
                    (
                        "",
                        emb_config.num_embeddings,
                        emb_config.embedding_dim,
                        data_type_to_sparse_type(emb_config.data_type),
                        EmbeddingLocation.HOST
                        if device.type == "cpu"
                        else EmbeddingLocation.DEVICE,
                    )
                ],
                pooling_mode=pooling_type_to_pooling_mode(emb_config.pooling),
                weight_lists=[table_name_to_quantized_weights[emb_config.name]],
                device=device,
            )
            self.embedding_bags.append(emb_module)
            if not emb_config.feature_names:
                emb_config.feature_names = [emb_config.name]
            <a id="change">for </a><a id="change">feature_name</a> in emb_config.feature_names<a id="change">:
                </a><a id="change">if feature_name not in shared_feature</a>:
                    <a id="change">shared_feature[feature_name] = </a>False
                else:
                    <a id="change">shared_feature[feature_name] = </a>True
                self._lengths_per_embedding.append(emb_config.embedding_dim)

        <a id="change">for </a><a id="change">emb_config</a> in self._embedding_bag_configs<a id="change">:
            </a><a id="change">for </a><a id="change">feature_name</a> in emb_config.feature_names<a id="change">:
                </a><a id="change">if shared_feature[feature_name]</a>:
                    <a id="change">self._embedding_names.append(</a>feature_name<a id="change"> + "@" + </a>emb_config.name<a id="change">)</a>
                else:
                    <a id="change">self._embedding_names.append(</a>feature_name<a id="change">)</a>

    def forward(
        self,
        features: KeyedJaggedTensor,</code></pre><h3>After Change</h3><pre><code class='java'>
                len(emb_config.feature_names) * [emb_config.embedding_dim]
            )

        self._embedding_names: List[str] = <a id="change">ebc_get_embedding_names(</a>embedding_configs<a id="change">)</a>

    def forward(
        self,
        features: KeyedJaggedTensor,</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/e5ba978877d387d5b5579f3c7781e57ad2822235#diff-30aabdcc064c9c3eee50a1dfb7ca08d561f2c524858be4eee22357c7e2befe70L149' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 22897887</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: e5ba978877d387d5b5579f3c7781e57ad2822235</div><div id='time'> Time: 2022-02-19</div><div id='author'> Author: xingl@fb.com</div><div id='file'> File Name: torchrec/quant/embedding_modules.py</div><div id='m_class'> M Class Name: EmbeddingBagCollection</div><div id='n_method'> N Class Name: EmbeddingBagCollection</div><div id='m_method'> M Method Name: __init__(5)</div><div id='n_method'> N Method Name: __init__(5)</div><div id='m_parent_class'> M Parent Class: EmbeddingBagCollectionInterface</div><div id='n_parent_class'> N Parent Class: EmbeddingBagCollectionInterface</div><div id='m_file'> M File Name: torchrec/quant/embedding_modules.py</div><div id='n_file'> N File Name: torchrec/quant/embedding_modules.py</div><div id='m_start'> M Start Line: 160</div><div id='m_end'> M End Line: 203</div><div id='n_start'> N Start Line: 163</div><div id='n_end'> N End Line: 190</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self._is_weighted = is_weighted
        self.embedding_bags: nn.ModuleDict = nn.ModuleDict()
        self._embedding_bag_configs = tables
        <a id="change">self._embedding_names: List[str] = </a><a id="change">[]</a>
        self._lengths_per_embedding: List[int] = []
        table_names = set()
        <a id="change">shared_feature</a>: Dict[str, bool] = <a id="change">{}</a>
        for embedding_config in tables:
            if embedding_config.name in table_names:
                raise ValueError(f"Duplicate table name {embedding_config.name}")
            table_names.add(embedding_config.name)
            dtype = (
                torch.float32
                if embedding_config.data_type == DataType.FP32
                else torch.float16
            )
            self.embedding_bags[embedding_config.name] = nn.EmbeddingBag(
                num_embeddings=embedding_config.num_embeddings,
                embedding_dim=embedding_config.embedding_dim,
                mode=_to_mode(embedding_config.pooling),
                device=device,
                include_last_offset=True,
                dtype=dtype,
            )
            if not embedding_config.feature_names:
                embedding_config.feature_names = [embedding_config.name]
            <a id="change">for </a><a id="change">feature_name</a> in embedding_config.feature_names<a id="change">:
                </a><a id="change">if feature_name not in shared_feature</a>:
                    <a id="change">shared_feature[feature_name] = </a>False
                else:
                    <a id="change">shared_feature[feature_name] = </a>True
                self._lengths_per_embedding.append(embedding_config.embedding_dim)

        <a id="change">for </a><a id="change">embedding_config</a> in tables<a id="change">:
            </a><a id="change">for </a><a id="change">feature_name</a> in embedding_config.feature_names<a id="change">:
                </a><a id="change">if shared_feature[feature_name]</a>:
                    <a id="change">self._embedding_names.append(
                        </a>feature_name<a id="change"> + "@" + </a>embedding_config.name<a id="change">
                    )</a>
                else:
                    <a id="change">self._embedding_names.append(</a>feature_name<a id="change">)</a>

    def forward(self, features: KeyedJaggedTensor) -&gt; KeyedTensor:
        
        Args:</code></pre><h3>After Change</h3><pre><code class='java'>
                len(embedding_config.feature_names) * [embedding_config.embedding_dim]
            )

        self._embedding_names: List[str] = <a id="change">ebc_get_embedding_names(</a>tables<a id="change">)</a>

    def forward(self, features: KeyedJaggedTensor) -&gt; KeyedTensor:
        
        Args:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/86cb29ffb86fc8f6c23190868269e3e2acb6a0f5#diff-5d3b5f2640c42dc99391b13cf90ae4597fe6b72a0705d5a9bdaae9c8e73d2fd8L116' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 22897883</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: 86cb29ffb86fc8f6c23190868269e3e2acb6a0f5</div><div id='time'> Time: 2022-02-16</div><div id='author'> Author: xingl@fb.com</div><div id='file'> File Name: torchrec/modules/embedding_modules.py</div><div id='m_class'> M Class Name: EmbeddingBagCollection</div><div id='n_method'> N Class Name: EmbeddingBagCollection</div><div id='m_method'> M Method Name: __init__(4)</div><div id='n_method'> N Method Name: __init__(4)</div><div id='m_parent_class'> M Parent Class: EmbeddingBagCollectionInterface</div><div id='n_parent_class'> N Parent Class: EmbeddingBagCollectionInterface</div><div id='m_file'> M File Name: torchrec/modules/embedding_modules.py</div><div id='n_file'> N File Name: torchrec/modules/embedding_modules.py</div><div id='m_start'> M Start Line: 127</div><div id='m_end'> M End Line: 166</div><div id='n_start'> N Start Line: 147</div><div id='n_end'> N End Line: 170</div><BR>