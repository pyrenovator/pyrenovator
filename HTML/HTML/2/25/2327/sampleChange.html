<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                &#47&#47 we use new save ways, only save the state_dict, and the extension changes to pt
                torch.save(net.state_dict(), SAVE_PATH + "" + ".pth")

            <a id="change">write(writer</a>, loss_value, loss_list, action_accuracy, 
                  move_camera_accuracy, non_camera_accuracy, short_important_accuracy,
                  <a id="change">batch_iter</a><a id="change">)</a>

            gc.collect()

        if True:</code></pre><h3>After Change</h3><pre><code class='java'>

    now = datetime.datetime.now()
    summary_path = "./log/" + now.strftime("%Y%m%d-%H%M%S") + "/"
    <a id="change">writer</a> = SummaryWriter(summary_path)

    &#47&#47 model path
    SAVE_PATH = os.path.join(MODEL_PATH, MODEL + "_" + time.strftime("%y-%m-%d_%H-%M-%S", time.localtime()))

    epoch_start = time.time()
    <a id="change">batch_iter</a> = 0

    for epoch in range(NUM_EPOCHS):

        loss_sum = 0

        for batch_idx, (features, labels) in enumerate(train_loader):
            &#47&#47 put model in train mode
            net.train()
            start = time.time()

            feature_tensor = features.to(device).float()
            labels_tensor = labels.to(device).float()
            del features, labels

            loss, loss_list, \
                acc_num_list = Loss.get_sl_loss_for_tensor(feature_tensor, 
                                                           labels_tensor, net, 
                                                           decrease_smart_opertaion=True,
                                                           return_important=True,
                                                           only_consider_small=True,
                                                           include_location_accuracy=True)
            del feature_tensor, labels_tensor

            optimizer.zero_grad()
            loss.backward() 
            optimizer.step()

            &#47&#47 add a grad clip
            parameters = [p for p in net.parameters() if p is not None and p.requires_grad]
            torch.nn.utils.clip_grad_norm_(parameters, CLIP)

            loss_value = float(loss.item())

            loss_sum += loss_value
            del loss

            action_accuracy = acc_num_list[0] / (acc_num_list[1] + 1e-9)
            move_camera_accuracy = acc_num_list[2] / (acc_num_list[3] + 1e-9)
            non_camera_accuracy = acc_num_list[4] / (acc_num_list[5] + 1e-9)
            short_important_accuracy = acc_num_list[6] / (acc_num_list[7] + 1e-9)

            location_accuracy<a id="change"> = acc_num_list</a><a id="change">[8]</a><a id="change"> / </a>(<a id="change">acc_num_list[9]</a><a id="change"> + 1e-9</a>)
            location_distance = acc_num_list[11] / (acc_num_list[9] + 1e-9)

            batch_time = time.time() - start

            batch_iter += 1
            print(&quotbatch_iter&quot, batch_iter)

            if batch_iter % EVAL_INTERFEVL == 0:
                print(&quotEpoch: [{}/{}]| loss: {:.3f} | acc: {:.3f} | batch time: {:.3f}s &quot.format(
                    batch_iter, epoch, loss_sum / (batch_iter + 1), action_accuracy, batch_time))

                &#47&#47 torch.save(net, SAVE_PATH + "" + ".pkl")
                &#47&#47 we use new save ways, only save the state_dict, and the extension changes to pt
                torch.save(net.state_dict(), SAVE_PATH + "" + ".pth")

            <a id="change">write(writer</a>, loss_value, loss_list, action_accuracy, 
                  move_camera_accuracy, non_camera_accuracy, short_important_accuracy,
                  location_accuracy, location_distance,
                  <a id="change">batch_iter</a><a id="change">)</a>

            gc.collect()

        if True:
            print(&quoteval begin&quot)
            val_loss, val_acc = eval(net, val_set, val_loader, device)
            print(&quoteval end&quot)

            print("Val loss: {:.6f}.".format(val_loss))
            writer.add_scalar(&quotVal/Loss&quot, val_loss, batch_iter)

            &#47&#47 for accuracy of actions in val
            print("Val action acc: {:.6f}.".format(val_acc[0]))
            writer.add_scalar(&quotVal/action Acc&quot, val_acc[0], batch_iter)
            print("Val move_camera acc: {:.6f}.".format(val_acc[1]))
            writer.add_scalar(&quotVal/move_camera Acc&quot, val_acc[1], batch_iter)
            print("Val non_camera acc: {:.6f}.".format(val_acc[2]))
            writer.add_scalar(&quotVal/non_camera Acc&quot, val_acc[2], batch_iter)
            print("Val short_important acc: {:.6f}.".format(val_acc[3]))
            writer.add_scalar(&quotVal/short_important Acc&quot, val_acc[3], batch_iter)
            <a id="change">print("Val location_accuracy acc: {:.6f}."</a><a id="change">.format(val_acc</a><a id="change">[4]))</a>
            writer.add_scalar(&quotVal/location_accuracy Acc&quot, val_acc[4], batch_iter)
            print("Val location_distance acc: {:.6f}.".format(val_acc[5]))
            <a id="change">writer.add_scalar(&quotVal/location_distance Acc&quot</a>, <a id="change">val_acc[5]</a>, <a id="change">batch_iter</a><a id="change">)</a>

            del val_loss, val_acc

            gc.collect()</code></pre>