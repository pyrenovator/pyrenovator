<html><h3>Pattern ID :21785
</h3><img src='69417059.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def forward(self, text, view=False):
        with torch.no_grad():
            phones = self.text2phone.string_to_tensor(text).squeeze(0).long().to(torch.device(self.device))
            mel = <a id="change">self.phone2mel(phones, speaker_embedding=self.speaker_embedding).transpose(0</a>, <a id="change">1</a><a id="change">)</a>
            wave = self.mel2wav(mel.unsqueeze(0)).squeeze(0).squeeze(0)
        if view:
            import matplotlib.pyplot as plt
            import librosa.display as lbd</code></pre><h3>After Change</h3><pre><code class='java'>
                         hop_length=256)
            ax[0].yaxis.set_visible(False)
            ax[1].yaxis.set_visible(False)
            <a id="change">duration_splits</a><a id="change">, label_positions</a> = cumsum_durations(durations.cpu().numpy())
            <a id="change">ax[1].set_xticks(duration_splits</a><a id="change">, minor=True)</a>
            <a id="change">ax[1]</a>.xaxis.grid(True, which=&quotminor&quot)
            <a id="change">ax[1].set_xticks(label_positions</a><a id="change">, minor=False)</a>
            <a id="change">ax[1]</a>.set_xticklabels(self.text2phone.get_phone_string(text))
            ax[0].set_title(text)
            plt.subplots_adjust(left=0.05, bottom=0.1, right=0.95, top=.9, wspace=0.0, hspace=0.0)
            plt.show()</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/4bbfcda47cf77f65ad0e557f8a06a174197d1bfc#diff-4b2f26f3d3287b567ffaec37b852ede237af7e76f93c389c33396f41ee6a1cf0L32' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 69417059</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 4bbfcda47cf77f65ad0e557f8a06a174197d1bfc</div><div id='time'> Time: 2021-07-24</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: InferenceInterfaces/Thorsten_FastSpeechInference.py</div><div id='m_class'> M Class Name: Thorsten_FastSpeechInference</div><div id='n_method'> N Class Name: Thorsten_FastSpeechInference</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: InferenceInterfaces/Thorsten_FastSpeechInference.py</div><div id='n_file'> N File Name: InferenceInterfaces/Thorsten_FastSpeechInference.py</div><div id='m_start'> M Start Line: 32</div><div id='m_end'> M End Line: 41</div><div id='n_start'> N Start Line: 32</div><div id='n_end'> N End Line: 56</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def forward(self, text, view=False):
        with torch.no_grad():
            phones = self.text2phone.string_to_tensor(text).squeeze(0).long().to(torch.device(self.device))
            mel = <a id="change">self.phone2mel(phones, speaker_embedding=self.speaker_embedding).transpose(0</a>, <a id="change">1</a><a id="change">)</a>
            wave = self.mel2wav(mel.unsqueeze(0)).squeeze(0).squeeze(0)
        if view:
            import matplotlib.pyplot as plt
            import librosa.display as lbd</code></pre><h3>After Change</h3><pre><code class='java'>
                         hop_length=256)
            ax[0].yaxis.set_visible(False)
            ax[1].yaxis.set_visible(False)
            duration_splits<a id="change">, label_positions</a> = cumsum_durations(durations.cpu().numpy())
            <a id="change">ax[1].set_xticks(</a>duration_splits<a id="change">, minor=True)</a>
            <a id="change">ax[1]</a>.xaxis.grid(True, which=&quotminor&quot)
            <a id="change">ax[1].set_xticks(</a>label_positions<a id="change">, minor=False)</a>
            <a id="change">ax[1]</a>.set_xticklabels(self.text2phone.get_phone_string(text))
            ax[0].set_title(text)
            plt.subplots_adjust(left=0.05, bottom=0.1, right=0.95, top=.9, wspace=0.0, hspace=0.0)
            plt.show()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/4bbfcda47cf77f65ad0e557f8a06a174197d1bfc#diff-98c0a7845dd35e306bac5af5963f246159ffc54c9cc93cdac53373929d69df73L31' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 69417058</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 4bbfcda47cf77f65ad0e557f8a06a174197d1bfc</div><div id='time'> Time: 2021-07-24</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: InferenceInterfaces/LJSpeech_FastSpeechInference.py</div><div id='m_class'> M Class Name: LJSpeech_FastSpeechInference</div><div id='n_method'> N Class Name: LJSpeech_FastSpeechInference</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: InferenceInterfaces/LJSpeech_FastSpeechInference.py</div><div id='n_file'> N File Name: InferenceInterfaces/LJSpeech_FastSpeechInference.py</div><div id='m_start'> M Start Line: 33</div><div id='m_end'> M End Line: 43</div><div id='n_start'> N Start Line: 33</div><div id='n_end'> N End Line: 57</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def forward(self, text, view=False):
        with torch.no_grad():
            phones = self.text2phone.string_to_tensor(text).squeeze(0).long().to(torch.device(self.device))
            mel = <a id="change">self.phone2mel(phones, speaker_embedding=self.speaker_embedding).transpose(0</a>, <a id="change">1</a><a id="change">)</a>
            wave = self.mel2wav(mel.unsqueeze(0)).squeeze(0).squeeze(0)
        if view:
            import matplotlib.pyplot as plt
            import librosa.display as lbd</code></pre><h3>After Change</h3><pre><code class='java'>
                         hop_length=256)
            ax[0].yaxis.set_visible(False)
            ax[1].yaxis.set_visible(False)
            duration_splits<a id="change">, label_positions</a> = cumsum_durations(durations.cpu().numpy())
            <a id="change">ax[1].set_xticks(</a>duration_splits<a id="change">, minor=True)</a>
            <a id="change">ax[1]</a>.xaxis.grid(True, which=&quotminor&quot)
            <a id="change">ax[1].set_xticks(</a>label_positions<a id="change">, minor=False)</a>
            <a id="change">ax[1]</a>.set_xticklabels(self.text2phone.get_phone_string(text))
            ax[0].set_title(text)
            plt.subplots_adjust(left=0.05, bottom=0.1, right=0.95, top=.9, wspace=0.0, hspace=0.0)
            plt.show()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/4bbfcda47cf77f65ad0e557f8a06a174197d1bfc#diff-7d3b87a93b0bc1f791ef878d1df587515e570f34cd1a868873e0fd6e7f941af5L31' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 69417057</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 4bbfcda47cf77f65ad0e557f8a06a174197d1bfc</div><div id='time'> Time: 2021-07-24</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: InferenceInterfaces/Nancy_FastSpeechInference.py</div><div id='m_class'> M Class Name: Nancy_FastSpeechInference</div><div id='n_method'> N Class Name: Nancy_FastSpeechInference</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: InferenceInterfaces/Nancy_FastSpeechInference.py</div><div id='n_file'> N File Name: InferenceInterfaces/Nancy_FastSpeechInference.py</div><div id='m_start'> M Start Line: 33</div><div id='m_end'> M End Line: 43</div><div id='n_start'> N Start Line: 33</div><div id='n_end'> N End Line: 57</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def forward(self, text, view=False):
        with torch.no_grad():
            phones = self.text2phone.string_to_tensor(text).squeeze(0).long().to(torch.device(self.device))
            mel = <a id="change">self.phone2mel(phones, speaker_embedding=self.speaker_embedding).transpose(0</a>, <a id="change">1</a><a id="change">)</a>
            wave = self.mel2wav(mel.unsqueeze(0)).squeeze(0).squeeze(0)
        if view:
            import matplotlib.pyplot as plt
            import librosa.display as lbd</code></pre><h3>After Change</h3><pre><code class='java'>
                         hop_length=256)
            ax[0].yaxis.set_visible(False)
            ax[1].yaxis.set_visible(False)
            duration_splits<a id="change">, label_positions</a> = cumsum_durations(durations.cpu().numpy())
            <a id="change">ax[1].set_xticks(</a>duration_splits<a id="change">, minor=True)</a>
            <a id="change">ax[1]</a>.xaxis.grid(True, which=&quotminor&quot)
            <a id="change">ax[1].set_xticks(</a>label_positions<a id="change">, minor=False)</a>
            <a id="change">ax[1]</a>.set_xticklabels(self.text2phone.get_phone_string(text))
            ax[0].set_title(text)
            plt.subplots_adjust(left=0.05, bottom=0.1, right=0.95, top=.9, wspace=0.0, hspace=0.0)
            plt.show()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/4bbfcda47cf77f65ad0e557f8a06a174197d1bfc#diff-4b2f26f3d3287b567ffaec37b852ede237af7e76f93c389c33396f41ee6a1cf0L30' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 69417063</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 4bbfcda47cf77f65ad0e557f8a06a174197d1bfc</div><div id='time'> Time: 2021-07-24</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: InferenceInterfaces/Thorsten_FastSpeechInference.py</div><div id='m_class'> M Class Name: Thorsten_FastSpeechInference</div><div id='n_method'> N Class Name: Thorsten_FastSpeechInference</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: InferenceInterfaces/Thorsten_FastSpeechInference.py</div><div id='n_file'> N File Name: InferenceInterfaces/Thorsten_FastSpeechInference.py</div><div id='m_start'> M Start Line: 32</div><div id='m_end'> M End Line: 41</div><div id='n_start'> N Start Line: 32</div><div id='n_end'> N End Line: 56</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def forward(self, text, view=False):
        with torch.no_grad():
            phones = self.text2phone.string_to_tensor(text).squeeze(0).long().to(torch.device(self.device))
            mel = <a id="change">self.phone2mel(phones, speaker_embedding=self.speaker_embedding).transpose(0</a>, <a id="change">1</a><a id="change">)</a>
            wave = self.mel2wav(mel.unsqueeze(0)).squeeze(0).squeeze(0)
        if view:
            import matplotlib.pyplot as plt
            import librosa.display as lbd</code></pre><h3>After Change</h3><pre><code class='java'>
                         hop_length=256)
            ax[0].yaxis.set_visible(False)
            ax[1].yaxis.set_visible(False)
            duration_splits<a id="change">, label_positions</a> = cumsum_durations(durations.cpu().numpy())
            <a id="change">ax[1].set_xticks(</a>duration_splits<a id="change">, minor=True)</a>
            <a id="change">ax[1]</a>.xaxis.grid(True, which=&quotminor&quot)
            <a id="change">ax[1].set_xticks(</a>label_positions<a id="change">, minor=False)</a>
            <a id="change">ax[1]</a>.set_xticklabels(self.text2phone.get_phone_string(text))
            ax[0].set_title(text)
            plt.subplots_adjust(left=0.05, bottom=0.1, right=0.95, top=.9, wspace=0.0, hspace=0.0)
            plt.show()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/digitalphonetics/ims-toucan/commit/4bbfcda47cf77f65ad0e557f8a06a174197d1bfc#diff-74abc2e7bebe273ec5fa359ca3a9ef79f13947e109a095575f6b818f4e7f438cL32' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 69417071</div><div id='project'> Project Name: digitalphonetics/ims-toucan</div><div id='commit'> Commit Name: 4bbfcda47cf77f65ad0e557f8a06a174197d1bfc</div><div id='time'> Time: 2021-07-24</div><div id='author'> Author: florian.lux@ims.uni-stuttgart.de</div><div id='file'> File Name: InferenceInterfaces/LibriTTS_FastSpeechInference.py</div><div id='m_class'> M Class Name: LibriTTS_FastSpeechInference</div><div id='n_method'> N Class Name: LibriTTS_FastSpeechInference</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: InferenceInterfaces/LibriTTS_FastSpeechInference.py</div><div id='n_file'> N File Name: InferenceInterfaces/LibriTTS_FastSpeechInference.py</div><div id='m_start'> M Start Line: 34</div><div id='m_end'> M End Line: 43</div><div id='n_start'> N Start Line: 34</div><div id='n_end'> N End Line: 58</div><BR>