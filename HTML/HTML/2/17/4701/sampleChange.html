<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            Tuple[PoolingType, DataType], List[EmbeddingBagConfig]
        ] = defaultdict(list)
        self._length_per_key: List[int] = []
        self._emb_modules: nn.ModuleList = <a id="change">nn.ModuleList()</a>
        self._output_dtype = output_dtype

        table_names = set()
        for table in self._embedding_bag_configs:</code></pre><h3>After Change</h3><pre><code class='java'>
        self._length_per_key: List[int] = []
        &#47&#47 Registering in a List instead of ModuleList because we want don&quott want them to be auto-registered.
        &#47&#47 Their states will be modified via self.embedding_bags
        self._emb_modules: List[nn.Module] = <a id="change">[]</a>
        self._output_dtype = output_dtype

        table_names = set()
        for table in self._embedding_bag_configs:
            if table.name in table_names:
                raise ValueError(f"Duplicate table name {table.name}")
            table_names.add(table.name)
            self._length_per_key.extend(
                [table.embedding_dim] * len(table.feature_names)
            )
            key = (table.pooling, table.data_type)
            self._key_to_tables[key].append(table)

        self._sum_length_per_key: int = sum(self._length_per_key)

        location = (
            EmbeddingLocation.HOST if device.type == "cpu" else EmbeddingLocation.DEVICE
        )

        for key, emb_configs in self._key_to_tables.items():
            (pooling, data_type) = key
            embedding_specs = []
            weight_lists = []
            for table in emb_configs:
                embedding_specs.append(
                    (
                        table.name,
                        table.num_embeddings,
                        table.embedding_dim,
                        data_type_to_sparse_type(data_type),
                        location,
                    )
                )
                weight_lists.append(table_name_to_quantized_weights[table.name])

            emb_module = IntNBitTableBatchedEmbeddingBagsCodegen(
                embedding_specs=embedding_specs,
                pooling_mode=pooling_type_to_pooling_mode(pooling),
                weight_lists=weight_lists,
                device=device,
                output_dtype=data_type_to_sparse_type(dtype_to_data_type(output_dtype)),
                row_alignment=16,
            )
            self._emb_modules.append(emb_module)

        self._embedding_names: List[str] = list(
            itertools.chain(*get_embedding_names_by_table(self._embedding_bag_configs))
        )
        &#47&#47 We map over the parameters from FBGEMM backed kernels to the canonical nn.EmbeddingBag
        &#47&#47 representation. This provides consistency between this class and the EmbeddingBagCollection
        &#47&#47 nn.Module API calls (state_dict, named_modules, etc)
        <a id="change">self.embedding_bags: nn.ModuleDict = </a><a id="change">nn.ModuleDict()</a>
        <a id="change">for </a>(_key, tables), <a id="change">emb_module</a> in <a id="change">zip(
            </a><a id="change">self._key_to_tables.items()</a>, self._emb_modules<a id="change">
        )</a><a id="change">:
            for </a><a id="change">embedding_config</a>, (weight, _) in <a id="change">zip(
                </a>tables, <a id="change">emb_module.split_embedding_weights(split_scale_shifts=False)
            ):
                self.embedding_bags</a><a id="change">[embedding_config.name] = torch.nn.Module()</a>
                &#47&#47 register as a buffer so it&quots exposed in state_dict.
                &#47&#47 however, since this is only needed for inference, we do not need to expose it as part of parameters.
                &#47&#47 Additionally, we cannot expose uint8 weights as parameters due to autograd restrictions.
                <a id="change">self.embedding_bags[embedding_config.name]</a>.register_buffer(
                    "weight", weight
                )
</code></pre>