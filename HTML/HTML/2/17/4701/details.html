<html><h3>Pattern ID :4701
</h3><img src='16759559.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            Tuple[PoolingType, DataType], List[EmbeddingBagConfig]
        ] = defaultdict(list)
        self._length_per_key: List[int] = []
        self._emb_modules: nn.ModuleList = <a id="change">nn.ModuleList()</a>
        self._output_dtype = output_dtype

        table_names = set()
        for table in self._embedding_bag_configs:</code></pre><h3>After Change</h3><pre><code class='java'>
        self._length_per_key: List[int] = []
        &#47&#47 Registering in a List instead of ModuleList because we want don&quott want them to be auto-registered.
        &#47&#47 Their states will be modified via self.embedding_bags
        self._emb_modules: List[nn.Module] = <a id="change">[]</a>
        self._output_dtype = output_dtype

        table_names = set()
        for table in self._embedding_bag_configs:
            if table.name in table_names:
                raise ValueError(f"Duplicate table name {table.name}")
            table_names.add(table.name)
            self._length_per_key.extend(
                [table.embedding_dim] * len(table.feature_names)
            )
            key = (table.pooling, table.data_type)
            self._key_to_tables[key].append(table)

        self._sum_length_per_key: int = sum(self._length_per_key)

        location = (
            EmbeddingLocation.HOST if device.type == "cpu" else EmbeddingLocation.DEVICE
        )

        for key, emb_configs in self._key_to_tables.items():
            (pooling, data_type) = key
            embedding_specs = []
            weight_lists = []
            for table in emb_configs:
                embedding_specs.append(
                    (
                        table.name,
                        table.num_embeddings,
                        table.embedding_dim,
                        data_type_to_sparse_type(data_type),
                        location,
                    )
                )
                weight_lists.append(table_name_to_quantized_weights[table.name])

            emb_module = IntNBitTableBatchedEmbeddingBagsCodegen(
                embedding_specs=embedding_specs,
                pooling_mode=pooling_type_to_pooling_mode(pooling),
                weight_lists=weight_lists,
                device=device,
                output_dtype=data_type_to_sparse_type(dtype_to_data_type(output_dtype)),
                row_alignment=16,
            )
            self._emb_modules.append(emb_module)

        self._embedding_names: List[str] = list(
            itertools.chain(*get_embedding_names_by_table(self._embedding_bag_configs))
        )
        &#47&#47 We map over the parameters from FBGEMM backed kernels to the canonical nn.EmbeddingBag
        &#47&#47 representation. This provides consistency between this class and the EmbeddingBagCollection
        &#47&#47 nn.Module API calls (state_dict, named_modules, etc)
        <a id="change">self.embedding_bags: nn.ModuleDict = </a><a id="change">nn.ModuleDict()</a>
        <a id="change">for </a>(_key, tables), <a id="change">emb_module</a> in <a id="change">zip(
            </a><a id="change">self._key_to_tables.items()</a>, self._emb_modules<a id="change">
        )</a><a id="change">:
            for </a><a id="change">embedding_config</a>, (weight, _) in <a id="change">zip(
                </a>tables, <a id="change">emb_module.split_embedding_weights(split_scale_shifts=False)
            ):
                self.embedding_bags</a><a id="change">[embedding_config.name] = torch.nn.Module()</a>
                &#47&#47 register as a buffer so it&quots exposed in state_dict.
                &#47&#47 however, since this is only needed for inference, we do not need to expose it as part of parameters.
                &#47&#47 Additionally, we cannot expose uint8 weights as parameters due to autograd restrictions.
                <a id="change">self.embedding_bags[embedding_config.name]</a>.register_buffer(
                    "weight", weight
                )
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 14</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/d4eb3e9556a0593ea23c6908df76445bb9843d79#diff-30aabdcc064c9c3eee50a1dfb7ca08d561f2c524858be4eee22357c7e2befe70L185' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 16759559</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: d4eb3e9556a0593ea23c6908df76445bb9843d79</div><div id='time'> Time: 2022-07-27</div><div id='author'> Author: yingliufb@fb.com</div><div id='file'> File Name: torchrec/quant/embedding_modules.py</div><div id='m_class'> M Class Name: EmbeddingBagCollection</div><div id='n_method'> N Class Name: EmbeddingBagCollection</div><div id='m_method'> M Method Name: __init__(6)</div><div id='n_method'> N Method Name: __init__(6)</div><div id='m_parent_class'> M Parent Class: EmbeddingBagCollectionInterface</div><div id='n_parent_class'> N Parent Class: EmbeddingBagCollectionInterface</div><div id='m_file'> M File Name: torchrec/quant/embedding_modules.py</div><div id='n_file'> N File Name: torchrec/quant/embedding_modules.py</div><div id='m_start'> M Start Line: 190</div><div id='m_end'> M End Line: 190</div><div id='n_start'> N Start Line: 185</div><div id='n_end'> N End Line: 258</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            Tuple[PoolingType, DataType], List[EmbeddingBagConfig]
        ] = defaultdict(list)
        self._length_per_key: List[int] = []
        self._emb_modules: nn.ModuleList = <a id="change">nn.ModuleList()</a>
        self._output_dtype = output_dtype

        table_names = set()
        for table in self._embedding_bag_configs:</code></pre><h3>After Change</h3><pre><code class='java'>
        self._length_per_key: List[int] = []
        &#47&#47 Registering in a List instead of ModuleList because we want don&quott want them to be auto-registered.
        &#47&#47 Their states will be modified via self.embedding_bags
        self._emb_modules: List[nn.Module] = <a id="change">[]</a>
        self._output_dtype = output_dtype

        table_names = set()
        for table in self._embedding_bag_configs:
            if table.name in table_names:
                raise ValueError(f"Duplicate table name {table.name}")
            table_names.add(table.name)
            self._length_per_key.extend(
                [table.embedding_dim] * len(table.feature_names)
            )
            key = (table.pooling, table.data_type)
            self._key_to_tables[key].append(table)

        self._sum_length_per_key: int = sum(self._length_per_key)

        location = (
            EmbeddingLocation.HOST if device.type == "cpu" else EmbeddingLocation.DEVICE
        )

        for key, emb_configs in self._key_to_tables.items():
            (pooling, data_type) = key
            embedding_specs = []
            weight_lists = []
            for table in emb_configs:
                embedding_specs.append(
                    (
                        table.name,
                        table.num_embeddings,
                        table.embedding_dim,
                        data_type_to_sparse_type(data_type),
                        location,
                    )
                )
                weight_lists.append(table_name_to_quantized_weights[table.name])

            emb_module = IntNBitTableBatchedEmbeddingBagsCodegen(
                embedding_specs=embedding_specs,
                pooling_mode=pooling_type_to_pooling_mode(pooling),
                weight_lists=weight_lists,
                device=device,
                output_dtype=data_type_to_sparse_type(dtype_to_data_type(output_dtype)),
                row_alignment=16,
            )
            self._emb_modules.append(emb_module)

        self._embedding_names: List[str] = list(
            itertools.chain(*get_embedding_names_by_table(self._embedding_bag_configs))
        )
        &#47&#47 We map over the parameters from FBGEMM backed kernels to the canonical nn.EmbeddingBag
        &#47&#47 representation. This provides consistency between this class and the EmbeddingBagCollection
        &#47&#47 nn.Module API calls (state_dict, named_modules, etc)
        <a id="change">self.embedding_bags: nn.ModuleDict = </a><a id="change">nn.ModuleDict()</a>
        <a id="change">for </a>(_key, tables), <a id="change">emb_module</a> in <a id="change">zip(
            </a><a id="change">self._key_to_tables.items()</a>, self._emb_modules<a id="change">
        )</a><a id="change">:
            for </a><a id="change">embedding_config</a>, (weight, _) in <a id="change">zip(
                </a>tables, <a id="change">emb_module.split_embedding_weights(split_scale_shifts=False)
            ):
                </a><a id="change">self.embedding_bags[embedding_config.name] = torch.nn.Module()</a>
                &#47&#47 register as a buffer so it&quots exposed in state_dict.
                &#47&#47 however, since this is only needed for inference, we do not need to expose it as part of parameters.
                &#47&#47 Additionally, we cannot expose uint8 weights as parameters due to autograd restrictions.
                <a id="change">self.embedding_bags[embedding_config.name]</a>.register_buffer(
                    "weight", weight
                )
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/d4eb3e9556a0593ea23c6908df76445bb9843d79#diff-30aabdcc064c9c3eee50a1dfb7ca08d561f2c524858be4eee22357c7e2befe70L175' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 16759558</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: d4eb3e9556a0593ea23c6908df76445bb9843d79</div><div id='time'> Time: 2022-07-27</div><div id='author'> Author: yingliufb@fb.com</div><div id='file'> File Name: torchrec/quant/embedding_modules.py</div><div id='m_class'> M Class Name: EmbeddingBagCollection</div><div id='n_method'> N Class Name: EmbeddingBagCollection</div><div id='m_method'> M Method Name: __init__(6)</div><div id='n_method'> N Method Name: __init__(6)</div><div id='m_parent_class'> M Parent Class: EmbeddingBagCollectionInterface</div><div id='n_parent_class'> N Parent Class: EmbeddingBagCollectionInterface</div><div id='m_file'> M File Name: torchrec/quant/embedding_modules.py</div><div id='n_file'> N File Name: torchrec/quant/embedding_modules.py</div><div id='m_start'> M Start Line: 190</div><div id='m_end'> M End Line: 190</div><div id='n_start'> N Start Line: 185</div><div id='n_end'> N End Line: 258</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        self._is_weighted = is_weighted
        self._embedding_bag_configs = tables
        self._emb_modules: nn.ModuleList = <a id="change">nn.ModuleList()</a>

        self._key_to_tables: Dict[
            Tuple[PoolingType, DataType], List[EmbeddingBagConfig]
        ] = defaultdict(list)</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 Registering in a List instead of ModuleList because we want don&quott want them to be auto-registered.
        &#47&#47 Their states will be modified via self.embedding_bags
        self._emb_modules: List[nn.Module] = <a id="change">[]</a>

        self._key_to_tables: Dict[
            Tuple[PoolingType, DataType], List[EmbeddingBagConfig]
        ] = defaultdict(list)

        self._embedding_names: List[str] = []

        self._length_per_key: List[int] = []

        for table in tables:
            self._length_per_key.extend(
                [table.embedding_dim] * len(table.feature_names)
            )

            key = (table.pooling, table.data_type)
            self._key_to_tables[key].append(table)

        optims = []
        for key, tables in self._key_to_tables.items():
            (pooling, data_type) = key
            emb_module = _BatchedFusedEmbeddingBag(
                tables,
                data_type=data_type,
                pooling=pooling,
                optimizer_type=emb_optim_type,
                optimizer_kwargs=emb_opt_kwargs,
                device=device,
                embedding_location=location,
            )
            self._emb_modules.append(emb_module)
            params: Dict[str, torch.Tensor] = {}
            for param_key, weight in emb_module.fused_optimizer().params.items():
                &#47&#47 pyre-fixme[6]: For 2nd param expected `Tensor` but got
                &#47&#47  `Union[Tensor, ShardedTensor]`.
                params[f"embedding_bags.{param_key}"] = weight
            optims.append(("", emb_module.fused_optimizer()))

        self._optim: CombinedOptimizer = CombinedOptimizer(optims)
        self._embedding_names = list(
            itertools.chain(*get_embedding_names_by_table(self._embedding_bag_configs))
        )

        &#47&#47 We map over the parameters from FBGEMM backed kernels to the canonical nn.EmbeddingBag
        &#47&#47 representation. This provides consistency between this class and the EmbeddingBagCollection&quots
        &#47&#47 nn.Module API calls (state_dict, named_modules, etc)
        <a id="change">self.embedding_bags: nn.ModuleDict = </a><a id="change">nn.ModuleDict()</a>
        <a id="change">for </a>(_key, tables), <a id="change">emb_module</a> in <a id="change">zip(
            </a><a id="change">self._key_to_tables.items()</a>, self._emb_modules<a id="change">
        )</a><a id="change">:
            for </a>embedding_config, <a id="change">weight</a> in <a id="change">zip(
                </a>tables, <a id="change">emb_module.split_embedding_weights()
            ):
                </a><a id="change">self.embedding_bags[embedding_config.name] = torch.nn.Module()</a>
                <a id="change">self.embedding_bags[embedding_config.name]</a>.register_parameter(
                    "weight", torch.nn.Parameter(weight)
                )
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/1d9c819d8683982e3b65ae83d73d5dee832077a1#diff-cc4a7e8d731e211e1403402d91cf122beaabbc1e556ad3f9208dddae5a8bf6a9L336' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 16759557</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: 1d9c819d8683982e3b65ae83d73d5dee832077a1</div><div id='time'> Time: 2022-07-07</div><div id='author'> Author: yingliufb@fb.com</div><div id='file'> File Name: torchrec/modules/fused_embedding_modules.py</div><div id='m_class'> M Class Name: FusedEmbeddingBagCollection</div><div id='n_method'> N Class Name: FusedEmbeddingBagCollection</div><div id='m_method'> M Method Name: __init__(7)</div><div id='n_method'> N Method Name: __init__(7)</div><div id='m_parent_class'> M Parent Class: FusedOptimizerModule,EmbeddingBagCollectionInterface</div><div id='n_parent_class'> N Parent Class: FusedOptimizerModule,EmbeddingBagCollectionInterface</div><div id='m_file'> M File Name: torchrec/modules/fused_embedding_modules.py</div><div id='n_file'> N File Name: torchrec/modules/fused_embedding_modules.py</div><div id='m_start'> M Start Line: 382</div><div id='m_end'> M End Line: 382</div><div id='n_start'> N Start Line: 384</div><div id='n_end'> N End Line: 442</div><BR>