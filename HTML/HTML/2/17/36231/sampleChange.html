<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        key_dim = make_divisible(cc * key_dim, divisor=8) // num_heads  &#47&#47 regard as key_dim_ratio
    else:
        key_dim = cc // num_heads  &#47&#47 Default value
    qk_scale = <a id="change">float(1.0</a><a id="change"> / tf.math.sqrt(</a><a id="change">tf.cast(</a>key_dim, <a id="change">"float32"</a><a id="change">))</a><a id="change">)</a>
    out_shape = cc if out_shape is None else out_shape
    emb_dim = num_heads * key_dim
    kv_kernel = block_size + halo_size * 2
    if block_size % strides != 0:
        strides = 1
        avg_pool_down = True
    else:
        avg_pool_down = False
    query_block = block_size // strides

    query = conv2d_no_bias(inputs, emb_dim, kernel_size=1, strides=strides, name=name and name + "query_")
    _, hh, ww, cc = query.shape
    &#47&#47 print(f"&gt;&gt;&gt;&gt; {inputs.shape = }, {query.shape = }, {block_size = }, {strides = }")
    &#47&#47 attn_query = rearrange(query, "B (h hb) (w wb) (hd c) -&gt; B hd h w (hb wb) c", hb=query_block, wb=query_block, hd=num_heads)
    &#47&#47 pos_query = rearrange(attn_query, "B hd h w (hb wb) c -&gt; B (hd h w) hb wb c", hb=query_block, wb=query_block)
    hh_qq, ww_qq, cc_qq = hh // query_block, ww // query_block, cc // num_heads
    query = tf.reshape(query, [-1, hh_qq, query_block, ww_qq, query_block, num_heads, cc_qq])
    query = tf.transpose(query, [0, 5, 1, 3, 2, 4, 6])  &#47&#47 [batch, num_heads, hh, ww, query_block, query_block, key_dim]
    &#47&#47 attn_query = [batch, num_heads, hh, ww, query_block * query_block, key_dim]
    attn_query = tf.reshape(query, [-1, num_heads, hh_qq, ww_qq, query_block * query_block, cc_qq]) * qk_scale  &#47&#47 [???] qk_scale NOT multiplied with pos_query
    &#47&#47 pos_query = [batch, num_heads * hh * ww, query_block, query_block, key_dim]
    pos_query = tf.reshape(query, [-1, num_heads * hh_qq * ww_qq, query_block, query_block, cc_qq])

    &#47&#47 key_value = [batch, height, width, key_dim + out_shape]
    key_value = conv2d_no_bias(inputs, emb_dim + out_shape, kernel_size=1, use_bias=False, name=name and name + "key_value_")
    kv_padded = tf.pad(key_value, [[0, 0], [halo_size, halo_size], [halo_size, halo_size], [0, 0]])
    sizes, strides = [1, kv_kernel, kv_kernel, 1], [1, block_size, block_size, 1]
    &#47&#47 kv_inp = [batch, hh, ww, kv_kernel * kv_kernel * (key_dim + out_shape)]
    &#47&#47 kv_inp = tf.image.extract_patches(kv_padded, sizes=sizes, strides=strides, rates=[1, 1, 1, 1], padding="VALID")
    kv_inp = CompatibleExtractPatches(sizes=sizes, strides=strides, rates=[1, 1, 1, 1], padding="VALID")(kv_padded)
    &#47&#47 kv_inp = rearrange(kv_inp, "B h w (hb wb hd c) -&gt; B hd h w (hb wb) c", hb=kv_kernel, wb=kv_kernel, hd=num_heads)
    _, hh_kk, ww_kk, cc = kv_inp.shape
    cc_kk = cc // num_heads // kv_kernel // kv_kernel
    kv_inp = tf.reshape(kv_inp, [-1, hh_kk, ww_kk, kv_kernel, kv_kernel, num_heads, cc_kk])
    kv_inp = tf.transpose(kv_inp, [0, 5, 1, 2, 3, 4, 6])
    kv_inp = tf.reshape(kv_inp, [-1, num_heads, hh_kk, ww_kk, kv_kernel * kv_kernel, cc_kk])

    &#47&#47 key = [batch, num_heads, hh, ww, kv_kernel * kv_kernel, key_dim]
    &#47&#47 value = [batch, num_heads, hh, ww, kv_kernel * kv_kernel, out_dim]
    key, value = tf.split(kv_inp, [emb_dim // num_heads, out_shape // num_heads], axis=-1)

    &#47&#47 scaled_dot_product_attention
    &#47&#47 print(f"&gt;&gt;&gt;&gt; {attn_query.shape = }, {key.shape = }, {value.shape = }, {kv_inp.shape = }, {pos_query.shape = }, {num_heads = }")
    &#47&#47 attention_scores = [batch, num_heads, hh, ww, query_block * query_block, kv_kernel * kv_kernel]
    attention_scores = <a id="change">keras.layers.Lambda(</a>lambda xx: tf.matmul(xx[0], xx[1], transpose_b=True)<a id="change">)</a>([attn_query, key])
    &#47&#47 pos = [batch, num_heads * hh * ww, query_block, query_block, kv_kernel, kv_kernel]
    pos = RelativePositionalEmbedding(position_height=kv_kernel, name=name and name + "pos_emb")(pos_query)
    &#47&#47 print(f"&gt;&gt;&gt;&gt; {pos.shape = }, {attention_scores.shape = }")
    pos = tf.reshape(pos, [-1, *attention_scores.shape[1:]])
    attention_scores = keras.layers.Add()([attention_scores, pos])
    &#47&#47 attention_scores = tf.nn.softmax(attention_scores, axis=-1)
    attention_scores = keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(attention_scores)

    if attn_dropout &gt; 0:
        attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)

    &#47&#47 attention_output = [batch, num_heads, hh, ww, query_block * query_block, out_dim]
    attention_output = <a id="change">keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))(</a><a id="change">[</a>attention_scores, value<a id="change"></a>]<a id="change">)</a>
    &#47&#47 attention_output = rearrange(attention_output, "B hd h w (hb wb) c -&gt; B (h hb) (w wb) (hd c)", hb=query_block, wb=query_block)
    _, heads, hh_aa, ww_aa, patch, cc_aa = attention_output.shape
    attention_output = tf.reshape(attention_output, [-1, heads, hh_aa, ww_aa, query_block, query_block, cc_aa])
    attention_output = tf.transpose(attention_output, [0, 2, 4, 3, 5, 1, 6])</code></pre><h3>After Change</h3><pre><code class='java'>
    else:
        key_dim = cc // num_heads  &#47&#47 Default value
    &#47&#47 qk_scale = float(1.0 / tf.math.sqrt(tf.cast(key_dim, "float32")))
    qk_scale = <a id="change">1.0</a><a id="change"> / </a>(<a id="change">float(</a>key_dim<a id="change">) ** 0.5</a>)
    out_shape = cc if out_shape is None else out_shape
    emb_dim = num_heads * key_dim
    kv_kernel = block_size + halo_size * 2
    if block_size % strides != 0:
        strides = 1
        avg_pool_down = True
    else:
        avg_pool_down = False
    query_block = block_size // strides

    query = conv2d_no_bias(inputs, emb_dim, kernel_size=1, strides=strides, name=name and name + "query_")
    _, hh, ww, cc = query.shape
    &#47&#47 print(f"&gt;&gt;&gt;&gt; {inputs.shape = }, {query.shape = }, {block_size = }, {strides = }")
    &#47&#47 attn_query = rearrange(query, "B (h hb) (w wb) (hd c) -&gt; B hd h w (hb wb) c", hb=query_block, wb=query_block, hd=num_heads)
    &#47&#47 pos_query = rearrange(attn_query, "B hd h w (hb wb) c -&gt; B (hd h w) hb wb c", hb=query_block, wb=query_block)
    hh_qq, ww_qq, cc_qq = hh // query_block, ww // query_block, cc // num_heads
    query = functional.reshape(query, [-1, hh_qq, query_block, ww_qq, query_block, num_heads, cc_qq])
    query = functional.transpose(query, [0, 5, 1, 3, 2, 4, 6])  &#47&#47 [batch, num_heads, hh, ww, query_block, query_block, key_dim]
    &#47&#47 attn_query = [batch, num_heads, hh, ww, query_block * query_block, key_dim]
    attn_query = functional.reshape(query, [-1, num_heads, hh_qq, ww_qq, query_block * query_block, cc_qq]) * qk_scale  &#47&#47 qk_scale NOT multiplied with pos_query
    &#47&#47 pos_query = [batch, num_heads * hh * ww, query_block, query_block, key_dim]
    pos_query = functional.reshape(query, [-1, num_heads * hh_qq * ww_qq, query_block, query_block, cc_qq])

    &#47&#47 key_value = [batch, height, width, key_dim + out_shape]
    key_value = conv2d_no_bias(inputs, emb_dim + out_shape, kernel_size=1, use_bias=False, name=name and name + "key_value_")
    kv_padded = functional.pad(key_value, [[0, 0], [halo_size, halo_size], [halo_size, halo_size], [0, 0]])
    sizes, strides = [1, kv_kernel, kv_kernel, 1], [1, block_size, block_size, 1]
    &#47&#47 kv_inp = [batch, hh, ww, kv_kernel * kv_kernel * (key_dim + out_shape)]
    &#47&#47 kv_inp = tf.image.extract_patches(kv_padded, sizes=sizes, strides=strides, rates=[1, 1, 1, 1], padding="VALID")
    kv_inp = CompatibleExtractPatches(sizes=sizes, strides=strides, rates=[1, 1, 1, 1], padding="VALID")(kv_padded)
    &#47&#47 kv_inp = rearrange(kv_inp, "B h w (hb wb hd c) -&gt; B hd h w (hb wb) c", hb=kv_kernel, wb=kv_kernel, hd=num_heads)
    _, hh_kk, ww_kk, cc = kv_inp.shape
    cc_kk = cc // num_heads // kv_kernel // kv_kernel
    kv_inp = functional.reshape(kv_inp, [-1, hh_kk, ww_kk, kv_kernel, kv_kernel, num_heads, cc_kk])
    kv_inp = functional.transpose(kv_inp, [0, 5, 1, 2, 3, 4, 6])
    kv_inp = functional.reshape(kv_inp, [-1, num_heads, hh_kk, ww_kk, kv_kernel * kv_kernel, cc_kk])

    &#47&#47 key = [batch, num_heads, hh, ww, kv_kernel * kv_kernel, key_dim]
    &#47&#47 value = [batch, num_heads, hh, ww, kv_kernel * kv_kernel, out_dim]
    key, value = functional.split(kv_inp, [emb_dim // num_heads, out_shape // num_heads], axis=-1)

    &#47&#47 scaled_dot_product_attention
    &#47&#47 print(f"&gt;&gt;&gt;&gt; {attn_query.shape = }, {key.shape = }, {value.shape = }, {kv_inp.shape = }, {pos_query.shape = }, {num_heads = }")
    &#47&#47 attention_scores = [batch, num_heads, hh, ww, query_block * query_block, kv_kernel * kv_kernel]
    &#47&#47 attention_scores = layers.Lambda(lambda xx: functional.matmul(xx[0], xx[1], transpose_b=True))([attn_query, key])
    attention_scores = attn_query<a id="change"> @ </a>functional.transpose(key, [0, 1, 2, 3, 5, 4])
    &#47&#47 pos = [batch, num_heads * hh * ww, query_block, query_block, kv_kernel, kv_kernel]
    pos = RelativePositionalEmbedding(position_height=kv_kernel, name=name and name + "pos_emb")(pos_query)
    &#47&#47 print(f"&gt;&gt;&gt;&gt; {pos.shape = }, {attention_scores.shape = }")
    pos = functional.reshape(pos, [-1, *attention_scores.shape[1:]])
    attention_scores = layers.Add()([attention_scores, pos])
    &#47&#47 attention_scores = tf.nn.softmax(attention_scores, axis=-1)
    attention_scores = layers.Softmax(axis=-1, name=name and name + "attention_scores")(attention_scores)

    if attn_dropout &gt; 0:
        attention_scores = layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)

    &#47&#47 attention_output = [batch, num_heads, hh, ww, query_block * query_block, out_dim]
    &#47&#47 attention_output = layers.Lambda(lambda xx: functional.matmul(xx[0], xx[1]))([attention_scores, value])
    attention_output = attention_scores<a id="change"> @ </a>value
    &#47&#47 attention_output = rearrange(attention_output, "B hd h w (hb wb) c -&gt; B (h hb) (w wb) (hd c)", hb=query_block, wb=query_block)
    _, heads, hh_aa, ww_aa, patch, cc_aa = attention_output.shape
    attention_output = functional.reshape(attention_output, [-1, heads, hh_aa, ww_aa, query_block, query_block, cc_aa])</code></pre>