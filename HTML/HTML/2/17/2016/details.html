<html><h3>Pattern ID :2016
</h3><img src='8909215.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    no_prox: bool,
    clip_global_grad_norm: Tensor,
):
    copy_grads<a id="change"> = </a><a id="change">[]</a>
    for i, param in enumerate(params):
        grad = grads[i]
        exp_avg = exp_avgs[i]
        exp_avg_sq = exp_avg_sqs[i]
        exp_avg_diff = exp_avg_diffs[i]
        pre_grad = pre_grads[i]

        grad = grad.mul_(clip_global_grad_norm)
        copy_grads.append(grad.clone())
        
        diff = grad - pre_grad
        update = grad + beta2 * diff
        
        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  &#47&#47 m_t
        exp_avg_diff.mul_(beta2).add_(diff, alpha=1 - beta2)  &#47&#47 diff_t
        exp_avg_sq.mul_(beta3).addcmul_(update, update, value=1 - beta3)  &#47&#47 n_t

        denom = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)
        update = ((exp_avg / bias_correction1 + beta2 * exp_avg_diff / bias_correction2)).div_(denom)

        if no_prox:
            param.mul_(1 - lr * weight_decay)
            param.add_(update, alpha=-lr)
        else:
            param.add_(update, alpha=-lr)
            param.div_(1 + lr * weight_decay)
    <a id="change">return </a>copy_grads

def _multi_tensor_adan(
    params: List[Tensor],</code></pre><h3>After Change</h3><pre><code class='java'>
        exp_avg_diff.mul_(beta2).add_(neg_grad_or_diff,
                                      alpha=1 - beta2)  &#47&#47 diff_t

        <a id="change">neg_grad_or_diff.mul_(beta2).add_(grad</a><a id="change">)</a>
        exp_avg_sq.mul_(beta3).addcmul_(neg_grad_or_diff,
                                        neg_grad_or_diff,
                                        value=1 - beta3)  &#47&#47 n_t

        denom = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)
        step_size_diff<a id="change"> = lr</a><a id="change"> * beta2 / </a>bias_correction2
        step_size = <a id="change">lr</a><a id="change"> / </a>bias_correction1

        if no_prox:
            param.mul_(1 - lr * weight_decay)
            param.addcdiv_(exp_avg, denom, value=-step_size)
            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)
        else:
            <a id="change">param.addcdiv_(</a>exp_avg, denom<a id="change">, value=-step_size)</a>
            <a id="change">param.addcdiv_(</a>exp_avg_diff, denom<a id="change">, value=-step_size_diff)</a>
            param.div_(1 + lr * weight_decay)

        <a id="change">neg_grad_or_diff.zero_().add_(grad</a><a id="change">, alpha=-1.0)</a>


def _multi_tensor_adan(
    params: List[Tensor],</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 13</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/sail-sg/adan/commit/9805dec1acf0151eecd1634a018c36239b0ad603#diff-bccd0d63d4b29d7d5170fdb4f6a4e95ad9bd8137b9a7ea1feb61848f4cabaeacL204' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8909215</div><div id='project'> Project Name: sail-sg/adan</div><div id='commit'> Commit Name: 9805dec1acf0151eecd1634a018c36239b0ad603</div><div id='time'> Time: 2022-12-10</div><div id='author'> Author: xyxie@pku.edu.cn</div><div id='file'> File Name: adan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _single_tensor_adan(0)</div><div id='n_method'> N Method Name: _single_tensor_adan(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: adan.py</div><div id='n_file'> N File Name: adan.py</div><div id='m_start'> M Start Line: 204</div><div id='m_end'> M End Line: 231</div><div id='n_start'> N Start Line: 221</div><div id='n_end'> N End Line: 256</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    no_prox: bool,
    clip_global_grad_norm: Tensor,
):
    copy_grads<a id="change"> = </a><a id="change">[]</a>
    for i, param in enumerate(params):
        grad = grads[i]
        exp_avg = exp_avgs[i]
        exp_avg_sq = exp_avg_sqs[i]
        exp_avg_diff = exp_avg_diffs[i]
        pre_grad = pre_grads[i]

        grad = grad.mul_(clip_global_grad_norm)
        copy_grads.append(grad.clone())
        
        diff = grad - pre_grad
        update = grad + beta2 * diff
        
        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  &#47&#47 m_t
        exp_avg_diff.mul_(beta2).add_(diff, alpha=1 - beta2)  &#47&#47 diff_t
        exp_avg_sq.mul_(beta3).addcmul_(update, update, value=1 - beta3)  &#47&#47 n_t

        denom = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)
        update = ((exp_avg / bias_correction1 + beta2 * exp_avg_diff / bias_correction2)).div_(denom)

        if no_prox:
            param.mul_(1 - lr * weight_decay)
            param.add_(update, alpha=-lr)
        else:
            param.add_(update, alpha=-lr)
            param.div_(1 + lr * weight_decay)
    <a id="change">return </a>copy_grads

def _multi_tensor_adan(
    params: List[Tensor],</code></pre><h3>After Change</h3><pre><code class='java'>
    no_prox: bool,
    clip_global_grad_norm: Tensor,
):
    for i, <a id="change">param</a> in enumerate(params):
        <a id="change">grad</a> = grads[i]
        exp_avg = exp_avgs[i]
        exp_avg_sq = exp_avg_sqs[i]
        exp_avg_diff = exp_avg_diffs[i]
        neg_grad_or_diff = neg_pre_grads[i]

        grad.mul_(clip_global_grad_norm)

        &#47&#47 for memory saving, we use `neg_grad_or_diff`
        &#47&#47 to get some temp variable in a inplace way
        neg_grad_or_diff.add_(grad)

        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  &#47&#47 m_t
        exp_avg_diff.mul_(beta2).add_(neg_grad_or_diff,
                                      alpha=1 - beta2)  &#47&#47 diff_t

        <a id="change">neg_grad_or_diff.mul_(beta2).add_(</a>grad<a id="change">)</a>
        exp_avg_sq.mul_(beta3).addcmul_(neg_grad_or_diff,
                                        neg_grad_or_diff,
                                        value=1 - beta3)  &#47&#47 n_t

        denom = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)
        step_size_diff<a id="change"> = </a>lr<a id="change"> * beta2 / </a>bias_correction2
        step_size = lr<a id="change"> / </a>bias_correction1

        if no_prox:
            param.mul_(1 - lr * weight_decay)
            param.addcdiv_(exp_avg, denom, value=-step_size)
            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)
        else:
            <a id="change">param.addcdiv_(</a>exp_avg, denom<a id="change">, value=-step_size)</a>
            <a id="change">param.addcdiv_(</a>exp_avg_diff, denom<a id="change">, value=-step_size_diff)</a>
            param.div_(1 + lr * weight_decay)

        <a id="change">neg_grad_or_diff.zero_().add_(</a>grad<a id="change">, alpha=-1.0)</a>


def _multi_tensor_adan(
    params: List[Tensor],</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sail-sg/adan/commit/6aafc8b3e81540fe52da3d70ced2bc3c9f17624a#diff-0b1b3ccdbc86485785aa958a04606abf754be337e06ac1f9c1b9e5471aec76b3L184' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8909219</div><div id='project'> Project Name: sail-sg/adan</div><div id='commit'> Commit Name: 6aafc8b3e81540fe52da3d70ced2bc3c9f17624a</div><div id='time'> Time: 2022-12-10</div><div id='author'> Author: xyxie@pku.edu.cn</div><div id='file'> File Name: NLP/Transformer-XL/adan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _single_tensor_adan(0)</div><div id='n_method'> N Method Name: _single_tensor_adan(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: NLP/Transformer-XL/adan.py</div><div id='n_file'> N File Name: NLP/Transformer-XL/adan.py</div><div id='m_start'> M Start Line: 204</div><div id='m_end'> M End Line: 231</div><div id='n_start'> N Start Line: 221</div><div id='n_end'> N End Line: 256</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    no_prox: bool,
    clip_global_grad_norm: Tensor,
):
    copy_grads<a id="change"> = </a><a id="change">[]</a>
    for i, param in enumerate(params):
        grad = grads[i]
        exp_avg = exp_avgs[i]
        exp_avg_sq = exp_avg_sqs[i]
        exp_avg_diff = exp_avg_diffs[i]
        pre_grad = pre_grads[i]

        grad = grad.mul_(clip_global_grad_norm)
        copy_grads.append(grad.clone())
        
        diff = grad - pre_grad
        update = grad + beta2 * diff
        
        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  &#47&#47 m_t
        exp_avg_diff.mul_(beta2).add_(diff, alpha=1 - beta2)  &#47&#47 diff_t
        exp_avg_sq.mul_(beta3).addcmul_(update, update, value=1 - beta3)  &#47&#47 n_t

        denom = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)
        update = ((exp_avg / bias_correction1 + beta2 * exp_avg_diff / bias_correction2)).div_(denom)

        if no_prox:
            param.mul_(1 - lr * weight_decay)
            param.add_(update, alpha=-lr)
        else:
            param.add_(update, alpha=-lr)
            param.div_(1 + lr * weight_decay)
    <a id="change">return </a>copy_grads

def _multi_tensor_adan(
    params: List[Tensor],</code></pre><h3>After Change</h3><pre><code class='java'>
    no_prox: bool,
    clip_global_grad_norm: Tensor,
):
    for i, <a id="change">param</a> in enumerate(params):
        <a id="change">grad</a> = grads[i]
        exp_avg = exp_avgs[i]
        exp_avg_sq = exp_avg_sqs[i]
        exp_avg_diff = exp_avg_diffs[i]
        neg_grad_or_diff = neg_pre_grads[i]

        grad.mul_(clip_global_grad_norm)

        &#47&#47 for memory saving, we use `neg_grad_or_diff`
        &#47&#47 to get some temp variable in a inplace way
        neg_grad_or_diff.add_(grad)

        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  &#47&#47 m_t
        exp_avg_diff.mul_(beta2).add_(neg_grad_or_diff,
                                      alpha=1 - beta2)  &#47&#47 diff_t

        <a id="change">neg_grad_or_diff.mul_(beta2).add_(</a>grad<a id="change">)</a>
        exp_avg_sq.mul_(beta3).addcmul_(neg_grad_or_diff,
                                        neg_grad_or_diff,
                                        value=1 - beta3)  &#47&#47 n_t

        denom = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)
        step_size_diff<a id="change"> = </a>lr<a id="change"> * beta2 / </a>bias_correction2
        step_size = lr<a id="change"> / </a>bias_correction1

        if no_prox:
            param.mul_(1 - lr * weight_decay)
            param.addcdiv_(exp_avg, denom, value=-step_size)
            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)
        else:
            <a id="change">param.addcdiv_(</a>exp_avg, denom<a id="change">, value=-step_size)</a>
            <a id="change">param.addcdiv_(</a>exp_avg_diff, denom<a id="change">, value=-step_size_diff)</a>
            param.div_(1 + lr * weight_decay)

        <a id="change">neg_grad_or_diff.zero_().add_(</a>grad<a id="change">, alpha=-1.0)</a>


def _multi_tensor_adan(
    params: List[Tensor],</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sail-sg/adan/commit/9805dec1acf0151eecd1634a018c36239b0ad603#diff-bccd0d63d4b29d7d5170fdb4f6a4e95ad9bd8137b9a7ea1feb61848f4cabaeacL184' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8909214</div><div id='project'> Project Name: sail-sg/adan</div><div id='commit'> Commit Name: 9805dec1acf0151eecd1634a018c36239b0ad603</div><div id='time'> Time: 2022-12-10</div><div id='author'> Author: xyxie@pku.edu.cn</div><div id='file'> File Name: adan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _single_tensor_adan(0)</div><div id='n_method'> N Method Name: _single_tensor_adan(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: adan.py</div><div id='n_file'> N File Name: adan.py</div><div id='m_start'> M Start Line: 204</div><div id='m_end'> M End Line: 231</div><div id='n_start'> N Start Line: 221</div><div id='n_end'> N End Line: 256</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    no_prox: bool,
    clip_global_grad_norm: Tensor,
):
    copy_grads<a id="change"> = </a><a id="change">[]</a>
    for i, param in enumerate(params):
        grad = grads[i]
        exp_avg = exp_avgs[i]
        exp_avg_sq = exp_avg_sqs[i]
        exp_avg_diff = exp_avg_diffs[i]
        pre_grad = pre_grads[i]

        grad = grad.mul_(clip_global_grad_norm)
        copy_grads.append(grad.clone())
        
        diff = grad - pre_grad
        update = grad + beta2 * diff
        
        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  &#47&#47 m_t
        exp_avg_diff.mul_(beta2).add_(diff, alpha=1 - beta2)  &#47&#47 diff_t
        exp_avg_sq.mul_(beta3).addcmul_(update, update, value=1 - beta3)  &#47&#47 n_t

        denom = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)
        update = ((exp_avg / bias_correction1 + beta2 * exp_avg_diff / bias_correction2)).div_(denom)

        if no_prox:
            param.mul_(1 - lr * weight_decay)
            param.add_(update, alpha=-lr)
        else:
            param.add_(update, alpha=-lr)
            param.div_(1 + lr * weight_decay)
    <a id="change">return </a>copy_grads

def _multi_tensor_adan(
    params: List[Tensor],</code></pre><h3>After Change</h3><pre><code class='java'>
    no_prox: bool,
    clip_global_grad_norm: Tensor,
):
    for i, <a id="change">param</a> in enumerate(params):
        <a id="change">grad</a> = grads[i]
        exp_avg = exp_avgs[i]
        exp_avg_sq = exp_avg_sqs[i]
        exp_avg_diff = exp_avg_diffs[i]
        neg_grad_or_diff = neg_pre_grads[i]

        grad.mul_(clip_global_grad_norm)

        &#47&#47 for memory saving, we use `neg_grad_or_diff`
        &#47&#47 to get some temp variable in a inplace way
        neg_grad_or_diff.add_(grad)

        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  &#47&#47 m_t
        exp_avg_diff.mul_(beta2).add_(neg_grad_or_diff,
                                      alpha=1 - beta2)  &#47&#47 diff_t

        <a id="change">neg_grad_or_diff.mul_(beta2).add_(</a>grad<a id="change">)</a>
        exp_avg_sq.mul_(beta3).addcmul_(neg_grad_or_diff,
                                        neg_grad_or_diff,
                                        value=1 - beta3)  &#47&#47 n_t

        denom = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)
        step_size_diff<a id="change"> = </a>lr<a id="change"> * beta2 / </a>bias_correction2
        step_size = lr<a id="change"> / </a>bias_correction1

        if no_prox:
            param.mul_(1 - lr * weight_decay)
            param.addcdiv_(exp_avg, denom, value=-step_size)
            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)
        else:
            <a id="change">param.addcdiv_(</a>exp_avg, denom<a id="change">, value=-step_size)</a>
            <a id="change">param.addcdiv_(</a>exp_avg_diff, denom<a id="change">, value=-step_size_diff)</a>
            param.div_(1 + lr * weight_decay)

        <a id="change">neg_grad_or_diff.zero_().add_(</a>grad<a id="change">, alpha=-1.0)</a>


def _multi_tensor_adan(
    params: List[Tensor],</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sail-sg/adan/commit/6aafc8b3e81540fe52da3d70ced2bc3c9f17624a#diff-61f7eb3450a10f3b466dcc44a8b30146ec63b38486822f81c29f75ac3088a06cL184' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8909211</div><div id='project'> Project Name: sail-sg/adan</div><div id='commit'> Commit Name: 6aafc8b3e81540fe52da3d70ced2bc3c9f17624a</div><div id='time'> Time: 2022-12-10</div><div id='author'> Author: xyxie@pku.edu.cn</div><div id='file'> File Name: CV/MAE/adan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _single_tensor_adan(0)</div><div id='n_method'> N Method Name: _single_tensor_adan(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: CV/MAE/adan.py</div><div id='n_file'> N File Name: CV/MAE/adan.py</div><div id='m_start'> M Start Line: 204</div><div id='m_end'> M End Line: 231</div><div id='n_start'> N Start Line: 221</div><div id='n_end'> N End Line: 256</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    no_prox: bool,
    clip_global_grad_norm: Tensor,
):
    copy_grads<a id="change"> = </a><a id="change">[]</a>
    for i, param in enumerate(params):
        grad = grads[i]
        exp_avg = exp_avgs[i]
        exp_avg_sq = exp_avg_sqs[i]
        exp_avg_diff = exp_avg_diffs[i]
        pre_grad = pre_grads[i]

        grad = grad.mul_(clip_global_grad_norm)
        copy_grads.append(grad.clone())
        
        diff = grad - pre_grad
        update = grad + beta2 * diff
        
        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  &#47&#47 m_t
        exp_avg_diff.mul_(beta2).add_(diff, alpha=1 - beta2)  &#47&#47 diff_t
        exp_avg_sq.mul_(beta3).addcmul_(update, update, value=1 - beta3)  &#47&#47 n_t

        denom = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)
        update = ((exp_avg / bias_correction1 + beta2 * exp_avg_diff / bias_correction2)).div_(denom)

        if no_prox:
            param.mul_(1 - lr * weight_decay)
            param.add_(update, alpha=-lr)
        else:
            param.add_(update, alpha=-lr)
            param.div_(1 + lr * weight_decay)
    <a id="change">return </a>copy_grads

def _multi_tensor_adan(
    params: List[Tensor],</code></pre><h3>After Change</h3><pre><code class='java'>
    no_prox: bool,
    clip_global_grad_norm: Tensor,
):
    for i, <a id="change">param</a> in enumerate(params):
        <a id="change">grad</a> = grads[i]
        exp_avg = exp_avgs[i]
        exp_avg_sq = exp_avg_sqs[i]
        exp_avg_diff = exp_avg_diffs[i]
        neg_grad_or_diff = neg_pre_grads[i]

        grad.mul_(clip_global_grad_norm)

        &#47&#47 for memory saving, we use `neg_grad_or_diff`
        &#47&#47 to get some temp variable in a inplace way
        neg_grad_or_diff.add_(grad)

        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  &#47&#47 m_t
        exp_avg_diff.mul_(beta2).add_(neg_grad_or_diff,
                                      alpha=1 - beta2)  &#47&#47 diff_t

        <a id="change">neg_grad_or_diff.mul_(beta2).add_(</a>grad<a id="change">)</a>
        exp_avg_sq.mul_(beta3).addcmul_(neg_grad_or_diff,
                                        neg_grad_or_diff,
                                        value=1 - beta3)  &#47&#47 n_t

        denom = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)
        step_size_diff<a id="change"> = </a>lr<a id="change"> * beta2 / </a>bias_correction2
        step_size = lr<a id="change"> / </a>bias_correction1

        if no_prox:
            param.mul_(1 - lr * weight_decay)
            param.addcdiv_(exp_avg, denom, value=-step_size)
            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)
        else:
            <a id="change">param.addcdiv_(</a>exp_avg, denom<a id="change">, value=-step_size)</a>
            <a id="change">param.addcdiv_(</a>exp_avg_diff, denom<a id="change">, value=-step_size_diff)</a>
            param.div_(1 + lr * weight_decay)

        <a id="change">neg_grad_or_diff.zero_().add_(</a>grad<a id="change">, alpha=-1.0)</a>


def _multi_tensor_adan(
    params: List[Tensor],</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sail-sg/adan/commit/6aafc8b3e81540fe52da3d70ced2bc3c9f17624a#diff-67c755dae37728c9303895a1af500d18552b3dbedbe392933fa05ddb6bf2837cL184' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 8909209</div><div id='project'> Project Name: sail-sg/adan</div><div id='commit'> Commit Name: 6aafc8b3e81540fe52da3d70ced2bc3c9f17624a</div><div id='time'> Time: 2022-12-10</div><div id='author'> Author: xyxie@pku.edu.cn</div><div id='file'> File Name: CV/timm/adan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _single_tensor_adan(0)</div><div id='n_method'> N Method Name: _single_tensor_adan(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: CV/timm/adan.py</div><div id='n_file'> N File Name: CV/timm/adan.py</div><div id='m_start'> M Start Line: 204</div><div id='m_end'> M End Line: 231</div><div id='n_start'> N Start Line: 221</div><div id='n_end'> N End Line: 256</div><BR>