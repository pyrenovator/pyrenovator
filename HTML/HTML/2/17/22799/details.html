<html><h3>Pattern ID :22799
</h3><img src='72406384.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        lengths_list: List[int] = self.lengths().tolist()
        N = max(lengths_list) if desired_length is None else desired_length
        offset = 0
        <a id="change">values</a><a id="change"> = []</a>
        <a id="change">for </a>length in lengths_list<a id="change">:
            </a>value<a id="change"> = self.values()[offset : offset + length]</a>
            if length &lt;= N:
                padding_tensor<a id="change"> = </a>torch.full(
                    [N - length], padding_value, device=self.values().device
                )
                value = (
                    torch.cat((padding_tensor, value), 0)
                    if pad_from_beginning
                    else torch.cat((value, padding_tensor), 0)
                )
            else:
                value = value[-N:] if chop_from_beginning else value[:N]
            <a id="change">values.append(</a>value<a id="change">)</a>
            offset<a id="change"> += </a>length
        final_tensor<a id="change"> = </a><a id="change">torch.stack(values</a><a id="change">)</a>
        <a id="change">return </a>final_tensor

    def lengths(self) -&gt; torch.Tensor:
        _lengths = _maybe_compute_lengths(self._lengths, self._offsets)</code></pre><h3>After Change</h3><pre><code class='java'>
        
        lengths_list: List[int] = self.lengths().tolist()
        N = max(lengths_list) if desired_length is None else desired_length
        <a id="change">return </a>torch.ops.fbgemm.jagged_to_padded_dense(
            self.values(), [self.offsets()], [N], padding_value
        )
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 14</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/52ddfd407a0e505682a053007893b3c24a158afa#diff-9b80c65f66abaf80b327456264ca7b031c6fa424946c489098b45aecf9b82c97L321' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 72406384</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: 52ddfd407a0e505682a053007893b3c24a158afa</div><div id='time'> Time: 2022-06-15</div><div id='author'> Author: joshuadeng@fb.com</div><div id='file'> File Name: torchrec/sparse/jagged_tensor.py</div><div id='m_class'> M Class Name: JaggedTensor</div><div id='n_method'> N Class Name: JaggedTensor</div><div id='m_method'> M Method Name: to_padded_dense(3)</div><div id='n_method'> N Method Name: to_padded_dense(5)</div><div id='m_parent_class'> M Parent Class: Pipelineable</div><div id='n_parent_class'> N Parent Class: Pipelineable</div><div id='m_file'> M File Name: torchrec/sparse/jagged_tensor.py</div><div id='n_file'> N File Name: torchrec/sparse/jagged_tensor.py</div><div id='m_start'> M Start Line: 321</div><div id='m_end'> M End Line: 378</div><div id='n_start'> N Start Line: 360</div><div id='n_end'> N End Line: 363</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        
        nums_nodes, id = graph.batch_num_nodes(), 0
        items_embedding = self.item_embedding(torch.tensor([i for i in range(self.items_total)]).to(nodes.device))
        <a id="change">batch_embedding</a><a id="change"> = []</a>
        <a id="change">for num_nodes</a> in nums_nodes<a id="change">:
            </a>output_node_features<a id="change"> = nodes_output[id:id + num_nodes, :]</a>
            output_nodes<a id="change"> = </a>nodes[id: id + num_nodes]
            beta = torch.zeros(self.items_total, 1).to(nodes.device)
            beta[output_nodes] = 1
            embed = (1 - beta * self.alpha) * items_embedding.clone()
            embed[output_nodes, :] = embed[output_nodes, :] + self.alpha[output_nodes] * output_node_features
            <a id="change">batch_embedding.append(</a>embed<a id="change">)</a>
            id<a id="change"> += </a>num_nodes
        batch_embedding<a id="change"> = </a><a id="change">torch.stack(</a>batch_embedding<a id="change">)</a>
        <a id="change">return </a>batch_embedding


class AggregateTemporalNodeFeatures(nn.Module):</code></pre><h3>After Change</h3><pre><code class='java'>
        items_embedding = self.item_embedding(torch.tensor([i for i in range(self.items_total)]).to(nodes.device))
        alpha = torch.sigmoid(self.alpha)
        embed = (1 - alpha) * items_embedding.clone() + alpha * nodes_output
        <a id="change">return </a>embed


class AggregateTemporalNodeFeatures(nn.Module):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/benedekrozemberczki/pytorch_geometric_temporal/commit/c402d2a14167bceaa3c8d3845879f8056e8aead7#diff-781837fb12ee6ac1f0b89d83a2500544e486bd58832bd4e4a8d1a779b2a0950eL78' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 72406385</div><div id='project'> Project Name: benedekrozemberczki/pytorch_geometric_temporal</div><div id='commit'> Commit Name: c402d2a14167bceaa3c8d3845879f8056e8aead7</div><div id='time'> Time: 2021-07-18</div><div id='author'> Author: benedek.rozemberczki@gmail.com</div><div id='file'> File Name: torch_geometric_temporal/nn/attention/dnntsp.py</div><div id='m_class'> M Class Name: GlobalGatedUpdater</div><div id='n_method'> N Class Name: GlobalGatedUpdater</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(4)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: torch_geometric_temporal/nn/attention/dnntsp.py</div><div id='n_file'> N File Name: torch_geometric_temporal/nn/attention/dnntsp.py</div><div id='m_start'> M Start Line: 78</div><div id='m_end'> M End Line: 98</div><div id='n_start'> N Start Line: 85</div><div id='n_end'> N End Line: 87</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        
        lengths_list: List[int] = self.lengths().tolist()
        N = max(lengths_list) if desired_length is None else desired_length
        <a id="change">offset</a> = 0
        <a id="change">values</a><a id="change"> = []</a>
        <a id="change">for length</a> in lengths_list<a id="change">:
            </a>value<a id="change"> = self.values()[offset : offset + length]</a>
            if length &lt;= N:
                padding_tensor<a id="change"> = </a>torch.full(
                    [N - length], padding_value, device=self.values().device
                )
                value = (
                    torch.cat((padding_tensor, value), 0)
                    if pad_from_beginning
                    else torch.cat((value, padding_tensor), 0)
                )
            else:
                value = value[-N:] if chop_from_beginning else value[:N]
            <a id="change">values.append(</a>value<a id="change">)</a>
            offset<a id="change"> += </a>length
        final_tensor<a id="change"> = </a><a id="change">torch.stack(</a>values<a id="change">)</a>
        <a id="change">return </a>final_tensor

    def lengths(self) -&gt; torch.Tensor:
        _lengths = _maybe_compute_lengths(self._lengths, self._offsets)</code></pre><h3>After Change</h3><pre><code class='java'>
        
        lengths_list: List[int] = self.lengths().tolist()
        N = max(lengths_list) if desired_length is None else desired_length
        <a id="change">return </a>torch.ops.fbgemm.jagged_to_padded_dense(
            self.values(), [self.offsets()], [N], padding_value
        )
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/52ddfd407a0e505682a053007893b3c24a158afa#diff-9b80c65f66abaf80b327456264ca7b031c6fa424946c489098b45aecf9b82c97L317' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 72406376</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: 52ddfd407a0e505682a053007893b3c24a158afa</div><div id='time'> Time: 2022-06-15</div><div id='author'> Author: joshuadeng@fb.com</div><div id='file'> File Name: torchrec/sparse/jagged_tensor.py</div><div id='m_class'> M Class Name: JaggedTensor</div><div id='n_method'> N Class Name: JaggedTensor</div><div id='m_method'> M Method Name: to_padded_dense(3)</div><div id='n_method'> N Method Name: to_padded_dense(5)</div><div id='m_parent_class'> M Parent Class: Pipelineable</div><div id='n_parent_class'> N Parent Class: Pipelineable</div><div id='m_file'> M File Name: torchrec/sparse/jagged_tensor.py</div><div id='n_file'> N File Name: torchrec/sparse/jagged_tensor.py</div><div id='m_start'> M Start Line: 321</div><div id='m_end'> M End Line: 378</div><div id='n_start'> N Start Line: 360</div><div id='n_end'> N End Line: 363</div><BR>