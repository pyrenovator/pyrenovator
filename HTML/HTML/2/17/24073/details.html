<html><h3>Pattern ID :24073
</h3><img src='74754164.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    x_n_fs_n_pad_n_outshp_n_res, with_v, dtype, as_variable, device, compile_graph, call
):
    tolerance_dict = {"float16": 1e-2, "float32": 1e-5, "float64": 1e-5, None: 1e-5}
    if <a id="change">call in [helpers.tf_call, helpers.tf_graph_call]</a> and "cpu" in device:
        &#47&#47 tf conv1d does not work when CUDA is installed, but array is on CPU
        return
    if <a id="change">call in [helpers.np_call, helpers.jnp_call]</a>:
        &#47&#47 numpy and jax do not yet support conv1d
        return
    if call in <a id="change">[</a>helpers.mx_call<a id="change"></a>] and "cpu" in device:
        &#47&#47 mxnet only supports 3d transpose convolutions with CUDNN
        return
    if call in [helpers.torch_call] and (dtype == "float16"):</code></pre><h3>After Change</h3><pre><code class='java'>
    x_n_fs_n_pad_n_outshp_n_res, with_v, dtype, as_variable, device, compile_graph
):
    tolerance_dict = {"float16": 1e-2, "float32": 1e-5, "float64": 1e-5, None: 1e-5}
    if <a id="change">ivy.current_backend_str() == "tensorflow"</a> and "cpu" in device:
        &#47&#47 tf conv1d does not work when CUDA is installed, but array is on CPU
        return
    if <a id="change">ivy.current_backend_str() in ("numpy", "jax")</a>:
        &#47&#47 numpy and jax do not yet support conv1d
        return
    if <a id="change">ivy.current_backend_str()</a> == "mxnet" and "cpu" in device:
        &#47&#47 mxnet only supports 3d transpose convolutions with CUDNN
        return
    if <a id="change">ivy.current_backend_str()</a> == "torch" and (dtype == "float16"):
        &#47&#47 we are skipping for float16 as it torch.nn.functional.conv2d
        &#47&#47 doesn&quott seem to be able to handle it
        return</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 10</div><BR><div id='size'>Non-data size: 12</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/ivy-dl/ivy/commit/3b5b029455aa9930dc81db5b5ea26d1079d363e9#diff-5a35dc499a7c8c451c6e504fe882c812ae3f01d96b6bc2319d5116759e1f17a6L1290' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 74754164</div><div id='project'> Project Name: ivy-dl/ivy</div><div id='commit'> Commit Name: 3b5b029455aa9930dc81db5b5ea26d1079d363e9</div><div id='time'> Time: 2022-08-18</div><div id='author'> Author: rashul.chutani@gmail.com</div><div id='file'> File Name: ivy_tests/test_ivy/test_stateful/test_layers.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: test_conv3d_transpose_layer(6)</div><div id='n_method'> N Method Name: test_conv3d_transpose_layer(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: ivy_tests/test_ivy/test_stateful/test_layers.py</div><div id='n_file'> N File Name: ivy_tests/test_ivy/test_stateful/test_layers.py</div><div id='m_start'> M Start Line: 1290</div><div id='m_end'> M End Line: 1302</div><div id='n_start'> N Start Line: 1302</div><div id='n_end'> N End Line: 1311</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    x_n_fs_n_pad_n_outshp_n_res, with_v, dtype, as_variable, device, compile_graph, call
):
    tolerance_dict = {"float16": 1e-2, "float32": 1e-5, "float64": 1e-5, None: 1e-5}
    if <a id="change">call in [helpers.tf_call, helpers.tf_graph_call]</a> and "cpu" in device:
        &#47&#47 tf conv1d does not work when CUDA is installed, but array is on CPU
        return
    if <a id="change">call in [helpers.np_call, helpers.jnp_call]</a>:
        &#47&#47 numpy and jax do not yet support conv1d
        return
    if call in <a id="change">[</a>helpers.torch_call<a id="change"></a>] and (dtype == "float16"):
        &#47&#47 we are skipping for float16 as it torch.nn.functional.conv2d
        &#47&#47 doesn&quott seem to be able to handle it
        return</code></pre><h3>After Change</h3><pre><code class='java'>
    x_n_fs_n_pad_n_outshp_n_res, with_v, dtype, as_variable, device, compile_graph
):
    tolerance_dict = {"float16": 1e-2, "float32": 1e-5, "float64": 1e-5, None: 1e-5}
    if <a id="change">ivy.current_backend_str() == "tensorflow"</a> and "cpu" in device:
        &#47&#47 tf conv1d does not work when CUDA is installed, but array is on CPU
        return
    if <a id="change">ivy.current_backend_str() in ("numpy", "jax")</a>:
        &#47&#47 numpy and jax do not yet support conv1d
        return
    if <a id="change">ivy.current_backend_str()</a> == "torch" and (dtype == "float16"):
        &#47&#47 we are skipping for float16 as it torch.nn.functional.conv2d
        &#47&#47 doesn&quott seem to be able to handle it
        return
    &#47&#47 smoke test
    x, filter_size, padding, out_shape, target = x_n_fs_n_pad_n_outshp_n_res
    if as_variable:
        x = ivy.variable(ivy.array(x, dtype=dtype, device=device))
    else:
        x = ivy.array(x, dtype=dtype, device=device)

    target = np.array(target)
    input_channels = x.shape[-1]
    output_channels = target.shape[-1]
    batch_size = x.shape[0]
    width = x.shape[1]
    if with_v and not dtype:
        np.random.seed(0)
        wlim = (6 / (output_channels + input_channels)) ** 0.5
        w = ivy.variable(
            ivy.array(
                np.random.uniform(
                    -wlim, wlim, (filter_size, output_channels, input_channels)
                ),
                dtype="float32",
                device=device,
            )
        )
        b = ivy.variable(
            ivy.zeros([1, 1, output_channels], device=device, dtype="float32")
        )
        v = Container({"w": w, "b": b})
    elif with_v:
        np.random.seed(0)
        wlim = (6 / (output_channels + input_channels)) ** 0.5
        w = ivy.variable(
            ivy.array(
                np.random.uniform(
                    -wlim, wlim, (filter_size, output_channels, input_channels)
                ),
                dtype=dtype,
                device=device,
            )
        )
        b = ivy.variable(ivy.zeros([1, 1, output_channels], device=device, dtype=dtype))
        v = Container({"w": w, "b": b})
    else:
        v = None
    conv1d_trans_layer = ivy.Conv1DTranspose(
        input_channels,
        output_channels,
        filter_size,
        1,
        padding,
        output_shape=out_shape,
        device=device,
        v=v,
        dtype=dtype,
    )
    ret = conv1d_trans_layer(x)
    &#47&#47 type test
    assert ivy.is_ivy_array(ret)
    &#47&#47 cardinality test
    new_width = width if padding == "SAME" else width + filter_size - 1
    assert ret.shape == (batch_size, new_width, output_channels)
    &#47&#47 value test
    if not with_v:
        return
    assert np.allclose(
        ivy.to_numpy(conv1d_trans_layer(x)), target, rtol=tolerance_dict[dtype]
    )
    &#47&#47 compilation test
    if <a id="change">ivy.current_backend_str()</a> == "torch":
        &#47&#47 pytest scripting does not **kwargs
        return
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/ivy-dl/ivy/commit/3b5b029455aa9930dc81db5b5ea26d1079d363e9#diff-5a35dc499a7c8c451c6e504fe882c812ae3f01d96b6bc2319d5116759e1f17a6L387' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 74754165</div><div id='project'> Project Name: ivy-dl/ivy</div><div id='commit'> Commit Name: 3b5b029455aa9930dc81db5b5ea26d1079d363e9</div><div id='time'> Time: 2022-08-18</div><div id='author'> Author: rashul.chutani@gmail.com</div><div id='file'> File Name: ivy_tests/test_ivy/test_stateful/test_layers.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: test_conv1d_transpose_layer(6)</div><div id='n_method'> N Method Name: test_conv1d_transpose_layer(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: ivy_tests/test_ivy/test_stateful/test_layers.py</div><div id='n_file'> N File Name: ivy_tests/test_ivy/test_stateful/test_layers.py</div><div id='m_start'> M Start Line: 388</div><div id='m_end'> M End Line: 467</div><div id='n_start'> N Start Line: 394</div><div id='n_end'> N End Line: 472</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    x_n_fs_n_pad_n_res, with_v, dtype, as_variable, device, compile_graph, call
):
    tolerance_dict = {"float16": 1e-2, "float32": 1e-5, "float64": 1e-5, None: 1e-5}
    if <a id="change">call in [helpers.tf_call, helpers.tf_graph_call]</a> and "cpu" in device:
        &#47&#47 tf conv1d does not work when CUDA is installed, but array is on CPU
        return
    if <a id="change">call in [helpers.np_call, helpers.jnp_call]</a>:
        &#47&#47 numpy and jax do not yet support conv1d
        return
    if call in <a id="change">[</a>helpers.torch_call<a id="change"></a>] and (dtype == "float16"):
        &#47&#47 we are skipping for float16 as it torch.nn.functional.conv2d
        &#47&#47 doesn&quott seem to be able to handle it
        return</code></pre><h3>After Change</h3><pre><code class='java'>
    x_n_fs_n_pad_n_res, with_v, dtype, as_variable, device, compile_graph
):
    tolerance_dict = {"float16": 1e-2, "float32": 1e-5, "float64": 1e-5, None: 1e-5}
    if <a id="change">ivy.current_backend_str() == "tensorflow"</a> and "cpu" in device:
        &#47&#47 tf conv1d does not work when CUDA is installed, but array is on CPU
        return
    if <a id="change">ivy.current_backend_str() in ("numpy", "jax")</a>:
        &#47&#47 numpy and jax do not yet support conv1d
        return
    if <a id="change">ivy.current_backend_str()</a> == "torch" and (dtype == "float16"):
        &#47&#47 we are skipping for float16 as it torch.nn.functional.conv2d
        &#47&#47 doesn&quott seem to be able to handle it
        return
    &#47&#47 smoke test
    x, filter_shape, padding, target = x_n_fs_n_pad_n_res
    if as_variable:
        x = ivy.variable(ivy.array(x, dtype=dtype, device=device))
    else:
        x = ivy.array(x, dtype=dtype, device=device)
    target = np.asarray(target)
    input_channels = x.shape[-1]
    output_channels = target.shape[-1]
    batch_size = x.shape[0]
    input_shape = list(x.shape[1:3])
    if with_v and not dtype:
        np.random.seed(0)
        wlim = (6 / (output_channels + input_channels)) ** 0.5
        w = ivy.variable(
            ivy.array(
                np.random.uniform(
                    -wlim, wlim, tuple(filter_shape + [output_channels, input_channels])
                ),
                dtype="float32",
                device=device,
            )
        )
        b = ivy.variable(
            ivy.zeros([1, 1, 1, output_channels], device=device, dtype="float32")
        )
        v = Container({"w": w, "b": b})
    elif with_v:
        np.random.seed(0)
        wlim = (6 / (output_channels + input_channels)) ** 0.5
        w = ivy.variable(
            ivy.array(
                np.random.uniform(
                    -wlim, wlim, tuple(filter_shape + [output_channels, input_channels])
                ),
                dtype=dtype,
                device=device,
            )
        )
        b = ivy.variable(
            ivy.zeros([1, 1, 1, output_channels], device=device, dtype=dtype)
        )
        v = Container({"w": w, "b": b})
    else:
        v = None
    conv2d_layer = ivy.Conv2D(
        input_channels,
        output_channels,
        filter_shape,
        1,
        padding,
        device=device,
        v=v,
        dtype=dtype,
    )
    ret = conv2d_layer(x)
    &#47&#47 type test
    assert ivy.is_ivy_array(ret)
    &#47&#47 cardinality test
    new_shape = (
        input_shape
        if padding == "SAME"
        else [item - filter_shape[i] + 1 for i, item in enumerate(input_shape)]
    )
    assert ret.shape == tuple([batch_size] + new_shape + [output_channels])
    &#47&#47 value test
    if not with_v:
        return
    assert np.allclose(
        ivy.to_numpy(conv2d_layer(x)), target, rtol=tolerance_dict[dtype]
    )
    &#47&#47 compilation test
    if <a id="change">ivy.current_backend_str()</a> == "torch":
        &#47&#47 pytest scripting does not **kwargs
        return
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/unifyai/ivy/commit/3b5b029455aa9930dc81db5b5ea26d1079d363e9#diff-5a35dc499a7c8c451c6e504fe882c812ae3f01d96b6bc2319d5116759e1f17a6L560' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 74754166</div><div id='project'> Project Name: unifyai/ivy</div><div id='commit'> Commit Name: 3b5b029455aa9930dc81db5b5ea26d1079d363e9</div><div id='time'> Time: 2022-08-18</div><div id='author'> Author: rashul.chutani@gmail.com</div><div id='file'> File Name: ivy_tests/test_ivy/test_stateful/test_layers.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: test_conv2d_layer(6)</div><div id='n_method'> N Method Name: test_conv2d_layer(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: ivy_tests/test_ivy/test_stateful/test_layers.py</div><div id='n_file'> N File Name: ivy_tests/test_ivy/test_stateful/test_layers.py</div><div id='m_start'> M Start Line: 561</div><div id='m_end'> M End Line: 644</div><div id='n_start'> N Start Line: 569</div><div id='n_end'> N End Line: 651</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    x_n_fs_n_pad_n_res, with_v, dtype, as_variable, device, compile_graph, call
):
    tolerance_dict = {"float16": 1e-2, "float32": 1e-5, "float64": 1e-5, None: 1e-5}
    if <a id="change">call in [helpers.tf_call, helpers.tf_graph_call]</a> and "cpu" in device:
        &#47&#47 tf conv1d does not work when CUDA is installed, but array is on CPU
        return
    if <a id="change">call in [helpers.np_call, helpers.jnp_call]</a>:
        &#47&#47 numpy and jax do not yet support conv1d
        return
    if call in <a id="change">[</a>helpers.torch_call<a id="change"></a>] and (dtype == "float16"):
        &#47&#47 we are skipping for float16 as it torch.nn.functional.conv1d
        &#47&#47 doesn&quott seem to be able to handle it
        return</code></pre><h3>After Change</h3><pre><code class='java'>
    x_n_fs_n_pad_n_res, with_v, dtype, as_variable, device, compile_graph
):
    tolerance_dict = {"float16": 1e-2, "float32": 1e-5, "float64": 1e-5, None: 1e-5}
    if <a id="change">ivy.current_backend_str() == "tensorflow"</a> and "cpu" in device:
        &#47&#47 tf conv1d does not work when CUDA is installed, but array is on CPU
        return
    if <a id="change">ivy.current_backend_str() in ("numpy", "jax")</a>:
        &#47&#47 numpy and jax do not yet support conv1d
        return
    if <a id="change">ivy.current_backend_str()</a> == "torch" and (dtype == "float16"):
        &#47&#47 we are skipping for float16 as it torch.nn.functional.conv1d
        &#47&#47 doesn&quott seem to be able to handle it
        return
    &#47&#47 smoke test
    x, filter_size, padding, target = x_n_fs_n_pad_n_res
    if as_variable:
        x = ivy.variable(ivy.array(x, dtype=dtype, device=device))
    else:
        x = ivy.array(x, dtype=dtype, device=device)

    target = np.array(target)
    input_channels = x.shape[-1]
    output_channels = target.shape[-1]
    batch_size = x.shape[0]

    width = x.shape[1]
    if with_v and not dtype:
        np.random.seed(0)
        wlim = (6 / (output_channels + input_channels)) ** 0.5
        w = ivy.variable(
            ivy.array(
                np.random.uniform(
                    -wlim, wlim, (filter_size, output_channels, input_channels)
                ),
                dtype="float32",
                device=device,
            )
        )
        b = ivy.variable(
            ivy.zeros([1, 1, output_channels], device=device, dtype="float32")
        )
        v = Container({"w": w, "b": b})
    elif with_v:
        np.random.seed(0)
        wlim = (6 / (output_channels + input_channels)) ** 0.5
        w = ivy.variable(
            ivy.array(
                np.random.uniform(
                    -wlim, wlim, (filter_size, output_channels, input_channels)
                ),
                dtype=dtype,
                device=device,
            )
        )
        b = ivy.variable(ivy.zeros([1, 1, output_channels], device=device, dtype=dtype))
        v = Container({"w": w, "b": b})
    else:
        v = None
    conv1d_layer = ivy.Conv1D(
        input_channels,
        output_channels,
        filter_size,
        1,
        padding,
        device=device,
        v=v,
        dtype=dtype,
    )
    ret = conv1d_layer(x)
    &#47&#47 type test
    assert ivy.is_ivy_array(ret)
    &#47&#47 cardinality test
    new_width = width if padding == "SAME" else width - filter_size + 1
    assert ret.shape == (batch_size, new_width, output_channels)
    &#47&#47 value test
    if not with_v:
        return
    assert np.allclose(
        ivy.to_numpy(conv1d_layer(x)), target, rtol=tolerance_dict[dtype]
    )
    &#47&#47 compilation test
    if <a id="change">ivy.current_backend_str()</a> == "torch":
        &#47&#47 pytest scripting does not **kwargs
        return
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/ivy-dl/ivy/commit/3b5b029455aa9930dc81db5b5ea26d1079d363e9#diff-5a35dc499a7c8c451c6e504fe882c812ae3f01d96b6bc2319d5116759e1f17a6L271' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 74754167</div><div id='project'> Project Name: ivy-dl/ivy</div><div id='commit'> Commit Name: 3b5b029455aa9930dc81db5b5ea26d1079d363e9</div><div id='time'> Time: 2022-08-18</div><div id='author'> Author: rashul.chutani@gmail.com</div><div id='file'> File Name: ivy_tests/test_ivy/test_stateful/test_layers.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: test_conv1d_layer(6)</div><div id='n_method'> N Method Name: test_conv1d_layer(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: ivy_tests/test_ivy/test_stateful/test_layers.py</div><div id='n_file'> N File Name: ivy_tests/test_ivy/test_stateful/test_layers.py</div><div id='m_start'> M Start Line: 272</div><div id='m_end'> M End Line: 351</div><div id='n_start'> N Start Line: 276</div><div id='n_end'> N End Line: 354</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    x_n_fs_n_pad_n_outshp_n_res, with_v, dtype, as_variable, device, compile_graph, call
):
    tolerance_dict = {"float16": 1e-2, "float32": 1e-5, "float64": 1e-5, None: 1e-5}
    if <a id="change">call in [helpers.tf_call, helpers.tf_graph_call]</a> and "cpu" in device:
        &#47&#47 tf conv1d does not work when CUDA is installed, but array is on CPU
        return
    if <a id="change">call in [helpers.np_call, helpers.jnp_call]</a>:
        &#47&#47 numpy and jax do not yet support conv1d
        return
    if call in <a id="change">[</a>helpers.torch_call<a id="change"></a>] and (dtype == "float16"):
        &#47&#47 we are skipping for float16 as it torch.nn.functional.conv_transpose2d
        &#47&#47 doesn&quott seem to be able to handle it
        return</code></pre><h3>After Change</h3><pre><code class='java'>
    x_n_fs_n_pad_n_outshp_n_res, with_v, dtype, as_variable, device, compile_graph
):
    tolerance_dict = {"float16": 1e-2, "float32": 1e-5, "float64": 1e-5, None: 1e-5}
    if <a id="change">ivy.current_backend_str() == "tensorflow"</a> and "cpu" in device:
        &#47&#47 tf conv1d does not work when CUDA is installed, but array is on CPU
        return
    if <a id="change">ivy.current_backend_str() in ("numpy", "jax")</a>:
        &#47&#47 numpy and jax do not yet support conv1d
        return
    if <a id="change">ivy.current_backend_str()</a> == "torch" and (dtype == "float16"):
        &#47&#47 we are skipping for float16 as it torch.nn.functional.conv2d
        &#47&#47 doesn&quott seem to be able to handle it
        return
    &#47&#47 smoke test
    x, filter_shape, padding, out_shape, target = x_n_fs_n_pad_n_outshp_n_res
    if as_variable:
        x = ivy.variable(ivy.array(x, dtype=dtype, device=device))
    else:
        x = ivy.array(x, dtype=dtype, device=device)
    target = np.asarray(target)
    input_channels = x.shape[-1]
    output_channels = target.shape[-1]
    batch_size = x.shape[0]
    input_shape = list(x.shape[1:3])
    if with_v and not dtype:
        np.random.seed(0)
        wlim = (6 / (output_channels + input_channels)) ** 0.5
        w = ivy.variable(
            ivy.array(
                np.random.uniform(
                    -wlim, wlim, tuple(filter_shape + [output_channels, input_channels])
                ),
                dtype="float32",
                device=device,
            )
        )
        b = ivy.variable(
            ivy.zeros([1, 1, 1, output_channels], device=device, dtype="float32")
        )
        v = Container({"w": w, "b": b})
    elif with_v:
        np.random.seed(0)
        wlim = (6 / (output_channels + input_channels)) ** 0.5
        w = ivy.variable(
            ivy.array(
                np.random.uniform(
                    -wlim, wlim, tuple(filter_shape + [output_channels, input_channels])
                ),
                dtype=dtype,
                device=device,
            )
        )
        b = ivy.variable(
            ivy.zeros([1, 1, 1, output_channels], device=device, dtype=dtype)
        )
        v = Container({"w": w, "b": b})
    else:
        v = None
    conv2d_transpose_layer = ivy.Conv2DTranspose(
        input_channels,
        output_channels,
        filter_shape,
        1,
        padding,
        output_shape=out_shape,
        device=device,
        v=v,
        dtype=dtype,
    )
    ret = conv2d_transpose_layer(x)
    &#47&#47 type test
    assert ivy.is_ivy_array(ret)
    &#47&#47 cardinality test
    new_shape = (
        input_shape
        if padding == "SAME"
        else [item + filter_shape[i] - 1 for i, item in enumerate(input_shape)]
    )
    assert ret.shape == tuple([batch_size] + new_shape + [output_channels])
    &#47&#47 value test
    if not with_v:
        return
    assert np.allclose(
        ivy.to_numpy(conv2d_transpose_layer(x)), target, rtol=tolerance_dict[dtype]
    )
    &#47&#47 compilation test
    if <a id="change">ivy.current_backend_str()</a> == "torch":
        &#47&#47 pytest scripting does not **kwargs
        return
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/ivy-dl/ivy/commit/3b5b029455aa9930dc81db5b5ea26d1079d363e9#diff-5a35dc499a7c8c451c6e504fe882c812ae3f01d96b6bc2319d5116759e1f17a6L708' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 74754160</div><div id='project'> Project Name: ivy-dl/ivy</div><div id='commit'> Commit Name: 3b5b029455aa9930dc81db5b5ea26d1079d363e9</div><div id='time'> Time: 2022-08-18</div><div id='author'> Author: rashul.chutani@gmail.com</div><div id='file'> File Name: ivy_tests/test_ivy/test_stateful/test_layers.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: test_conv2d_transpose_layer(6)</div><div id='n_method'> N Method Name: test_conv2d_transpose_layer(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: ivy_tests/test_ivy/test_stateful/test_layers.py</div><div id='n_file'> N File Name: ivy_tests/test_ivy/test_stateful/test_layers.py</div><div id='m_start'> M Start Line: 709</div><div id='m_end'> M End Line: 795</div><div id='n_start'> N Start Line: 719</div><div id='n_end'> N End Line: 802</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    x_n_fs_n_pad_n_outshp_n_res, with_v, dtype, as_variable, device, compile_graph, call
):
    tolerance_dict = {"float16": 1e-2, "float32": 1e-5, "float64": 1e-5, None: 1e-5}
    if <a id="change">call in [helpers.tf_call, helpers.tf_graph_call]</a> and "cpu" in device:
        &#47&#47 tf conv1d does not work when CUDA is installed, but array is on CPU
        return
    if <a id="change">call in [helpers.np_call, helpers.jnp_call]</a>:
        &#47&#47 numpy and jax do not yet support conv1d
        return
    if call in [helpers.mx_call] and "cpu" in device:
        &#47&#47 mxnet only supports 3d transpose convolutions with CUDNN
        return
    if call in <a id="change">[</a>helpers.torch_call<a id="change"></a>] and (dtype == "float16"):
        &#47&#47 we are skipping for float16 as it torch.nn.functional.conv_transpose3d
        &#47&#47 doesn&quott seem to be able to handle it
        return</code></pre><h3>After Change</h3><pre><code class='java'>
    x_n_fs_n_pad_n_outshp_n_res, with_v, dtype, as_variable, device, compile_graph
):
    tolerance_dict = {"float16": 1e-2, "float32": 1e-5, "float64": 1e-5, None: 1e-5}
    if <a id="change">ivy.current_backend_str() == "tensorflow"</a> and "cpu" in device:
        &#47&#47 tf conv1d does not work when CUDA is installed, but array is on CPU
        return
    if <a id="change">ivy.current_backend_str() in ("numpy", "jax")</a>:
        &#47&#47 numpy and jax do not yet support conv1d
        return
    if <a id="change">ivy.current_backend_str()</a> == "mxnet" and "cpu" in device:
        &#47&#47 mxnet only supports 3d transpose convolutions with CUDNN
        return
    if <a id="change">ivy.current_backend_str()</a> == "torch" and (dtype == "float16"):
        &#47&#47 we are skipping for float16 as it torch.nn.functional.conv2d
        &#47&#47 doesn&quott seem to be able to handle it
        return</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/unifyai/ivy/commit/3b5b029455aa9930dc81db5b5ea26d1079d363e9#diff-5a35dc499a7c8c451c6e504fe882c812ae3f01d96b6bc2319d5116759e1f17a6L1289' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 74754163</div><div id='project'> Project Name: unifyai/ivy</div><div id='commit'> Commit Name: 3b5b029455aa9930dc81db5b5ea26d1079d363e9</div><div id='time'> Time: 2022-08-18</div><div id='author'> Author: rashul.chutani@gmail.com</div><div id='file'> File Name: ivy_tests/test_ivy/test_stateful/test_layers.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: test_conv3d_transpose_layer(6)</div><div id='n_method'> N Method Name: test_conv3d_transpose_layer(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: ivy_tests/test_ivy/test_stateful/test_layers.py</div><div id='n_file'> N File Name: ivy_tests/test_ivy/test_stateful/test_layers.py</div><div id='m_start'> M Start Line: 1290</div><div id='m_end'> M End Line: 1302</div><div id='n_start'> N Start Line: 1302</div><div id='n_end'> N End Line: 1311</div><BR>