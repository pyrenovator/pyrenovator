<html><h3>Pattern ID :34456
</h3><img src='98813282.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Encode if needed (training, first prediction pass)
        &#47&#47NOTE is None
        &#47&#47 if encoder_outputs is None:
        <a id="change">if </a><a id="change">is_None(</a>encoder_outputs<a id="change">)</a>:
            <a id="change">encoder_outputs</a><a id="change"> = </a>self.encoder(shared_embedding=self.shared_embed_weight,
                input_ids=input_ids, 
                attention_mask=attention_mask,
                inputs_embeds=inputs_embeds,
                head_mask=head_mask
            )

        hidden_states<a id="change"> = encoder_outputs</a><a id="change">[0]</a>

        &#47&#47 If decoding with past key value states, only the last tokens
        &#47&#47 should be given as an input
        &#47&#47NOTE is not None
        &#47&#47 if decoder_past_key_value_states is not None:
        if is_not_None(decoder_past_key_value_states):
            &#47&#47NOTE is not None
            &#47&#47 if decoder_input_ids is not None:
            if is_not_None(decoder_input_ids):
                decoder_input_ids = decoder_input_ids[:, -1:]
            &#47&#47NOTE is not None
            &#47&#47 if decoder_inputs_embeds is not None:
            if is_not_None(decoder_inputs_embeds):
                decoder_inputs_embeds = decoder_inputs_embeds[:, -1:]

        &#47&#47 Decode
        decoder_outputs = self.decoder(
            shared_embedding=self.shared_embed_weight,
            input_ids=decoder_input_ids,
            attention_mask=decoder_attention_mask,
            inputs_embeds=decoder_inputs_embeds,
            past_key_value_states=decoder_past_key_value_states,
            encoder_hidden_states=hidden_states,
            encoder_attention_mask=attention_mask,
            head_mask=head_mask,
            use_cache=use_cache,
        )

        &#47&#47 if use_cache is True:
        <a id="change">if use_cache</a>:
            past = ((<a id="change">encoder_outputs</a><a id="change">, decoder_outputs[1]</a>)<a id="change"></a>,)
            decoder_outputs<a id="change"> = </a><a id="change">decoder_outputs[:1]</a><a id="change"> + past + decoder_outputs[2:]</a>

        if self.output_only:
            return <a id="change">decoder_outputs[0]</a>
        <a id="change">return </a>decoder_outputs<a id="change"> + encoder_outputs</a>


@add_start_docstrings(T5 Model with a `language modeling` head on top. , T5_START_DOCSTRING)
class T5ForConditionalGeneration(T5PreTrainedModel):</code></pre><h3>After Change</h3><pre><code class='java'>
        

        &#47&#47 Encode
        <a id="change">encoder_hidden_states</a><a id="change"> = </a>self.encoder(input_ids=input_ids,shared_embedding=self.shared_embed_weight,
                                            attention_mask=attention_mask,head_mask=head_mask)

        &#47&#47 Decode
        decoder_hidden_states = self.decoder(
            input_ids=decoder_input_ids,
            shared_embedding=self.shared_embed_weight,
            attention_mask=decoder_attention_mask,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=attention_mask,
            head_mask=head_mask
        )

        if self.output_only:
            return decoder_hidden_states

        <a id="change">return </a>decoder_hidden_states<a id="change">,encoder_hidden_states</a>


@add_start_docstrings(T5 Model with a `language modeling` head on top. , T5_START_DOCSTRING)
class T5ForConditionalGeneration(T5PreTrainedModel):</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 22</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/cf27d37dff379f4b4e0abadd66985c1d78a55887#diff-8874e7c07d189804a7ce4eb1055b74111f1acffe8182f750b80a1621ca3d0a2dL196' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 98813282</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: cf27d37dff379f4b4e0abadd66985c1d78a55887</div><div id='time'> Time: 2020-06-24</div><div id='author'> Author: alondej@gmail.com</div><div id='file'> File Name: models/normal/NLP_models/modeling_t5_tied_weights.py</div><div id='m_class'> M Class Name: T5Model</div><div id='n_method'> N Class Name: T5Model</div><div id='m_method'> M Method Name: forward(6)</div><div id='n_method'> N Method Name: forward(11)</div><div id='m_parent_class'> M Parent Class: T5PreTrainedModel</div><div id='n_parent_class'> N Parent Class: T5PreTrainedModel</div><div id='m_file'> M File Name: models/normal/NLP_models/modeling_t5_tied_weights.py</div><div id='n_file'> N File Name: models/normal/NLP_models/modeling_t5_tied_weights.py</div><div id='m_start'> M Start Line: 265</div><div id='m_end'> M End Line: 358</div><div id='n_start'> N Start Line: 196</div><div id='n_end'> N End Line: 254</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Encode if needed (training, first prediction pass)
        &#47&#47NOTE is None
        &#47&#47 if encoder_outputs is None:
        <a id="change">if </a><a id="change">is_None(</a>encoder_outputs<a id="change">)</a>:
            <a id="change">encoder_outputs</a><a id="change"> = </a>self.encoder(
                input_ids=input_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, head_mask=head_mask
            )

        hidden_states<a id="change"> = </a><a id="change">encoder_outputs[0]</a>

        &#47&#47 If decoding with past key value states, only the last tokens
        &#47&#47 should be given as an input
        &#47&#47NOTE is not None
        &#47&#47 if decoder_past_key_value_states is not None:
        if is_not_None(decoder_past_key_value_states):
            &#47&#47NOTE is not None
            &#47&#47 if decoder_input_ids is not None:
            if is_not_None(decoder_input_ids):
                decoder_input_ids = decoder_input_ids[:, -1:]
            &#47&#47NOTE is not None
            &#47&#47 if decoder_inputs_embeds is not None:
            if is_not_None(decoder_inputs_embeds):
                decoder_inputs_embeds = decoder_inputs_embeds[:, -1:]

        &#47&#47 Decode
        decoder_outputs = self.decoder(
            input_ids=decoder_input_ids,
            attention_mask=decoder_attention_mask,
            inputs_embeds=decoder_inputs_embeds,
            past_key_value_states=decoder_past_key_value_states,
            encoder_hidden_states=hidden_states,
            encoder_attention_mask=attention_mask,
            head_mask=head_mask,
            use_cache=use_cache,
        )

        &#47&#47 if use_cache is True:
        <a id="change">if use_cache</a>:
            past = ((encoder_outputs<a id="change">, decoder_outputs[1]</a>)<a id="change"></a>,)
            decoder_outputs<a id="change"> = </a><a id="change">decoder_outputs[:1]</a><a id="change"> + past + decoder_outputs[2:]</a>
        
        if self.output_only:
            return <a id="change">decoder_outputs[0]</a>
        <a id="change">return </a>decoder_outputs<a id="change"> + </a>encoder_outputs


@add_start_docstrings(T5 Model with a `language modeling` head on top. , T5_START_DOCSTRING)</code></pre><h3>After Change</h3><pre><code class='java'>
        

        &#47&#47 Encode
        <a id="change">encoder_hidden_states</a><a id="change"> = </a>self.encoder(input_ids=input_ids,attention_mask=attention_mask,head_mask=head_mask)

        &#47&#47 Decode
        decoder_hidden_states = self.decoder(
            input_ids=decoder_input_ids,
            attention_mask=decoder_attention_mask,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=attention_mask,
            head_mask=head_mask
        )

        if self.output_only:
            return decoder_hidden_states

        <a id="change">return </a>decoder_hidden_states<a id="change">,encoder_hidden_states</a>


@add_start_docstrings(T5 Model with a `language modeling` head on top. , T5_START_DOCSTRING)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/7ac977abea2c3ebc7af0cc51d0b4bba4653d6c9f#diff-86d860e66a6925f6c0f5eddcbc09a214146aaf7d51edec93751c364b30042ee4L1295' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 98813280</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 7ac977abea2c3ebc7af0cc51d0b4bba4653d6c9f</div><div id='time'> Time: 2020-07-07</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: models/normal/NLP_models/modeling_t5.py</div><div id='m_class'> M Class Name: T5Model</div><div id='n_method'> N Class Name: T5Model</div><div id='m_method'> M Method Name: forward(6)</div><div id='n_method'> N Method Name: forward(11)</div><div id='m_parent_class'> M Parent Class: T5PreTrainedModel</div><div id='n_parent_class'> N Parent Class: T5PreTrainedModel</div><div id='m_file'> M File Name: models/normal/NLP_models/modeling_t5.py</div><div id='n_file'> N File Name: models/normal/NLP_models/modeling_t5.py</div><div id='m_start'> M Start Line: 1296</div><div id='m_end'> M End Line: 1385</div><div id='n_start'> N Start Line: 874</div><div id='n_end'> N End Line: 930</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Encode if needed (training, first prediction pass)
        &#47&#47NOTE is None
        &#47&#47 if encoder_outputs is None:
        <a id="change">if </a><a id="change">is_None(</a>encoder_outputs<a id="change">)</a>:
            <a id="change">encoder_outputs</a><a id="change"> = </a>self.encoder(shared_embedding=self.shared_embed_weight,
                input_ids=input_ids, 
                attention_mask=attention_mask,
                inputs_embeds=inputs_embeds,
                head_mask=head_mask
            )

        hidden_states<a id="change"> = </a><a id="change">encoder_outputs[0]</a>

        &#47&#47 If decoding with past key value states, only the last tokens
        &#47&#47 should be given as an input
        &#47&#47NOTE is not None
        &#47&#47 if decoder_past_key_value_states is not None:
        if is_not_None(decoder_past_key_value_states):
            &#47&#47NOTE is not None
            &#47&#47 if decoder_input_ids is not None:
            if is_not_None(decoder_input_ids):
                decoder_input_ids = decoder_input_ids[:, -1:]
            &#47&#47NOTE is not None
            &#47&#47 if decoder_inputs_embeds is not None:
            if is_not_None(decoder_inputs_embeds):
                decoder_inputs_embeds = decoder_inputs_embeds[:, -1:]

        &#47&#47 Decode
        decoder_outputs = self.decoder(
            shared_embedding=self.shared_embed_weight,
            input_ids=decoder_input_ids,
            attention_mask=decoder_attention_mask,
            inputs_embeds=decoder_inputs_embeds,
            past_key_value_states=decoder_past_key_value_states,
            encoder_hidden_states=hidden_states,
            encoder_attention_mask=attention_mask,
            head_mask=head_mask,
            use_cache=use_cache,
        )

        &#47&#47 if use_cache is True:
        <a id="change">if use_cache</a>:
            past = ((encoder_outputs<a id="change">, decoder_outputs[1]</a>)<a id="change"></a>,)
            decoder_outputs<a id="change"> = </a><a id="change">decoder_outputs[:1]</a><a id="change"> + past + decoder_outputs[2:]</a>

        if self.output_only:
            return <a id="change">decoder_outputs[0]</a>
        <a id="change">return </a>decoder_outputs<a id="change"> + </a>encoder_outputs


@add_start_docstrings(T5 Model with a `language modeling` head on top. , T5_START_DOCSTRING)</code></pre><h3>After Change</h3><pre><code class='java'>
        

        &#47&#47 Encode
        <a id="change">encoder_hidden_states</a><a id="change"> = </a>self.encoder(input_ids=input_ids,shared_embedding=self.shared_embed_weight,
                                            attention_mask=attention_mask,head_mask=head_mask)

        &#47&#47 Decode
        decoder_hidden_states = self.decoder(
            input_ids=decoder_input_ids,
            shared_embedding=self.shared_embed_weight,
            attention_mask=decoder_attention_mask,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=attention_mask,
            head_mask=head_mask
        )

        if self.output_only:
            return decoder_hidden_states

        <a id="change">return </a>decoder_hidden_states<a id="change">,encoder_hidden_states</a>


@add_start_docstrings(T5 Model with a `language modeling` head on top. , T5_START_DOCSTRING)
class T5ForConditionalGeneration(T5PreTrainedModel):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/cf27d37dff379f4b4e0abadd66985c1d78a55887#diff-8874e7c07d189804a7ce4eb1055b74111f1acffe8182f750b80a1621ca3d0a2dL264' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 98813286</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: cf27d37dff379f4b4e0abadd66985c1d78a55887</div><div id='time'> Time: 2020-06-24</div><div id='author'> Author: alondej@gmail.com</div><div id='file'> File Name: models/normal/NLP_models/modeling_t5_tied_weights.py</div><div id='m_class'> M Class Name: T5Model</div><div id='n_method'> N Class Name: T5Model</div><div id='m_method'> M Method Name: forward(6)</div><div id='n_method'> N Method Name: forward(11)</div><div id='m_parent_class'> M Parent Class: T5PreTrainedModel</div><div id='n_parent_class'> N Parent Class: T5PreTrainedModel</div><div id='m_file'> M File Name: models/normal/NLP_models/modeling_t5_tied_weights.py</div><div id='n_file'> N File Name: models/normal/NLP_models/modeling_t5_tied_weights.py</div><div id='m_start'> M Start Line: 265</div><div id='m_end'> M End Line: 358</div><div id='n_start'> N Start Line: 196</div><div id='n_end'> N End Line: 254</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Encode if needed (training, first prediction pass)
        &#47&#47NOTE is none
        &#47&#47 if encoder_outputs is None:
        <a id="change">if </a><a id="change">is_None(</a>encoder_outputs<a id="change">)</a>:
            &#47&#47 Convert encoder inputs in embeddings if needed
            <a id="change">encoder_outputs</a><a id="change"> = </a>self.encoder(shared_embedding=self.shared_embed_weight,
                input_ids=input_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, head_mask=head_mask
            )

        hidden_states<a id="change"> = </a><a id="change">encoder_outputs[0]</a>

        &#47&#47NOTE is not none, is none, is none
        &#47&#47 if lm_labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:
        if is_not_None(lm_labels) and is_None(decoder_input_ids) and is_None(decoder_inputs_embeds):
            &#47&#47 get decoder inputs from shifting lm labels to the right
            decoder_input_ids = self._shift_right(lm_labels)

        &#47&#47 If decoding with past key value states, only the last tokens
        &#47&#47 should be given as an input
        &#47&#47NOTE is not None
        &#47&#47 if decoder_past_key_value_states is not None:
        if is_not_None(decoder_past_key_value_states):
            &#47&#47NOTE is None
            &#47&#47 assert lm_labels is None, "Decoder should not use cached key value states when training."
            assert is_None(lm_labels), "Decoder should not use cached key value states when training."
            &#47&#47NOTE is not None
            &#47&#47 if decoder_input_ids is not None:
            if is_not_None(decoder_input_ids):
                decoder_input_ids = decoder_input_ids[:, -1:]
            &#47&#47NOTE is not None    
            &#47&#47 if decoder_inputs_embeds is not None:
            if is_not_None(decoder_inputs_embeds):
                decoder_inputs_embeds = decoder_inputs_embeds[:, -1:]

        &#47&#47 Decode
        decoder_outputs = self.decoder(
            shared_embedding=self.shared_embed_weight,
            input_ids=decoder_input_ids,
            attention_mask=decoder_attention_mask,
            inputs_embeds=decoder_inputs_embeds,
            past_key_value_states=decoder_past_key_value_states,
            encoder_hidden_states=hidden_states,
            encoder_attention_mask=attention_mask,
            head_mask=head_mask,
            use_cache=use_cache,
        )

        &#47&#47 insert decoder past at right place
        &#47&#47 to speed up decoding
        &#47&#47 if use_cache is True:
        <a id="change">if use_cache</a>:
            past = ((encoder_outputs<a id="change">, decoder_outputs[1]</a>)<a id="change"></a>,)
            decoder_outputs<a id="change"> = </a><a id="change">decoder_outputs[:1]</a><a id="change"> + past + decoder_outputs[2:]</a>

        sequence_output = <a id="change">decoder_outputs[0]</a>
        &#47&#47 Rescale output before projecting on vocab
        &#47&#47 See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py&#47&#47L586
        sequence_output = sequence_output * (self.model_dim ** -0.5)
        lm_logits = self.lm_head(sequence_output)

        decoder_outputs = (lm_logits,) + decoder_outputs[1:]  &#47&#47 Add hidden states and attention if they are here
        &#47&#47NOTE is not None
        &#47&#47 if lm_labels is not None:
        if is_not_None(lm_labels):
            &#47&#47 loss_fct = CrossEntropyLoss(ignore_index=-100)
            loss_fct = self.lm_loss
            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))
            &#47&#47 TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py&#47&#47L666
            decoder_outputs = (loss,) + decoder_outputs

        if self.output_only:
            return decoder_outputs[0]

        <a id="change">return </a>decoder_outputs<a id="change"> + </a>encoder_outputs

    def prepare_inputs_for_generation(self, input_ids, past, attention_mask, use_cache, **kwargs):
        &#47&#47NOTE is not None</code></pre><h3>After Change</h3><pre><code class='java'>
        


        <a id="change">encoder_hidden_states</a><a id="change"> = </a>self.encoder(input_ids=input_ids,shared_embedding=self.shared_embed_weight,
                                            attention_mask=attention_mask, head_mask=head_mask)

        &#47&#47NOTE is not none, is none, is none
        &#47&#47 if lm_labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:
        if is_not_None(lm_labels) and is_None(decoder_input_ids):
            &#47&#47 get decoder inputs from shifting lm labels to the right
            decoder_input_ids = self._shift_right(lm_labels)

        &#47&#47 Decode
        decoder_hidden_states = self.decoder(
            input_ids=decoder_input_ids,
            shared_embedding=self.shared_embed_weight,
            attention_mask=decoder_attention_mask,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=attention_mask,
            head_mask=head_mask
        )

        &#47&#47 Rescale output before projecting on vocab
        &#47&#47 See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py&#47&#47L586
        sequence_output = decoder_hidden_states * (self.model_dim ** -0.5)
        lm_logits = self.lm_head(sequence_output)

        decoder_outputs = (lm_logits,decoder_hidden_states) &#47&#47 Add hidden states
        &#47&#47NOTE is not None
        &#47&#47 if lm_labels is not None:
        if is_not_None(lm_labels):
            &#47&#47 loss_fct = CrossEntropyLoss(ignore_index=-100)
            loss_fct = self.lm_loss
            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))
            &#47&#47 TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py&#47&#47L666
            decoder_outputs = (loss,) + decoder_outputs
        
        if self.output_only:
            return decoder_outputs[0]
        <a id="change">return </a>decoder_outputs + (encoder_hidden_states<a id="change"></a>,)

    def prepare_inputs_for_generation(self, input_ids, past, attention_mask, use_cache, **kwargs):
        &#47&#47NOTE is not None</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/cf27d37dff379f4b4e0abadd66985c1d78a55887#diff-8874e7c07d189804a7ce4eb1055b74111f1acffe8182f750b80a1621ca3d0a2dL414' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 98813254</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: cf27d37dff379f4b4e0abadd66985c1d78a55887</div><div id='time'> Time: 2020-06-24</div><div id='author'> Author: alondej@gmail.com</div><div id='file'> File Name: models/normal/NLP_models/modeling_t5_tied_weights.py</div><div id='m_class'> M Class Name: T5ForConditionalGeneration</div><div id='n_method'> N Class Name: T5ForConditionalGeneration</div><div id='m_method'> M Method Name: forward(7)</div><div id='n_method'> N Method Name: forward(12)</div><div id='m_parent_class'> M Parent Class: T5PreTrainedModel</div><div id='n_parent_class'> N Parent Class: T5PreTrainedModel</div><div id='m_file'> M File Name: models/normal/NLP_models/modeling_t5_tied_weights.py</div><div id='n_file'> N File Name: models/normal/NLP_models/modeling_t5_tied_weights.py</div><div id='m_start'> M Start Line: 418</div><div id='m_end'> M End Line: 544</div><div id='n_start'> N Start Line: 363</div><div id='n_end'> N End Line: 399</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Encode if needed (training, first prediction pass)
        &#47&#47NOTE is none
        &#47&#47 if encoder_outputs is None:
        <a id="change">if </a><a id="change">is_None(</a>encoder_outputs<a id="change">)</a>:
            &#47&#47 Convert encoder inputs in embeddings if needed
            <a id="change">encoder_outputs</a><a id="change"> = </a>self.encoder(
                input_ids=input_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, head_mask=head_mask
            )

        hidden_states<a id="change"> = </a><a id="change">encoder_outputs[0]</a>

        &#47&#47NOTE is not none, is none, is none
        &#47&#47 if lm_labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:
        if is_not_None(lm_labels) and is_None(decoder_input_ids) and is_None(decoder_inputs_embeds):
            &#47&#47 get decoder inputs from shifting lm labels to the right
            decoder_input_ids = self._shift_right(lm_labels)

        &#47&#47 If decoding with past key value states, only the last tokens
        &#47&#47 should be given as an input
        &#47&#47NOTE is not None
        &#47&#47 if decoder_past_key_value_states is not None:
        if is_not_None(decoder_past_key_value_states):
            &#47&#47NOTE is None
            &#47&#47 assert lm_labels is None, "Decoder should not use cached key value states when training."
            assert is_None(lm_labels), "Decoder should not use cached key value states when training."
            &#47&#47NOTE is not None
            &#47&#47 if decoder_input_ids is not None:
            if is_not_None(decoder_input_ids):
                decoder_input_ids = decoder_input_ids[:, -1:]
            &#47&#47NOTE is not None    
            &#47&#47 if decoder_inputs_embeds is not None:
            if is_not_None(decoder_inputs_embeds):
                decoder_inputs_embeds = decoder_inputs_embeds[:, -1:]

        &#47&#47 Decode
        decoder_outputs = self.decoder(
            input_ids=decoder_input_ids,
            attention_mask=decoder_attention_mask,
            inputs_embeds=decoder_inputs_embeds,
            past_key_value_states=decoder_past_key_value_states,
            encoder_hidden_states=hidden_states,
            encoder_attention_mask=attention_mask,
            head_mask=head_mask,
            use_cache=use_cache,
        )

        &#47&#47 insert decoder past at right place
        &#47&#47 to speed up decoding
        &#47&#47 if use_cache is True:
        <a id="change">if use_cache</a>:
            past = ((encoder_outputs<a id="change">, decoder_outputs[1]</a>)<a id="change"></a>,)
            decoder_outputs<a id="change"> = </a><a id="change">decoder_outputs[:1]</a><a id="change"> + past + decoder_outputs[2:]</a>

        sequence_output = <a id="change">decoder_outputs[0]</a>
        &#47&#47 Rescale output before projecting on vocab
        &#47&#47 See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py&#47&#47L586
        sequence_output = sequence_output * (self.model_dim ** -0.5)
        lm_logits = self.lm_head(sequence_output)

        decoder_outputs = (lm_logits,) + decoder_outputs[1:]  &#47&#47 Add hidden states and attention if they are here
        &#47&#47NOTE is not None
        &#47&#47 if lm_labels is not None:
        if is_not_None(lm_labels):
            &#47&#47 loss_fct = CrossEntropyLoss(ignore_index=-100)
            loss_fct = self.lm_loss
            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))
            &#47&#47 TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py&#47&#47L666
            decoder_outputs = (loss,) + decoder_outputs
        
        if self.output_only:
            return decoder_outputs[0]
        <a id="change">return </a>decoder_outputs<a id="change"> + </a>encoder_outputs

    def prepare_inputs_for_generation(self, input_ids, past, attention_mask, use_cache, **kwargs):
        &#47&#47NOTE is not None</code></pre><h3>After Change</h3><pre><code class='java'>
        


        <a id="change">encoder_hidden_states</a><a id="change"> = </a>self.encoder(input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask)

        &#47&#47NOTE is not none, is none, is none
        &#47&#47 if lm_labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:
        if is_not_None(lm_labels) and is_None(decoder_input_ids):
            &#47&#47 get decoder inputs from shifting lm labels to the right
            decoder_input_ids = self._shift_right(lm_labels)

        &#47&#47 Decode
        decoder_hidden_states = self.decoder(
            input_ids=decoder_input_ids,
            attention_mask=decoder_attention_mask,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=attention_mask,
            head_mask=head_mask
        )

        &#47&#47 Rescale output before projecting on vocab
        &#47&#47 See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py&#47&#47L586
        sequence_output = decoder_hidden_states * (self.model_dim ** -0.5)
        lm_logits = self.lm_head(sequence_output)

        decoder_outputs = (lm_logits,decoder_hidden_states) &#47&#47 Add hidden states
        &#47&#47NOTE is not None
        &#47&#47 if lm_labels is not None:
        if is_not_None(lm_labels):
            &#47&#47 loss_fct = CrossEntropyLoss(ignore_index=-100)
            loss_fct = self.lm_loss
            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))
            &#47&#47 TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py&#47&#47L666
            decoder_outputs = (loss,) + decoder_outputs
        
        if self.output_only:
            return decoder_outputs[0]
        <a id="change">return </a>decoder_outputs + (encoder_hidden_states<a id="change"></a>,)

    def prepare_inputs_for_generation(self, input_ids, past, attention_mask, use_cache, **kwargs):
        &#47&#47NOTE is not None</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/saareliad/ftpipe/commit/7ac977abea2c3ebc7af0cc51d0b4bba4653d6c9f#diff-86d860e66a6925f6c0f5eddcbc09a214146aaf7d51edec93751c364b30042ee4L1429' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 98813289</div><div id='project'> Project Name: saareliad/ftpipe</div><div id='commit'> Commit Name: 7ac977abea2c3ebc7af0cc51d0b4bba4653d6c9f</div><div id='time'> Time: 2020-07-07</div><div id='author'> Author: saareliad@campus.technion.ac.il</div><div id='file'> File Name: models/normal/NLP_models/modeling_t5.py</div><div id='m_class'> M Class Name: T5ForConditionalGeneration</div><div id='n_method'> N Class Name: T5ForConditionalGeneration</div><div id='m_method'> M Method Name: forward(7)</div><div id='n_method'> N Method Name: forward(12)</div><div id='m_parent_class'> M Parent Class: T5PreTrainedModel</div><div id='n_parent_class'> N Parent Class: T5PreTrainedModel</div><div id='m_file'> M File Name: models/normal/NLP_models/modeling_t5.py</div><div id='n_file'> N File Name: models/normal/NLP_models/modeling_t5.py</div><div id='m_start'> M Start Line: 1433</div><div id='m_end'> M End Line: 1557</div><div id='n_start'> N Start Line: 1027</div><div id='n_end'> N End Line: 1061</div><BR>