<html><h3>Pattern ID :28729
</h3><img src='84626315.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        keep_vars: bool = False,
    ) -&gt; Dict[str, Any]:
        self.flush()
        <a id="change">if destination is None</a>:
            <a id="change">destination = </a><a id="change">OrderedDict()</a>
            &#47&#47 pyre-ignore [16]
            destination._metadata<a id="change"> = </a><a id="change">OrderedDict()</a>

        <a id="change">for </a>config, <a id="change">param</a> in <a id="change">zip(
            </a>self._config.embedding_tables,
            <a id="change">self.split_embedding_weights()</a><a id="change">,
        )</a><a id="change">:
            key</a><a id="change"> = </a>prefix<a id="change"> + f"{config.name}.weight"</a>
            <a id="change">assert </a>config.local_rows == param.size(0)
            <a id="change">assert </a>config.local_cols == param.size(1)
            <a id="change">if config.global_metadata is not None</a>:
                &#47&#47 set additional field of sharded tensor based on local tensor properties
                config.global_metadata.tensor_properties.dtype<a id="change"> = </a>param.dtype
                config.global_metadata.tensor_properties.requires_grad<a id="change"> = </a>(
                    param.requires_grad
                )
                <a id="change">destination[
                    key
                ]</a><a id="change"> = ShardedTensor._init_from_local_shards_and_global_metadata(
                    local_shards=[Shard(param, config.local_metadata)],
                    sharded_tensor_metadata=config.global_metadata,
                    process_group=self._pg,
                )</a>
            else:
                <a id="change">destination[key] = param</a>
        <a id="change">return destination</a>

    def split_embedding_weights(self) -&gt; List[torch.Tensor]:
        return self.emb_module.split_embedding_weights()
</code></pre><h3>After Change</h3><pre><code class='java'>
        keep_vars: bool = False,
    ) -&gt; Dict[str, Any]:
        self.flush()
        <a id="change">return _get_state_dict(
            </a>self._config.embedding_tables,
            <a id="change">self.split_embedding_weights()</a>,
            self._pg,
            destination,
            prefix<a id="change">,
        )</a>

    def split_embedding_weights(self) -&gt; List[torch.Tensor]:
        return self.emb_module.split_embedding_weights()
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 26</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/a4440f145ac9df2ba467e9f5d591b4773779d28f#diff-19b6f02a42a3b6b147a160975a984ac3c98ebffdc15b720887f337ede50257b7L488' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 84626315</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: a4440f145ac9df2ba467e9f5d591b4773779d28f</div><div id='time'> Time: 2022-01-06</div><div id='author'> Author: yingliufb@fb.com</div><div id='file'> File Name: torchrec/distributed/embedding_lookup.py</div><div id='m_class'> M Class Name: BaseBatchedEmbedding</div><div id='n_method'> N Class Name: BaseBatchedEmbedding</div><div id='m_method'> M Method Name: state_dict(4)</div><div id='n_method'> N Method Name: state_dict(4)</div><div id='m_parent_class'> M Parent Class: BaseEmbedding</div><div id='n_parent_class'> N Parent Class: BaseEmbedding</div><div id='m_file'> M File Name: torchrec/distributed/embedding_lookup.py</div><div id='n_file'> N File Name: torchrec/distributed/embedding_lookup.py</div><div id='m_start'> M Start Line: 488</div><div id='m_end'> M End Line: 515</div><div id='n_start'> N Start Line: 521</div><div id='n_end'> N End Line: 527</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        keep_vars: bool = False,
    ) -&gt; Dict[str, Any]:
        self.flush()
        <a id="change">if destination is None</a>:
            <a id="change">destination = </a><a id="change">OrderedDict()</a>
            &#47&#47 pyre-ignore [16]
            destination._metadata<a id="change"> = </a><a id="change">OrderedDict()</a>

        <a id="change">for </a>config, <a id="change">param</a> in <a id="change">zip(
            </a>self._config.embedding_tables,
            <a id="change">self.split_embedding_weights()</a><a id="change">,
        )</a><a id="change">:
            key</a><a id="change"> = </a>prefix<a id="change"> + f"{config.name}.weight"</a>
            <a id="change">assert </a>config.local_rows == param.size(0)
            <a id="change">assert </a>config.local_cols == param.size(1)
            <a id="change">if config.global_metadata is not None</a>:
                &#47&#47 set additional field of sharded tensor based on local tensor properties
                config.global_metadata.tensor_properties.dtype<a id="change"> = </a>param.dtype
                config.global_metadata.tensor_properties.requires_grad<a id="change"> = </a>(
                    param.requires_grad
                )
                <a id="change">destination[
                    key
                ]</a><a id="change"> = ShardedTensor._init_from_local_shards_and_global_metadata(
                    local_shards=[Shard(param, config.local_metadata)],
                    sharded_tensor_metadata=config.global_metadata,
                    process_group=self._pg,
                )</a>
            else:
                <a id="change">destination[key] = </a>param
        <a id="change">return </a>destination

    def split_embedding_weights(self) -&gt; List[torch.Tensor]:
        return self.emb_module.split_embedding_weights()</code></pre><h3>After Change</h3><pre><code class='java'>
        keep_vars: bool = False,
    ) -&gt; Dict[str, Any]:
        self.flush()
        <a id="change">return _get_state_dict(
            </a>self._config.embedding_tables,
            <a id="change">self.split_embedding_weights()</a>,
            self._pg,
            destination,
            prefix<a id="change">,
        )</a>

    def split_embedding_weights(self) -&gt; List[torch.Tensor]:
        return self.emb_module.split_embedding_weights()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/a4440f145ac9df2ba467e9f5d591b4773779d28f#diff-19b6f02a42a3b6b147a160975a984ac3c98ebffdc15b720887f337ede50257b7L481' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 84626314</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: a4440f145ac9df2ba467e9f5d591b4773779d28f</div><div id='time'> Time: 2022-01-06</div><div id='author'> Author: yingliufb@fb.com</div><div id='file'> File Name: torchrec/distributed/embedding_lookup.py</div><div id='m_class'> M Class Name: BaseBatchedEmbedding</div><div id='n_method'> N Class Name: BaseBatchedEmbedding</div><div id='m_method'> M Method Name: state_dict(4)</div><div id='n_method'> N Method Name: state_dict(4)</div><div id='m_parent_class'> M Parent Class: BaseEmbedding</div><div id='n_parent_class'> N Parent Class: BaseEmbedding</div><div id='m_file'> M File Name: torchrec/distributed/embedding_lookup.py</div><div id='n_file'> N File Name: torchrec/distributed/embedding_lookup.py</div><div id='m_start'> M Start Line: 488</div><div id='m_end'> M End Line: 515</div><div id='n_start'> N Start Line: 521</div><div id='n_end'> N End Line: 527</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        keep_vars: bool = False,
    ) -&gt; Dict[str, Any]:
        self.flush()
        <a id="change">if destination is None</a>:
            <a id="change">destination = </a><a id="change">OrderedDict()</a>
            &#47&#47 pyre-ignore [16]
            destination._metadata<a id="change"> = </a><a id="change">OrderedDict()</a>

        <a id="change">for </a>config, <a id="change">param</a> in <a id="change">zip(
            </a>self._config.embedding_tables,
            <a id="change">self.split_embedding_weights()</a><a id="change">,
        )</a><a id="change">:
            key</a><a id="change"> = </a>prefix<a id="change"> + f"{config.name}.weight"</a>
            <a id="change">assert </a>config.local_rows == param.size(0)
            <a id="change">assert </a>config.local_cols == param.size(1)
            <a id="change">if config.global_metadata is not None</a>:
                &#47&#47 set additional field of sharded tensor based on local tensor properties
                config.global_metadata.tensor_properties.dtype<a id="change"> = </a>param.dtype
                config.global_metadata.tensor_properties.requires_grad<a id="change"> = </a>(
                    param.requires_grad
                )
                <a id="change">destination[
                    key
                ]</a><a id="change"> = ShardedTensor._init_from_local_shards_and_global_metadata(
                    local_shards=[Shard(param, config.local_metadata)],
                    sharded_tensor_metadata=config.global_metadata,
                    process_group=self._pg,
                )</a>
            else:
                <a id="change">destination[key] = </a>param
        <a id="change">return </a>destination

    def split_embedding_weights(self) -&gt; List[torch.Tensor]:
        return self.emb_module.split_embedding_weights()</code></pre><h3>After Change</h3><pre><code class='java'>
        keep_vars: bool = False,
    ) -&gt; Dict[str, Any]:
        self.flush()
        <a id="change">return _get_state_dict(
            </a>self._config.embedding_tables,
            <a id="change">self.split_embedding_weights()</a>,
            self._pg,
            destination,
            prefix<a id="change">,
        )</a>

    def split_embedding_weights(self) -&gt; List[torch.Tensor]:
        return self.emb_module.split_embedding_weights()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/a4440f145ac9df2ba467e9f5d591b4773779d28f#diff-19b6f02a42a3b6b147a160975a984ac3c98ebffdc15b720887f337ede50257b7L1055' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 84626313</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: a4440f145ac9df2ba467e9f5d591b4773779d28f</div><div id='time'> Time: 2022-01-06</div><div id='author'> Author: yingliufb@fb.com</div><div id='file'> File Name: torchrec/distributed/embedding_lookup.py</div><div id='m_class'> M Class Name: BaseBatchedEmbeddingBag</div><div id='n_method'> N Class Name: BaseBatchedEmbeddingBag</div><div id='m_method'> M Method Name: state_dict(4)</div><div id='n_method'> N Method Name: state_dict(4)</div><div id='m_parent_class'> M Parent Class: BaseEmbeddingBag</div><div id='n_parent_class'> N Parent Class: BaseEmbeddingBag</div><div id='m_file'> M File Name: torchrec/distributed/embedding_lookup.py</div><div id='n_file'> N File Name: torchrec/distributed/embedding_lookup.py</div><div id='m_start'> M Start Line: 1062</div><div id='m_end'> M End Line: 1089</div><div id='n_start'> N Start Line: 1051</div><div id='n_end'> N End Line: 1057</div><BR>