<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

def cross_entropy_loss(ctx: Context, src: jnp.ndarray, tgt: jnp.ndarray) -&gt; jnp.ndarray:
    src = src.reshape(ctx.dims.sizes.heads, -1, src.shape[-1])
    src<a id="change"> = </a>psum_scatter(src)  &#47&#47 model parallel -&gt; data parallel for loss computation

    normalization = ctx.dims.sizes.batch / tgt.size
    tgt = one_hot(tgt.astype(src.dtype), src.shape[-1])
    shifted = src<a id="change"> - </a>lax.stop_gradient(src).max(-1, keepdims=True)
    exp_shifted = <a id="change">jnp.exp(</a>shifted<a id="change">)</a>
    sum_exp = <a id="change">exp_shifted.sum(-1</a><a id="change">, keepdims=True)</a>
    out<a id="change"> = </a>((<a id="change">jnp.log(</a>sum_exp<a id="change">)</a><a id="change"> - </a>shifted) * tgt).sum(tuple(range(1, tgt.ndim)))
    <a id="change">return </a>lax.pmean(out * normalization, ParallelAxes.model)


def momentumnet_main(ctx: Context, fn: typing.Callable[[Context, jnp.ndarray, jnp.ndarray], jnp.ndarray]):</code></pre><h3>After Change</h3><pre><code class='java'>
    src = lax.psum(src, ParallelAxes.model)

    max_logit = lax.stop_gradient(src).max(-1, keepdims=True)
    log_z<a id="change"> = </a>lax.log(lax.exp(src - max_logit).sum(-1, keepdims=True)) + max_logit
    loss = (log_z - src) * one_hot(tgt.astype(src.dtype), src.shape[-1])
    loss = loss.sum() / tgt.size
    if ctx.training.z_loss:
        loss += jnp.square(log_z) * (ctx.training.z_loss / tgt.size)
    <a id="change">return </a>loss


def momentumnet_main(ctx: Context, fn: typing.Callable[[Context, jnp.ndarray, jnp.ndarray], jnp.ndarray]):</code></pre>