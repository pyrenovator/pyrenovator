<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        List[Metadata],
        Optional[List[TextDocument]],
    ]:
        inputs<a id="change"> = []</a>
        expanded_documents = []
        <a id="change">for </a>doc in documents<a id="change">:
            partitions: </a>Sequence[Span]
            if self.single_sentence:
                partitions = doc[self.sentence_annotation]
            else:
                partitions = [Span(start=0, end=len(doc.text))]
            for partition in partitions:
                encoding = self.tokenizer(
                    doc.text[partition.start : partition.end],
                    padding=False,
                    truncation=self.truncation,
                    max_length=self.max_length,
                    is_split_into_words=False,
                    return_offsets_mapping=True,
                    return_special_tokens_mask=True,
                )
                <a id="change">inputs.append(</a>encoding<a id="change">)</a>
                expanded_documents.append(doc)

        metadata<a id="change"> = </a>[
            {
                "offset_mapping": inp.pop("offset_mapping"),
                "special_tokens_mask": inp.pop("special_tokens_mask"),
            }
            for inp in inputs
        ]

        if self.single_sentence:
            i = 0
            for document in documents:
                for sentence_index in range(len(document[self.sentence_annotation])):
                    metadata[i]["sentence_index"]<a id="change"> = </a>sentence_index
                    i += 1

        <a id="change">return </a>inputs<a id="change">, metadata, expanded_documents</a>

    def encode_target(
        self,
        documents: List[TextDocument],</code></pre><h3>After Change</h3><pre><code class='java'>
        else:
            partitions = [Span(start=0, end=len(document.text))]

        task_encodings<a id="change">: List[TaskEncoding] = </a><a id="change">[]</a>
        for partition_idx, partition in enumerate(partitions):
            inputs = self.tokenizer(
                document.text[partition.start : partition.end],
                padding=False,
                truncation=self.truncation,
                max_length=self.max_length,
                is_split_into_words=False,
                return_offsets_mapping=True,
                return_special_tokens_mask=True,
            )

            metadata = {
                "offset_mapping": inputs.pop("offset_mapping"),
                "special_tokens_mask": inputs.pop("special_tokens_mask"),
            }

            if self.single_sentence:
                metadata["partition_idx"] = partition_idx

            task_encodings.append(
                TaskEncoding(
                    document=document,
                    inputs=inputs,
                    metadata=metadata,
                )
            )

        <a id="change">return </a>task_encodings

    def encode_target(
        self,</code></pre>