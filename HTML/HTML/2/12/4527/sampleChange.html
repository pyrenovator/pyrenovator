<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                                               position_ids=position_ids,
                                               head_mask=head_mask)

        hidden_states<a id="change"> = </a>transformer_outputs[0]
        &#47&#47 hidden_states should be torch.Size([1, 1024, 768]) after reshape
        &#47&#47 hidden_states=hidden_states.view(-1,self.n_positions,self.n_embed)
        lm_logits = self.lm_head(hidden_states)
        outputs = (lm_logits,) + transformer_outputs[1:]
        if labels is not None:
            &#47&#47 Shift so that tokens &lt; n predict n
            shift_logits<a id="change"> = </a>lm_logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            &#47&#47 Flatten the tokens
            &#47&#47 loss_fct = CrossEntropyLoss(ignore_index=-100)
            def loss_fct(logits, labels): return nn.functional.cross_entropy(
                logits, labels, ignore_index=-100)
            loss<a id="change"> = </a>loss_fct(<a id="change">shift_logits.view(-1</a>, <a id="change">shift_logits.size(-1</a><a id="change">)</a><a id="change">)</a>,
                            <a id="change">shift_labels.view(-1</a><a id="change">)</a>)
            
            &#47&#47 HACK: changed to output just the loss, somehow this improves partitioning results,
            &#47&#47 need to understand why.
            outputs = (loss,)
            &#47&#47 outputs = (loss,) + outputs

        &#47&#47 (loss), lm_logits, presents, (all hidden_states), (attentions)
        <a id="change">return </a>outputs


@add_start_docstrings(The GPT2 Model transformer with a language modeling and a multiple-choice classification</code></pre><h3>After Change</h3><pre><code class='java'>
        lm_logits = self.lm_head(hidden_states)
        &#47&#47 hidden_states should be torch.Size([1, 1024, 768]) after reshape
        &#47&#47 hidden_states=hidden_states.view(-1,self.n_positions,self.n_embed)
        output<a id="change"> = </a>self.compute_output(lm_logits,
                                     labels=labels)

        &#47&#47 loss or logits
        <a id="change">return </a>output


@add_start_docstrings(The GPT2 Model transformer with a language modeling and a multiple-choice classification</code></pre>