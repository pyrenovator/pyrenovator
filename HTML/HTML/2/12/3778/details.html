<html><h3>Pattern ID :3778
</h3><img src='14136375.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    train_target_dataset = dataset(root=args.root, task=args.target, download=True, transform=train_transform)
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_dataset<a id="change"> = </a><a id="change">dataset(root=args.root, task=args.target, download=True, transform=val_transform)</a>
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    <a id="change">if args.data == &quotDomainNet&quot</a>:
        test_dataset<a id="change"> = </a><a id="change">dataset(root=args.root, task=args.target, split=&quottest&quot, download=True, transform=val_transform)</a>
        test_loader<a id="change"> = </a>DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    else:
        test_loader<a id="change"> = </a>val_loader

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)</code></pre><h3>After Change</h3><pre><code class='java'>
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    test_loader<a id="change"> = </a>DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)

    &#47&#47 create model
    print("=&gt; using pre-trained model &quot{}&quot".format(args.arch))
    backbone = utils.get_model(args.arch)
    if args.add_pool:
        pool_layer = nn.Sequential(
            nn.AdaptiveAvgPool2d(output_size=(1, 1)),
            nn.Flatten()
        )
    else:
        pool_layer = nn.Identity()
    classifier = ImageClassifier(backbone, num_classes, bottleneck_dim=args.bottleneck_dim, pool_layer=pool_layer).to(device)
    domain_discri = DomainDiscriminator(in_feature=classifier.features_dim, hidden_size=1024).to(device)

    &#47&#47 define optimizer and lr scheduler
    optimizer = SGD(classifier.get_parameters() + domain_discri.get_parameters(),
                    args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True)
    lr_scheduler = LambdaLR(optimizer, lambda x:  args.lr * (1. + args.lr_gamma * float(x)) ** (-args.lr_decay))

    &#47&#47 define loss function
    domain_adv = DomainAdversarialLoss(domain_discri).to(device)

    &#47&#47 resume from the best checkpoint
    if args.phase != &quottrain&quot:
        checkpoint = torch.load(logger.get_checkpoint_path(&quotbest&quot), map_location=&quotcpu&quot)
        classifier.load_state_dict(checkpoint)

    &#47&#47 analysis the model
    if args.phase == &quotanalysis&quot:
        &#47&#47 extract features from both domains
        feature_extractor = nn.Sequential(classifier.backbone, classifier.bottleneck).to(device)
        source_feature = collect_feature(train_source_loader, feature_extractor, device)
        target_feature = collect_feature(train_target_loader, feature_extractor, device)
        &#47&#47 plot t-SNE
        tSNE_filename = osp.join(logger.visualize_directory, &quotTSNE.png&quot)
        tsne.visualize(source_feature, target_feature, tSNE_filename)
        print("Saving t-SNE to", tSNE_filename)
        &#47&#47 calculate A-distance, which is a measure for distribution discrepancy
        A_distance = a_distance.calculate(source_feature, target_feature, device)
        print("A-distance =", A_distance)
        return

    if args.phase == &quottest&quot:
        acc1 = utils.validate(test_loader, classifier, args, device)
        print(acc1)
        return

    &#47&#47 start training
    best_acc1 = 0.
    for epoch in range(args.epochs):
        &#47&#47 train for one epoch
        train(train_source_iter, train_target_iter, classifier, domain_adv, optimizer,
              lr_scheduler, epoch, args)

        &#47&#47 evaluate on validation set
        acc1 = utils.validate(val_loader, classifier, args, device)

        &#47&#47 remember best acc@1 and save checkpoint
        torch.save(classifier.state_dict(), logger.get_checkpoint_path(&quotlatest&quot))
        if acc1 &gt; best_acc1:
            shutil.copy(logger.get_checkpoint_path(&quotlatest&quot), logger.get_checkpoint_path(&quotbest&quot))
        best_acc1 = max(acc1, best_acc1)

    print("best_acc1 = {:3.1f}".format(best_acc1))

    &#47&#47 evaluate on test set
    classifier.load_state_dict(torch.load(logger.get_checkpoint_path(&quotbest&quot)))
    acc1<a id="change"> = </a><a id="change">utils.validate(</a>test_loader, classifier, args, device<a id="change">)</a>
    print("test_acc1 = {:3.1f}".format(acc1))

    logger.close()
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 9</div><BR><div id='size'>Non-data size: 11</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/thuml/transfer-learning-library/commit/f7950ee17554dc6b3e19c13851106cda21041bca#diff-e1577c32e8bf31b8e3667c1b72d19356bc2d5101bc1fc28aaa6ed91700d11bd7L68' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14136375</div><div id='project'> Project Name: thuml/transfer-learning-library</div><div id='commit'> Commit Name: f7950ee17554dc6b3e19c13851106cda21041bca</div><div id='time'> Time: 2021-07-26</div><div id='author'> Author: 13126830206@163.com</div><div id='file'> File Name: examples/domain_adaptation/classification/dann.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/domain_adaptation/classification/dann.py</div><div id='n_file'> N File Name: examples/domain_adaptation/classification/dann.py</div><div id='m_start'> M Start Line: 74</div><div id='m_end'> M End Line: 151</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 153</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    train_target_dataset = dataset(root=args.root, task=args.target, download=True, transform=MultipleApply([train_transform, val_transform]))
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_dataset<a id="change"> = </a><a id="change">dataset(root=args.root, task=args.target, download=True, transform=val_transform)</a>
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    <a id="change">if args.data == &quotDomainNet&quot</a>:
        test_dataset<a id="change"> = </a><a id="change">dataset(root=args.root, task=args.target, split=&quottest&quot, download=True,
                               transform=val_transform)</a>
        test_loader<a id="change"> = </a>DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    else:
        test_loader<a id="change"> = </a>val_loader

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)</code></pre><h3>After Change</h3><pre><code class='java'>
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    test_loader<a id="change"> = </a>DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)

    &#47&#47 create model
    backbone = utils.get_model(args.arch)
    pool_layer = nn.Identity() if args.no_pool else None
    classifier = ImageClassifier(backbone, num_classes, bottleneck_dim=args.bottleneck_dim, pool_layer=pool_layer).to(device)

    &#47&#47 define optimizer and lr scheduler
    optimizer = Adam(classifier.get_parameters(), args.lr)
    lr_scheduler = LambdaLR(optimizer, lambda x: args.lr * (1. + args.lr_gamma * float(x)) ** (-args.lr_decay))

    &#47&#47 resume from the best checkpoint
    if args.phase != &quottrain&quot:
        checkpoint = torch.load(logger.get_checkpoint_path(&quotbest&quot), map_location=&quotcpu&quot)
        classifier.load_state_dict(checkpoint)

    &#47&#47 analysis the model
    if args.phase == &quotanalysis&quot:
        &#47&#47 extract features from both domains
        feature_extractor = nn.Sequential(classifier.backbone, classifier.bottleneck).to(device)
        source_feature = collect_feature(train_source_loader, feature_extractor, device)
        target_feature = collect_feature(train_target_loader, feature_extractor, device)
        &#47&#47 plot t-SNE
        tSNE_filename = osp.join(logger.visualize_directory, &quotTSNE.png&quot)
        tsne.visualize(source_feature, target_feature, tSNE_filename)
        print("Saving t-SNE to", tSNE_filename)
        &#47&#47 calculate A-distance, which is a measure for distribution discrepancy
        A_distance = a_distance.calculate(source_feature, target_feature, device)
        print("A-distance =", A_distance)
        return

    if args.phase == &quottest&quot:
        acc1 = utils.validate(test_loader, classifier, args, device)
        print(acc1)
        return

    if args.pretrain is None:
        &#47&#47 first pretrain the classifier wish source data
        print("Pretraining the model on source domain.")
        args.pretrain = logger.get_checkpoint_path(&quotpretrain&quot)
        pretrained_model = ImageClassifier(backbone, num_classes, args.bottleneck_dim).to(device)
        pretrain_optimizer = Adam(pretrained_model.get_parameters(), args.pretrain_lr)
        pretrain_lr_scheduler = LambdaLR(pretrain_optimizer,
                                         lambda x: args.pretrain_lr * (1. + args.lr_gamma * float(x)) ** (
                                             -args.lr_decay))

        &#47&#47 start pretraining
        for epoch in range(args.pretrain_epochs):
            &#47&#47 pretrain for one epoch
            pretrain(train_source_iter, pretrained_model, pretrain_optimizer, pretrain_lr_scheduler, epoch, args)
            &#47&#47 validate to show pretrain process
            utils.validate(val_loader, pretrained_model, args, device)

        torch.save(pretrained_model.state_dict(), args.pretrain)

    checkpoint = torch.load(args.pretrain, map_location=&quotcpu&quot)
    classifier.load_state_dict(checkpoint)
    teacher = EmaTeacher(classifier, alpha=args.alpha)
    consistent_loss = L2ConsistencyLoss().to(device)
    class_balance_loss = ClassBalanceLoss(num_classes).to(device)

    &#47&#47 start training
    best_acc1 = 0.
    for epoch in range(args.epochs):
        print(lr_scheduler.get_lr())
        &#47&#47 train for one epoch
        train(train_source_iter, train_target_iter, classifier, teacher, consistent_loss, class_balance_loss, optimizer,
              lr_scheduler, epoch, args)

        &#47&#47 evaluate on validation set
        acc1 = utils.validate(val_loader, classifier, args, device)

        &#47&#47 remember best acc@1 and save checkpoint
        torch.save(classifier.state_dict(), logger.get_checkpoint_path(&quotlatest&quot))
        if acc1 &gt; best_acc1:
            shutil.copy(logger.get_checkpoint_path(&quotlatest&quot), logger.get_checkpoint_path(&quotbest&quot))
        best_acc1 = max(acc1, best_acc1)

    print("best_acc1 = {:3.1f}".format(best_acc1))

    &#47&#47 evaluate on test set
    classifier.load_state_dict(torch.load(logger.get_checkpoint_path(&quotbest&quot)))
    acc1<a id="change"> = </a><a id="change">utils.validate(</a>test_loader, classifier, args, device<a id="change">)</a>
    print("test_acc1 = {:3.1f}".format(acc1))

    logger.close()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thuml/transfer-learning-library/commit/ac06563cc969a7128fe67f950eafea6c805ef10d#diff-a53dd9060d9eb51c8de23097a754193edef5d750fd5f860a3e598e0b2939efa2L32' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14136359</div><div id='project'> Project Name: thuml/transfer-learning-library</div><div id='commit'> Commit Name: ac06563cc969a7128fe67f950eafea6c805ef10d</div><div id='time'> Time: 2021-07-26</div><div id='author'> Author: 13126830206@163.com</div><div id='file'> File Name: examples/domain_adaptation/classification/self_ensemble.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/domain_adaptation/classification/self_ensemble.py</div><div id='n_file'> N File Name: examples/domain_adaptation/classification/self_ensemble.py</div><div id='m_start'> M Start Line: 66</div><div id='m_end'> M End Line: 169</div><div id='n_start'> N Start Line: 51</div><div id='n_end'> N End Line: 160</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    train_source_dataset = dataset(root=args.root, task=args.source, download=True, transform=train_transform)
    train_source_loader = DataLoader(train_source_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_dataset<a id="change"> = </a><a id="change">dataset(root=args.root, task=args.target, download=True, transform=val_transform)</a>
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    <a id="change">if args.data == &quotDomainNet&quot</a>:
        test_dataset<a id="change"> = </a><a id="change">dataset(root=args.root, task=args.target, split=&quottest&quot, download=True, transform=val_transform)</a>
        test_loader<a id="change"> = </a>DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    else:
        test_loader<a id="change"> = </a>val_loader

    train_source_iter = ForeverDataIterator(train_source_loader)
</code></pre><h3>After Change</h3><pre><code class='java'>
    train_source_loader = DataLoader(train_source_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    test_loader<a id="change"> = </a>DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)

    train_source_iter = ForeverDataIterator(train_source_loader)

    &#47&#47 create model
    print("=&gt; using pre-trained model &quot{}&quot".format(args.arch))
    &#47&#47 backbone = models.__dict__[args.arch](pretrained=True)
    backbone = utils.get_models(args.arch)
    classifier = Classifier(backbone, num_classes).to(device)

    &#47&#47 define optimizer and lr scheduler
    optimizer = SGD(classifier.get_parameters(), args.lr, momentum=args.momentum, weight_decay=args.wd, nesterov=True)
    lr_scheduler = LambdaLR(optimizer, lambda x:  args.lr * (1. + args.lr_gamma * float(x)) ** (-args.lr_decay))

    &#47&#47 resume from the best checkpoint
    if args.phase != &quottrain&quot:
        checkpoint = torch.load(logger.get_checkpoint_path(&quotbest&quot), map_location=&quotcpu&quot)
        classifier.load_state_dict(checkpoint)

    &#47&#47 analysis the model
    if args.phase == &quotanalysis&quot:
        &#47&#47 using shuffled val loader
        val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers)
        &#47&#47 extract features from both domains
        feature_extractor = nn.Sequential(classifier.backbone, classifier.bottleneck).to(device)
        source_feature = collect_feature(train_source_loader, feature_extractor, device)
        target_feature = collect_feature(val_loader, feature_extractor, device)
        &#47&#47 plot t-SNE
        tSNE_filename = osp.join(logger.visualize_directory, &quotTSNE.png&quot)
        tsne.visualize(source_feature, target_feature, tSNE_filename)
        print("Saving t-SNE to", tSNE_filename)
        &#47&#47 calculate A-distance, which is a measure for distribution discrepancy
        A_distance = a_distance.calculate(source_feature, target_feature, device)
        print("A-distance =", A_distance)
        return

    if args.phase == &quottest&quot:
        acc1 = utils.validate(test_loader, classifier, args, device)
        print(acc1)
        return

    &#47&#47 start training
    best_acc1 = 0.
    for epoch in range(args.epochs):
        &#47&#47 train for one epoch
        train(train_source_iter, classifier, optimizer,
              lr_scheduler, epoch, args)

        &#47&#47 evaluate on validation set
        acc1 = utils.validate(val_loader, classifier, args, device)

        &#47&#47 remember best acc@1 and save checkpoint
        torch.save(classifier.state_dict(), logger.get_checkpoint_path(&quotlatest&quot))
        if acc1 &gt; best_acc1:
            shutil.copy(logger.get_checkpoint_path(&quotlatest&quot), logger.get_checkpoint_path(&quotbest&quot))
        best_acc1 = max(acc1, best_acc1)

    print("best_acc1 = {:3.1f}".format(best_acc1))

    &#47&#47 evaluate on test set
    classifier.load_state_dict(torch.load(logger.get_checkpoint_path(&quotbest&quot)))
    acc1<a id="change"> = </a><a id="change">utils.validate(</a>test_loader, classifier, args, device<a id="change">)</a>
    print("test_acc1 = {:3.1f}".format(acc1))

    logger.close()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thuml/transfer-learning-library/commit/0520dbca23da2054e0264ce23b6639bad44c22f8#diff-fdaf3651e882d2abfd0cad255cddb03cf31f7d91056b9116b6e9ac0eacfe5403L32' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14136373</div><div id='project'> Project Name: thuml/transfer-learning-library</div><div id='commit'> Commit Name: 0520dbca23da2054e0264ce23b6639bad44c22f8</div><div id='time'> Time: 2021-07-25</div><div id='author'> Author: 13126830206@163.com</div><div id='file'> File Name: examples/domain_adaptation/classification/source_only.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/domain_adaptation/classification/source_only.py</div><div id='n_file'> N File Name: examples/domain_adaptation/classification/source_only.py</div><div id='m_start'> M Start Line: 73</div><div id='m_end'> M End Line: 144</div><div id='n_start'> N Start Line: 69</div><div id='n_end'> N End Line: 152</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    train_target_dataset = dataset(root=args.root, task=args.target, download=True, transform=train_transform)
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_dataset<a id="change"> = </a><a id="change">dataset(root=args.root, task=args.target, download=True, transform=val_transform)</a>
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    <a id="change">if args.data == &quotDomainNet&quot</a>:
        test_dataset<a id="change"> = </a><a id="change">dataset(root=args.root, task=args.target, split=&quottest&quot, download=True, transform=val_transform)</a>
        test_loader<a id="change"> = </a>DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    else:
        test_loader<a id="change"> = </a>val_loader

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)</code></pre><h3>After Change</h3><pre><code class='java'>
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    test_loader<a id="change"> = </a>DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)

    &#47&#47 create model
    print("=&gt; using pre-trained model &quot{}&quot".format(args.arch))
    backbone = utils.get_model(args.arch)
    pool_layer = nn.Identity() if args.no_pool else None
    classifier = ImageClassifier(backbone, num_classes, bottleneck_dim=args.bottleneck_dim, pool_layer=pool_layer).to(device)

    &#47&#47 define loss function
    if args.adversarial:
        thetas = [Theta(dim).to(device) for dim in (classifier.features_dim, num_classes)]
    else:
        thetas = None
    jmmd_loss = JointMultipleKernelMaximumMeanDiscrepancy(
        kernels=(
            [GaussianKernel(alpha=2 ** k) for k in range(-3, 2)],
            (GaussianKernel(sigma=0.92, track_running_stats=False),)
        ),
        linear=args.linear, thetas=thetas
    ).to(device)

    parameters = classifier.get_parameters()
    if thetas is not None:
        parameters += [{"params": theta.parameters(), &quotlr&quot: 0.1} for theta in thetas]

    &#47&#47 define optimizer
    optimizer = SGD(parameters, args.lr, momentum=args.momentum, weight_decay=args.wd, nesterov=True)
    lr_scheduler = LambdaLR(optimizer, lambda x:  args.lr * (1. + args.lr_gamma * float(x)) ** (-args.lr_decay))

    &#47&#47 resume from the best checkpoint
    if args.phase != &quottrain&quot:
        checkpoint = torch.load(logger.get_checkpoint_path(&quotbest&quot), map_location=&quotcpu&quot)
        classifier.load_state_dict(checkpoint)

    &#47&#47 analysis the model
    if args.phase == &quotanalysis&quot:
        &#47&#47 extract features from both domains
        feature_extractor = nn.Sequential(classifier.backbone, classifier.bottleneck).to(device)
        source_feature = collect_feature(train_source_loader, feature_extractor, device)
        target_feature = collect_feature(train_target_loader, feature_extractor, device)
        &#47&#47 plot t-SNE
        tSNE_filename = osp.join(logger.visualize_directory, &quotTSNE.png&quot)
        tsne.visualize(source_feature, target_feature, tSNE_filename)
        print("Saving t-SNE to", tSNE_filename)
        &#47&#47 calculate A-distance, which is a measure for distribution discrepancy
        A_distance = a_distance.calculate(source_feature, target_feature, device)
        print("A-distance =", A_distance)
        return

    if args.phase == &quottest&quot:
        acc1 = utils.validate(test_loader, classifier, args, device)
        print(acc1)
        return

    &#47&#47 start training
    best_acc1 = 0.
    for epoch in range(args.epochs):
        &#47&#47 train for one epoch
        train(train_source_iter, train_target_iter, classifier, jmmd_loss, optimizer,
              lr_scheduler, epoch, args)

        &#47&#47 evaluate on validation set
        acc1 = utils.validate(val_loader, classifier, args, device)

        &#47&#47 remember best acc@1 and save checkpoint
        torch.save(classifier.state_dict(), logger.get_checkpoint_path(&quotlatest&quot))
        if acc1 &gt; best_acc1:
            shutil.copy(logger.get_checkpoint_path(&quotlatest&quot), logger.get_checkpoint_path(&quotbest&quot))
        best_acc1 = max(acc1, best_acc1)

    print("best_acc1 = {:3.1f}".format(best_acc1))

    &#47&#47 evaluate on test set
    classifier.load_state_dict(torch.load(logger.get_checkpoint_path(&quotbest&quot)))
    acc1<a id="change"> = </a><a id="change">utils.validate(</a>test_loader, classifier, args, device<a id="change">)</a>
    print("test_acc1 = {:3.1f}".format(acc1))

    logger.close()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thuml/transfer-learning-library/commit/ac06563cc969a7128fe67f950eafea6c805ef10d#diff-498f235b539b3e5184453e7bbf1bbca220258d3ca12dabdf7175d59730c23391L33' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14136389</div><div id='project'> Project Name: thuml/transfer-learning-library</div><div id='commit'> Commit Name: ac06563cc969a7128fe67f950eafea6c805ef10d</div><div id='time'> Time: 2021-07-26</div><div id='author'> Author: 13126830206@163.com</div><div id='file'> File Name: examples/domain_adaptation/classification/jan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/domain_adaptation/classification/jan.py</div><div id='n_file'> N File Name: examples/domain_adaptation/classification/jan.py</div><div id='m_start'> M Start Line: 74</div><div id='m_end'> M End Line: 164</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 159</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    train_target_dataset = dataset(root=args.root, task=args.target, download=True, transform=train_transform)
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_dataset<a id="change"> = </a><a id="change">dataset(root=args.root, task=args.target, download=True, transform=val_transform)</a>
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    <a id="change">if args.data == &quotDomainNet&quot</a>:
        test_dataset<a id="change"> = </a><a id="change">dataset(root=args.root, task=args.target, split=&quottest&quot, download=True, transform=val_transform)</a>
        test_loader<a id="change"> = </a>DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    else:
        test_loader<a id="change"> = </a>val_loader

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)</code></pre><h3>After Change</h3><pre><code class='java'>
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    test_loader<a id="change"> = </a>DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)

    &#47&#47 create model
    print("=&gt; using pre-trained model &quot{}&quot".format(args.arch))
    backbone = utils.get_model(args.arch)
    if args.add_pool:
        pool_layer = nn.Sequential(
            nn.AdaptiveAvgPool2d(output_size=(1, 1)),
            nn.Flatten()
        )
    else:
        pool_layer = nn.Identity()
    classifier = ImageClassifier(backbone, num_classes, bottleneck_dim=args.bottleneck_dim,
                                 width=args.bottleneck_dim, pool_layer=pool_layer).to(device)
    mdd = MarginDisparityDiscrepancy(args.margin).to(device)

    &#47&#47 define optimizer and lr_scheduler
    &#47&#47 The learning rate of the classiﬁers are set 10 times to that of the feature extractor by default.
    optimizer = SGD(classifier.get_parameters(), args.lr, momentum=args.momentum, weight_decay=args.wd, nesterov=True)
    lr_scheduler = LambdaLR(optimizer, lambda x:  args.lr * (1. + args.lr_gamma * float(x)) ** (-args.lr_decay))

    &#47&#47 resume from the best checkpoint
    if args.phase != &quottrain&quot:
        checkpoint = torch.load(logger.get_checkpoint_path(&quotbest&quot), map_location=&quotcpu&quot)
        classifier.load_state_dict(checkpoint)

    &#47&#47 analysis the model
    if args.phase == &quotanalysis&quot:
        &#47&#47 extract features from both domains
        feature_extractor = nn.Sequential(classifier.backbone, classifier.bottleneck).to(device)
        source_feature = collect_feature(train_source_loader, feature_extractor, device)
        target_feature = collect_feature(train_target_loader, feature_extractor, device)
        &#47&#47 plot t-SNE
        tSNE_filename = osp.join(logger.visualize_directory, &quotTSNE.png&quot)
        tsne.visualize(source_feature, target_feature, tSNE_filename)
        print("Saving t-SNE to", tSNE_filename)
        &#47&#47 calculate A-distance, which is a measure for distribution discrepancy
        A_distance = a_distance.calculate(source_feature, target_feature, device)
        print("A-distance =", A_distance)
        return

    if args.phase == &quottest&quot:
        acc1 = utils.validate(test_loader, classifier, args, device)
        print(acc1)
        return

    &#47&#47 start training
    best_acc1 = 0.
    for epoch in range(args.epochs):
        &#47&#47 train for one epoch
        train(train_source_iter, train_target_iter, classifier, mdd, optimizer,
              lr_scheduler, epoch, args)

        &#47&#47 evaluate on validation set
        acc1 = utils.validate(val_loader, classifier, args, device)

        &#47&#47 remember best acc@1 and save checkpoint
        torch.save(classifier.state_dict(), logger.get_checkpoint_path(&quotlatest&quot))
        if acc1 &gt; best_acc1:
            shutil.copy(logger.get_checkpoint_path(&quotlatest&quot), logger.get_checkpoint_path(&quotbest&quot))
        best_acc1 = max(acc1, best_acc1)

    print("best_acc1 = {:3.1f}".format(best_acc1))

    &#47&#47 evaluate on test set
    classifier.load_state_dict(torch.load(logger.get_checkpoint_path(&quotbest&quot)))
    acc1<a id="change"> = </a><a id="change">utils.validate(</a>test_loader, classifier, args, device<a id="change">)</a>
    print("test_acc1 = {:3.1f}".format(acc1))

    logger.close()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thuml/transfer-learning-library/commit/f7950ee17554dc6b3e19c13851106cda21041bca#diff-e8a8c7adef9dde1bb6da9910af77d5815a0ffae2ded9b5a78c6b16a4e7fc0e02L33' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14136368</div><div id='project'> Project Name: thuml/transfer-learning-library</div><div id='commit'> Commit Name: f7950ee17554dc6b3e19c13851106cda21041bca</div><div id='time'> Time: 2021-07-26</div><div id='author'> Author: 13126830206@163.com</div><div id='file'> File Name: examples/domain_adaptation/classification/mdd.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/domain_adaptation/classification/mdd.py</div><div id='n_file'> N File Name: examples/domain_adaptation/classification/mdd.py</div><div id='m_start'> M Start Line: 74</div><div id='m_end'> M End Line: 150</div><div id='n_start'> N Start Line: 67</div><div id='n_end'> N End Line: 150</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    train_target_dataset = dataset(root=args.root, task=args.target, download=True, transform=train_transform)
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_dataset<a id="change"> = </a><a id="change">dataset(root=args.root, task=args.target, download=True, transform=val_transform)</a>
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    <a id="change">if args.data == &quotDomainNet&quot</a>:
        test_dataset<a id="change"> = </a><a id="change">dataset(root=args.root, task=args.target, split=&quottest&quot, download=True, transform=val_transform)</a>
        test_loader<a id="change"> = </a>DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    else:
        test_loader<a id="change"> = </a>val_loader

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)</code></pre><h3>After Change</h3><pre><code class='java'>
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    test_loader<a id="change"> = </a>DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)

    &#47&#47 create model
    print("=&gt; using pre-trained model &quot{}&quot".format(args.arch))
    backbone = utils.get_model(args.arch)
    pool_layer = nn.Identity() if args.no_pool else None
    classifier = ImageClassifier(backbone, num_classes, bottleneck_dim=args.bottleneck_dim, pool_layer=pool_layer).to(device)
    classifier_feature_dim = classifier.features_dim

    if args.randomized:
        domain_discri = DomainDiscriminator(args.randomized_dim, hidden_size=1024).to(device)
    else:
        domain_discri = DomainDiscriminator(classifier_feature_dim * num_classes, hidden_size=1024).to(device)

    all_parameters = classifier.get_parameters() + domain_discri.get_parameters()
    &#47&#47 define optimizer and lr scheduler
    optimizer = SGD(all_parameters, args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True)
    lr_scheduler = LambdaLR(optimizer, lambda x: args.lr * (1. + args.lr_gamma * float(x)) ** (-args.lr_decay))

    &#47&#47 define loss function
    domain_adv = ConditionalDomainAdversarialLoss(
        domain_discri, entropy_conditioning=args.entropy,
        num_classes=num_classes, features_dim=classifier_feature_dim, randomized=args.randomized,
        randomized_dim=args.randomized_dim
    ).to(device)

    &#47&#47 resume from the best checkpoint
    if args.phase != &quottrain&quot:
        checkpoint = torch.load(logger.get_checkpoint_path(&quotbest&quot), map_location=&quotcpu&quot)
        classifier.load_state_dict(checkpoint)

    &#47&#47 analysis the model
    if args.phase == &quotanalysis&quot:
        &#47&#47 extract features from both domains
        feature_extractor = nn.Sequential(classifier.backbone, classifier.bottleneck).to(device)
        source_feature = collect_feature(train_source_loader, feature_extractor, device)
        target_feature = collect_feature(train_target_loader, feature_extractor, device)
        &#47&#47 plot t-SNE
        tSNE_filename = osp.join(logger.visualize_directory, &quotTSNE.png&quot)
        tsne.visualize(source_feature, target_feature, tSNE_filename)
        print("Saving t-SNE to", tSNE_filename)
        &#47&#47 calculate A-distance, which is a measure for distribution discrepancy
        A_distance = a_distance.calculate(source_feature, target_feature, device)
        print("A-distance =", A_distance)
        return

    if args.phase == &quottest&quot:
        acc1 = utils.validate(test_loader, classifier, args, device)
        print(acc1)
        return

    &#47&#47 start training
    best_acc1 = 0.
    for epoch in range(args.epochs):
        print("lr:", lr_scheduler.get_last_lr()[0])
        &#47&#47 train for one epoch
        train(train_source_iter, train_target_iter, classifier, domain_adv, optimizer,
              lr_scheduler, epoch, args)

        &#47&#47 evaluate on validation set
        acc1 = utils.validate(val_loader, classifier, args, device)

        &#47&#47 remember best acc@1 and save checkpoint
        torch.save(classifier.state_dict(), logger.get_checkpoint_path(&quotlatest&quot))
        if acc1 &gt; best_acc1:
            shutil.copy(logger.get_checkpoint_path(&quotlatest&quot), logger.get_checkpoint_path(&quotbest&quot))
        best_acc1 = max(acc1, best_acc1)

    print("best_acc1 = {:3.1f}".format(best_acc1))

    &#47&#47 evaluate on test set
    classifier.load_state_dict(torch.load(logger.get_checkpoint_path(&quotbest&quot)))
    acc1<a id="change"> = </a><a id="change">utils.validate(</a>test_loader, classifier, args, device<a id="change">)</a>
    print("test_acc1 = {:3.1f}".format(acc1))

    logger.close()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thuml/transfer-learning-library/commit/ac06563cc969a7128fe67f950eafea6c805ef10d#diff-3ef94bff8ea381a404378a0a2a2df338dd26da2fa9da1a66d3033bcac665bd5aL33' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14136367</div><div id='project'> Project Name: thuml/transfer-learning-library</div><div id='commit'> Commit Name: ac06563cc969a7128fe67f950eafea6c805ef10d</div><div id='time'> Time: 2021-07-26</div><div id='author'> Author: 13126830206@163.com</div><div id='file'> File Name: examples/domain_adaptation/classification/cdan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/domain_adaptation/classification/cdan.py</div><div id='n_file'> N File Name: examples/domain_adaptation/classification/cdan.py</div><div id='m_start'> M Start Line: 74</div><div id='m_end'> M End Line: 162</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 157</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    train_target_dataset = dataset(root=args.root, task=args.target, download=True, transform=train_transform)
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_dataset<a id="change"> = </a><a id="change">dataset(root=args.root, task=args.target, download=True, transform=val_transform)</a>
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    <a id="change">if args.data == &quotDomainNet&quot</a>:
        test_dataset<a id="change"> = </a><a id="change">dataset(root=args.root, task=args.target, split=&quottest&quot, download=True, transform=val_transform)</a>
        test_loader<a id="change"> = </a>DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    else:
        test_loader<a id="change"> = </a>val_loader

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)</code></pre><h3>After Change</h3><pre><code class='java'>
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    test_loader<a id="change"> = </a>DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)

    &#47&#47 create model
    print("=&gt; using pre-trained model &quot{}&quot".format(args.arch))
    backbone = utils.get_model(args.arch)
    pool_layer = nn.Identity() if args.no_pool else None
    classifier = ImageClassifier(backbone, num_classes, args.num_blocks,
                                 bottleneck_dim=args.bottleneck_dim, dropout_p=args.dropout_p, pool_layer=pool_layer).to(device)
    adaptive_feature_norm = AdaptiveFeatureNorm(args.delta).to(device)

    &#47&#47 define optimizer
    &#47&#47 the learning rate is fixed according to origin paper
    optimizer = SGD(classifier.get_parameters(), args.lr, weight_decay=args.weight_decay)

    &#47&#47 resume from the best checkpoint
    if args.phase != &quottrain&quot:
        checkpoint = torch.load(logger.get_checkpoint_path(&quotbest&quot), map_location=&quotcpu&quot)
        classifier.load_state_dict(checkpoint)

    &#47&#47 analysis the model
    if args.phase == &quotanalysis&quot:
        &#47&#47 extract features from both domains
        feature_extractor = nn.Sequential(classifier.backbone, classifier.bottleneck).to(device)
        source_feature = collect_feature(train_source_loader, feature_extractor, device)
        target_feature = collect_feature(train_target_loader, feature_extractor, device)
        &#47&#47 plot t-SNE
        tSNE_filename = osp.join(logger.visualize_directory, &quotTSNE.png&quot)
        tsne.visualize(source_feature, target_feature, tSNE_filename)
        print("Saving t-SNE to", tSNE_filename)
        &#47&#47 calculate A-distance, which is a measure for distribution discrepancy
        A_distance = a_distance.calculate(source_feature, target_feature, device)
        print("A-distance =", A_distance)
        return

    if args.phase == &quottest&quot:
        acc1 = utils.validate(test_loader, classifier, args, device)
        print(acc1)
        return

    &#47&#47 start training
    best_acc1 = 0.
    for epoch in range(args.epochs):
        &#47&#47 train for one epoch
        train(train_source_iter, train_target_iter, classifier, adaptive_feature_norm, optimizer, epoch, args)

        &#47&#47 evaluate on validation set
        acc1 = utils.validate(val_loader, classifier, args, device)

        &#47&#47 remember best acc@1 and save checkpoint
        torch.save(classifier.state_dict(), logger.get_checkpoint_path(&quotlatest&quot))
        if acc1 &gt; best_acc1:
            shutil.copy(logger.get_checkpoint_path(&quotlatest&quot), logger.get_checkpoint_path(&quotbest&quot))
        best_acc1 = max(acc1, best_acc1)

    print("best_acc1 = {:3.1f}".format(best_acc1))

    &#47&#47 evaluate on test set
    classifier.load_state_dict(torch.load(logger.get_checkpoint_path(&quotbest&quot)))
    acc1<a id="change"> = </a><a id="change">utils.validate(</a>test_loader, classifier, args, device<a id="change">)</a>
    print("test_acc1 = {:3.1f}".format(acc1))

    logger.close()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thuml/transfer-learning-library/commit/ac06563cc969a7128fe67f950eafea6c805ef10d#diff-b392ddc855b086aad555bf78134cde06c28182d621e3c322e997196a81f8de59L32' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14136380</div><div id='project'> Project Name: thuml/transfer-learning-library</div><div id='commit'> Commit Name: ac06563cc969a7128fe67f950eafea6c805ef10d</div><div id='time'> Time: 2021-07-26</div><div id='author'> Author: 13126830206@163.com</div><div id='file'> File Name: examples/domain_adaptation/classification/afn.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/domain_adaptation/classification/afn.py</div><div id='n_file'> N File Name: examples/domain_adaptation/classification/afn.py</div><div id='m_start'> M Start Line: 72</div><div id='m_end'> M End Line: 145</div><div id='n_start'> N Start Line: 66</div><div id='n_end'> N End Line: 141</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    train_target_dataset = dataset(root=args.root, task=args.target, download=True, transform=train_transform)
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_dataset<a id="change"> = </a><a id="change">dataset(root=args.root, task=args.target, download=True, transform=val_transform)</a>
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    <a id="change">if args.data == &quotDomainNet&quot</a>:
        test_dataset<a id="change"> = </a><a id="change">dataset(root=args.root, task=args.target, split=&quottest&quot, download=True, transform=val_transform)</a>
        test_loader<a id="change"> = </a>DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    else:
        test_loader<a id="change"> = </a>val_loader

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)</code></pre><h3>After Change</h3><pre><code class='java'>
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    test_loader<a id="change"> = </a>DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)

    &#47&#47 create model
    print("=&gt; using pre-trained model &quot{}&quot".format(args.arch))
    backbone = utils.get_model(args.arch)
    pool_layer = nn.Identity() if args.no_pool else None
    classifier = ImageClassifier(backbone, num_classes, bottleneck_dim=args.bottleneck_dim, pool_layer=pool_layer).to(device)
    &#47&#47 define optimizer and lr scheduler
    optimizer = SGD(classifier.get_parameters(), args.lr, momentum=args.momentum, weight_decay=args.wd, nesterov=True)
    lr_scheduler = LambdaLR(optimizer, lambda x:  args.lr * (1. + args.lr_gamma * float(x)) ** (-args.lr_decay))

    &#47&#47 define loss function
    mkmmd_loss = MultipleKernelMaximumMeanDiscrepancy(
        kernels=[GaussianKernel(alpha=2 ** k) for k in range(-3, 2)],
        linear=not args.non_linear
    )

    &#47&#47 resume from the best checkpoint
    if args.phase != &quottrain&quot:
        checkpoint = torch.load(logger.get_checkpoint_path(&quotbest&quot), map_location=&quotcpu&quot)
        classifier.load_state_dict(checkpoint)

    &#47&#47 analysis the model
    if args.phase == &quotanalysis&quot:
        &#47&#47 extract features from both domains
        feature_extractor = nn.Sequential(classifier.backbone, classifier.bottleneck).to(device)
        source_feature = collect_feature(train_source_loader, feature_extractor, device)
        target_feature = collect_feature(train_target_loader, feature_extractor, device)
        &#47&#47 plot t-SNE
        tSNE_filename = osp.join(logger.visualize_directory, &quotTSNE.png&quot)
        tsne.visualize(source_feature, target_feature, tSNE_filename)
        print("Saving t-SNE to", tSNE_filename)
        &#47&#47 calculate A-distance, which is a measure for distribution discrepancy
        A_distance = a_distance.calculate(source_feature, target_feature, device)
        print("A-distance =", A_distance)
        return

    if args.phase == &quottest&quot:
        acc1 = utils.validate(test_loader, classifier, args, device)
        print(acc1)
        return

    &#47&#47 start training
    best_acc1 = 0.
    for epoch in range(args.epochs):
        &#47&#47 train for one epoch
        train(train_source_iter, train_target_iter, classifier, mkmmd_loss, optimizer,
              lr_scheduler, epoch, args)

        &#47&#47 evaluate on validation set
        acc1 = utils.validate(val_loader, classifier, args, device)

        &#47&#47 remember best acc@1 and save checkpoint
        torch.save(classifier.state_dict(), logger.get_checkpoint_path(&quotlatest&quot))
        if acc1 &gt; best_acc1:
            shutil.copy(logger.get_checkpoint_path(&quotlatest&quot), logger.get_checkpoint_path(&quotbest&quot))
        best_acc1 = max(acc1, best_acc1)

    print("best_acc1 = {:3.1f}".format(best_acc1))

    &#47&#47 evaluate on test set
    classifier.load_state_dict(torch.load(logger.get_checkpoint_path(&quotbest&quot)))
    acc1<a id="change"> = </a><a id="change">utils.validate(</a>test_loader, classifier, args, device<a id="change">)</a>
    print("test_acc1 = {:3.1f}".format(acc1))

    logger.close()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thuml/transfer-learning-library/commit/ac06563cc969a7128fe67f950eafea6c805ef10d#diff-1ae36f420944e9532982dfb8a2799a8201c1fd550018567bcf7a3b5b428449ccL33' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14136378</div><div id='project'> Project Name: thuml/transfer-learning-library</div><div id='commit'> Commit Name: ac06563cc969a7128fe67f950eafea6c805ef10d</div><div id='time'> Time: 2021-07-26</div><div id='author'> Author: 13126830206@163.com</div><div id='file'> File Name: examples/domain_adaptation/classification/dan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/domain_adaptation/classification/dan.py</div><div id='n_file'> N File Name: examples/domain_adaptation/classification/dan.py</div><div id='m_start'> M Start Line: 74</div><div id='m_end'> M End Line: 153</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 147</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    train_target_dataset = dataset(root=args.root, task=args.target, download=True, transform=train_transform)
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_dataset<a id="change"> = </a><a id="change">dataset(root=args.root, task=args.target, download=True, transform=val_transform)</a>
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    <a id="change">if args.data == &quotDomainNet&quot</a>:
        test_dataset<a id="change"> = </a><a id="change">dataset(root=args.root, task=args.target, split=&quottest&quot, download=True, transform=val_transform)</a>
        test_loader<a id="change"> = </a>DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    else:
        test_loader<a id="change"> = </a>val_loader

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)</code></pre><h3>After Change</h3><pre><code class='java'>
    train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size,
                                     shuffle=True, num_workers=args.workers, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    test_loader<a id="change"> = </a>DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)

    train_source_iter = ForeverDataIterator(train_source_loader)
    train_target_iter = ForeverDataIterator(train_target_loader)

    &#47&#47 create model
    print("=&gt; using pre-trained model &quot{}&quot".format(args.arch))
    backbone = utils.get_model(args.arch)
    pool_layer = nn.Identity() if args.no_pool else None
    classifier = ImageClassifier(backbone, num_classes, bottleneck_dim=args.bottleneck_dim, pool_layer=pool_layer).to(device)

    &#47&#47 define optimizer and lr scheduler
    optimizer = SGD(classifier.get_parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True)
    lr_scheduler = LambdaLR(optimizer, lambda x:  args.lr * (1. + args.lr_gamma * float(x)) ** (-args.lr_decay))

    &#47&#47 define loss function
    mcc_loss = MinimumClassConfusionLoss(temperature=args.temperature)

    &#47&#47 resume from the best checkpoint
    if args.phase != &quottrain&quot:
        checkpoint = torch.load(logger.get_checkpoint_path(&quotbest&quot), map_location=&quotcpu&quot)
        classifier.load_state_dict(checkpoint)

    &#47&#47 analysis the model
    if args.phase == &quotanalysis&quot:
        &#47&#47 extract features from both domains
        feature_extractor = nn.Sequential(classifier.backbone, classifier.bottleneck).to(device)
        source_feature = collect_feature(train_source_loader, feature_extractor, device)
        target_feature = collect_feature(train_target_loader, feature_extractor, device)
        &#47&#47 plot t-SNE
        tSNE_filename = osp.join(logger.visualize_directory, &quotTSNE.png&quot)
        tsne.visualize(source_feature, target_feature, tSNE_filename)
        print("Saving t-SNE to", tSNE_filename)
        &#47&#47 calculate A-distance, which is a measure for distribution discrepancy
        A_distance = a_distance.calculate(source_feature, target_feature, device)
        print("A-distance =", A_distance)
        return

    if args.phase == &quottest&quot:
        acc1 = utils.validate(test_loader, classifier, args, device)
        print(acc1)
        return

    &#47&#47 start training
    best_acc1 = 0.
    for epoch in range(args.epochs):
        print("lr:", lr_scheduler.get_last_lr()[0])
        &#47&#47 train for one epoch
        train(train_source_iter, train_target_iter, classifier, mcc_loss, optimizer,
              lr_scheduler, epoch, args)

        &#47&#47 evaluate on validation set
        acc1 = utils.validate(val_loader, classifier, args, device)

        &#47&#47 remember best acc@1 and save checkpoint
        torch.save(classifier.state_dict(), logger.get_checkpoint_path(&quotlatest&quot))
        if acc1 &gt; best_acc1:
            shutil.copy(logger.get_checkpoint_path(&quotlatest&quot), logger.get_checkpoint_path(&quotbest&quot))
        best_acc1 = max(acc1, best_acc1)

    print("best_acc1 = {:3.1f}".format(best_acc1))

    &#47&#47 evaluate on test set
    classifier.load_state_dict(torch.load(logger.get_checkpoint_path(&quotbest&quot)))
    acc1<a id="change"> = </a><a id="change">utils.validate(</a>test_loader, classifier, args, device<a id="change">)</a>
    print("test_acc1 = {:3.1f}".format(acc1))

    logger.close()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/thuml/transfer-learning-library/commit/ac06563cc969a7128fe67f950eafea6c805ef10d#diff-755dc1e477172b28265e20a909a7d962e3ecf3aaf955072856ad8b524603ab4eL32' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 14136377</div><div id='project'> Project Name: thuml/transfer-learning-library</div><div id='commit'> Commit Name: ac06563cc969a7128fe67f950eafea6c805ef10d</div><div id='time'> Time: 2021-07-26</div><div id='author'> Author: 13126830206@163.com</div><div id='file'> File Name: examples/domain_adaptation/classification/mcc.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: examples/domain_adaptation/classification/mcc.py</div><div id='n_file'> N File Name: examples/domain_adaptation/classification/mcc.py</div><div id='m_start'> M Start Line: 73</div><div id='m_end'> M End Line: 150</div><div id='n_start'> N Start Line: 67</div><div id='n_end'> N End Line: 145</div><BR>