<html><h3>Pattern ID :39803
</h3><img src='113354083.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    param_groups,
    states
):
    <a id="change">params</a><a id="change"> = </a><a id="change">list(</a>params<a id="change">)</a>
    for group_idx, group_mapping in enumerate(param_mapping):
        group = param_groups[group_idx]

        amsgrad = group[&quotamsgrad&quot]
        beta1, beta2 = group[&quotbetas&quot]

        for param_idx in group_mapping:
            p = params[param_idx]

            if p.gradient is None:
                continue
            grad = p.gradient

            p = p * (1 - group[&quotlr&quot] * group[&quotweight_decay&quot])
            state = states[param_idx]

            state[&quotstep&quot] += 1
            bias_correction1 = 1 - beta1**state[&quotstep&quot]
            bias_correction2 = 1 - beta2**state[&quotstep&quot]

            state[&quotexp_avg&quot] = state[&quotexp_avg&quot] * beta1 + (1 - beta1) * grad
            state[&quotexp_avg_sq&quot] = state[&quotexp_avg_sq&quot] * beta2 + (1 - beta2) * grad * grad

            if amsgrad:
                state[&quotmax_exp_avg_sq&quot] = torch.max(state[&quotmax_exp_avg_sq&quot], state[&quotexp_avg_sq&quot])
                denom = state[&quotmax_exp_avg_sq&quot] / math.sqrt(bias_correction2) + group[&quoteps&quot]
            else:
                denom = state[&quotexp_avg_sq&quot] / math.sqrt(bias_correction2) + group[&quoteps&quot]

            step_size = group[&quotlr&quot] / bias_correction1
            <a id="change">params[param_idx]</a> = p<a id="change"> - </a>step_size * (state[&quotexp_avg&quot] / denom)

    <a id="change">return </a><a id="change">tuple(params</a><a id="change">)</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
            step_size = group[&quotlr&quot] / bias_correction1
            p.update = step_size * (state[&quotexp_avg&quot] / denom)

    out = <a id="change">tuple(p</a><a id="change"> - p.update for p in params if hasattr(p, &quotupdate&quot))</a>
    for p in params:
        if hasattr(p, &quotupdate&quot):
            del p.update
    <a id="change">return </a>out
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 10</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/leopard-ai/betty/commit/6d731ca13bf887bb15eefc85c120c681d0d9a1d0#diff-14cb4988c9063d54b880c71701a8d37ef78cfbe2d6c604b241ce7ccf55dd6d48L11' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 113354083</div><div id='project'> Project Name: leopard-ai/betty</div><div id='commit'> Commit Name: 6d731ca13bf887bb15eefc85c120c681d0d9a1d0</div><div id='time'> Time: 2022-02-17</div><div id='author'> Author: sangkeuc@andrew.cmu.edu</div><div id='file'> File Name: betty/optim/adamw.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: fadamw(4)</div><div id='n_method'> N Method Name: fadamw(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: betty/optim/adamw.py</div><div id='n_file'> N File Name: betty/optim/adamw.py</div><div id='m_start'> M Start Line: 11</div><div id='m_end'> M End Line: 44</div><div id='n_start'> N Start Line: 24</div><div id='n_end'> N End Line: 47</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    param_groups,
    states
):
    <a id="change">params</a><a id="change"> = </a><a id="change">list(</a>params<a id="change">)</a>
    for group_idx, group_mapping in enumerate(param_mapping):
        group = param_groups[group_idx]

        amsgrad = group[&quotamsgrad&quot]
        beta1, beta2 = group[&quotbetas&quot]
        weight_decay = group[&quotweight_decay&quot]

        for <a id="change">param_idx</a> in group_mapping:
            p = params[param_idx]

            if p.gradient is None:
                continue
            grad = p.gradient
            print(&quotadam grad:&quot, grad)

            state = states[param_idx]

            state[&quotstep&quot] += 1
            bias_correction1 = 1 - beta1**state[&quotstep&quot]
            bias_correction2 = 1 - beta2**state[&quotstep&quot]

            if weight_decay != 0:
                grad = grad + (weight_decay * p)

            state[&quotexp_avg&quot] = state[&quotexp_avg&quot] * beta1 + (1 - beta1) * grad
            state[&quotexp_avg_sq&quot] =  state[&quotexp_avg_sq&quot] * beta2 + (1 - beta2) * grad * grad

            if amsgrad:
                state[&quotmax_exp_avg_sq&quot] = torch.max(state[&quotmax_exp_avg_sq&quot], state[&quotexp_avg_sq&quot])
                denom = state[&quotmax_exp_avg_sq&quot].sqrt() / math.sqrt(bias_correction2) + group[&quoteps&quot]
            else:
                denom = state[&quotexp_avg_sq&quot].sqrt() / math.sqrt(bias_correction2) + group[&quoteps&quot]

            step_size = group[&quotlr&quot] / bias_correction1
            <a id="change">params[param_idx]</a> = p<a id="change"> - </a>step_size * (state[&quotexp_avg&quot] / denom)

    <a id="change">return </a><a id="change">tuple(</a>params<a id="change">)</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
            step_size = group[&quotlr&quot] / bias_correction1
            p.update = step_size * (state[&quotexp_avg&quot] / denom)

    out = <a id="change">tuple(p</a><a id="change"> - p.update for p in params if hasattr(p, &quotupdate&quot))</a>
    for p in params:
        if hasattr(p, &quotupdate&quot):
            del p.update
    <a id="change">return </a>out
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leopard-ai/betty/commit/6d731ca13bf887bb15eefc85c120c681d0d9a1d0#diff-02175463e720b8e991266c8411306e432bbc787fd5644b2cd38a122de76fa8cbL6' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 113354085</div><div id='project'> Project Name: leopard-ai/betty</div><div id='commit'> Commit Name: 6d731ca13bf887bb15eefc85c120c681d0d9a1d0</div><div id='time'> Time: 2022-02-17</div><div id='author'> Author: sangkeuc@andrew.cmu.edu</div><div id='file'> File Name: betty/optim/adam.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: fadam(4)</div><div id='n_method'> N Method Name: fadam(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: betty/optim/adam.py</div><div id='n_file'> N File Name: betty/optim/adam.py</div><div id='m_start'> M Start Line: 12</div><div id='m_end'> M End Line: 49</div><div id='n_start'> N Start Line: 20</div><div id='n_end'> N End Line: 51</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    param_groups,
    states
):
    <a id="change">params</a><a id="change"> = </a><a id="change">list(</a>params<a id="change">)</a>
    for group_idx, group_mapping in enumerate(param_mapping):
        group = param_groups[group_idx]

        weight_decay = group[&quotweight_decay&quot]
        momentum = group[&quotmomentum&quot]
        dampening = group[&quotdampening&quot]
        nesterov = group[&quotnesterov&quot]

        for <a id="change">param_idx</a> in group_mapping:
            p = params[param_idx]

            if p.gradient is None:
                continue
            grad = p.gradient
            if weight_decay != 0:
                grad = grad + weight_decay * p

            param_state = states[param_idx]
            if &quotmomentum_buffer&quot not in param_state or param_state[&quotmomentum_buffer&quot] is None:
                buf = param_state[&quotmomentum_buffer&quot] = grad
            else:
                buf = param_state[&quotmomentum_buffer&quot]
                buf = momentum * buf + (1 - dampening) * grad
                param_state[&quotmomentum_buffer&quot] = buf
            if nesterov:
                grad = grad + momentum * buf
            else:
                grad = buf

            <a id="change">params[param_idx]</a> = p<a id="change"> - </a>group[&quotlr&quot] * grad
    <a id="change">return </a><a id="change">tuple(</a>params<a id="change">)</a>
</code></pre><h3>After Change</h3><pre><code class='java'>
            p.update = group[&quotlr&quot] * grad
            &#47&#47params[param_idx] = p - group[&quotlr&quot] * grad
    &#47&#47return tuple(params)
    <a id="change">return </a><a id="change">tuple(p</a><a id="change"> - p.update for p in params)</a>
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/leopard-ai/betty/commit/5cbbbc04c6f80884ee93fc9e0403026f2ecd328e#diff-b3c9f0b436bdc36193200d7691f5f4f2c1800a6cdc5bb9caf0f1a2d10f38b409L4' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 113354084</div><div id='project'> Project Name: leopard-ai/betty</div><div id='commit'> Commit Name: 5cbbbc04c6f80884ee93fc9e0403026f2ecd328e</div><div id='time'> Time: 2022-02-12</div><div id='author'> Author: sangkeuc@andrew.cmu.edu</div><div id='file'> File Name: betty/optim/sgd.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: fsgd(4)</div><div id='n_method'> N Method Name: fsgd(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: betty/optim/sgd.py</div><div id='n_file'> N File Name: betty/optim/sgd.py</div><div id='m_start'> M Start Line: 10</div><div id='m_end'> M End Line: 41</div><div id='n_start'> N Start Line: 11</div><div id='n_end'> N End Line: 43</div><BR>