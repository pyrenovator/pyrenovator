<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
class CNNLayer(nn.Module):
    def __init__(self, n_in, num_filters=(128,), filter_size=(7,), pool_size=(1,), dilation=1, dropout_rate=0.0):
        super(CNNLayer, self).__init__()
        l = <a id="change">[]</a>
        for n_out, ksize, p in zip(num_filters, filter_size, pool_size):
            l<a id="change"> += [ 
                </a><a id="change">nn.Conv1d(</a>n_in, <a id="change">n_out</a><a id="change">, kernel_size=ksize, dilation=2**dilation, padding=2**dilation*(ksize//2))</a>,
                nn.MaxPool1d(p, stride=1, padding=p//2) if p &gt; 1 else nn.Identity(),
                <a id="change">nn.GroupNorm(1</a>, <a id="change">n_out</a><a id="change">)</a>, &#47&#47 same as LayerNorm?
                <a id="change">nn.CELU()</a>, 
                <a id="change">nn.Dropout(p=dropout_rate)</a> ]
            n_in = n_out
        self.net<a id="change"> = nn</a><a id="change">.Sequential(</a>*<a id="change">l)</a>

    def forward(self, x): &#47&#47 (B=1, 4, N)
        return self.net(x)
</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self, n_in, num_filters=(128,), filter_size=(7,), pool_size=(1,), dilation=1, dropout_rate=0.0, resnet=False):
        super(CNNLayer, self).__init__()
        self.resnet = resnet
        self.net<a id="change"> = </a><a id="change">[]</a>
        for <a id="change">n_out</a>, ksize, p in zip(num_filters, filter_size, pool_size):
            <a id="change">self.net.append( 
                nn</a><a id="change">.Sequential( 
                    </a><a id="change">nn.Conv1d(</a>n_in, <a id="change">n_out</a><a id="change">, kernel_size=ksize, dilation=2**dilation, padding=2**dilation*(ksize//2))</a>,
                    nn.MaxPool1d(p, stride=1, padding=p//2) if p &gt; 1 else nn.Identity(),
                    <a id="change">nn.GroupNorm(1</a>, <a id="change">n_out</a><a id="change">)</a>, &#47&#47 same as LayerNorm?
                    <a id="change">nn.CELU()</a>, 
                    <a id="change">nn.Dropout(p=dropout_rate) ) )</a>
            n_in = n_out

    def forward(self, x): &#47&#47 (B=1, 4, N)
        n_in = x.shape[1]</code></pre>