<html><h3>Pattern ID :6509
</h3><img src='22554968.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
):
    if clip_global_grad_norm&lt;1.0:
        torch._foreach_mul_(grads, clip_global_grad_norm.item())
    copy_grads<a id="change"> = </a><a id="change">[g.clone() for g in grads]</a>

    diff = torch._foreach_sub(grads, pre_grads)
    &#47&#47 NOTE: line below while looking identical gives different result, due to float precision errors.
    &#47&#47 using mul+add produces identical results to single-tensor, using add+alpha doesn&quott
    &#47&#47 On cuda this difference doesn&quott matter due to its&quot own precision non-determinism
    &#47&#47 update = torch._foreach_add(grads, torch._foreach_mul(diff, beta2))
    update = torch._foreach_add(grads, diff, alpha=beta2)

    torch._foreach_mul_(exp_avgs, beta1)
    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1) &#47&#47 m_t

    torch._foreach_mul_(exp_avg_diffs, beta2)
    torch._foreach_add_(exp_avg_diffs, diff, alpha=1 - beta2) &#47&#47 diff_t

    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs, update, update, value=1 - beta3) &#47&#47 n_t

    denom = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    update = torch._foreach_div(exp_avgs, bias_correction1)
    &#47&#47 NOTE: same issue as above. beta2 * diff / bias_correction2 != diff * (beta2 / bias_correction2)
    &#47&#47 using faster version by default. 
    &#47&#47 torch._foreach_add_(update, torch._foreach_div(torch._foreach_mul(exp_avg_diffs, beta2), bias_correction2))
    torch._foreach_add_(update, torch._foreach_mul(exp_avg_diffs, beta2 / bias_correction2))
    torch._foreach_div_(update, denom)

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        torch._foreach_add_(params, update, alpha=-lr)
    else:
        torch._foreach_add_(params, update, alpha=-lr)
        torch._foreach_div_(params, 1 + lr * weight_decay)
    <a id="change">return </a>copy_grads</code></pre><h3>After Change</h3><pre><code class='java'>
    torch._foreach_add_(exp_avg_diffs, neg_pre_grads,
                        alpha=1 - beta2)  &#47&#47 diff_t

    <a id="change">torch._foreach_mul_(neg_pre_grads</a>, <a id="change">beta2</a><a id="change">)</a>
    torch._foreach_add_(neg_pre_grads, grads)
    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs,
                            neg_pre_grads,
                            neg_pre_grads,
                            value=1 - beta3)  &#47&#47 n_t

    denom = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    step_size_diff = <a id="change">lr</a><a id="change"> * beta2 / </a>bias_correction2
    step_size<a id="change"> = lr</a><a id="change"> / </a>bias_correction1

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        <a id="change">torch._foreach_addcdiv_(params</a>, <a id="change">exp_avgs</a>, denom<a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(params</a>,
                                <a id="change">exp_avg_diffs</a>,
                                denom<a id="change">,
                                value=-step_size_diff)</a>
    else:
        <a id="change">torch._foreach_addcdiv_(params</a>, <a id="change">exp_avgs</a>, denom<a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(params</a>,
                                <a id="change">exp_avg_diffs</a>,
                                denom<a id="change">,
                                value=-step_size_diff)</a>
        torch._foreach_div_(params, 1 + lr * weight_decay)
    <a id="change">torch._foreach_zero_(neg_pre_grads</a><a id="change">)</a>
    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 15</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/sail-sg/adan/commit/6aafc8b3e81540fe52da3d70ced2bc3c9f17624a#diff-0b1b3ccdbc86485785aa958a04606abf754be337e06ac1f9c1b9e5471aec76b3L253' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 22554968</div><div id='project'> Project Name: sail-sg/adan</div><div id='commit'> Commit Name: 6aafc8b3e81540fe52da3d70ced2bc3c9f17624a</div><div id='time'> Time: 2022-12-10</div><div id='author'> Author: xyxie@pku.edu.cn</div><div id='file'> File Name: NLP/Transformer-XL/adan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _multi_tensor_adan(0)</div><div id='n_method'> N Method Name: _multi_tensor_adan(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: NLP/Transformer-XL/adan.py</div><div id='n_file'> N File Name: NLP/Transformer-XL/adan.py</div><div id='m_start'> M Start Line: 253</div><div id='m_end'> M End Line: 290</div><div id='n_start'> N Start Line: 280</div><div id='n_end'> N End Line: 323</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
):
    if clip_global_grad_norm&lt;1.0:
        torch._foreach_mul_(grads, clip_global_grad_norm.item())
    copy_grads<a id="change"> = </a><a id="change">[g.clone() for g in grads]</a>

    diff = torch._foreach_sub(grads, pre_grads)
    &#47&#47 NOTE: line below while looking identical gives different result, due to float precision errors.
    &#47&#47 using mul+add produces identical results to single-tensor, using add+alpha doesn&quott
    &#47&#47 On cuda this difference doesn&quott matter due to its&quot own precision non-determinism
    &#47&#47 update = torch._foreach_add(grads, torch._foreach_mul(diff, beta2))
    update = torch._foreach_add(grads, diff, alpha=beta2)

    torch._foreach_mul_(exp_avgs, beta1)
    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1) &#47&#47 m_t

    torch._foreach_mul_(exp_avg_diffs, beta2)
    torch._foreach_add_(exp_avg_diffs, diff, alpha=1 - beta2) &#47&#47 diff_t

    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs, update, update, value=1 - beta3) &#47&#47 n_t

    denom = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    update = torch._foreach_div(exp_avgs, bias_correction1)
    &#47&#47 NOTE: same issue as above. beta2 * diff / bias_correction2 != diff * (beta2 / bias_correction2)
    &#47&#47 using faster version by default. 
    &#47&#47 torch._foreach_add_(update, torch._foreach_div(torch._foreach_mul(exp_avg_diffs, beta2), bias_correction2))
    torch._foreach_add_(update, torch._foreach_mul(exp_avg_diffs, beta2 / bias_correction2))
    torch._foreach_div_(update, denom)

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        torch._foreach_add_(params, update, alpha=-lr)
    else:
        torch._foreach_add_(params, update, alpha=-lr)
        torch._foreach_div_(params, 1 + lr * weight_decay)
    <a id="change">return </a>copy_grads</code></pre><h3>After Change</h3><pre><code class='java'>
    torch._foreach_add_(exp_avg_diffs, neg_pre_grads,
                        alpha=1 - beta2)  &#47&#47 diff_t

    <a id="change">torch._foreach_mul_(</a>neg_pre_grads, beta2<a id="change">)</a>
    torch._foreach_add_(neg_pre_grads, grads)
    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs,
                            neg_pre_grads,
                            neg_pre_grads,
                            value=1 - beta3)  &#47&#47 n_t

    denom = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    step_size_diff = lr<a id="change"> * beta2 / </a>bias_correction2
    step_size<a id="change"> = </a>lr<a id="change"> / </a>bias_correction1

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        <a id="change">torch._foreach_addcdiv_(</a>params, exp_avgs, denom<a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(</a>params,
                                exp_avg_diffs,
                                denom<a id="change">,
                                value=-step_size_diff)</a>
    else:
        <a id="change">torch._foreach_addcdiv_(</a>params, exp_avgs, denom<a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(</a>params,
                                exp_avg_diffs,
                                denom<a id="change">,
                                value=-step_size_diff)</a>
        torch._foreach_div_(params, 1 + lr * weight_decay)
    <a id="change">torch._foreach_zero_(</a>neg_pre_grads<a id="change">)</a>
    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sail-sg/adan/commit/9805dec1acf0151eecd1634a018c36239b0ad603#diff-bccd0d63d4b29d7d5170fdb4f6a4e95ad9bd8137b9a7ea1feb61848f4cabaeacL233' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 22554969</div><div id='project'> Project Name: sail-sg/adan</div><div id='commit'> Commit Name: 9805dec1acf0151eecd1634a018c36239b0ad603</div><div id='time'> Time: 2022-12-10</div><div id='author'> Author: xyxie@pku.edu.cn</div><div id='file'> File Name: adan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _multi_tensor_adan(0)</div><div id='n_method'> N Method Name: _multi_tensor_adan(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: adan.py</div><div id='n_file'> N File Name: adan.py</div><div id='m_start'> M Start Line: 253</div><div id='m_end'> M End Line: 290</div><div id='n_start'> N Start Line: 280</div><div id='n_end'> N End Line: 323</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
):
    if clip_global_grad_norm&lt;1.0:
        torch._foreach_mul_(grads, clip_global_grad_norm.item())
    copy_grads<a id="change"> = </a><a id="change">[g.clone() for g in grads]</a>

    diff = torch._foreach_sub(grads, pre_grads)
    &#47&#47 NOTE: line below while looking identical gives different result, due to float precision errors.
    &#47&#47 using mul+add produces identical results to single-tensor, using add+alpha doesn&quott
    &#47&#47 On cuda this difference doesn&quott matter due to its&quot own precision non-determinism
    &#47&#47 update = torch._foreach_add(grads, torch._foreach_mul(diff, beta2))
    update = torch._foreach_add(grads, diff, alpha=beta2)

    torch._foreach_mul_(exp_avgs, beta1)
    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1) &#47&#47 m_t

    torch._foreach_mul_(exp_avg_diffs, beta2)
    torch._foreach_add_(exp_avg_diffs, diff, alpha=1 - beta2) &#47&#47 diff_t

    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs, update, update, value=1 - beta3) &#47&#47 n_t

    denom = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    update = torch._foreach_div(exp_avgs, bias_correction1)
    &#47&#47 NOTE: same issue as above. beta2 * diff / bias_correction2 != diff * (beta2 / bias_correction2)
    &#47&#47 using faster version by default. 
    &#47&#47 torch._foreach_add_(update, torch._foreach_div(torch._foreach_mul(exp_avg_diffs, beta2), bias_correction2))
    torch._foreach_add_(update, torch._foreach_mul(exp_avg_diffs, beta2 / bias_correction2))
    torch._foreach_div_(update, denom)

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        torch._foreach_add_(params, update, alpha=-lr)
    else:
        torch._foreach_add_(params, update, alpha=-lr)
        torch._foreach_div_(params, 1 + lr * weight_decay)
    <a id="change">return </a>copy_grads</code></pre><h3>After Change</h3><pre><code class='java'>
    torch._foreach_add_(exp_avg_diffs, neg_pre_grads,
                        alpha=1 - beta2)  &#47&#47 diff_t

    <a id="change">torch._foreach_mul_(</a>neg_pre_grads, beta2<a id="change">)</a>
    torch._foreach_add_(neg_pre_grads, grads)
    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs,
                            neg_pre_grads,
                            neg_pre_grads,
                            value=1 - beta3)  &#47&#47 n_t

    denom = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    step_size_diff = lr<a id="change"> * beta2 / </a>bias_correction2
    step_size<a id="change"> = </a>lr<a id="change"> / </a>bias_correction1

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        <a id="change">torch._foreach_addcdiv_(</a>params, exp_avgs, denom<a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(</a>params,
                                exp_avg_diffs,
                                denom<a id="change">,
                                value=-step_size_diff)</a>
    else:
        <a id="change">torch._foreach_addcdiv_(</a>params, exp_avgs, denom<a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(</a>params,
                                exp_avg_diffs,
                                denom<a id="change">,
                                value=-step_size_diff)</a>
        torch._foreach_div_(params, 1 + lr * weight_decay)
    <a id="change">torch._foreach_zero_(</a>neg_pre_grads<a id="change">)</a>
    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sail-sg/adan/commit/6aafc8b3e81540fe52da3d70ced2bc3c9f17624a#diff-67c755dae37728c9303895a1af500d18552b3dbedbe392933fa05ddb6bf2837cL233' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 22554970</div><div id='project'> Project Name: sail-sg/adan</div><div id='commit'> Commit Name: 6aafc8b3e81540fe52da3d70ced2bc3c9f17624a</div><div id='time'> Time: 2022-12-10</div><div id='author'> Author: xyxie@pku.edu.cn</div><div id='file'> File Name: CV/timm/adan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _multi_tensor_adan(0)</div><div id='n_method'> N Method Name: _multi_tensor_adan(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: CV/timm/adan.py</div><div id='n_file'> N File Name: CV/timm/adan.py</div><div id='m_start'> M Start Line: 253</div><div id='m_end'> M End Line: 290</div><div id='n_start'> N Start Line: 280</div><div id='n_end'> N End Line: 323</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
):
    if clip_global_grad_norm&lt;1.0:
        torch._foreach_mul_(grads, clip_global_grad_norm.item())
    copy_grads<a id="change"> = </a><a id="change">[g.clone() for g in grads]</a>

    diff = torch._foreach_sub(grads, pre_grads)
    &#47&#47 NOTE: line below while looking identical gives different result, due to float precision errors.
    &#47&#47 using mul+add produces identical results to single-tensor, using add+alpha doesn&quott
    &#47&#47 On cuda this difference doesn&quott matter due to its&quot own precision non-determinism
    &#47&#47 update = torch._foreach_add(grads, torch._foreach_mul(diff, beta2))
    update = torch._foreach_add(grads, diff, alpha=beta2)

    torch._foreach_mul_(exp_avgs, beta1)
    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1) &#47&#47 m_t

    torch._foreach_mul_(exp_avg_diffs, beta2)
    torch._foreach_add_(exp_avg_diffs, diff, alpha=1 - beta2) &#47&#47 diff_t

    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs, update, update, value=1 - beta3) &#47&#47 n_t

    denom = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    update = torch._foreach_div(exp_avgs, bias_correction1)
    &#47&#47 NOTE: same issue as above. beta2 * diff / bias_correction2 != diff * (beta2 / bias_correction2)
    &#47&#47 using faster version by default. 
    &#47&#47 torch._foreach_add_(update, torch._foreach_div(torch._foreach_mul(exp_avg_diffs, beta2), bias_correction2))
    torch._foreach_add_(update, torch._foreach_mul(exp_avg_diffs, beta2 / bias_correction2))
    torch._foreach_div_(update, denom)

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        torch._foreach_add_(params, update, alpha=-lr)
    else:
        torch._foreach_add_(params, update, alpha=-lr)
        torch._foreach_div_(params, 1 + lr * weight_decay)
    <a id="change">return </a>copy_grads</code></pre><h3>After Change</h3><pre><code class='java'>
    torch._foreach_add_(exp_avg_diffs, neg_pre_grads,
                        alpha=1 - beta2)  &#47&#47 diff_t

    <a id="change">torch._foreach_mul_(</a>neg_pre_grads, beta2<a id="change">)</a>
    torch._foreach_add_(neg_pre_grads, grads)
    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs,
                            neg_pre_grads,
                            neg_pre_grads,
                            value=1 - beta3)  &#47&#47 n_t

    denom = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    step_size_diff = lr<a id="change"> * beta2 / </a>bias_correction2
    step_size<a id="change"> = </a>lr<a id="change"> / </a>bias_correction1

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        <a id="change">torch._foreach_addcdiv_(</a>params, exp_avgs, denom<a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(</a>params,
                                exp_avg_diffs,
                                denom<a id="change">,
                                value=-step_size_diff)</a>
    else:
        <a id="change">torch._foreach_addcdiv_(</a>params, exp_avgs, denom<a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(</a>params,
                                exp_avg_diffs,
                                denom<a id="change">,
                                value=-step_size_diff)</a>
        torch._foreach_div_(params, 1 + lr * weight_decay)
    <a id="change">torch._foreach_zero_(</a>neg_pre_grads<a id="change">)</a>
    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sail-sg/adan/commit/6aafc8b3e81540fe52da3d70ced2bc3c9f17624a#diff-0b1b3ccdbc86485785aa958a04606abf754be337e06ac1f9c1b9e5471aec76b3L233' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 22554971</div><div id='project'> Project Name: sail-sg/adan</div><div id='commit'> Commit Name: 6aafc8b3e81540fe52da3d70ced2bc3c9f17624a</div><div id='time'> Time: 2022-12-10</div><div id='author'> Author: xyxie@pku.edu.cn</div><div id='file'> File Name: NLP/Transformer-XL/adan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _multi_tensor_adan(0)</div><div id='n_method'> N Method Name: _multi_tensor_adan(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: NLP/Transformer-XL/adan.py</div><div id='n_file'> N File Name: NLP/Transformer-XL/adan.py</div><div id='m_start'> M Start Line: 253</div><div id='m_end'> M End Line: 290</div><div id='n_start'> N Start Line: 280</div><div id='n_end'> N End Line: 323</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
):
    if clip_global_grad_norm&lt;1.0:
        torch._foreach_mul_(grads, clip_global_grad_norm.item())
    copy_grads<a id="change"> = </a><a id="change">[g.clone() for g in grads]</a>

    diff = torch._foreach_sub(grads, pre_grads)
    &#47&#47 NOTE: line below while looking identical gives different result, due to float precision errors.
    &#47&#47 using mul+add produces identical results to single-tensor, using add+alpha doesn&quott
    &#47&#47 On cuda this difference doesn&quott matter due to its&quot own precision non-determinism
    &#47&#47 update = torch._foreach_add(grads, torch._foreach_mul(diff, beta2))
    update = torch._foreach_add(grads, diff, alpha=beta2)

    torch._foreach_mul_(exp_avgs, beta1)
    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1) &#47&#47 m_t

    torch._foreach_mul_(exp_avg_diffs, beta2)
    torch._foreach_add_(exp_avg_diffs, diff, alpha=1 - beta2) &#47&#47 diff_t

    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs, update, update, value=1 - beta3) &#47&#47 n_t

    denom = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    update = torch._foreach_div(exp_avgs, bias_correction1)
    &#47&#47 NOTE: same issue as above. beta2 * diff / bias_correction2 != diff * (beta2 / bias_correction2)
    &#47&#47 using faster version by default. 
    &#47&#47 torch._foreach_add_(update, torch._foreach_div(torch._foreach_mul(exp_avg_diffs, beta2), bias_correction2))
    torch._foreach_add_(update, torch._foreach_mul(exp_avg_diffs, beta2 / bias_correction2))
    torch._foreach_div_(update, denom)

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        torch._foreach_add_(params, update, alpha=-lr)
    else:
        torch._foreach_add_(params, update, alpha=-lr)
        torch._foreach_div_(params, 1 + lr * weight_decay)
    <a id="change">return </a>copy_grads</code></pre><h3>After Change</h3><pre><code class='java'>
    torch._foreach_add_(exp_avg_diffs, neg_pre_grads,
                        alpha=1 - beta2)  &#47&#47 diff_t

    <a id="change">torch._foreach_mul_(</a>neg_pre_grads, beta2<a id="change">)</a>
    torch._foreach_add_(neg_pre_grads, grads)
    torch._foreach_mul_(exp_avg_sqs, beta3)
    torch._foreach_addcmul_(exp_avg_sqs,
                            neg_pre_grads,
                            neg_pre_grads,
                            value=1 - beta3)  &#47&#47 n_t

    denom = torch._foreach_sqrt(exp_avg_sqs)
    torch._foreach_div_(denom, bias_correction3_sqrt)
    torch._foreach_add_(denom, eps)

    step_size_diff = lr<a id="change"> * beta2 / </a>bias_correction2
    step_size<a id="change"> = </a>lr<a id="change"> / </a>bias_correction1

    if no_prox:
        torch._foreach_mul_(params, 1 - lr * weight_decay)
        <a id="change">torch._foreach_addcdiv_(</a>params, exp_avgs, denom<a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(</a>params,
                                exp_avg_diffs,
                                denom<a id="change">,
                                value=-step_size_diff)</a>
    else:
        <a id="change">torch._foreach_addcdiv_(</a>params, exp_avgs, denom<a id="change">, value=-step_size)</a>
        <a id="change">torch._foreach_addcdiv_(</a>params,
                                exp_avg_diffs,
                                denom<a id="change">,
                                value=-step_size_diff)</a>
        torch._foreach_div_(params, 1 + lr * weight_decay)
    <a id="change">torch._foreach_zero_(</a>neg_pre_grads<a id="change">)</a>
    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sail-sg/adan/commit/6aafc8b3e81540fe52da3d70ced2bc3c9f17624a#diff-61f7eb3450a10f3b466dcc44a8b30146ec63b38486822f81c29f75ac3088a06cL233' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 22554965</div><div id='project'> Project Name: sail-sg/adan</div><div id='commit'> Commit Name: 6aafc8b3e81540fe52da3d70ced2bc3c9f17624a</div><div id='time'> Time: 2022-12-10</div><div id='author'> Author: xyxie@pku.edu.cn</div><div id='file'> File Name: CV/MAE/adan.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _multi_tensor_adan(0)</div><div id='n_method'> N Method Name: _multi_tensor_adan(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: CV/MAE/adan.py</div><div id='n_file'> N File Name: CV/MAE/adan.py</div><div id='m_start'> M Start Line: 253</div><div id='m_end'> M End Line: 290</div><div id='n_start'> N Start Line: 280</div><div id='n_end'> N End Line: 323</div><BR>