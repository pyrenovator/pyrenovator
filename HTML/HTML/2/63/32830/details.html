<html><h3>Pattern ID :32830
</h3><img src='95234989.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if (index + 1) % 100 == 0 or (index + 1) == batches:
            iters = index + epoch * batches + 1
            writer.add_scalar("Train/MSE_Loss", loss.item(), iters)
            <a id="change">print(f"Epoch[{epoch + 1:05d}/{config.epochs:05d}]({index + 1:05d}/{batches:05d}) MSE loss: {loss.item():.6f}."</a><a id="change">)</a>


def validate(model, valid_dataloader, criterion, epoch, writer) -&gt; float:
    &#47&#47 Calculate how many iterations there are under Epoch.</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 Calculate how many iterations there are under epoch
    batches = len(train_dataloader)

    <a id="change">batch_time = AverageMeter("Time"</a>, <a id="change">":6.3f"</a><a id="change">)</a>
    <a id="change">data_time = </a><a id="change">AverageMeter("Data"</a>, <a id="change">":6.3f"</a><a id="change">)</a>
    <a id="change">losses = AverageMeter("Loss"</a>, <a id="change">":6.6f"</a><a id="change">)</a>
    <a id="change">psnres = </a><a id="change">AverageMeter("PSNR"</a>, <a id="change">":4.2f"</a><a id="change">)</a>
    progress<a id="change"> = </a><a id="change">ProgressMeter(</a>batches, <a id="change">[batch_time</a>, <a id="change">data_time</a>, <a id="change">losses</a>, <a id="change">psnres</a>]<a id="change">, prefix=f"Epoch: [{epoch}]")</a>

    &#47&#47 Put the generator in training mode
    model.train()

    <a id="change">end = time</a><a id="change">.time()</a>
    for index, (lr, hr) in enumerate(train_dataloader):
        &#47&#47 measure data loading time
        <a id="change">data_time.update(time</a><a id="change">.time()</a><a id="change"> - end</a><a id="change">)</a>

        lr = lr.to(config.device, non_blocking=True)
        hr = hr.to(config.device, non_blocking=True)

        &#47&#47 Initialize the generator gradient
        model.zero_grad()

        &#47&#47 Mixed precision training
        with amp.autocast():
            sr = model(lr)
            loss = criterion(sr, hr)
        &#47&#47 Gradient zoom
        scaler.scale(loss).backward()
        &#47&#47 Update generator weight
        scaler.step(optimizer)
        scaler.update()

        &#47&#47 measure accuracy and record loss
        psnr<a id="change"> = 10.</a><a id="change"> * torch.log10(1.</a><a id="change"> / torch</a><a id="change">.mean(</a>(sr<a id="change"> - </a>hr)<a id="change"> ** 2</a><a id="change">))</a>
        <a id="change">losses.update(loss.item()</a>, <a id="change">lr.size(0</a><a id="change">)</a><a id="change">)</a>
        <a id="change">psnres.update(psnr.item()</a>, <a id="change">lr.size(0</a><a id="change">)</a><a id="change">)</a>

        &#47&#47 measure elapsed time
        <a id="change">batch_time.update(time</a><a id="change">.time()</a><a id="change"> - end</a><a id="change">)</a>
        end<a id="change"> = time.time()</a>

        <a id="change">if index % config.print_frequency == 0</a>:
            <a id="change">progress.display(index</a><a id="change">)</a>

        &#47&#47 In this Epoch, every one hundred iterations and the last iteration print the loss function
        &#47&#47 and write it to Tensorboard at the same time
        if (index + 1) % 100 == 0 or (index + 1) == batches:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 39</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lornatang/fsrcnn-pytorch/commit/43cfaf9e5af149c81b55f722658550dbbc5f6c88#diff-ed183d67207df065a11e1289f19d34cc2abbc5448dea952683cfe9728c342b95L105' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95234989</div><div id='project'> Project Name: lornatang/fsrcnn-pytorch</div><div id='commit'> Commit Name: 43cfaf9e5af149c81b55f722658550dbbc5f6c88</div><div id='time'> Time: 2021-11-25</div><div id='author'> Author: liuchangyu1111@gmail.com</div><div id='file'> File Name: train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train(7)</div><div id='n_method'> N Method Name: train(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: train.py</div><div id='n_file'> N File Name: train.py</div><div id='m_start'> M Start Line: 105</div><div id='m_end'> M End Line: 107</div><div id='n_start'> N Start Line: 147</div><div id='n_end'> N End Line: 194</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if (index + 1) % 100 == 0 or (index + 1) == batches:
            iters = index + epoch * batches + 1
            writer.add_scalar("Train/MSE_Loss", loss.item(), iters)
            <a id="change">print(f"Epoch[{epoch + 1:05d}/{config.epochs:05d}]({index + 1:05d}/{batches:05d}) MSE loss: {loss.item():.6f}."</a><a id="change">)</a>


def validate(model, valid_dataloader, criterion, epoch, writer) -&gt; float:
    &#47&#47 Calculate how many iterations there are under Epoch.</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 Calculate how many iterations there are under epoch
    batches = len(train_dataloader)

    <a id="change">batch_time = AverageMeter("Time"</a>, <a id="change">":6.3f"</a><a id="change">)</a>
    <a id="change">data_time = </a><a id="change">AverageMeter("Data"</a>, <a id="change">":6.3f"</a><a id="change">)</a>
    <a id="change">losses = AverageMeter("Loss"</a>, <a id="change">":6.6f"</a><a id="change">)</a>
    <a id="change">psnres = </a><a id="change">AverageMeter("PSNR"</a>, <a id="change">":4.2f"</a><a id="change">)</a>
    progress<a id="change"> = </a><a id="change">ProgressMeter(</a>batches, <a id="change">[</a>batch_time, data_time, losses, psnres<a id="change"></a>]<a id="change">, prefix=f"Epoch: [{epoch}]")</a>

    &#47&#47 Put the generator in training mode
    model.train()

    <a id="change">end = </a><a id="change">time.time()</a>
    for <a id="change">index</a>, (lr, hr) in enumerate(train_dataloader):
        &#47&#47 measure data loading time
        <a id="change">data_time.update(</a><a id="change">time.time()</a><a id="change"> - </a>end<a id="change">)</a>

        <a id="change">lr</a> = lr.to(config.device, non_blocking=True)
        hr = hr.to(config.device, non_blocking=True)

        &#47&#47 Initialize the generator gradient
        model.zero_grad()

        &#47&#47 Mixed precision training
        with amp.autocast():
            sr = model(lr)
            loss = criterion(sr, hr)
        &#47&#47 Gradient zoom
        scaler.scale(loss).backward()
        &#47&#47 Update generator weight
        scaler.step(optimizer)
        scaler.update()

        &#47&#47 measure accuracy and record loss
        psnr<a id="change"> = 10.</a><a id="change"> * torch.log10(1.</a><a id="change"> / </a><a id="change">torch.mean(</a>(sr<a id="change"> - </a>hr)<a id="change"> ** 2</a><a id="change">))</a>
        <a id="change">losses.update(loss.item()</a>, <a id="change">lr.size(0</a><a id="change">)</a><a id="change">)</a>
        <a id="change">psnres.update(psnr.item()</a>, <a id="change">lr.size(0</a><a id="change">)</a><a id="change">)</a>

        &#47&#47 measure elapsed time
        <a id="change">batch_time.update(</a><a id="change">time.time()</a><a id="change"> - </a>end<a id="change">)</a>
        end<a id="change"> = time.time()</a>

        <a id="change">if index % config.print_frequency == 0</a>:
            <a id="change">progress.display(</a>index<a id="change">)</a>

        &#47&#47 In this Epoch, every one hundred iterations and the last iteration print the loss function
        &#47&#47 and write it to Tensorboard at the same time
        if (index + 1) % 100 == 0 or (index + 1) == batches:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lornatang/fsrcnn-pytorch/commit/43cfaf9e5af149c81b55f722658550dbbc5f6c88#diff-ed183d67207df065a11e1289f19d34cc2abbc5448dea952683cfe9728c342b95L79' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95234988</div><div id='project'> Project Name: lornatang/fsrcnn-pytorch</div><div id='commit'> Commit Name: 43cfaf9e5af149c81b55f722658550dbbc5f6c88</div><div id='time'> Time: 2021-11-25</div><div id='author'> Author: liuchangyu1111@gmail.com</div><div id='file'> File Name: train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train(7)</div><div id='n_method'> N Method Name: train(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: train.py</div><div id='n_file'> N File Name: train.py</div><div id='m_start'> M Start Line: 105</div><div id='m_end'> M End Line: 107</div><div id='n_start'> N Start Line: 147</div><div id='n_end'> N End Line: 194</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if (index + 1) % 100 == 0 or (index + 1) == batches:
            iters = index + epoch * batches + 1
            writer.add_scalar("Train/MSE_Loss", loss.item(), iters)
            <a id="change">print(f"Epoch[{epoch + 1:05d}/{config.epochs:05d}]({index + 1:05d}/{batches:05d}) MSE loss: {loss.item():.6f}."</a><a id="change">)</a>

    &#47&#47 Update lr
    scheduler.step()
</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 Calculate how many iterations there are under epoch
    batches = len(train_dataloader)

    <a id="change">batch_time = AverageMeter("Time"</a>, <a id="change">":6.3f"</a><a id="change">)</a>
    <a id="change">data_time = </a><a id="change">AverageMeter("Data"</a>, <a id="change">":6.3f"</a><a id="change">)</a>
    <a id="change">losses = </a><a id="change">AverageMeter("Loss"</a>, <a id="change">":6.6f"</a><a id="change">)</a>
    <a id="change">psnres = AverageMeter("PSNR"</a>, <a id="change">":4.2f"</a><a id="change">)</a>
    progress<a id="change"> = </a><a id="change">ProgressMeter(</a>batches, <a id="change">[</a>batch_time, data_time, losses, psnres<a id="change"></a>]<a id="change">, prefix=f"Epoch: [{epoch + 1}]")</a>

    &#47&#47 Put the generator in training mode
    model.train()

    <a id="change">end = </a><a id="change">time.time()</a>
    for <a id="change">index</a>, (lr, hr) in enumerate(train_dataloader):
        &#47&#47 measure data loading time
        <a id="change">data_time.update(</a><a id="change">time.time()</a><a id="change"> - </a>end<a id="change">)</a>

        <a id="change">lr</a> = lr.to(config.device, non_blocking=True)
        hr = hr.to(config.device, non_blocking=True)

        &#47&#47 Initialize the generator gradient
        model.zero_grad()

        &#47&#47 Mixed precision training
        with amp.autocast():
            sr = model(lr)
            loss = criterion(sr, hr)
        &#47&#47 Gradient zoom
        scaler.scale(loss).backward()
        &#47&#47 Update generator weight
        scaler.step(optimizer)
        scaler.update()

        &#47&#47 measure accuracy and record loss
        psnr<a id="change"> = 10.</a><a id="change"> * torch.log10(1.</a><a id="change"> / </a><a id="change">torch.mean(</a>(sr<a id="change"> - </a>hr)<a id="change"> ** 2</a><a id="change">))</a>
        <a id="change">losses.update(loss.item()</a>, <a id="change">lr.size(0</a><a id="change">)</a><a id="change">)</a>
        <a id="change">psnres.update(psnr.item()</a>, <a id="change">lr.size(0</a><a id="change">)</a><a id="change">)</a>

        &#47&#47 measure elapsed time
        <a id="change">batch_time.update(</a><a id="change">time.time()</a><a id="change"> - </a>end<a id="change">)</a>
        end<a id="change"> = time.time()</a>

        <a id="change">if index % config.print_frequency == 0</a>:
            <a id="change">progress.display(</a>index<a id="change">)</a>

        &#47&#47 In this Epoch, every one hundred iterations and the last iteration print the loss function
        &#47&#47 and write it to Tensorboard at the same time
        if (index + 1) % 100 == 0 or (index + 1) == batches:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lornatang/espcn-pytorch/commit/3d7da32ace2da2b908bad2a32243b464f206e72a#diff-ed183d67207df065a11e1289f19d34cc2abbc5448dea952683cfe9728c342b95L83' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95234990</div><div id='project'> Project Name: lornatang/espcn-pytorch</div><div id='commit'> Commit Name: 3d7da32ace2da2b908bad2a32243b464f206e72a</div><div id='time'> Time: 2021-11-30</div><div id='author'> Author: liuchangyu1111@gmail.com</div><div id='file'> File Name: train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train(7)</div><div id='n_method'> N Method Name: train(8)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: train.py</div><div id='n_file'> N File Name: train.py</div><div id='m_start'> M Start Line: 83</div><div id='m_end'> M End Line: 117</div><div id='n_start'> N Start Line: 156</div><div id='n_end'> N End Line: 203</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if (index + 1) % 100 == 0 or (index + 1) == batches:
            iters = index + epoch * batches + 1
            writer.add_scalar("Train/MSE_Loss", loss.item(), iters)
            <a id="change">print(f"Epoch[{epoch + 1:05d}/{config.epochs:05d}]({index + 1:05d}/{batches:05d}) MSE loss: {loss.item():.6f}."</a><a id="change">)</a>


def validate(model, valid_dataloader, epoch, writer) -&gt; float:
    &#47&#47 Calculate how many iterations there are under Epoch.</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 Calculate how many iterations there are under epoch
    batches = len(train_dataloader)

    <a id="change">batch_time = AverageMeter("Time"</a>, <a id="change">":6.3f"</a><a id="change">)</a>
    <a id="change">data_time = </a><a id="change">AverageMeter("Data"</a>, <a id="change">":6.3f"</a><a id="change">)</a>
    <a id="change">losses = AverageMeter("Loss"</a>, <a id="change">":6.6f"</a><a id="change">)</a>
    <a id="change">psnres = </a><a id="change">AverageMeter("PSNR"</a>, <a id="change">":4.2f"</a><a id="change">)</a>
    progress<a id="change"> = </a><a id="change">ProgressMeter(</a>batches, <a id="change">[</a>batch_time, data_time, losses, psnres<a id="change"></a>]<a id="change">, prefix=f"Epoch: [{epoch}]")</a>

    &#47&#47 Put the generator in training mode
    model.train()

    <a id="change">end = </a><a id="change">time.time()</a>
    for <a id="change">index</a>, (lr, hr) in enumerate(train_dataloader):
        &#47&#47 measure data loading time
        <a id="change">data_time.update(</a><a id="change">time.time()</a><a id="change"> - </a>end<a id="change">)</a>

        <a id="change">lr</a> = lr.to(config.device, non_blocking=True)
        hr = hr.to(config.device, non_blocking=True)

        &#47&#47 Initialize the generator gradient
        model.zero_grad()

        &#47&#47 Mixed precision training
        with amp.autocast():
            sr = model(lr)
            loss = criterion(sr, hr)
        &#47&#47 Gradient zoom + gradient clipping
        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.model_clip_gradient, norm_type=2.0)
        &#47&#47 Update generator weight
        scaler.step(optimizer)
        scaler.update()

        &#47&#47 measure accuracy and record loss
        psnr<a id="change"> = 10.</a><a id="change"> * torch.log10(1.</a><a id="change"> / </a><a id="change">torch.mean(</a>(sr<a id="change"> - </a>hr)<a id="change"> ** 2</a><a id="change">))</a>
        <a id="change">losses.update(loss.item()</a>, <a id="change">lr.size(0</a><a id="change">)</a><a id="change">)</a>
        <a id="change">psnres.update(psnr.item()</a>, <a id="change">lr.size(0</a><a id="change">)</a><a id="change">)</a>

        &#47&#47 measure elapsed time
        <a id="change">batch_time.update(</a><a id="change">time.time()</a><a id="change"> - </a>end<a id="change">)</a>
        end<a id="change"> = time.time()</a>

        <a id="change">if index % config.print_frequency == 0</a>:
            <a id="change">progress.display(</a>index<a id="change">)</a>

        &#47&#47 In this Epoch, every one hundred iterations and the last iteration print the loss function
        &#47&#47 and write it to Tensorboard at the same time
        if (index + 1) % 100 == 0 or (index + 1) == batches:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lornatang/vdsr-pytorch/commit/5091c519595825293d2807e74173657b5dbe83a4#diff-ed183d67207df065a11e1289f19d34cc2abbc5448dea952683cfe9728c342b95L89' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 95234976</div><div id='project'> Project Name: lornatang/vdsr-pytorch</div><div id='commit'> Commit Name: 5091c519595825293d2807e74173657b5dbe83a4</div><div id='time'> Time: 2021-11-24</div><div id='author'> Author: liuchangyu1111@gmail.com</div><div id='file'> File Name: train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train(7)</div><div id='n_method'> N Method Name: train(7)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: train.py</div><div id='n_file'> N File Name: train.py</div><div id='m_start'> M Start Line: 109</div><div id='m_end'> M End Line: 119</div><div id='n_start'> N Start Line: 155</div><div id='n_end'> N End Line: 204</div><BR>