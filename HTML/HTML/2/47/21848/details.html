<html><h3>Pattern ID :21848
</h3><img src='69673219.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            dataset.values("frames.predictions.detections.eval"),
            [[], [None], [None, [None]], [[None], [None]]],
        )
        <a id="change">self.assertNotIn("eval_tp"</a>, <a id="change">dataset.get_field_schema())</a>
        self.assertNotIn("eval_tp", dataset.get_frame_field_schema())
        <a id="change">self.assertNotIn("eval_fp"</a>, dataset.get_field_schema()<a id="change">)</a>
        self.assertNotIn("eval_fp", dataset.get_frame_field_schema())
        self.assertNotIn("eval_fn", <a id="change">dataset.get_field_schema()</a>)
        self.assertNotIn("eval_fn", dataset.get_frame_field_schema())

        &#47&#47</code></pre><h3>After Change</h3><pre><code class='java'>
            compute_mAP=True,
        )

        schema<a id="change"> = </a><a id="change">dataset.get_field_schema(flat=True)</a>
        <a id="change">self.assertIn("eval_tp"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("eval_fp"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("eval_fn"</a>, schema<a id="change">)</a>

        schema = dataset.get_frame_field_schema(flat=True)
        <a id="change">self.assertIn("eval_tp"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("eval_fp"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("eval_fn"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("ground_truth.detections.eval"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("ground_truth.detections.eval_id"</a>, schema<a id="change">)</a>
        self.assertIn("ground_truth.detections.eval_iou", schema)
        self.assertIn("predictions.detections.eval", schema)
        <a id="change">self.assertIn("predictions.detections.eval_id"</a>, schema<a id="change">)</a>
        self.assertIn("predictions.detections.eval_iou", schema)

        empty_view.load_evaluation_view("eval")
        empty_view.get_evaluation_info("eval")

        results.report()
        results.print_report()
        results.mAP()

        metrics = results.metrics()
        self.assertEqual(metrics["support"], 0)

        actual = results.confusion_matrix()
        self.assertEqual(actual.shape, (0, 0))

        &#47&#47
        &#47&#47 Test classwise evaluation (including missing data)
        &#47&#47

        results = dataset.evaluate_detections(
            "frames.predictions",
            gt_field="frames.ground_truth",
            eval_key="eval",
            method="coco",
            compute_mAP=True,
            classwise=True,  &#47&#47 don&quott allow matches w/ different classes
        )

        dataset.load_evaluation_view("eval")
        dataset.get_evaluation_info("eval")

        results.report()
        results.print_report()
        results.mAP()

        metrics = results.metrics()
        self.assertEqual(metrics["support"], 3)

        &#47&#47 rows = GT, cols = predicted, labels = [cat, dog]
        actual = results.confusion_matrix()
        expected = np.array([[1, 0], [0, 0]], dtype=int)
        self.assertEqual(actual.shape, expected.shape)
        self.assertTrue((actual == expected).all())

        &#47&#47 rows = GT, cols = predicted, labels = [cat, dog, None]
        classes = list(results.classes) + [results.missing]
        actual = results.confusion_matrix(classes=classes)
        expected = np.array([[1, 0, 2], [0, 0, 0], [1, 1, 0]], dtype=int)
        self.assertEqual(actual.shape, expected.shape)
        self.assertTrue((actual == expected).all())

        self.assertIn("eval", dataset.list_evaluations())
        self.assertListEqual(
            dataset.values("frames.ground_truth.detections.eval"),
            [[], [None], [["fn"], None], [["tp"], ["fn"]]],
        )
        self.assertListEqual(
            dataset.values("frames.predictions.detections.eval"),
            [[], [None], [None, ["fp"]], [["tp"], ["fp"]]],
        )
        self.assertIn("eval_tp", dataset.get_field_schema())
        self.assertIn("eval_tp", dataset.get_frame_field_schema())
        self.assertListEqual(
            dataset.values("frames.eval_tp"),
            [[], [0], [0, 0], [1, 0]],
        )
        self.assertIn("eval_fp", dataset.get_field_schema())
        self.assertIn("eval_fp", dataset.get_frame_field_schema())
        self.assertListEqual(
            dataset.values("frames.eval_fp"),
            [[], [0], [0, 1], [0, 1]],
        )
        self.assertIn("eval_fn", dataset.get_field_schema())
        self.assertIn("eval_fn", dataset.get_frame_field_schema())
        self.assertListEqual(
            dataset.values("frames.eval_fn"),
            [[], [0], [1, 0], [0, 1]],
        )

        dataset.delete_evaluation("eval")

        self.assertNotIn("eval", dataset.list_evaluations())
        self.assertListEqual(
            dataset.values("frames.ground_truth.detections.eval"),
            [[], [None], [[None], None], [[None], [None]]],
        )
        self.assertListEqual(
            dataset.values("frames.predictions.detections.eval"),
            [[], [None], [None, [None]], [[None], [None]]],
        )

        schema = dataset.get_field_schema(flat=True)
        self.assertNotIn("eval_tp", schema)
        self.assertNotIn("eval_fp", schema)
        self.assertNotIn("eval_fn", schema)

        <a id="change">schema = </a>dataset.get_frame_field_schema(flat=True)
        self.assertNotIn("eval_tp", schema)
        <a id="change">self.assertNotIn("eval_fp"</a>, <a id="change">schema</a><a id="change">)</a>
        <a id="change">self.assertNotIn("eval_fn"</a>, <a id="change">schema</a><a id="change">)</a>
        <a id="change">self.assertNotIn("ground_truth.detections.eval"</a>, <a id="change">schema</a><a id="change">)</a>
        <a id="change">self.assertNotIn("ground_truth.detections.eval_id"</a>, <a id="change">schema</a><a id="change">)</a>
        <a id="change">self.assertNotIn("ground_truth.detections.eval_iou"</a>, <a id="change">schema</a><a id="change">)</a>
        <a id="change">self.assertNotIn("predictions.detections.eval"</a>, <a id="change">schema</a><a id="change">)</a>
        <a id="change">self.assertNotIn("predictions.detections.eval_id"</a>, <a id="change">schema</a><a id="change">)</a>
        <a id="change">self.assertNotIn("predictions.detections.eval_iou"</a>, <a id="change">schema</a><a id="change">)</a>

        &#47&#47
        &#47&#47 Test non-classwise evaluation (including missing data)
        &#47&#47</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 24</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/voxel51/fiftyone/commit/74378e96e4e8208e9c17876740c2b6bc29ebde11#diff-b4dc8e475d33460e6cbbfeab1dc2b4b7cff019a7b3bd1d29d5401aa2e4cdedacL1493' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 69673219</div><div id='project'> Project Name: voxel51/fiftyone</div><div id='commit'> Commit Name: 74378e96e4e8208e9c17876740c2b6bc29ebde11</div><div id='time'> Time: 2022-10-14</div><div id='author'> Author: brimoor@umich.edu</div><div id='file'> File Name: tests/unittests/evaluation_tests.py</div><div id='m_class'> M Class Name: VideoDetectionsTests</div><div id='n_method'> N Class Name: VideoDetectionsTests</div><div id='m_method'> M Method Name: test_evaluate_video_detections_coco(1)</div><div id='n_method'> N Method Name: test_evaluate_video_detections_coco(1)</div><div id='m_parent_class'> M Parent Class: unittest.TestCase</div><div id='n_parent_class'> N Parent Class: unittest.TestCase</div><div id='m_file'> M File Name: tests/unittests/evaluation_tests.py</div><div id='n_file'> N File Name: tests/unittests/evaluation_tests.py</div><div id='m_start'> M Start Line: 1493</div><div id='m_end'> M End Line: 1603</div><div id='n_start'> N Start Line: 1568</div><div id='n_end'> N End Line: 1687</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            dataset.values(pred_eval_field),
            [None, None, [None], [None], [None]],
        )
        <a id="change">self.assertNotIn("eval_tp"</a>, <a id="change">dataset.get_field_schema())</a>
        <a id="change">self.assertNotIn("eval_fp"</a>, dataset.get_field_schema()<a id="change">)</a>
        self.assertNotIn("eval_fn", <a id="change">dataset.get_field_schema()</a>)

        &#47&#47
        &#47&#47 Test non-classwise evaluation (including missing data)</code></pre><h3>After Change</h3><pre><code class='java'>
            **kwargs,
        )

        schema<a id="change"> = </a><a id="change">dataset.get_field_schema(flat=True)</a>
        <a id="change">self.assertIn("eval_tp"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("eval_fp"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("eval_fn"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("ground_truth.detections.eval"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("ground_truth.detections.eval_id"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("ground_truth.detections.eval_iou"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("predictions.detections.eval"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("predictions.detections.eval_id"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("predictions.detections.eval_iou"</a>, schema<a id="change">)</a>

        empty_view.load_evaluation_view("eval")
        empty_view.get_evaluation_info("eval")

        results.report()
        results.print_report()
        results.mAP()

        metrics = results.metrics()
        self.assertEqual(metrics["support"], 0)

        actual = results.confusion_matrix()
        self.assertEqual(actual.shape, (0, 0))

        &#47&#47
        &#47&#47 Test classwise evaluation (including missing data)
        &#47&#47

        results = dataset.evaluate_detections(
            "predictions",
            gt_field="ground_truth",
            eval_key="eval",
            method="open-images",
            classwise=True,  &#47&#47 don&quott allow matches w/ different classes
            **kwargs,
        )

        dataset.load_evaluation_view("eval")
        dataset.get_evaluation_info("eval")

        results.report()
        results.print_report()
        results.mAP()

        metrics = results.metrics()
        self.assertEqual(metrics["support"], 3)

        &#47&#47 rows = GT, cols = predicted, labels = [cat, dog]
        actual = results.confusion_matrix()
        expected = np.array([[1, 0], [0, 0]], dtype=int)
        self.assertEqual(actual.shape, expected.shape)
        self.assertTrue((actual == expected).all())

        &#47&#47 rows = GT, cols = predicted, labels = [cat, dog, None]
        classes = list(results.classes) + [results.missing]
        actual = results.confusion_matrix(classes=classes)
        expected = np.array([[1, 0, 2], [0, 0, 0], [1, 1, 0]], dtype=int)
        self.assertEqual(actual.shape, expected.shape)
        self.assertTrue((actual == expected).all())

        self.assertIn("eval", dataset.list_evaluations())
        self.assertListEqual(
            dataset.values(gt_eval_field),
            [None, ["fn"], None, ["tp"], ["fn"]],
        )
        self.assertListEqual(
            dataset.values(pred_eval_field),
            [None, None, ["fp"], ["tp"], ["fp"]],
        )
        self.assertIn("eval_tp", dataset.get_field_schema())
        self.assertListEqual(dataset.values("eval_tp"), [0, 0, 0, 1, 0])
        self.assertIn("eval_fp", dataset.get_field_schema())
        self.assertListEqual(dataset.values("eval_fp"), [0, 0, 1, 0, 1])
        self.assertIn("eval_fn", dataset.get_field_schema())
        self.assertListEqual(dataset.values("eval_fn"), [0, 1, 0, 0, 1])

        dataset.delete_evaluation("eval")

        self.assertNotIn("eval", dataset.list_evaluations())
        self.assertListEqual(
            dataset.values(gt_eval_field),
            [None, [None], None, [None], [None]],
        )
        self.assertListEqual(
            dataset.values(pred_eval_field),
            [None, None, [None], [None], [None]],
        )

        <a id="change">schema = </a>dataset.get_field_schema(flat=True)
        self.assertNotIn("eval_tp", schema)
        <a id="change">self.assertNotIn("eval_fp"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("eval_fn"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("ground_truth.detections.eval"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("ground_truth.detections.eval_id"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("ground_truth.detections.eval_iou"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("predictions.detections.eval"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("predictions.detections.eval_id"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("predictions.detections.eval_iou"</a>, schema<a id="change">)</a>

        &#47&#47
        &#47&#47 Test non-classwise evaluation (including missing data)
        &#47&#47</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/voxel51/fiftyone/commit/74378e96e4e8208e9c17876740c2b6bc29ebde11#diff-b4dc8e475d33460e6cbbfeab1dc2b4b7cff019a7b3bd1d29d5401aa2e4cdedacL1205' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 69673217</div><div id='project'> Project Name: voxel51/fiftyone</div><div id='commit'> Commit Name: 74378e96e4e8208e9c17876740c2b6bc29ebde11</div><div id='time'> Time: 2022-10-14</div><div id='author'> Author: brimoor@umich.edu</div><div id='file'> File Name: tests/unittests/evaluation_tests.py</div><div id='m_class'> M Class Name: DetectionsTests</div><div id='n_method'> N Class Name: DetectionsTests</div><div id='m_method'> M Method Name: _evaluate_open_images(3)</div><div id='n_method'> N Method Name: _evaluate_open_images(3)</div><div id='m_parent_class'> M Parent Class: unittest.TestCase</div><div id='n_parent_class'> N Parent Class: unittest.TestCase</div><div id='m_file'> M File Name: tests/unittests/evaluation_tests.py</div><div id='n_file'> N File Name: tests/unittests/evaluation_tests.py</div><div id='m_start'> M Start Line: 1304</div><div id='m_end'> M End Line: 1306</div><div id='n_start'> N Start Line: 1267</div><div id='n_end'> N End Line: 1364</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            dataset.values(pred_eval_field),
            [None, None, [None], [None], [None]],
        )
        <a id="change">self.assertNotIn("eval_tp"</a>, <a id="change">dataset.get_field_schema())</a>
        <a id="change">self.assertNotIn("eval_fp"</a>, dataset.get_field_schema()<a id="change">)</a>
        self.assertNotIn("eval_fn", <a id="change">dataset.get_field_schema()</a>)

        &#47&#47
        &#47&#47 Test non-classwise evaluation (including missing data)</code></pre><h3>After Change</h3><pre><code class='java'>
            **kwargs,
        )

        schema<a id="change"> = </a><a id="change">dataset.get_field_schema(flat=True)</a>
        <a id="change">self.assertIn("eval_tp"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("eval_fp"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("eval_fn"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("ground_truth.detections.eval"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("ground_truth.detections.eval_id"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("ground_truth.detections.eval_iou"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("predictions.detections.eval"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("predictions.detections.eval_id"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("predictions.detections.eval_iou"</a>, schema<a id="change">)</a>

        empty_view.load_evaluation_view("eval")
        empty_view.get_evaluation_info("eval")

        results.report()
        results.print_report()
        results.mAP()

        metrics = results.metrics()
        self.assertEqual(metrics["support"], 0)

        actual = results.confusion_matrix()
        self.assertEqual(actual.shape, (0, 0))

        &#47&#47
        &#47&#47 Test classwise evaluation (including missing data)
        &#47&#47

        results = dataset.evaluate_detections(
            "predictions",
            gt_field="ground_truth",
            eval_key="eval",
            method="coco",
            compute_mAP=True,
            classwise=True,  &#47&#47 don&quott allow matches w/ different classes
            **kwargs,
        )

        dataset.load_evaluation_view("eval")
        dataset.get_evaluation_info("eval")

        results.report()
        results.print_report()
        results.mAP()

        metrics = results.metrics()
        self.assertEqual(metrics["support"], 3)

        &#47&#47 rows = GT, cols = predicted, labels = [cat, dog]
        actual = results.confusion_matrix()
        expected = np.array([[1, 0], [0, 0]], dtype=int)
        self.assertEqual(actual.shape, expected.shape)
        self.assertTrue((actual == expected).all())

        &#47&#47 rows = GT, cols = predicted, labels = [cat, dog, None]
        classes = list(results.classes) + [results.missing]
        actual = results.confusion_matrix(classes=classes)
        expected = np.array([[1, 0, 2], [0, 0, 0], [1, 1, 0]], dtype=int)
        self.assertEqual(actual.shape, expected.shape)
        self.assertTrue((actual == expected).all())

        self.assertIn("eval", dataset.list_evaluations())
        self.assertListEqual(
            dataset.values(gt_eval_field),
            [None, ["fn"], None, ["tp"], ["fn"]],
        )
        self.assertListEqual(
            dataset.values(pred_eval_field),
            [None, None, ["fp"], ["tp"], ["fp"]],
        )
        self.assertIn("eval_tp", dataset.get_field_schema())
        self.assertListEqual(dataset.values("eval_tp"), [0, 0, 0, 1, 0])
        self.assertIn("eval_fp", dataset.get_field_schema())
        self.assertListEqual(dataset.values("eval_fp"), [0, 0, 1, 0, 1])
        self.assertIn("eval_fn", dataset.get_field_schema())
        self.assertListEqual(dataset.values("eval_fn"), [0, 1, 0, 0, 1])

        dataset.delete_evaluation("eval")

        self.assertNotIn("eval", dataset.list_evaluations())
        self.assertListEqual(
            dataset.values(gt_eval_field),
            [None, [None], None, [None], [None]],
        )
        self.assertListEqual(
            dataset.values(pred_eval_field),
            [None, None, [None], [None], [None]],
        )

        <a id="change">schema = </a>dataset.get_field_schema(flat=True)
        self.assertNotIn("eval_tp", schema)
        <a id="change">self.assertNotIn("eval_fp"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("eval_fn"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("ground_truth.detections.eval"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("ground_truth.detections.eval_id"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("ground_truth.detections.eval_iou"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("predictions.detections.eval"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("predictions.detections.eval_id"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("predictions.detections.eval_iou"</a>, schema<a id="change">)</a>

        &#47&#47
        &#47&#47 Test non-classwise evaluation (including missing data)
        &#47&#47</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/voxel51/fiftyone/commit/74378e96e4e8208e9c17876740c2b6bc29ebde11#diff-b4dc8e475d33460e6cbbfeab1dc2b4b7cff019a7b3bd1d29d5401aa2e4cdedacL1061' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 69673216</div><div id='project'> Project Name: voxel51/fiftyone</div><div id='commit'> Commit Name: 74378e96e4e8208e9c17876740c2b6bc29ebde11</div><div id='time'> Time: 2022-10-14</div><div id='author'> Author: brimoor@umich.edu</div><div id='file'> File Name: tests/unittests/evaluation_tests.py</div><div id='m_class'> M Class Name: DetectionsTests</div><div id='n_method'> N Class Name: DetectionsTests</div><div id='m_method'> M Method Name: _evaluate_coco(3)</div><div id='n_method'> N Method Name: _evaluate_coco(3)</div><div id='m_parent_class'> M Parent Class: unittest.TestCase</div><div id='n_parent_class'> N Parent Class: unittest.TestCase</div><div id='m_file'> M File Name: tests/unittests/evaluation_tests.py</div><div id='n_file'> N File Name: tests/unittests/evaluation_tests.py</div><div id='m_start'> M Start Line: 1162</div><div id='m_end'> M End Line: 1164</div><div id='n_start'> N Start Line: 1105</div><div id='n_end'> N End Line: 1203</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        )

    def test_evaluate_video_detections_open_images(self):
        <a id="change">dataset</a> = self._make_video_detections_dataset()

        &#47&#47
        &#47&#47 Test empty view
        &#47&#47

        empty_view = dataset.limit(0)
        self.assertEqual(len(empty_view), 0)

        results = empty_view.evaluate_detections(
            "frames.predictions",
            gt_field="frames.ground_truth",
            eval_key="eval",
            method="open-images",
        )

        empty_view.load_evaluation_view("eval")
        empty_view.get_evaluation_info("eval")

        results.report()
        results.print_report()
        results.mAP()

        metrics = results.metrics()
        self.assertEqual(metrics["support"], 0)

        actual = results.confusion_matrix()
        self.assertEqual(actual.shape, (0, 0))

        &#47&#47
        &#47&#47 Test classwise evaluation (including missing data)
        &#47&#47

        results = dataset.evaluate_detections(
            "frames.predictions",
            gt_field="frames.ground_truth",
            eval_key="eval",
            method="open-images",
            classwise=True,  &#47&#47 don&quott allow matches w/ different classes
        )

        dataset.load_evaluation_view("eval")
        dataset.get_evaluation_info("eval")

        results.report()
        results.print_report()
        results.mAP()

        metrics = results.metrics()
        self.assertEqual(metrics["support"], 3)

        &#47&#47 rows = GT, cols = predicted, labels = [cat, dog]
        actual = results.confusion_matrix()
        expected = np.array([[1, 0], [0, 0]], dtype=int)
        self.assertEqual(actual.shape, expected.shape)
        self.assertTrue((actual == expected).all())

        &#47&#47 rows = GT, cols = predicted, labels = [cat, dog, None]
        classes = list(results.classes) + [results.missing]
        actual = results.confusion_matrix(classes=classes)
        expected = np.array([[1, 0, 2], [0, 0, 0], [1, 1, 0]], dtype=int)
        self.assertEqual(actual.shape, expected.shape)
        self.assertTrue((actual == expected).all())

        self.assertIn("eval", dataset.list_evaluations())
        self.assertListEqual(
            dataset.values("frames.ground_truth.detections.eval"),
            [[], [None], [["fn"], None], [["tp"], ["fn"]]],
        )
        self.assertListEqual(
            dataset.values("frames.predictions.detections.eval"),
            [[], [None], [None, ["fp"]], [["tp"], ["fp"]]],
        )
        self.assertIn("eval_tp", dataset.get_field_schema())
        self.assertIn("eval_tp", dataset.get_frame_field_schema())
        self.assertListEqual(
            dataset.values("frames.eval_tp"),
            [[], [0], [0, 0], [1, 0]],
        )
        self.assertIn("eval_fp", dataset.get_field_schema())
        self.assertIn("eval_fp", dataset.get_frame_field_schema())
        self.assertListEqual(
            dataset.values("frames.eval_fp"),
            [[], [0], [0, 1], [0, 1]],
        )
        self.assertIn("eval_fn", dataset.get_field_schema())
        self.assertIn("eval_fn", dataset.get_frame_field_schema())
        self.assertListEqual(
            dataset.values("frames.eval_fn"),
            [[], [0], [1, 0], [0, 1]],
        )

        dataset.delete_evaluation("eval")

        self.assertNotIn("eval", dataset.list_evaluations())
        self.assertListEqual(
            dataset.values("frames.ground_truth.detections.eval"),
            [[], [None], [[None], None], [[None], [None]]],
        )
        self.assertListEqual(
            dataset.values("frames.predictions.detections.eval"),
            [[], [None], [None, [None]], [[None], [None]]],
        )
        <a id="change">self.assertNotIn("eval_tp"</a>, <a id="change">dataset.get_field_schema())</a>
        self.assertNotIn("eval_tp", dataset.get_frame_field_schema())
        <a id="change">self.assertNotIn("eval_fp"</a>, dataset.get_field_schema()<a id="change">)</a>
        self.assertNotIn("eval_fp", dataset.get_frame_field_schema())
        self.assertNotIn("eval_fn", <a id="change">dataset.get_field_schema()</a>)
        self.assertNotIn("eval_fn", dataset.get_frame_field_schema())

        &#47&#47</code></pre><h3>After Change</h3><pre><code class='java'>
            method="open-images",
        )

        schema<a id="change"> = </a><a id="change">dataset.get_field_schema(flat=True)</a>
        <a id="change">self.assertIn("eval_tp"</a>, schema<a id="change">)</a>
        self.assertIn("eval_fp", schema)
        <a id="change">self.assertIn("eval_fn"</a>, schema<a id="change">)</a>

        schema = dataset.get_frame_field_schema(flat=True)
        <a id="change">self.assertIn("eval_tp"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("eval_fp"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("eval_fn"</a>, schema<a id="change">)</a>
        self.assertIn("ground_truth.detections.eval", schema)
        <a id="change">self.assertIn("ground_truth.detections.eval_id"</a>, schema<a id="change">)</a>
        self.assertIn("ground_truth.detections.eval_iou", schema)
        <a id="change">self.assertIn("predictions.detections.eval"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("predictions.detections.eval_id"</a>, schema<a id="change">)</a>
        <a id="change">self.assertIn("predictions.detections.eval_iou"</a>, schema<a id="change">)</a>

        empty_view.load_evaluation_view("eval")
        empty_view.get_evaluation_info("eval")

        results.report()
        results.print_report()
        results.mAP()

        metrics = results.metrics()
        self.assertEqual(metrics["support"], 0)

        actual = results.confusion_matrix()
        self.assertEqual(actual.shape, (0, 0))

        &#47&#47
        &#47&#47 Test classwise evaluation (including missing data)
        &#47&#47

        results = dataset.evaluate_detections(
            "frames.predictions",
            gt_field="frames.ground_truth",
            eval_key="eval",
            method="open-images",
            classwise=True,  &#47&#47 don&quott allow matches w/ different classes
        )

        dataset.load_evaluation_view("eval")
        dataset.get_evaluation_info("eval")

        results.report()
        results.print_report()
        results.mAP()

        metrics = results.metrics()
        self.assertEqual(metrics["support"], 3)

        &#47&#47 rows = GT, cols = predicted, labels = [cat, dog]
        actual = results.confusion_matrix()
        expected = np.array([[1, 0], [0, 0]], dtype=int)
        self.assertEqual(actual.shape, expected.shape)
        self.assertTrue((actual == expected).all())

        &#47&#47 rows = GT, cols = predicted, labels = [cat, dog, None]
        classes = list(results.classes) + [results.missing]
        actual = results.confusion_matrix(classes=classes)
        expected = np.array([[1, 0, 2], [0, 0, 0], [1, 1, 0]], dtype=int)
        self.assertEqual(actual.shape, expected.shape)
        self.assertTrue((actual == expected).all())

        self.assertIn("eval", dataset.list_evaluations())
        self.assertListEqual(
            dataset.values("frames.ground_truth.detections.eval"),
            [[], [None], [["fn"], None], [["tp"], ["fn"]]],
        )
        self.assertListEqual(
            dataset.values("frames.predictions.detections.eval"),
            [[], [None], [None, ["fp"]], [["tp"], ["fp"]]],
        )
        self.assertIn("eval_tp", dataset.get_field_schema())
        self.assertIn("eval_tp", dataset.get_frame_field_schema())
        self.assertListEqual(
            dataset.values("frames.eval_tp"),
            [[], [0], [0, 0], [1, 0]],
        )
        self.assertIn("eval_fp", dataset.get_field_schema())
        self.assertIn("eval_fp", dataset.get_frame_field_schema())
        self.assertListEqual(
            dataset.values("frames.eval_fp"),
            [[], [0], [0, 1], [0, 1]],
        )
        self.assertIn("eval_fn", dataset.get_field_schema())
        self.assertIn("eval_fn", dataset.get_frame_field_schema())
        self.assertListEqual(
            dataset.values("frames.eval_fn"),
            [[], [0], [1, 0], [0, 1]],
        )

        dataset.delete_evaluation("eval")

        self.assertNotIn("eval", dataset.list_evaluations())
        self.assertListEqual(
            dataset.values("frames.ground_truth.detections.eval"),
            [[], [None], [[None], None], [[None], [None]]],
        )
        self.assertListEqual(
            dataset.values("frames.predictions.detections.eval"),
            [[], [None], [None, [None]], [[None], [None]]],
        )

        schema = dataset.get_field_schema(flat=True)
        self.assertNotIn("eval_tp", schema)
        self.assertNotIn("eval_fp", schema)
        self.assertNotIn("eval_fn", schema)

        <a id="change">schema = </a>dataset.get_frame_field_schema(flat=True)
        <a id="change">self.assertNotIn("eval_tp"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("eval_fp"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("eval_fn"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("ground_truth.detections.eval"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("ground_truth.detections.eval_id"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("ground_truth.detections.eval_iou"</a>, schema<a id="change">)</a>
        self.assertNotIn("predictions.detections.eval", schema)
        <a id="change">self.assertNotIn("predictions.detections.eval_id"</a>, schema<a id="change">)</a>
        <a id="change">self.assertNotIn("predictions.detections.eval_iou"</a>, schema<a id="change">)</a>

        &#47&#47
        &#47&#47 Test non-classwise evaluation (including missing data)
        &#47&#47</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/voxel51/fiftyone/commit/74378e96e4e8208e9c17876740c2b6bc29ebde11#diff-b4dc8e475d33460e6cbbfeab1dc2b4b7cff019a7b3bd1d29d5401aa2e4cdedacL1652' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 69673213</div><div id='project'> Project Name: voxel51/fiftyone</div><div id='commit'> Commit Name: 74378e96e4e8208e9c17876740c2b6bc29ebde11</div><div id='time'> Time: 2022-10-14</div><div id='author'> Author: brimoor@umich.edu</div><div id='file'> File Name: tests/unittests/evaluation_tests.py</div><div id='m_class'> M Class Name: VideoDetectionsTests</div><div id='n_method'> N Class Name: VideoDetectionsTests</div><div id='m_method'> M Method Name: test_evaluate_video_detections_open_images(1)</div><div id='n_method'> N Method Name: test_evaluate_video_detections_open_images(1)</div><div id='m_parent_class'> M Parent Class: unittest.TestCase</div><div id='n_parent_class'> N Parent Class: unittest.TestCase</div><div id='m_file'> M File Name: tests/unittests/evaluation_tests.py</div><div id='n_file'> N File Name: tests/unittests/evaluation_tests.py</div><div id='m_start'> M Start Line: 1653</div><div id='m_end'> M End Line: 1761</div><div id='n_start'> N Start Line: 1753</div><div id='n_end'> N End Line: 1871</div><BR>