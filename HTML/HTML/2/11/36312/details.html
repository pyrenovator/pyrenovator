<html><h3>Pattern ID :36312
</h3><img src='102897378.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

            &#47&#47 group.setdefault(&quotparams_old&quot, group[&quotparams&quot])

            <a id="change">for </a>p in group[&quotparams&quot]<a id="change">:
                </a><a id="change">if </a>p.grad is None:
                    <a id="change">continue</a>

                p_t = p
                param_state = self.state[p]
                <a id="change">if &quotparam_old&quot not in param_state</a>:
                    param_state[&quotparam_old&quot]<a id="change"> = </a>torch.clone(p).detach()
                else:
                    p_t = param_state[&quotparam_old&quot]

                d_p<a id="change"> = </a>p.grad.data + lamda*(p-p_t)
                p.add_(d_p, alpha=-lr)
</code></pre><h3>After Change</h3><pre><code class='java'>
    @torch.no_grad()
    def step(self, global_params, device):
        for group in self.param_groups:
            <a id="change">for </a>p, g in <a id="change">zip(</a>group[&quotparams&quot], global_params<a id="change">):
                </a>g = g.to(device)
                d_p<a id="change"> = </a>p.grad.data + group[&quotmu&quot] * (p.data - g.data)
                p.data.add_(d_p, alpha=-group[&quotlr&quot])
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 10</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tsingz0/pfl-non-iid/commit/330ce2d6169fd4a54cec28895418bd38c206b6a4#diff-a99c19163c19c03eede69bdda22f1a2d923b8eb7d0f4b0463d797d66d5fc8e87L89' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 102897378</div><div id='project'> Project Name: tsingz0/pfl-non-iid</div><div id='commit'> Commit Name: 330ce2d6169fd4a54cec28895418bd38c206b6a4</div><div id='time'> Time: 2021-03-24</div><div id='author'> Author: 2719584131@qq.com</div><div id='file'> File Name: system/flcore/optimizers/fedoptimizer.py</div><div id='m_class'> M Class Name: PerturbedGradientDescent</div><div id='n_method'> N Class Name: PerturbedGradientDescent</div><div id='m_method'> M Method Name: step(3)</div><div id='n_method'> N Method Name: step(1)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: system/flcore/optimizers/fedoptimizer.py</div><div id='n_file'> N File Name: system/flcore/optimizers/fedoptimizer.py</div><div id='m_start'> M Start Line: 89</div><div id='m_end'> M End Line: 108</div><div id='n_start'> N Start Line: 116</div><div id='n_end'> N End Line: 121</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                loss = closure()

        for group in self.param_groups:
            <a id="change">for </a><a id="change">p</a> in group[&quotparams&quot]<a id="change">:
                </a><a id="change">if </a>p.grad is None:
                    <a id="change">continue</a>
                grad = p.grad
                if grad.is_sparse:
                    raise RuntimeError(&quotASGD does not support sparse gradients&quot)
                state<a id="change"> = </a>self.state[p]

                &#47&#47 State initialization
                if len(state) == 0:
                    state[&quotstep&quot] = 0
                    state[&quoteta&quot] = group[&quotlr&quot]
                    state[&quotmu&quot] = 1
                    state[&quotax&quot] = torch.zeros_like(p, memory_format=torch.preserve_format)

                state[&quotstep&quot] += 1

                <a id="change">if group[&quotweight_decay&quot] != 0</a>:
                    grad<a id="change"> = </a>grad.add(p, alpha=group[&quotweight_decay&quot])

                &#47&#47 decay term
                p.mul_(1 - group[&quotlambd&quot] * state[&quoteta&quot])</code></pre><h3>After Change</h3><pre><code class='java'>
                   lambd=group[&quotlambd&quot])

            &#47&#47 update eta and mu
            <a id="change">for </a>p, mu, eta in <a id="change">zip(</a>params_with_grad, mus, etas<a id="change">):
                </a>state<a id="change"> = </a>self.state[p]
                state[&quoteta&quot] = (group[&quotlr&quot] /
                                math.pow((1 + group[&quotlambd&quot] * group[&quotlr&quot] * state[&quotstep&quot]), group[&quotalpha&quot]))
                state[&quotmu&quot] = 1 / max(1, state[&quotstep&quot] - group[&quott0&quot])</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/9a622f4cd9318d69896462823891588ef0bf719c#diff-b5441178f7d0764941fd9b5dc89b596d57da114d4665b7b31138beac59e93c7dL36' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 102897382</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 9a622f4cd9318d69896462823891588ef0bf719c</div><div id='time'> Time: 2021-05-19</div><div id='author'> Author: iramazanli@fb.com</div><div id='file'> File Name: torch/optim/asgd.py</div><div id='m_class'> M Class Name: ASGD</div><div id='n_method'> N Class Name: ASGD</div><div id='m_method'> M Method Name: step(2)</div><div id='n_method'> N Method Name: step(2)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: torch/optim/asgd.py</div><div id='n_file'> N File Name: torch/optim/asgd.py</div><div id='m_start'> M Start Line: 48</div><div id='m_end'> M End Line: 86</div><div id='n_start'> N Start Line: 49</div><div id='n_end'> N End Line: 94</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            dampening = group[&quotdampening&quot]
            nesterov = group[&quotnesterov&quot]

            <a id="change">for </a><a id="change">p</a> in group[&quotparams&quot]<a id="change">:
                </a><a id="change">if </a>p.grad is None:
                    <a id="change">continue</a>
                d_p = p.grad
                <a id="change">if weight_decay != 0</a>:
                    d_p<a id="change"> = </a>d_p.add(p, alpha=weight_decay)
                if momentum != 0:
                    param_state = self.state[p]
                    if &quotmomentum_buffer&quot not in param_state:
                        buf = param_state[&quotmomentum_buffer&quot] = torch.clone(d_p).detach()
                    else:
                        buf<a id="change"> = </a>param_state[&quotmomentum_buffer&quot]
                        buf.mul_(momentum).add_(d_p, alpha=1 - dampening)
                    if nesterov:
                        d_p = d_p.add(buf, alpha=momentum)</code></pre><h3>After Change</h3><pre><code class='java'>
                  nesterov)

            &#47&#47 update momentum_buffers in state
            <a id="change">for </a>p, momentum_buffer in <a id="change">zip(</a>params_with_grad, momentum_buffer_list<a id="change">):
                </a>state = self.state[p]
                state[&quotmomentum_buffer&quot]<a id="change"> = </a>momentum_buffer

        return loss
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/a0cf5566d88533c5caa7a490beb6eb0760eee9b4#diff-e5ea47a2193f1cfb039210c5c0ff83e8175739afc0551866052f6ad31bb91482L76' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 102897386</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: a0cf5566d88533c5caa7a490beb6eb0760eee9b4</div><div id='time'> Time: 2021-01-21</div><div id='author'> Author: wanchaol@users.noreply.github.com</div><div id='file'> File Name: torch/optim/sgd.py</div><div id='m_class'> M Class Name: SGD</div><div id='n_method'> N Class Name: SGD</div><div id='m_method'> M Method Name: step(2)</div><div id='n_method'> N Method Name: step(2)</div><div id='m_parent_class'> M Parent Class: Optimizer</div><div id='n_parent_class'> N Parent Class: Optimizer</div><div id='m_file'> M File Name: torch/optim/sgd.py</div><div id='n_file'> N File Name: torch/optim/sgd.py</div><div id='m_start'> M Start Line: 88</div><div id='m_end'> M End Line: 114</div><div id='n_start'> N Start Line: 89</div><div id='n_end'> N End Line: 124</div><BR>