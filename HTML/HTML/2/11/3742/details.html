<html><h3>Pattern ID :3742
</h3><img src='13971701.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        parameter_lr = lr * ctx.parameter_variance.get(param_name, 1)
        grad = grad.astype(ctx.model.storage_dtype)
        grad = adaptive_gradient_clipping(inner_ctx, param_name, grad)
        <a id="change">if "norm" in param_name.lower() or "rezero" in param_name.lower() or grad.ndim &lt; 2</a>:
            grad<a id="change"> = </a>adam(inner_ctx, param_name, grad, current_step)  &#47&#47 Do adam update for small parameters
        else:
            grad = sm3(inner_ctx, param_name, grad)
            grad = ema(inner_ctx, param_name, grad, current_step, 1 - ctx.optimizer.momentum_beta, "momentum", True)</code></pre><h3>After Change</h3><pre><code class='java'>

    lr = -get_current_lr(ctx, current_step)
    count = ctx.parameters[&quotshampoo/count&quot]
    stats = {<a id="change">k[len(&quotshampoo/stats/&quot):]</a>: p for k, p in ctx.parameters.items() if k.startswith(&quotshampoo/stats/&quot)}
    state = ShampooState(count=count, stats=stats)
    init, update_fn = distributed_shampoo(1, 1024)
    grads = {k: adaptive_gradient_clipping(ctx, k, v.astype(ctx.model.storage_dtype)) for k, v in grads.items()}</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/homebrewnlp/homebrewnlp-jax/commit/c9ecfae9c07532698f99f867beea0394b618a860#diff-4540d7c8541eb25568b448ea775d64c43be83ce020bc67dd8340da4d4e804846L75' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 13971701</div><div id='project'> Project Name: homebrewnlp/homebrewnlp-jax</div><div id='commit'> Commit Name: c9ecfae9c07532698f99f867beea0394b618a860</div><div id='time'> Time: 2022-05-10</div><div id='author'> Author: 39779310+ClashLuke@users.noreply.github.com</div><div id='file'> File Name: src/optimizer.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: update(3)</div><div id='n_method'> N Method Name: update(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/optimizer.py</div><div id='n_file'> N File Name: src/optimizer.py</div><div id='m_start'> M Start Line: 75</div><div id='m_end'> M End Line: 91</div><div id='n_start'> N Start Line: 76</div><div id='n_end'> N End Line: 89</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    ctx = ctx.add_to_prefix("optimizer")
    lr = -get_current_lr(ctx, current_step)

    for param_name, <a id="change">grad</a> in grads.items():
        inner_ctx = ctx.add_to_prefix(param_name, count=False)
        if "optimizer" in param_name:
            continue
        parameter_lr = lr * ctx.parameter_variance.get(param_name, 1)
        grad = grad.astype(ctx.model.storage_dtype)
        grad = adaptive_gradient_clipping(inner_ctx, param_name, grad)
        <a id="change">if "norm" in param_name.lower() or "rezero" in param_name.lower() or grad.ndim &lt; 2</a>:
            grad<a id="change"> = </a>adam(inner_ctx, param_name, grad, current_step)  &#47&#47 Do adam update for small parameters
        else:
            grad = sm3(inner_ctx, param_name, grad)
            grad = ema(inner_ctx, param_name, grad, current_step, 1 - ctx.optimizer.momentum_beta, "momentum", True)</code></pre><h3>After Change</h3><pre><code class='java'>

    lr = -get_current_lr(ctx, current_step)
    count = ctx.parameters[&quotshampoo/count&quot]
    stats = {<a id="change">k[len(&quotshampoo/stats/&quot):]</a>: p for k, p in ctx.parameters.items() if k.startswith(&quotshampoo/stats/&quot)}
    state = ShampooState(count=count, stats=stats)
    init, update_fn = distributed_shampoo(1, 1024)
    grads = {k: adaptive_gradient_clipping(ctx, k, v.astype(ctx.model.storage_dtype)) for k, v in grads.items()}</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/homebrewnlp/homebrewnlp-jax/commit/c9ecfae9c07532698f99f867beea0394b618a860#diff-4540d7c8541eb25568b448ea775d64c43be83ce020bc67dd8340da4d4e804846L74' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 13971702</div><div id='project'> Project Name: homebrewnlp/homebrewnlp-jax</div><div id='commit'> Commit Name: c9ecfae9c07532698f99f867beea0394b618a860</div><div id='time'> Time: 2022-05-10</div><div id='author'> Author: 39779310+ClashLuke@users.noreply.github.com</div><div id='file'> File Name: src/optimizer.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: update(3)</div><div id='n_method'> N Method Name: update(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/optimizer.py</div><div id='n_file'> N File Name: src/optimizer.py</div><div id='m_start'> M Start Line: 75</div><div id='m_end'> M End Line: 91</div><div id='n_start'> N Start Line: 76</div><div id='n_end'> N End Line: 89</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 logger.info(&quotInput: %s&quot, str(batch_input_sequences))

        batch_outputs = [[] for _ in range(batch_size)]
        for <a id="change">hyperparameter_idx</a> in range(len(args.temperature)):
            out = model.generate(input_ids=batch_context_tensor,
                                 bad_words_ids=None,
                                 attention_mask=attention_mask,
                                 min_length=args.min_output_length,
                                 max_length=batch_context_tensor.shape[1]+args.length,
                                 num_beams=args.num_beams[hyperparameter_idx],
                                 top_k=args.top_k[hyperparameter_idx],
                                 top_p=args.top_p[hyperparameter_idx],
                                 early_stopping=True,
                                 num_return_sequences=args.num_samples[hyperparameter_idx],
                                 repetition_penalty=args.repetition_penalty[hyperparameter_idx],
                                 no_repeat_ngram_size=args.no_repeat_ngram_size[hyperparameter_idx],
                                 do_sample=args.temperature[hyperparameter_idx]!=0,
                                 temperature=args.temperature[hyperparameter_idx] if args.temperature[hyperparameter_idx] &gt; 0 else 1.0, &#47&#47 if temperature==0, we do not sample
                                 eos_token_id=end_token_id,
                                 pad_token_id=pad_token_id,
                                )
            if not isinstance(out, list):
                out = out[:, :].tolist()
            for i, o in enumerate(out):
                <a id="change">if args.model_type==&quotbart&quot or args.model_type==&quotmbart&quot</a>:
                    o<a id="change"> = </a>o[1:] &#47&#47 remove &lt;s&gt; start token
                if not args.output_prompt:
                    o = o[len(batch_prompt_tokens[(i//args.num_samples[hyperparameter_idx]) % batch_size]):]
                min_index = len(o)-1</code></pre><h3>After Change</h3><pre><code class='java'>
                sample_index = (i//args.num_samples[hyperparameter_idx]) % batch_size
                
                if not args.output_prompt:
                    o = <a id="change">o[len(batch_prompt_tokens[sample_index]):]</a>
                min_index = len(o)-1
                for stop_token_id in stop_token_ids+[end_token_id]:
                    try:
                        index = o.index(stop_token_id)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/stanford-oval/genienlp/commit/a50063bc3ba95c58b516d8149b90dc35b36605fb#diff-de31570d158d9fc989a3d6319905880ffc88335e5b475b35a267f19580a38c71L237' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 13971699</div><div id='project'> Project Name: stanford-oval/genienlp</div><div id='commit'> Commit Name: a50063bc3ba95c58b516d8149b90dc35b36605fb</div><div id='time'> Time: 2020-07-16</div><div id='author'> Author: mehrad@stanford.edu</div><div id='file'> File Name: genienlp/paraphrase/run_generation.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: run_single_process_generation(1)</div><div id='n_method'> N Method Name: run_single_process_generation(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: genienlp/paraphrase/run_generation.py</div><div id='n_file'> N File Name: genienlp/paraphrase/run_generation.py</div><div id='m_start'> M Start Line: 277</div><div id='m_end'> M End Line: 339</div><div id='n_start'> N Start Line: 263</div><div id='n_end'> N End Line: 324</div><BR>