<html><h3>Pattern ID :33824
</h3><img src='97062601.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    o = []
    &#47&#47 Manual zero pad (due to padding=1 in Conv3d)
    zeros = torch.zeros_like(example_clip[:, :, 0])
    o.append(<a id="change">cotrans.forward(</a>zeros<a id="change">)</a>)
    for i in range(example_clip.shape[2]):
        o.append(<a id="change">cotrans.forward(example_clip[:, :, i]</a><a id="change">)</a>)
    o.append(cotrans.forward(zeros))

    &#47&#47 For debugging:</code></pre><h3>After Change</h3><pre><code class='java'>
    target = trans.forward(example_clip)

    &#47&#47 Recurrent block
    <a id="change">cotrans</a> = CoX3DTransform(
        dim_in=2,
        dim_out=2,
        temp_kernel_size=3,
        stride=2,
        dim_inner=5,
        num_groups=5,
        stride_1x1=False,
        inplace_relu=True,
        eps=1e-5,
        bn_mmt=0.1,
        dilation=1,
        norm_module=torch.nn.BatchNorm3d,
        se_ratio=0.1,
        swish_inner=True,
        block_idx=0,
        temporal_window_size=example_clip.shape[2],  &#47&#47 +2 from padding
        temporal_fill="zeros",
        se_scope="clip",
    )
    co.load_state_dict(cotrans, trans.state_dict(), flatten=True)
    cotrans.eval()  &#47&#47 This has a major effect on BatchNorm result

    &#47&#47 forward
    output = cotrans.forward(example_clip)
    assert torch.allclose(target, output)

    &#47&#47 forward_steps
    output = cotrans.forward_steps(example_clip, pad_end=True)
    assert torch.allclose(target, output)

    &#47&#47 forward_steps - broken up
    cotrans.clean_state()
    nothing = cotrans.forward_steps(example_clip[:, :, :-1], pad_end=False)  &#47&#47 init
    assert isinstance(nothing, TensorPlaceholder)

    lasts = cotrans.forward_steps(example_clip[:, :, -1:], pad_end=True)
    assert torch.allclose(lasts, target)

    &#47&#47 forward_step
    <a id="change">cotrans.clean_state()</a>
    nothing = cotrans.forward_steps(example_clip[:, :, :], pad_end=False)  &#47&#47 init
    assert isinstance(nothing, TensorPlaceholder)

    &#47&#47 Manual pad end.
    zeros = torch.zeros_like(example_clip[:, :, 0])
    step = cotrans.forward_step(zeros)
    assert torch.allclose(step, target[:, :, 0])  &#47&#47 (4/4) correct in SE pool
    step = <a id="change">cotrans.forward_step(</a>zeros<a id="change">)</a>
    assert torch.allclose(step, target[:, :, 1], atol=5e-3)  &#47&#47 (3/4) correct in pool
    step<a id="change"> = cotrans</a><a id="change">.forward_step(</a>zeros<a id="change">)</a>
    assert torch.allclose(step, target[:, :, 2], atol=5e-3)  &#47&#47 (2/4) correct in pool
    step<a id="change"> = cotrans</a><a id="change">.forward_step(</a>zeros<a id="change">)</a>
    assert torch.allclose(step, target[:, :, 3], atol=5e-3)  &#47&#47 (1/4) correct in pool
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 9</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lukashedegaard/co3d/commit/3a1ca5de4898fd89bc774492cf0eeaed905baba1#diff-81ebfd3b7da0e2057c99c1622bd2ed14cb38bdb040741812bc0eb3de8e60005eL598' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 97062601</div><div id='project'> Project Name: lukashedegaard/co3d</div><div id='commit'> Commit Name: 3a1ca5de4898fd89bc774492cf0eeaed905baba1</div><div id='time'> Time: 2021-09-10</div><div id='author'> Author: lh@eng.au.dk</div><div id='file'> File Name: tests/cox3d/test_x3d.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: test_CoX3DTransform(0)</div><div id='n_method'> N Method Name: test_CoX3DTransform(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: tests/cox3d/test_x3d.py</div><div id='n_file'> N File Name: tests/cox3d/test_x3d.py</div><div id='m_start'> M Start Line: 601</div><div id='m_end'> M End Line: 657</div><div id='n_start'> N Start Line: 598</div><div id='n_end'> N End Line: 672</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    )

    &#47&#47 Continual model
    <a id="change">comodel</a> = CoX3D(
        dim_in=3,
        image_size=160,
        frames_per_clip=frames_per_clip,
        num_classes=400,
        x3d_conv1_dim=12,
        x3d_conv5_dim=2048,
        x3d_num_groups=1,
        x3d_width_per_group=64,
        x3d_width_factor=2.0,
        x3d_depth_factor=2.2,
        x3d_bottleneck_factor=2.25,
        x3d_use_channelwise_3x3x3=1,
        x3d_dropout_rate=0.5,
        x3d_head_activation="softmax",
        x3d_head_batchnorm=0,
        x3d_fc_std_init=0.01,
        x3d_final_batchnorm_zero_init=1,
        temporal_fill="zeros",
        se_scope="clip",
    )

    &#47&#47 Load weights
    model_state = torch.load(weights_path, map_location="cpu")["model_state"]
    model.load_state_dict(model_state)
    comodel.load_state_dict(model_state)

    &#47&#47 Training mode has large effect on BatchNorm result - test will fail otherwise
    model.eval()
    comodel.eval()

    &#47&#47 Forward
    target = model(input)
    target_top10 = torch.topk(target, k=10)[1][0].tolist()

    &#47&#47 forward_steps produces same outputs
    output = comodel.forward_steps(input)
    assert torch.allclose(target, output, atol=5e-3)

    &#47&#47 Continuation is not exact due to zero paddings (even for boring video)
    output_next_frame = <a id="change">comodel.forward(</a>input[:, :, -1]<a id="change">)</a>

    &#47&#47 assert torch.allclose(target, output_next_frame, atol=5e-4)

    next_frame_top10 = torch.topk(output_next_frame, k=10)[1][0].tolist()
    &#47&#47 Top 2 is the same
    assert len(set(target_top10[:2]) - set(next_frame_top10[:2])) == 0
    &#47&#47 Top 4 is overlapping
    assert len(set(target_top10[:4]) - set(next_frame_top10[:4])) == 0
    &#47&#47 Top 10 is mostly overlapping
    assert len(set(target_top10) - set(next_frame_top10)) == 1

    &#47&#47 Check output if we continue for a long time
    for _ in range(20):
        output_next_frame = <a id="change">comodel.forward(input[:, :, -1]</a><a id="change">)</a>

    &#47&#47 They are further apart now
    assert torch.allclose(target, output_next_frame, atol=5e-2)
    _, next_frame_top10 = torch.topk(output_next_frame, k=10)</code></pre><h3>After Change</h3><pre><code class='java'>
    )

    &#47&#47 Continual model
    <a id="change">comodel</a> = CoX3D(
        dim_in=3,
        image_size=160,
        frames_per_clip=frames_per_clip,
        num_classes=400,
        x3d_conv1_dim=12,
        x3d_conv5_dim=2048,
        x3d_num_groups=1,
        x3d_width_per_group=64,
        x3d_width_factor=2.0,
        x3d_depth_factor=2.2,
        x3d_bottleneck_factor=2.25,
        x3d_use_channelwise_3x3x3=1,
        x3d_dropout_rate=0.5,
        x3d_head_activation="softmax",
        x3d_head_batchnorm=0,
        x3d_fc_std_init=0.01,
        x3d_final_batchnorm_zero_init=1,
        temporal_fill="zeros",
        se_scope="clip",
    )
    assert comodel.delay == 220

    &#47&#47 Load weights
    model_state = torch.load(weights_path, map_location="cpu")["model_state"]
    model.load_state_dict(model_state)
    comodel.load_state_dict(model_state, flatten=True)

    &#47&#47 Training mode has large effect on BatchNorm result - test will fail otherwise
    model.eval()
    comodel.eval()

    target = model.forward(sample)

    &#47&#47 forward
    output = comodel.forward(sample)
    assert torch.allclose(target, output)

    &#47&#47 forward_steps
    output = comodel.forward_steps(sample, pad_end=True)
    assert torch.allclose(target, output)

    &#47&#47 forward
    <a id="change">comodel.clean_state()</a>
    &#47&#47 init
    for i in range(frames_per_clip):
        <a id="change">comodel.forward_step(</a>sample[:, :, i]<a id="change">)</a>

    &#47&#47 zero-pad end manually
    zeros = torch.zeros_like(sample[:, :, 0])
    for _ in range(comodel.delay - frames_per_clip):
        comodel.forward_step(zeros)

    &#47&#47 final result
    output<a id="change"> = </a><a id="change">comodel.forward_step(</a>zeros<a id="change">)</a>
    torch.allclose(output, target, atol=5e-3)

    target_top10 = torch.topk(target, k=10)[1][0].tolist()
    output_top10 = torch.topk(output, k=10)[1][0].tolist()

    assert len(set(target_top10) - set(output_top10)) &lt;= 3

    &#47&#47 Another step - now out of comparable operation
    output<a id="change"> = </a><a id="change">comodel.forward_step(</a>zeros<a id="change">)</a>
    output_top10 = torch.topk(output, k=10)[1][0].tolist()
    assert len(set(target_top10) - set(output_top10)) &lt;= 3

</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lukashedegaard/co3d/commit/44e4624108b83f8ba94cfb960cdf74f36bbd5703#diff-81ebfd3b7da0e2057c99c1622bd2ed14cb38bdb040741812bc0eb3de8e60005eL598' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 97062633</div><div id='project'> Project Name: lukashedegaard/co3d</div><div id='commit'> Commit Name: 44e4624108b83f8ba94cfb960cdf74f36bbd5703</div><div id='time'> Time: 2021-09-10</div><div id='author'> Author: lh@eng.au.dk</div><div id='file'> File Name: tests/cox3d/test_x3d.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: test_CoX3D(0)</div><div id='n_method'> N Method Name: test_CoX3D(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: tests/cox3d/test_x3d.py</div><div id='n_file'> N File Name: tests/cox3d/test_x3d.py</div><div id='m_start'> M Start Line: 601</div><div id='m_end'> M End Line: 687</div><div id='n_start'> N Start Line: 599</div><div id='n_end'> N End Line: 688</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    target = trans(example_clip)

    &#47&#47 Recurrent block
    <a id="change">cotrans</a> = CoX3DTransform(
        dim_in=2,
        dim_out=2,
        temp_kernel_size=3,
        stride=2,
        dim_inner=5,
        num_groups=5,
        stride_1x1=False,
        inplace_relu=True,
        eps=1e-5,
        bn_mmt=0.1,
        dilation=1,
        norm_module=torch.nn.BatchNorm3d,
        se_ratio=0.1,
        swish_inner=True,
        block_idx=0,
        temporal_window_size=example_clip.shape[2],
        temporal_fill="zeros",
    )
    cotrans.load_state_dict(trans.state_dict())
    cotrans.eval()  &#47&#47 This has a major effect on BatchNorm result

    &#47&#47 Forward 3D works like the original
    output3d = cotrans.forward_regular(example_clip)
    assert torch.allclose(target, output3d)

    o = []
    &#47&#47 Manual zero pad (due to padding=1 in Conv3d)
    zeros = torch.zeros_like(example_clip[:, :, 0])
    o.append(<a id="change">cotrans.forward(</a>zeros<a id="change">)</a>)
    for i in range(example_clip.shape[2]):
        o.append(<a id="change">cotrans.forward(example_clip[:, :, i]</a><a id="change">)</a>)
    o.append(cotrans.forward(zeros))

    &#47&#47 For debugging:</code></pre><h3>After Change</h3><pre><code class='java'>
    target = trans.forward(example_clip)

    &#47&#47 Recurrent block
    <a id="change">cotrans</a> = CoX3DTransform(
        dim_in=2,
        dim_out=2,
        temp_kernel_size=3,
        stride=2,
        dim_inner=5,
        num_groups=5,
        stride_1x1=False,
        inplace_relu=True,
        eps=1e-5,
        bn_mmt=0.1,
        dilation=1,
        norm_module=torch.nn.BatchNorm3d,
        se_ratio=0.1,
        swish_inner=True,
        block_idx=0,
        temporal_window_size=example_clip.shape[2],  &#47&#47 +2 from padding
        temporal_fill="zeros",
        se_scope="clip",
    )
    co.load_state_dict(cotrans, trans.state_dict(), flatten=True)
    cotrans.eval()  &#47&#47 This has a major effect on BatchNorm result

    &#47&#47 forward
    output = cotrans.forward(example_clip)
    assert torch.allclose(target, output)

    &#47&#47 forward_steps
    output = cotrans.forward_steps(example_clip, pad_end=True)
    assert torch.allclose(target, output)

    &#47&#47 forward_steps - broken up
    cotrans.clean_state()
    nothing = cotrans.forward_steps(example_clip[:, :, :-1], pad_end=False)  &#47&#47 init
    assert isinstance(nothing, TensorPlaceholder)

    lasts = cotrans.forward_steps(example_clip[:, :, -1:], pad_end=True)
    assert torch.allclose(lasts, target)

    &#47&#47 forward_step
    <a id="change">cotrans.clean_state()</a>
    nothing = cotrans.forward_steps(example_clip[:, :, :], pad_end=False)  &#47&#47 init
    assert isinstance(nothing, TensorPlaceholder)

    &#47&#47 Manual pad end.
    zeros = torch.zeros_like(example_clip[:, :, 0])
    step = <a id="change">cotrans.forward_step(</a>zeros<a id="change">)</a>
    assert torch.allclose(step, target[:, :, 0])  &#47&#47 (4/4) correct in SE pool
    step = cotrans.forward_step(zeros)
    assert torch.allclose(step, target[:, :, 1], atol=5e-3)  &#47&#47 (3/4) correct in pool
    step<a id="change"> = </a><a id="change">cotrans.forward_step(</a>zeros<a id="change">)</a>
    assert torch.allclose(step, target[:, :, 2], atol=5e-3)  &#47&#47 (2/4) correct in pool
    step<a id="change"> = </a><a id="change">cotrans.forward_step(</a>zeros<a id="change">)</a>
    assert torch.allclose(step, target[:, :, 3], atol=5e-3)  &#47&#47 (1/4) correct in pool
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lukashedegaard/co3d/commit/3a1ca5de4898fd89bc774492cf0eeaed905baba1#diff-81ebfd3b7da0e2057c99c1622bd2ed14cb38bdb040741812bc0eb3de8e60005eL581' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 97062600</div><div id='project'> Project Name: lukashedegaard/co3d</div><div id='commit'> Commit Name: 3a1ca5de4898fd89bc774492cf0eeaed905baba1</div><div id='time'> Time: 2021-09-10</div><div id='author'> Author: lh@eng.au.dk</div><div id='file'> File Name: tests/cox3d/test_x3d.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: test_CoX3DTransform(0)</div><div id='n_method'> N Method Name: test_CoX3DTransform(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: tests/cox3d/test_x3d.py</div><div id='n_file'> N File Name: tests/cox3d/test_x3d.py</div><div id='m_start'> M Start Line: 601</div><div id='m_end'> M End Line: 657</div><div id='n_start'> N Start Line: 598</div><div id='n_end'> N End Line: 672</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    target = trans(example_clip)

    &#47&#47 Recurrent block
    <a id="change">cotrans</a> = CoX3DTransform(
        dim_in=2,
        dim_out=2,
        temp_kernel_size=3,
        stride=2,
        dim_inner=5,
        num_groups=5,
        stride_1x1=False,
        inplace_relu=True,
        eps=1e-5,
        bn_mmt=0.1,
        dilation=1,
        norm_module=torch.nn.BatchNorm3d,
        se_ratio=0.1,
        swish_inner=True,
        block_idx=0,
        temporal_window_size=example_clip.shape[2],
        temporal_fill="zeros",
    )
    cotrans.load_state_dict(trans.state_dict())
    cotrans.eval()  &#47&#47 This has a major effect on BatchNorm result

    &#47&#47 Forward 3D works like the original
    output3d = cotrans.forward_regular(example_clip)
    assert torch.allclose(target, output3d)

    o = []
    &#47&#47 Manual zero pad (due to padding=1 in Conv3d)
    zeros = torch.zeros_like(example_clip[:, :, 0])
    o.append(<a id="change">cotrans.forward(</a>zeros<a id="change">)</a>)
    for i in range(example_clip.shape[2]):
        o.append(<a id="change">cotrans.forward(example_clip[:, :, i]</a><a id="change">)</a>)
    o.append(cotrans.forward(zeros))

    &#47&#47 For debugging:</code></pre><h3>After Change</h3><pre><code class='java'>
    target = trans.forward(example_clip)

    &#47&#47 Recurrent block
    <a id="change">cotrans</a> = CoX3DTransform(
        dim_in=2,
        dim_out=2,
        temp_kernel_size=3,
        stride=2,
        dim_inner=5,
        num_groups=5,
        stride_1x1=False,
        inplace_relu=True,
        eps=1e-5,
        bn_mmt=0.1,
        dilation=1,
        norm_module=torch.nn.BatchNorm3d,
        se_ratio=0.1,
        swish_inner=True,
        block_idx=0,
        temporal_window_size=example_clip.shape[2],  &#47&#47 +2 from padding
        temporal_fill="zeros",
        se_scope="clip",
    )
    co.load_state_dict(cotrans, trans.state_dict(), flatten=True)
    cotrans.eval()  &#47&#47 This has a major effect on BatchNorm result

    &#47&#47 forward
    output = cotrans.forward(example_clip)
    assert torch.allclose(target, output)

    &#47&#47 forward_steps
    output = cotrans.forward_steps(example_clip, pad_end=True)
    assert torch.allclose(target, output)

    &#47&#47 forward_steps - broken up
    cotrans.clean_state()
    nothing = cotrans.forward_steps(example_clip[:, :, :-1], pad_end=False)  &#47&#47 init
    assert isinstance(nothing, TensorPlaceholder)

    lasts = cotrans.forward_steps(example_clip[:, :, -1:], pad_end=True)
    assert torch.allclose(lasts, target)

    &#47&#47 forward_step
    <a id="change">cotrans.clean_state()</a>
    nothing = cotrans.forward_steps(example_clip[:, :, :], pad_end=False)  &#47&#47 init
    assert isinstance(nothing, TensorPlaceholder)

    &#47&#47 Manual pad end.
    zeros = torch.zeros_like(example_clip[:, :, 0])
    step<a id="change"> = </a><a id="change">cotrans.forward_step(</a>zeros<a id="change">)</a>
    assert torch.allclose(step, target[:, :, 0])  &#47&#47 (4/4) correct in SE pool
    step<a id="change"> = </a><a id="change">cotrans.forward_step(</a>zeros<a id="change">)</a>
    assert torch.allclose(step, target[:, :, 1], atol=5e-3)  &#47&#47 (3/4) correct in pool
    step = <a id="change">cotrans.forward_step(</a>zeros<a id="change">)</a>
    assert torch.allclose(step, target[:, :, 2], atol=5e-3)  &#47&#47 (2/4) correct in pool
    step = cotrans.forward_step(zeros)
    assert torch.allclose(step, target[:, :, 3], atol=5e-3)  &#47&#47 (1/4) correct in pool
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lukashedegaard/co3d/commit/85d83724404879a086f4b9ef26aee5f4398830ac#diff-81ebfd3b7da0e2057c99c1622bd2ed14cb38bdb040741812bc0eb3de8e60005eL581' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 97062603</div><div id='project'> Project Name: lukashedegaard/co3d</div><div id='commit'> Commit Name: 85d83724404879a086f4b9ef26aee5f4398830ac</div><div id='time'> Time: 2021-08-25</div><div id='author'> Author: lh@eng.au.dk</div><div id='file'> File Name: tests/cox3d/test_x3d.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: test_CoX3DTransform(0)</div><div id='n_method'> N Method Name: test_CoX3DTransform(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: tests/cox3d/test_x3d.py</div><div id='n_file'> N File Name: tests/cox3d/test_x3d.py</div><div id='m_start'> M Start Line: 601</div><div id='m_end'> M End Line: 657</div><div id='n_start'> N Start Line: 598</div><div id='n_end'> N End Line: 672</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    )

    &#47&#47 Continual model
    <a id="change">comodel</a> = CoX3D(
        dim_in=3,
        image_size=160,
        frames_per_clip=frames_per_clip,
        num_classes=400,
        x3d_conv1_dim=12,
        x3d_conv5_dim=2048,
        x3d_num_groups=1,
        x3d_width_per_group=64,
        x3d_width_factor=2.0,
        x3d_depth_factor=2.2,
        x3d_bottleneck_factor=2.25,
        x3d_use_channelwise_3x3x3=1,
        x3d_dropout_rate=0.5,
        x3d_head_activation="softmax",
        x3d_head_batchnorm=0,
        x3d_fc_std_init=0.01,
        x3d_final_batchnorm_zero_init=1,
        temporal_fill="zeros",
        se_scope="clip",
    )

    &#47&#47 Load weights
    model_state = torch.load(weights_path, map_location="cpu")["model_state"]
    model.load_state_dict(model_state)
    comodel.load_state_dict(model_state)

    &#47&#47 Training mode has large effect on BatchNorm result - test will fail otherwise
    model.eval()
    comodel.eval()

    &#47&#47 Forward
    target = model(input)
    target_top10 = torch.topk(target, k=10)[1][0].tolist()

    &#47&#47 forward_steps produces same outputs
    output = comodel.forward_steps(input)
    assert torch.allclose(target, output, atol=5e-3)

    &#47&#47 Continuation is not exact due to zero paddings (even for boring video)
    output_next_frame = <a id="change">comodel.forward(</a>input[:, :, -1]<a id="change">)</a>

    &#47&#47 assert torch.allclose(target, output_next_frame, atol=5e-4)

    next_frame_top10 = torch.topk(output_next_frame, k=10)[1][0].tolist()
    &#47&#47 Top 2 is the same
    assert len(set(target_top10[:2]) - set(next_frame_top10[:2])) == 0
    &#47&#47 Top 4 is overlapping
    assert len(set(target_top10[:4]) - set(next_frame_top10[:4])) == 0
    &#47&#47 Top 10 is mostly overlapping
    assert len(set(target_top10) - set(next_frame_top10)) == 1

    &#47&#47 Check output if we continue for a long time
    for _ in range(20):
        output_next_frame = <a id="change">comodel.forward(input[:, :, -1]</a><a id="change">)</a>

    &#47&#47 They are further apart now
    assert torch.allclose(target, output_next_frame, atol=5e-2)
    _, next_frame_top10 = torch.topk(output_next_frame, k=10)</code></pre><h3>After Change</h3><pre><code class='java'>
    )

    &#47&#47 Continual model
    <a id="change">comodel</a> = CoX3D(
        dim_in=3,
        image_size=160,
        frames_per_clip=frames_per_clip,
        num_classes=400,
        x3d_conv1_dim=12,
        x3d_conv5_dim=2048,
        x3d_num_groups=1,
        x3d_width_per_group=64,
        x3d_width_factor=2.0,
        x3d_depth_factor=2.2,
        x3d_bottleneck_factor=2.25,
        x3d_use_channelwise_3x3x3=1,
        x3d_dropout_rate=0.5,
        x3d_head_activation="softmax",
        x3d_head_batchnorm=0,
        x3d_fc_std_init=0.01,
        x3d_final_batchnorm_zero_init=1,
        temporal_fill="zeros",
        se_scope="clip",
    )
    assert comodel.delay == 220

    &#47&#47 Load weights
    model_state = torch.load(weights_path, map_location="cpu")["model_state"]
    model.load_state_dict(model_state)
    comodel.load_state_dict(model_state, flatten=True)

    &#47&#47 Training mode has large effect on BatchNorm result - test will fail otherwise
    model.eval()
    comodel.eval()

    target = model.forward(sample)

    &#47&#47 forward
    output = comodel.forward(sample)
    assert torch.allclose(target, output)

    &#47&#47 forward_steps
    output = comodel.forward_steps(sample, pad_end=True)
    assert torch.allclose(target, output)

    &#47&#47 forward
    <a id="change">comodel.clean_state()</a>
    &#47&#47 init
    for i in range(frames_per_clip):
        comodel.forward_step(sample[:, :, i])

    &#47&#47 zero-pad end manually
    zeros = torch.zeros_like(sample[:, :, 0])
    for _ in range(comodel.delay - frames_per_clip):
        <a id="change">comodel.forward_step(</a>zeros<a id="change">)</a>

    &#47&#47 final result
    output<a id="change"> = </a><a id="change">comodel.forward_step(</a>zeros<a id="change">)</a>
    torch.allclose(output, target, atol=5e-3)

    target_top10 = torch.topk(target, k=10)[1][0].tolist()
    output_top10 = torch.topk(output, k=10)[1][0].tolist()

    assert len(set(target_top10) - set(output_top10)) &lt;= 3

    &#47&#47 Another step - now out of comparable operation
    output<a id="change"> = </a><a id="change">comodel.forward_step(</a>zeros<a id="change">)</a>
    output_top10 = torch.topk(output, k=10)[1][0].tolist()
    assert len(set(target_top10) - set(output_top10)) &lt;= 3

</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lukashedegaard/co3d/commit/0b7612c3dc839d75811d3b7ccb59a7a0b453f051#diff-81ebfd3b7da0e2057c99c1622bd2ed14cb38bdb040741812bc0eb3de8e60005eL598' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 97062607</div><div id='project'> Project Name: lukashedegaard/co3d</div><div id='commit'> Commit Name: 0b7612c3dc839d75811d3b7ccb59a7a0b453f051</div><div id='time'> Time: 2021-08-31</div><div id='author'> Author: lh@eng.au.dk</div><div id='file'> File Name: tests/cox3d/test_x3d.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: test_CoX3D(0)</div><div id='n_method'> N Method Name: test_CoX3D(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: tests/cox3d/test_x3d.py</div><div id='n_file'> N File Name: tests/cox3d/test_x3d.py</div><div id='m_start'> M Start Line: 601</div><div id='m_end'> M End Line: 687</div><div id='n_start'> N Start Line: 599</div><div id='n_end'> N End Line: 688</div><BR>