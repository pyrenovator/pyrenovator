<html><h3>Pattern ID :12866
</h3><img src='43532936.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            nn.ReLU()
        ])

        <a id="change">for </a>l in <a id="change">range(</a>num_layers_linear_hidden - 1<a id="change">)</a><a id="change">:
            </a>self.operators.append(nn.Linear(hidden_dim, hidden_dim))
            self.operators.append(nn.ReLU())
            &#47&#47 self.operators.append(nn.Dropout(0.2))
</code></pre><h3>After Change</h3><pre><code class='java'>
            if layer == &quotlinear&quot:
                self.operators.append(nn.Linear(prev_object[1], argument))
                prev_object = (layer, argument)
            elif <a id="change">layer == &quotrelu&quot</a>:
                assert argument is None, &quotNo argument for ReLU please&quot
                self.operators.append(nn.ReLU())
            elif <a id="change">layer == &quotdropout&quot</a>:
                self.operators.append(nn.Dropout(argument))
            else:
                <a id="change">raise </a><a id="change">NotImplementedError(f&quot{layer} not known&quot</a><a id="change">)</a>

        self.operators.append(nn.Linear(prev_object[1], 2 * action_dim))

        self.operators.apply(init_xavier_uniform)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/tmdt-buw/karolos/commit/bda97ff0aa40ddff62e42733856c4c66cc37b8b6#diff-29037086e0dfbb6a2262a556dce65661316ec86ddf4f52bf9de230586208ab9dL65' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43532936</div><div id='project'> Project Name: tmdt-buw/karolos</div><div id='commit'> Commit Name: bda97ff0aa40ddff62e42733856c4c66cc37b8b6</div><div id='time'> Time: 2020-05-13</div><div id='author'> Author: timo.thun@ima-ifu.rwth-aachen.de</div><div id='file'> File Name: agents/nnfactory/sac.py</div><div id='m_class'> M Class Name: Policy</div><div id='n_method'> N Class Name: Policy</div><div id='m_method'> M Method Name: __init__(6)</div><div id='n_method'> N Method Name: __init__(7)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: agents/nnfactory/sac.py</div><div id='n_file'> N File Name: agents/nnfactory/sac.py</div><div id='m_start'> M Start Line: 65</div><div id='m_end'> M End Line: 87</div><div id='n_start'> N Start Line: 88</div><div id='n_end'> N End Line: 108</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
def mask_image(x, config):
    height, width, _ = config[&quotimage_shape&quot]
    mask_all = []
    <a id="change">for </a>i in <a id="change">range(</a>x.size(0)<a id="change">)</a><a id="change">:
        </a>mask = Masks.get_ff_mask(height, width)
        mask_all.append(mask)
    mask = torch.from_numpy(np.asarray(mask_all)).unsqueeze(1).float()
    ones = torch.ones(x.size(0), 1, x.size(2), x.size(3))</code></pre><h3>After Change</h3><pre><code class='java'>
    if x.is_cuda:
        mask = mask.cuda()

    <a id="change">if config[&quotmask_type&quot] == &quothole&quot</a>:
        result = x * (1. - mask)
    elif <a id="change">config[&quotmask_type&quot] == &quotmosaic&quot</a>:
        &#47&#47 TODO: Matching the mosaic patch size and the mask size
        mosaic_unit_size = config[&quotmosaic_unit_size&quot]
        downsampled_image = F.interpolate(x, scale_factor=1. / mosaic_unit_size, mode=&quotnearest&quot)
        upsampled_image = F.interpolate(downsampled_image, size=(height, width), mode=&quotnearest&quot)
        result = upsampled_image * mask + x * (1. - mask)
    else:
        <a id="change">raise </a><a id="change">NotImplementedError(&quotNot implemented mask type.&quot</a><a id="change">)</a>

    return result, mask</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sayednadim/global-and-local-attention-based-free-form-image-inpainting/commit/aaa17ed332dc95db0f5900a43be179e26569b50c#diff-4a9d05cc3eadd7aa0b347854fe63c55a626198aae56f6d3482457196186e5d60L38' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43532938</div><div id='project'> Project Name: sayednadim/global-and-local-attention-based-free-form-image-inpainting</div><div id='commit'> Commit Name: aaa17ed332dc95db0f5900a43be179e26569b50c</div><div id='time'> Time: 2020-08-16</div><div id='author'> Author: smnadimuddin@gmail.com</div><div id='file'> File Name: model/mask.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: mask_image(2)</div><div id='n_method'> N Method Name: mask_image(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: model/mask.py</div><div id='n_file'> N File Name: model/mask.py</div><div id='m_start'> M Start Line: 40</div><div id='m_end'> M End Line: 49</div><div id='n_start'> N Start Line: 56</div><div id='n_end'> N End Line: 72</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            nn.ReLU()
        ])

        <a id="change">for </a>l in <a id="change">range(</a>num_layers_linear_hidden - 1<a id="change">)</a><a id="change">:
            </a>self.operators.append(nn.Linear(hidden_dim, hidden_dim))
            self.operators.append(nn.ReLU())

        self.operators.append(nn.Linear(hidden_dim, 1))</code></pre><h3>After Change</h3><pre><code class='java'>
            if layer == &quotlinear&quot:
                self.operators.append(nn.Linear(prev_object[1], argument))
                prev_object = (layer, argument)
            elif <a id="change">layer == &quotrelu&quot</a>:
                assert argument is None, &quotNo argument for ReLU please&quot
                self.operators.append(nn.ReLU())
            elif <a id="change">layer == &quotdropout&quot</a>:
                self.operators.append(nn.Dropout(argument))
            else:
                <a id="change">raise </a><a id="change">NotImplementedError(f&quot{layer} not known&quot</a><a id="change">)</a>

        self.operators.append(nn.Linear(prev_object[1], 1))

        self.operators.apply(init_xavier_uniform)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tmdt-buw/karolos/commit/bda97ff0aa40ddff62e42733856c4c66cc37b8b6#diff-29037086e0dfbb6a2262a556dce65661316ec86ddf4f52bf9de230586208ab9dL34' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43532939</div><div id='project'> Project Name: tmdt-buw/karolos</div><div id='commit'> Commit Name: bda97ff0aa40ddff62e42733856c4c66cc37b8b6</div><div id='time'> Time: 2020-05-13</div><div id='author'> Author: timo.thun@ima-ifu.rwth-aachen.de</div><div id='file'> File Name: agents/nnfactory/sac.py</div><div id='m_class'> M Class Name: Critic</div><div id='n_method'> N Class Name: Critic</div><div id='m_method'> M Method Name: __init__(4)</div><div id='n_method'> N Method Name: __init__(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: agents/nnfactory/sac.py</div><div id='n_file'> N File Name: agents/nnfactory/sac.py</div><div id='m_start'> M Start Line: 34</div><div id='m_end'> M End Line: 53</div><div id='n_start'> N Start Line: 45</div><div id='n_end'> N End Line: 65</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    height, width, _ = config[&quotimage_shape&quot]
    mask = torch.ones(x.size(0), 1, x.size(2), x.size(3))
    temp = torch.ones(x.size(0), 1, x.size(2), x.size(3))
    <a id="change">for </a>i in <a id="change">range(</a>x.size(0)<a id="change">)</a><a id="change">:
        </a>mask_temp = Masks.get_ff_mask(height, width)
        mask_temp = torch.from_numpy(mask_temp)
        mask[i,:,:,:] = temp[i,:,:,:] * mask_temp
    if x.is_cuda:</code></pre><h3>After Change</h3><pre><code class='java'>
    if x.is_cuda:
        mask = mask.cuda()

    <a id="change">if config[&quotmask_type&quot] == &quothole&quot</a>:
        result = x * (1. - mask)
    elif <a id="change">config[&quotmask_type&quot] == &quotmosaic&quot</a>:
        &#47&#47 TODO: Matching the mosaic patch size and the mask size
        mosaic_unit_size = config[&quotmosaic_unit_size&quot]
        downsampled_image = F.interpolate(x, scale_factor=1. / mosaic_unit_size, mode=&quotnearest&quot)
        upsampled_image = F.interpolate(downsampled_image, size=(height, width), mode=&quotnearest&quot)
        result = upsampled_image * mask + x * (1. - mask)
    else:
        <a id="change">raise </a><a id="change">NotImplementedError(&quotNot implemented mask type.&quot</a><a id="change">)</a>

    return result, mask</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/sayednadim/global-and-local-attention-based-free-form-image-inpainting/commit/2e453ae0b658395a88acb8db67115db86d9274ea#diff-4a9d05cc3eadd7aa0b347854fe63c55a626198aae56f6d3482457196186e5d60L38' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 43532941</div><div id='project'> Project Name: sayednadim/global-and-local-attention-based-free-form-image-inpainting</div><div id='commit'> Commit Name: 2e453ae0b658395a88acb8db67115db86d9274ea</div><div id='time'> Time: 2020-08-16</div><div id='author'> Author: smnadimuddin@gmail.com</div><div id='file'> File Name: model/mask.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: mask_image(2)</div><div id='n_method'> N Method Name: mask_image(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: model/mask.py</div><div id='n_file'> N File Name: model/mask.py</div><div id='m_start'> M Start Line: 40</div><div id='m_end'> M End Line: 48</div><div id='n_start'> N Start Line: 56</div><div id='n_end'> N End Line: 72</div><BR>