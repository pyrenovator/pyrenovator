<html><h3>Pattern ID :26363
</h3><img src='79167031.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        quantizer_channels = extract_channels[-1]
        post_quantizer_channels = context_channels[-1]
        extra_args = (
            {"num_residuals": quantizer_num_residuals}<a id="change">
            if </a>quantizer_type == "rvq"<a id="change">
            else </a>{}
        )

        self.quantizer = Quantizer1d(</code></pre><h3>After Change</h3><pre><code class='java'>
        quantizer_channels = extract_channels[-1]
        post_quantizer_channels = context_channels[-1]

        <a id="change">if self.quantizer_type == "timewise"</a>:
            self.quantizer = Quantizer1d(
                channels=quantizer_channels,
                num_groups=quantizer_groups,
                codebook_size=codebook_size,
                expire_threshold=quantizer_expire_threshold,
                num_residuals=quantizer_num_residuals,
            )
        elif <a id="change">self.quantizer_type == "channelwise"</a>:
            assert_message<a id="change"> = </a>"quantizer_split_size required with channelwise type"
            assert quantizer_split_size is not None, assert_message
            self.quantizer<a id="change"> = </a>QuantizerChannelwise1d(
                channels=quantizer_channels,
                split_size=quantizer_split_size,
                num_groups=quantizer_groups,
                codebook_size=codebook_size,
                expire_threshold=quantizer_expire_threshold,
                num_residuals=quantizer_num_residuals,
            )
        else:
            <a id="change">raise ValueError("Quantizer type must be timewise or channelwise"</a><a id="change">)</a>

        self.post_quantizer = nn.Sequential(
            ResnetBlock1d(
                in_channels=quantizer_channels,</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 9</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/archinetai/audio-diffusion-pytorch-trainer/commit/d95282a5569b4aecd3bd9a11e751ecf659093bde#diff-c571f52981352b8ae331df88d5933bcdf64567980f4af6de2a94a37dbee42040L58' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 79167031</div><div id='project'> Project Name: archinetai/audio-diffusion-pytorch-trainer</div><div id='commit'> Commit Name: d95282a5569b4aecd3bd9a11e751ecf659093bde</div><div id='time'> Time: 2022-09-09</div><div id='author'> Author: flavio.schneider.97@gmail.com</div><div id='file'> File Name: main/module_diffqe.py</div><div id='m_class'> M Class Name: Model</div><div id='n_method'> N Class Name: Model</div><div id='m_method'> M Method Name: __init__(38)</div><div id='n_method'> N Method Name: __init__(38)</div><div id='m_parent_class'> M Parent Class: pl.LightningModule</div><div id='n_parent_class'> N Parent Class: pl.LightningModule</div><div id='m_file'> M File Name: main/module_diffqe.py</div><div id='n_file'> N File Name: main/module_diffqe.py</div><div id='m_start'> M Start Line: 58</div><div id='m_end'> M End Line: 107</div><div id='n_start'> N Start Line: 63</div><div id='n_end'> N End Line: 116</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        sparse_layer = cast_tuple(sparse_attn, depth)

        for _, sparse_attn in zip(range(depth), sparse_layer):
            attn_class = Attention<a id="change"> if </a>not sparse_attn<a id="change"> else </a>partial(SparseAttention, sparse_attn_global_indices = sparse_attn_global_indices)

            layers.append(nn.ModuleList([
                PreNorm(dim, attn_class(dim, causal = causal, seq_len = seq_len, heads = heads, dim_head = dim_head, dropout = attn_dropout, noncausal_attn_len = noncausal_attn_len)),</code></pre><h3>After Change</h3><pre><code class='java'>
        attn_type_layer = islice(cycle(attn_types), depth)

        for _, sparse_attn, attn_type in zip(range(depth), sparse_layer, attn_type_layer):
            <a id="change">if attn_type == &quotfull&quot</a>:
                attn_class = partial(Attention, noncausal_attn_len = noncausal_attn_len)
            elif <a id="change">attn_type == &quotsparse&quot</a>:
                attn_class<a id="change"> = </a>partial(SparseAttention, sparse_attn_global_indices = sparse_attn_global_indices)
            elif attn_type == &quotaxial_row&quot:
                attn_class = partial(SparseAxialCausalAttention, seq_len = seq_len, axis = 0, image_size = image_fmap_size)
            elif attn_type == &quotaxial_col&quot:
                attn_class = partial(SparseAxialCausalAttention, seq_len = seq_len, axis = 1, image_size = image_fmap_size)
            elif attn_type == &quotconv_like&quot:
                attn_class<a id="change"> = </a>partial(SparseConvCausalAttention, seq_len = seq_len, image_size = image_fmap_size)
            else:
                <a id="change">raise ValueError(f&quotattention type "{attn_type}" is not valid&quot</a><a id="change">)</a>

            layers.append(nn.ModuleList([
                PreNorm(dim, attn_class(dim, causal = causal, seq_len = seq_len, heads = heads, dim_head = dim_head, dropout = attn_dropout)),
                PreNorm(dim, FeedForward(dim, mult = ff_mult, dropout = ff_dropout))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/dalle-pytorch/commit/de732e8756750e161f0e51fac8baf9bcdb13182e#diff-ee397916e2ccc5603bb55b6ea44f20d581aae722f1e26d17108234ff0411b1beL46' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 79167030</div><div id='project'> Project Name: lucidrains/dalle-pytorch</div><div id='commit'> Commit Name: de732e8756750e161f0e51fac8baf9bcdb13182e</div><div id='time'> Time: 2021-02-10</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: dalle_pytorch/transformer.py</div><div id='m_class'> M Class Name: Transformer</div><div id='n_method'> N Class Name: Transformer</div><div id='m_method'> M Method Name: __init__(1)</div><div id='n_method'> N Method Name: __init__(1)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: dalle_pytorch/transformer.py</div><div id='n_file'> N File Name: dalle_pytorch/transformer.py</div><div id='m_start'> M Start Line: 67</div><div id='m_end'> M End Line: 68</div><div id='n_start'> N Start Line: 75</div><div id='n_end'> N End Line: 92</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 build first step conv 1x1.
        self.conv_list.append(ConvBNReLU(in_channels, out_channels // 2, kernel_size=1, bias=False))
        &#47&#47 avg pool in skip if stride = 2.
        self.skip_step1 = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)<a id="change"> if </a>stride == 2<a id="change"> else </a>nn.Identity()

        in_channels = out_channels // 2
        mid_channels = in_channels</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 build first step conv 1x1.
        self.conv_list.append(ConvBNReLU(in_channels, out_channels // 2, kernel_size=1, bias=False))
        &#47&#47 build skip connection after first convolution.
        <a id="change">if stride == 1</a>:
            self.skip_step1 = nn.Identity()
        elif <a id="change">stdc_downsample_mode == "avg_pool"</a>:
            self.skip_step1<a id="change"> = </a>nn.AvgPool2d(kernel_size=3, stride=2, padding=1)
        elif stdc_downsample_mode == "dw_conv":
            self.skip_step1<a id="change"> = </a>ConvBNReLU(
                out_channels // 2, out_channels // 2, kernel_size=3, stride=2, padding=1, bias=False, groups=out_channels // 2, use_activation=False
            )
        else:
            <a id="change">raise ValueError(f"stdc_downsample mode is not supported: found {stdc_downsample_mode}," f" must be in [avg_pool, dw_conv]"</a><a id="change">)</a>

        in_channels = out_channels // 2
        mid_channels = in_channels
        &#47&#47 build rest conv3x3 layers.</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/deci-ai/super-gradients/commit/98083d4303f887cb29f39029871612fe5e2e4753#diff-8f12293ee8d59da4f2d8522208a7f27d9b4f90502789d26e213681b949e5a90cL33' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 79167026</div><div id='project'> Project Name: deci-ai/super-gradients</div><div id='commit'> Commit Name: 98083d4303f887cb29f39029871612fe5e2e4753</div><div id='time'> Time: 2022-11-02</div><div id='author'> Author: 88616312+lkdci@users.noreply.github.com</div><div id='file'> File Name: src/super_gradients/training/models/segmentation_models/stdc.py</div><div id='m_class'> M Class Name: STDCBlock</div><div id='n_method'> N Class Name: STDCBlock</div><div id='m_method'> M Method Name: __init__(6)</div><div id='n_method'> N Method Name: __init__(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: src/super_gradients/training/models/segmentation_models/stdc.py</div><div id='n_file'> N File Name: src/super_gradients/training/models/segmentation_models/stdc.py</div><div id='m_start'> M Start Line: 34</div><div id='m_end'> M End Line: 43</div><div id='n_start'> N Start Line: 29</div><div id='n_end'> N End Line: 53</div><BR>