<html><h3>Pattern ID :27952
</h3><img src='82883618.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            self.doc_model = SentenceTransformer(model_path[1])
    
    def start_multi_process_pool(self, target_devices: List[str] = None) -&gt; Dict[str, object]:
        <a id="change">return </a>self.doc_model.start_multi_process_pool(target_devices=target_devices)

    def stop_multi_process_pool(self, pool: Dict[str, object]):
        output_queue = pool[&quotoutput&quot]</code></pre><h3>After Change</h3><pre><code class='java'>
    def start_multi_process_pool(self, target_devices: List[str] = None) -&gt; Dict[str, object]:
        logger.info("Start multi-process pool on devices: {}".format(&quot, &quot.join(map(str, target_devices))))

        ctx = <a id="change">mp.get_context(&quotspawn&quot</a><a id="change">)</a>
        input_queue = ctx.Queue()
        output_queue = ctx.Queue()
        processes = <a id="change">[]</a>

        <a id="change">for </a>process_id, <a id="change">device_name</a> in enumerate(target_devices)<a id="change">:
            p</a><a id="change"> = ctx.Process(target=SentenceTransformer._encode_multi_process_worker, args=(process_id, device_name, self.doc_model, input_queue, output_queue), daemon=True)</a>
            <a id="change">p.start()</a>
            <a id="change">processes.append(p</a><a id="change">)</a>

        return {&quotinput&quot: input_queue, &quotoutput&quot: output_queue, &quotprocesses&quot: processes}

    def stop_multi_process_pool(self, pool: Dict[str, object]):</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/ukplab/beir/commit/d858a38384efabf6ef5992cfe454745d884c1055#diff-d40a036e97d6861d35db98a125c891e5877649cd49c45e40eb591ce9dd188e77L25' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 82883618</div><div id='project'> Project Name: ukplab/beir</div><div id='commit'> Commit Name: d858a38384efabf6ef5992cfe454745d884c1055</div><div id='time'> Time: 2022-06-15</div><div id='author'> Author: nouamane98@gmail.com</div><div id='file'> File Name: beir/retrieval/models/sentence_bert.py</div><div id='m_class'> M Class Name: SentenceBERT</div><div id='n_method'> N Class Name: SentenceBERT</div><div id='m_method'> M Method Name: start_multi_process_pool(2)</div><div id='n_method'> N Method Name: start_multi_process_pool(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: beir/retrieval/models/sentence_bert.py</div><div id='n_file'> N File Name: beir/retrieval/models/sentence_bert.py</div><div id='m_start'> M Start Line: 25</div><div id='m_end'> M End Line: 25</div><div id='n_start'> N Start Line: 26</div><div id='n_end'> N End Line: 38</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def test_algorithm(self):
        if not torch.cuda.is_available():
            print("skip tests since cuda is not available")
            <a id="change">return</a>

        nprocs = torch.cuda.device_count()
        os.environ["WORLD_SIZE"] = str(nprocs)
        os.environ["LOCAL_WORLD_SIZE"] = str(nprocs)</code></pre><h3>After Change</h3><pre><code class='java'>
            "BAGUA_SERVICE_PORT": str(find_free_port(9000, 9100)),
        }

        mp = <a id="change">multiprocessing.get_context("spawn"</a><a id="change">)</a>
        processes = <a id="change">[]</a>
        <a id="change">for </a><a id="change">i</a> in range(nprocs)<a id="change">:
            p</a><a id="change"> = mp.Process(target=run_model, args=(i, env))</a>
            <a id="change">p.start()</a>
            <a id="change">processes.append(</a>p<a id="change">)</a>

        for p in processes:
            p.join(timeout=60)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/baguasys/bagua/commit/a9529bef66e367884316a2b2ebc917ff35bf6334#diff-c46b7f4c320ba27d2783f4d47a513380419a948da235a60e6b47ccf8be3f194bL61' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 82883619</div><div id='project'> Project Name: baguasys/bagua</div><div id='commit'> Commit Name: a9529bef66e367884316a2b2ebc917ff35bf6334</div><div id='time'> Time: 2021-08-26</div><div id='author'> Author: 45031995+wangraying@users.noreply.github.com</div><div id='file'> File Name: tests/torch_api/test_async_model_average.py</div><div id='m_class'> M Class Name: TestAsyncModelAverage</div><div id='n_method'> N Class Name: TestAsyncModelAverage</div><div id='m_method'> M Method Name: test_algorithm(1)</div><div id='n_method'> N Method Name: test_algorithm(1)</div><div id='m_parent_class'> M Parent Class: unittest.TestCase</div><div id='n_parent_class'> N Parent Class: unittest.TestCase</div><div id='m_file'> M File Name: tests/torch_api/test_async_model_average.py</div><div id='n_file'> N File Name: tests/torch_api/test_async_model_average.py</div><div id='m_start'> M Start Line: 62</div><div id='m_end'> M End Line: 76</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 87</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            self.doc_model = SentenceTransformer(model_path[1])
    
    def start_multi_process_pool(self, target_devices: List[str] = None) -&gt; Dict[str, object]:
        <a id="change">return </a>self.doc_model.start_multi_process_pool(target_devices=target_devices)

    def stop_multi_process_pool(self, pool: Dict[str, object]):
        output_queue = pool[&quotoutput&quot]</code></pre><h3>After Change</h3><pre><code class='java'>
    def start_multi_process_pool(self, target_devices: List[str] = None) -&gt; Dict[str, object]:
        logger.info("Start multi-process pool on devices: {}".format(&quot, &quot.join(map(str, target_devices))))

        ctx = <a id="change">mp.get_context(&quotspawn&quot</a><a id="change">)</a>
        input_queue = ctx.Queue()
        output_queue = ctx.Queue()
        processes = <a id="change">[]</a>

        <a id="change">for </a>process_id, <a id="change">device_name</a> in enumerate(target_devices)<a id="change">:
            p</a><a id="change"> = ctx.Process(target=SentenceTransformer._encode_multi_process_worker, args=(process_id, device_name, self.doc_model, input_queue, output_queue), daemon=True)</a>
            <a id="change">p.start()</a>
            <a id="change">processes.append(</a>p<a id="change">)</a>

        return {&quotinput&quot: input_queue, &quotoutput&quot: output_queue, &quotprocesses&quot: processes}

    def stop_multi_process_pool(self, pool: Dict[str, object]):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/beir-cellar/beir/commit/d858a38384efabf6ef5992cfe454745d884c1055#diff-d40a036e97d6861d35db98a125c891e5877649cd49c45e40eb591ce9dd188e77L24' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 82883620</div><div id='project'> Project Name: beir-cellar/beir</div><div id='commit'> Commit Name: d858a38384efabf6ef5992cfe454745d884c1055</div><div id='time'> Time: 2022-06-15</div><div id='author'> Author: nouamane98@gmail.com</div><div id='file'> File Name: beir/retrieval/models/sentence_bert.py</div><div id='m_class'> M Class Name: SentenceBERT</div><div id='n_method'> N Class Name: SentenceBERT</div><div id='m_method'> M Method Name: start_multi_process_pool(2)</div><div id='n_method'> N Method Name: start_multi_process_pool(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: beir/retrieval/models/sentence_bert.py</div><div id='n_file'> N File Name: beir/retrieval/models/sentence_bert.py</div><div id='m_start'> M Start Line: 25</div><div id='m_end'> M End Line: 25</div><div id='n_start'> N Start Line: 26</div><div id='n_end'> N End Line: 38</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
def run_test_locally(fn):
    if not torch.cuda.is_available():
        print("skip tests since cuda is not available")
        <a id="change">return </a>[]

    nprocs = torch.cuda.device_count()
    os.environ["WORLD_SIZE"] = str(nprocs)</code></pre><h3>After Change</h3><pre><code class='java'>
        "BAGUA_SERVICE_PORT": str(find_free_port(9000, 9100)),
    }

    mp = <a id="change">multiprocessing.get_context("spawn"</a><a id="change">)</a>
    results = [Result() for _ in range(nprocs)]
    processes = <a id="change">[]</a>
    <a id="change">for </a><a id="change">i</a> in range(nprocs)<a id="change">:
        p</a><a id="change"> = mp.Process(
            target=fn,
            args=(i, nprocs, results, env),
        )</a>
        <a id="change">p.start()</a>
        <a id="change">processes.append(</a>p<a id="change">)</a>

    for p in processes:
        p.join(timeout=60)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/baguasys/bagua/commit/a9529bef66e367884316a2b2ebc917ff35bf6334#diff-7a16c2809e9e060c1dc2b8f83387654f4a97459627405dbf5948e60b05bcea7dL105' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 82883621</div><div id='project'> Project Name: baguasys/bagua</div><div id='commit'> Commit Name: a9529bef66e367884316a2b2ebc917ff35bf6334</div><div id='time'> Time: 2021-08-26</div><div id='author'> Author: 45031995+wangraying@users.noreply.github.com</div><div id='file'> File Name: tests/comm/test_communicator.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: run_test_locally(1)</div><div id='n_method'> N Method Name: run_test_locally(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: tests/comm/test_communicator.py</div><div id='n_file'> N File Name: tests/comm/test_communicator.py</div><div id='m_start'> M Start Line: 106</div><div id='m_end'> M End Line: 122</div><div id='n_start'> N Start Line: 115</div><div id='n_end'> N End Line: 138</div><BR>