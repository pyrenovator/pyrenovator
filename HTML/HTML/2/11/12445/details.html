<html><h3>Pattern ID :12445
</h3><img src='42370183.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    normalization = ctx.dims.sizes.batch / tgt.size
    tgt = one_hot(tgt.astype(src.dtype), src.shape[-1])
    shifted = src<a id="change"> - </a>lax.stop_gradient(src).max(-1, keepdims=True)
    exp_shifted = <a id="change">jnp.exp(</a>shifted<a id="change">)</a>
    sum_exp<a id="change"> = </a>exp_shifted.sum(-1, keepdims=True)
    out = ((<a id="change">jnp.log(</a>sum_exp<a id="change">) - </a>shifted) * tgt).sum(tuple(range(1, tgt.ndim)))
    return lax.pmean(out * normalization, ParallelAxes.model)

</code></pre><h3>After Change</h3><pre><code class='java'>
    src = lax.psum(src, ParallelAxes.model)

    max_logit = lax.stop_gradient(src).max(-1, keepdims=True)
    log_z = lax.log(<a id="change">lax.exp(src - max_logit).sum(</a>-1<a id="change">, keepdims=True)</a>)<a id="change"> + </a>max_logit
    loss = (log_z - src) * one_hot(tgt.astype(src.dtype), src.shape[-1])
    loss = loss.sum() / tgt.size
    if ctx.training.z_loss:
        loss<a id="change"> += </a><a id="change">jnp.square(</a>log_z<a id="change">) * </a>(ctx.training.z_loss / tgt.size)
    return loss

</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 10</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/homebrewnlp/homebrewnlp-jax/commit/f0031ea763d3a229cac4e176f5d191444555669e#diff-6159e4e86b1b15ffd250a78dbdf29a41b004b4963154e49f8b96609b8ea977ceL195' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 42370183</div><div id='project'> Project Name: homebrewnlp/homebrewnlp-jax</div><div id='commit'> Commit Name: f0031ea763d3a229cac4e176f5d191444555669e</div><div id='time'> Time: 2022-02-20</div><div id='author'> Author: 39779310+ClashLuke@users.noreply.github.com</div><div id='file'> File Name: src/model.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: cross_entropy_loss(3)</div><div id='n_method'> N Method Name: cross_entropy_loss(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/model.py</div><div id='n_file'> N File Name: src/model.py</div><div id='m_start'> M Start Line: 195</div><div id='m_end'> M End Line: 204</div><div id='n_start'> N Start Line: 195</div><div id='n_end'> N End Line: 203</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    normalization = ctx.dims.sizes.batch / tgt.size
    tgt = one_hot(tgt.astype(src.dtype), src.shape[-1])
    shifted = src<a id="change"> - </a>lax.stop_gradient(src).max(-1, keepdims=True)
    exp_shifted<a id="change"> = jnp.exp(</a>shifted<a id="change">)</a>
    sum_exp = exp_shifted.sum(-1, keepdims=True)
    out = ((<a id="change">jnp.log(</a>sum_exp<a id="change">)</a> - shifted)<a id="change"> * </a>tgt).sum(tuple(range(1, tgt.ndim)))
    return lax.pmean(out * normalization, ParallelAxes.model)

</code></pre><h3>After Change</h3><pre><code class='java'>
    src = lax.psum(src, ParallelAxes.model)

    max_logit = lax.stop_gradient(src).max(-1, keepdims=True)
    log_z = lax.log(<a id="change">lax.exp(src - max_logit).sum(</a>-1<a id="change">, keepdims=True)</a>)<a id="change"> + </a>max_logit
    loss = (log_z - src) * one_hot(tgt.astype(src.dtype), src.shape[-1])
    loss = loss.sum() / tgt.size
    if ctx.training.z_loss:
        loss<a id="change"> += </a><a id="change">jnp.square(</a>log_z<a id="change">) * </a>(ctx.training.z_loss / tgt.size)
    return loss

</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/homebrewnlp/olmax/commit/f0031ea763d3a229cac4e176f5d191444555669e#diff-6159e4e86b1b15ffd250a78dbdf29a41b004b4963154e49f8b96609b8ea977ceL194' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 42370148</div><div id='project'> Project Name: homebrewnlp/olmax</div><div id='commit'> Commit Name: f0031ea763d3a229cac4e176f5d191444555669e</div><div id='time'> Time: 2022-02-20</div><div id='author'> Author: 39779310+ClashLuke@users.noreply.github.com</div><div id='file'> File Name: src/model.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: cross_entropy_loss(3)</div><div id='n_method'> N Method Name: cross_entropy_loss(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/model.py</div><div id='n_file'> N File Name: src/model.py</div><div id='m_start'> M Start Line: 195</div><div id='m_end'> M End Line: 204</div><div id='n_start'> N Start Line: 195</div><div id='n_end'> N End Line: 203</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 Numerical stability mixture and loglik
        log_max = torch.amax(log, dim=2, keepdim=True)  &#47&#47 [1,1,K] (collapsed joints)
        lik<a id="change"> = </a>weights * <a id="change">torch.exp(</a>log<a id="change"> - </a>log_max<a id="change">)</a>  &#47&#47 Take max
        loglik = <a id="change">torch.log(</a>torch.sum(lik, dim=2, keepdim=True)<a id="change">) + </a>log_max  &#47&#47 Return max
        loglik = loglik * mask  &#47&#47 replace with mask

        loss = -torch.mean(loglik)</code></pre><h3>After Change</h3><pre><code class='java'>
        loglik = torch.logsumexp((torch.log(weights) + log_pi), dim=2, keepdim=True)
        loglik = loglik * mask

        mean = <a id="change">torch.sum(</a>weights * lambdas<a id="change">, axis=-1, keepdims=True)</a>
        reglrz<a id="change"> = </a>torch.mean(<a id="change">torch.square(</a>y<a id="change"> - </a>mean<a id="change">) * </a>mask)
        loss = -torch.mean(loglik) + 0.001 * reglrz
        return loss
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/nixtla/neuralforecast/commit/e0011be5ea3be6d2eb3291c8c12b4f130e244002#diff-b802b4aae962f9e4c5e03302b63a22d5ed334b8e60bc54cfdb206ee1ac4d25dfL1389' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 42370210</div><div id='project'> Project Name: nixtla/neuralforecast</div><div id='commit'> Commit Name: e0011be5ea3be6d2eb3291c8c12b4f130e244002</div><div id='time'> Time: 2023-03-30</div><div id='author'> Author: kin.gtz.olivares@gmail.com</div><div id='file'> File Name: neuralforecast/losses/pytorch.py</div><div id='m_class'> M Class Name: PMM</div><div id='n_method'> N Class Name: PMM</div><div id='m_method'> M Method Name: neglog_likelihood(4)</div><div id='n_method'> N Method Name: neglog_likelihood(4)</div><div id='m_parent_class'> M Parent Class: torch.nn.Module</div><div id='n_parent_class'> N Parent Class: torch.nn.Module</div><div id='m_file'> M File Name: neuralforecast/losses/pytorch.py</div><div id='n_file'> N File Name: neuralforecast/losses/pytorch.py</div><div id='m_start'> M Start Line: 1404</div><div id='m_end'> M End Line: 1422</div><div id='n_start'> N Start Line: 1413</div><div id='n_end'> N End Line: 1438</div><BR>