<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if torch_device is None:
            torch_device = "cuda" if torch.cuda.is_available() else "cpu"

        <a id="change">self.unet.to(</a>torch_device<a id="change">)</a>
        self.vqvae.to(torch_device)

        latents = torch.randn(
            (batch_size, self.unet.in_channels, self.unet.sample_size, self.unet.sample_size),</code></pre><h3>After Change</h3><pre><code class='java'>
    def __call__(self, batch_size=1, generator=None, eta=0.0, num_inference_steps=50, output_type="pil", **kwargs):
        &#47&#47 eta corresponds to Î· in paper and should be between [0, 1]

        <a id="change">if "torch_device" in kwargs</a>:
            device<a id="change"> = kwargs</a><a id="change">.pop("torch_device"</a><a id="change">)</a>
            <a id="change">warnings.warn(
                "`torch_device` is deprecated as an input argument to `__call__` and will be removed in v0.3.0."
                " Consider using `pipe.to(torch_device)` instead."</a><a id="change">
            )</a>

            &#47&#47 Set device as before (to be removed in 0.3.0)
            if device is None:
                device = "cuda" if torch.cuda.is_available() else "cpu"
            <a id="change">self.to(</a>device<a id="change">)</a>

        latents = torch.randn(
            (batch_size, self.unet.in_channels, self.unet.sample_size, self.unet.sample_size),
            generator=generator,</code></pre>