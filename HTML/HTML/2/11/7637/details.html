<html><h3>Pattern ID :7637
</h3><img src='25366851.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        self.optimizer.zero_grad()

        &#47&#47 The actor ids of a given batch should be all the same. Thus we can debatch them.
        actor_ids<a id="change"> = </a><a id="change">debatch_actor_ids(</a>actor_ids<a id="change">)</a>

        &#47&#47 Convert only actions to torch, since observations are converted in policy.compute_substep_policy_output method
        actions = convert_to_torch(actions, device=self.policy.device, cast=None, in_place=True)
        total_loss = self.loss.calculate_loss(policy=self.policy, observations=observations,
                                              actions=actions, actor_ids=actor_ids, events=self.imitation_events)
        total_loss.backward()
        self.optimizer.step()

        &#47&#47 Report additional policy-related stats
        for <a id="change">actor_id</a> in actor_ids:
            l2_norm = sum([param.norm() for param in <a id="change">self.policy.network_for(actor_id</a><a id="change">)</a>.parameters()])
            grad_norm = compute_gradient_norm(<a id="change">self.policy.network_for(actor_id</a><a id="change">)</a>.parameters())

            self.imitation_events.policy_l2_norm(step_id=actor_id.step_key, agent_id=actor_id.agent_id,
                                                 value=l2_norm.item())</code></pre><h3>After Change</h3><pre><code class='java'>
        self.policy.train()
        self.optimizer.zero_grad()

        <a id="change">observation_dict</a><a id="change">, action_dict</a> = data
        convert_to_torch(action_dict, device=self.policy.device, cast=None, in_place=True)

        total_loss = self.loss.calculate_loss(policy=self.policy, observation_dict=observation_dict,
                                              action_dict=action_dict, events=self.imitation_events)
        total_loss.backward()
        self.optimizer.step()

        &#47&#47 Report additional policy-related stats
        for <a id="change">policy_id</a> in <a id="change">observation_dict.keys()</a>:
            l2_norm = sum([param.norm() for param in <a id="change">self.policy.networks[policy_id]</a>.parameters()])
            grad_norm = compute_gradient_norm(<a id="change">self.policy.networks[policy_id]</a>.parameters())

            self.imitation_events.policy_l2_norm(step_id=policy_id, value=l2_norm.item())
            self.imitation_events.policy_grad_norm(step_id=policy_id, value=grad_norm)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 5</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/enlite-ai/maze/commit/2ebc028ca49702404f06e6d6875800d5b05b26c9#diff-edb9b60873d67ce0e47cb8c1b0d00f124e0b49eb26165cb6ad88b2bd39923138L120' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 25366851</div><div id='project'> Project Name: enlite-ai/maze</div><div id='commit'> Commit Name: 2ebc028ca49702404f06e6d6875800d5b05b26c9</div><div id='time'> Time: 2021-12-13</div><div id='author'> Author: office@enlite.ai</div><div id='file'> File Name: maze/train/trainers/imitation/bc_trainer.py</div><div id='m_class'> M Class Name: BCTrainer</div><div id='n_method'> N Class Name: BCTrainer</div><div id='m_method'> M Method Name: _run_iteration(2)</div><div id='n_method'> N Method Name: _run_iteration(4)</div><div id='m_parent_class'> M Parent Class: Trainer</div><div id='n_parent_class'> N Parent Class: Trainer</div><div id='m_file'> M File Name: maze/train/trainers/imitation/bc_trainer.py</div><div id='n_file'> N File Name: maze/train/trainers/imitation/bc_trainer.py</div><div id='m_start'> M Start Line: 125</div><div id='m_end'> M End Line: 149</div><div id='n_start'> N Start Line: 120</div><div id='n_end'> N End Line: 135</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.optimizer.zero_grad()

        &#47&#47 The actor ids of a given batch should be all the same. Thus we can debatch them.
        actor_ids<a id="change"> = </a><a id="change">debatch_actor_ids(</a>actor_ids<a id="change">)</a>

        &#47&#47 Convert only actions to torch, since observations are converted in policy.compute_substep_policy_output method
        actions = convert_to_torch(actions, device=self.policy.device, cast=None, in_place=True)
        total_loss = self.loss.calculate_loss(policy=self.policy, observations=observations,
                                              actions=actions, actor_ids=actor_ids, events=self.imitation_events)
        total_loss.backward()
        self.optimizer.step()

        &#47&#47 Report additional policy-related stats
        for <a id="change">actor_id</a> in actor_ids:
            l2_norm = sum([param.norm() for param in <a id="change">self.policy.network_for(</a>actor_id<a id="change">)</a>.parameters()])
            grad_norm = compute_gradient_norm(<a id="change">self.policy.network_for(</a>actor_id<a id="change">)</a>.parameters())

            self.imitation_events.policy_l2_norm(step_id=actor_id.step_key, agent_id=actor_id.agent_id,
                                                 value=l2_norm.item())</code></pre><h3>After Change</h3><pre><code class='java'>
        self.policy.train()
        self.optimizer.zero_grad()

        observation_dict<a id="change">, action_dict</a> = data
        convert_to_torch(action_dict, device=self.policy.device, cast=None, in_place=True)

        total_loss = self.loss.calculate_loss(policy=self.policy, observation_dict=observation_dict,
                                              action_dict=action_dict, events=self.imitation_events)
        total_loss.backward()
        self.optimizer.step()

        &#47&#47 Report additional policy-related stats
        for <a id="change">policy_id</a> in <a id="change">observation_dict.keys()</a>:
            l2_norm = sum([param.norm() for param in <a id="change">self.policy.networks[policy_id]</a>.parameters()])
            grad_norm = compute_gradient_norm(<a id="change">self.policy.networks[policy_id]</a>.parameters())

            self.imitation_events.policy_l2_norm(step_id=policy_id, value=l2_norm.item())
            self.imitation_events.policy_grad_norm(step_id=policy_id, value=grad_norm)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/enlite-ai/maze/commit/2ebc028ca49702404f06e6d6875800d5b05b26c9#diff-edb9b60873d67ce0e47cb8c1b0d00f124e0b49eb26165cb6ad88b2bd39923138L125' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 25366850</div><div id='project'> Project Name: enlite-ai/maze</div><div id='commit'> Commit Name: 2ebc028ca49702404f06e6d6875800d5b05b26c9</div><div id='time'> Time: 2021-12-13</div><div id='author'> Author: office@enlite.ai</div><div id='file'> File Name: maze/train/trainers/imitation/bc_trainer.py</div><div id='m_class'> M Class Name: BCTrainer</div><div id='n_method'> N Class Name: BCTrainer</div><div id='m_method'> M Method Name: _run_iteration(2)</div><div id='n_method'> N Method Name: _run_iteration(4)</div><div id='m_parent_class'> M Parent Class: Trainer</div><div id='n_parent_class'> N Parent Class: Trainer</div><div id='m_file'> M File Name: maze/train/trainers/imitation/bc_trainer.py</div><div id='n_file'> N File Name: maze/train/trainers/imitation/bc_trainer.py</div><div id='m_start'> M Start Line: 125</div><div id='m_end'> M End Line: 149</div><div id='n_start'> N Start Line: 120</div><div id='n_end'> N End Line: 135</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.optimizer.zero_grad()

        &#47&#47 The actor ids of a given batch should be all the same. Thus we can debatch them.
        actor_ids<a id="change"> = </a><a id="change">debatch_actor_ids(</a>actor_ids<a id="change">)</a>

        &#47&#47 Convert only actions to torch, since observations are converted in policy.compute_substep_policy_output method
        actions = convert_to_torch(actions, device=self.policy.device, cast=None, in_place=True)
        total_loss = self.loss.calculate_loss(policy=self.policy, observations=observations,
                                              actions=actions, actor_ids=actor_ids, events=self.imitation_events)
        total_loss.backward()
        self.optimizer.step()

        &#47&#47 Report additional policy-related stats
        for <a id="change">actor_id</a> in actor_ids:
            l2_norm = sum([param.norm() for param in <a id="change">self.policy.network_for(</a>actor_id<a id="change">)</a>.parameters()])
            grad_norm = compute_gradient_norm(<a id="change">self.policy.network_for(</a>actor_id<a id="change">)</a>.parameters())

            self.imitation_events.policy_l2_norm(step_id=actor_id.step_key, agent_id=actor_id.agent_id,
                                                 value=l2_norm.item())</code></pre><h3>After Change</h3><pre><code class='java'>
        self.policy.train()
        self.optimizer.zero_grad()

        observation_dict<a id="change">, action_dict</a> = data
        convert_to_torch(action_dict, device=self.policy.device, cast=None, in_place=True)

        total_loss = self.loss.calculate_loss(policy=self.policy, observation_dict=observation_dict,
                                              action_dict=action_dict, events=self.imitation_events)
        total_loss.backward()
        self.optimizer.step()

        &#47&#47 Report additional policy-related stats
        for <a id="change">policy_id</a> in <a id="change">observation_dict.keys()</a>:
            l2_norm = sum([param.norm() for param in <a id="change">self.policy.networks[policy_id]</a>.parameters()])
            grad_norm = compute_gradient_norm(<a id="change">self.policy.networks[policy_id]</a>.parameters())

            self.imitation_events.policy_l2_norm(step_id=policy_id, value=l2_norm.item())
            self.imitation_events.policy_grad_norm(step_id=policy_id, value=grad_norm)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/enlite-ai/maze/commit/00c9f42179f3a30775a1de05729c7acdf85f5449#diff-edb9b60873d67ce0e47cb8c1b0d00f124e0b49eb26165cb6ad88b2bd39923138L125' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 25366849</div><div id='project'> Project Name: enlite-ai/maze</div><div id='commit'> Commit Name: 00c9f42179f3a30775a1de05729c7acdf85f5449</div><div id='time'> Time: 2021-12-13</div><div id='author'> Author: office@enlite.ai</div><div id='file'> File Name: maze/train/trainers/imitation/bc_trainer.py</div><div id='m_class'> M Class Name: BCTrainer</div><div id='n_method'> N Class Name: BCTrainer</div><div id='m_method'> M Method Name: _run_iteration(2)</div><div id='n_method'> N Method Name: _run_iteration(4)</div><div id='m_parent_class'> M Parent Class: Trainer</div><div id='n_parent_class'> N Parent Class: Trainer</div><div id='m_file'> M File Name: maze/train/trainers/imitation/bc_trainer.py</div><div id='n_file'> N File Name: maze/train/trainers/imitation/bc_trainer.py</div><div id='m_start'> M Start Line: 125</div><div id='m_end'> M End Line: 149</div><div id='n_start'> N Start Line: 120</div><div id='n_end'> N End Line: 135</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.optimizer.zero_grad()

        &#47&#47 The actor ids of a given batch should be all the same. Thus we can debatch them.
        actor_ids<a id="change"> = </a><a id="change">debatch_actor_ids(</a>actor_ids<a id="change">)</a>

        &#47&#47 Convert only actions to torch, since observations are converted in policy.compute_substep_policy_output method
        actions = convert_to_torch(actions, device=self.policy.device, cast=None, in_place=True)
        total_loss = self.loss.calculate_loss(policy=self.policy, observations=observations,
                                              actions=actions, actor_ids=actor_ids, events=self.imitation_events)
        total_loss.backward()
        self.optimizer.step()

        &#47&#47 Report additional policy-related stats
        for <a id="change">actor_id</a> in actor_ids:
            l2_norm = sum([param.norm() for param in <a id="change">self.policy.network_for(</a>actor_id<a id="change">)</a>.parameters()])
            grad_norm = compute_gradient_norm(<a id="change">self.policy.network_for(</a>actor_id<a id="change">)</a>.parameters())

            self.imitation_events.policy_l2_norm(step_id=actor_id.step_key, agent_id=actor_id.agent_id,
                                                 value=l2_norm.item())</code></pre><h3>After Change</h3><pre><code class='java'>
        self.policy.train()
        self.optimizer.zero_grad()

        observation_dict<a id="change">, action_dict</a> = data
        convert_to_torch(action_dict, device=self.policy.device, cast=None, in_place=True)

        total_loss = self.loss.calculate_loss(policy=self.policy, observation_dict=observation_dict,
                                              action_dict=action_dict, events=self.imitation_events)
        total_loss.backward()
        self.optimizer.step()

        &#47&#47 Report additional policy-related stats
        for <a id="change">policy_id</a> in <a id="change">observation_dict.keys()</a>:
            l2_norm = sum([param.norm() for param in <a id="change">self.policy.networks[policy_id]</a>.parameters()])
            grad_norm = compute_gradient_norm(<a id="change">self.policy.networks[policy_id]</a>.parameters())

            self.imitation_events.policy_l2_norm(step_id=policy_id, value=l2_norm.item())
            self.imitation_events.policy_grad_norm(step_id=policy_id, value=grad_norm)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/enlite-ai/maze/commit/39e5c561cdb5c6e52c435d18831daaa0b901b0b6#diff-edb9b60873d67ce0e47cb8c1b0d00f124e0b49eb26165cb6ad88b2bd39923138L125' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 25366853</div><div id='project'> Project Name: enlite-ai/maze</div><div id='commit'> Commit Name: 39e5c561cdb5c6e52c435d18831daaa0b901b0b6</div><div id='time'> Time: 2021-12-13</div><div id='author'> Author: office@enlite.ai</div><div id='file'> File Name: maze/train/trainers/imitation/bc_trainer.py</div><div id='m_class'> M Class Name: BCTrainer</div><div id='n_method'> N Class Name: BCTrainer</div><div id='m_method'> M Method Name: _run_iteration(2)</div><div id='n_method'> N Method Name: _run_iteration(4)</div><div id='m_parent_class'> M Parent Class: Trainer</div><div id='n_parent_class'> N Parent Class: Trainer</div><div id='m_file'> M File Name: maze/train/trainers/imitation/bc_trainer.py</div><div id='n_file'> N File Name: maze/train/trainers/imitation/bc_trainer.py</div><div id='m_start'> M Start Line: 125</div><div id='m_end'> M End Line: 149</div><div id='n_start'> N Start Line: 120</div><div id='n_end'> N End Line: 135</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.optimizer.zero_grad()

        &#47&#47 The actor ids of a given batch should be all the same. Thus we can debatch them.
        actor_ids<a id="change"> = </a><a id="change">debatch_actor_ids(</a>actor_ids<a id="change">)</a>

        &#47&#47 Convert only actions to torch, since observations are converted in policy.compute_substep_policy_output method
        actions = convert_to_torch(actions, device=self.policy.device, cast=None, in_place=True)
        total_loss = self.loss.calculate_loss(policy=self.policy, observations=observations,
                                              actions=actions, actor_ids=actor_ids, events=self.imitation_events)
        total_loss.backward()
        self.optimizer.step()

        &#47&#47 Report additional policy-related stats
        for <a id="change">actor_id</a> in actor_ids:
            l2_norm = sum([param.norm() for param in <a id="change">self.policy.network_for(</a>actor_id<a id="change">)</a>.parameters()])
            grad_norm = compute_gradient_norm(<a id="change">self.policy.network_for(</a>actor_id<a id="change">)</a>.parameters())

            self.imitation_events.policy_l2_norm(step_id=actor_id.step_key, agent_id=actor_id.agent_id,
                                                 value=l2_norm.item())</code></pre><h3>After Change</h3><pre><code class='java'>
        self.policy.train()
        self.optimizer.zero_grad()

        observation_dict<a id="change">, action_dict</a> = data
        convert_to_torch(action_dict, device=self.policy.device, cast=None, in_place=True)

        total_loss = self.loss.calculate_loss(policy=self.policy, observation_dict=observation_dict,
                                              action_dict=action_dict, events=self.imitation_events)
        total_loss.backward()
        self.optimizer.step()

        &#47&#47 Report additional policy-related stats
        for <a id="change">policy_id</a> in <a id="change">observation_dict.keys()</a>:
            l2_norm = sum([param.norm() for param in <a id="change">self.policy.networks[policy_id]</a>.parameters()])
            grad_norm = compute_gradient_norm(<a id="change">self.policy.networks[policy_id]</a>.parameters())

            self.imitation_events.policy_l2_norm(step_id=policy_id, value=l2_norm.item())
            self.imitation_events.policy_grad_norm(step_id=policy_id, value=grad_norm)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/enlite-ai/maze/commit/c29839f14d1f1b2da97c0fb85dbaad20b7f7e435#diff-edb9b60873d67ce0e47cb8c1b0d00f124e0b49eb26165cb6ad88b2bd39923138L125' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 25366859</div><div id='project'> Project Name: enlite-ai/maze</div><div id='commit'> Commit Name: c29839f14d1f1b2da97c0fb85dbaad20b7f7e435</div><div id='time'> Time: 2021-12-13</div><div id='author'> Author: office@enlite.ai</div><div id='file'> File Name: maze/train/trainers/imitation/bc_trainer.py</div><div id='m_class'> M Class Name: BCTrainer</div><div id='n_method'> N Class Name: BCTrainer</div><div id='m_method'> M Method Name: _run_iteration(2)</div><div id='n_method'> N Method Name: _run_iteration(4)</div><div id='m_parent_class'> M Parent Class: Trainer</div><div id='n_parent_class'> N Parent Class: Trainer</div><div id='m_file'> M File Name: maze/train/trainers/imitation/bc_trainer.py</div><div id='n_file'> N File Name: maze/train/trainers/imitation/bc_trainer.py</div><div id='m_start'> M Start Line: 125</div><div id='m_end'> M End Line: 149</div><div id='n_start'> N Start Line: 120</div><div id='n_end'> N End Line: 135</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self.optimizer.zero_grad()

        &#47&#47 The actor ids of a given batch should be all the same. Thus we can debatch them.
        actor_ids<a id="change"> = </a><a id="change">debatch_actor_ids(</a>actor_ids<a id="change">)</a>

        &#47&#47 Convert only actions to torch, since observations are converted in policy.compute_substep_policy_output method
        actions = convert_to_torch(actions, device=self.policy.device, cast=None, in_place=True)
        total_loss = self.loss.calculate_loss(policy=self.policy, observations=observations,
                                              actions=actions, actor_ids=actor_ids, events=self.imitation_events)
        total_loss.backward()
        self.optimizer.step()

        &#47&#47 Report additional policy-related stats
        for <a id="change">actor_id</a> in actor_ids:
            l2_norm = sum([param.norm() for param in <a id="change">self.policy.network_for(</a>actor_id<a id="change">)</a>.parameters()])
            grad_norm = compute_gradient_norm(<a id="change">self.policy.network_for(</a>actor_id<a id="change">)</a>.parameters())

            self.imitation_events.policy_l2_norm(step_id=actor_id.step_key, agent_id=actor_id.agent_id,
                                                 value=l2_norm.item())</code></pre><h3>After Change</h3><pre><code class='java'>
        self.policy.train()
        self.optimizer.zero_grad()

        observation_dict<a id="change">, action_dict</a> = data
        convert_to_torch(action_dict, device=self.policy.device, cast=None, in_place=True)

        total_loss = self.loss.calculate_loss(policy=self.policy, observation_dict=observation_dict,
                                              action_dict=action_dict, events=self.imitation_events)
        total_loss.backward()
        self.optimizer.step()

        &#47&#47 Report additional policy-related stats
        for <a id="change">policy_id</a> in <a id="change">observation_dict.keys()</a>:
            l2_norm = sum([param.norm() for param in <a id="change">self.policy.networks[policy_id]</a>.parameters()])
            grad_norm = compute_gradient_norm(<a id="change">self.policy.networks[policy_id]</a>.parameters())

            self.imitation_events.policy_l2_norm(step_id=policy_id, value=l2_norm.item())
            self.imitation_events.policy_grad_norm(step_id=policy_id, value=grad_norm)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/enlite-ai/maze/commit/dcea82711937efcec43bf117f592488fae18f096#diff-edb9b60873d67ce0e47cb8c1b0d00f124e0b49eb26165cb6ad88b2bd39923138L125' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 25366861</div><div id='project'> Project Name: enlite-ai/maze</div><div id='commit'> Commit Name: dcea82711937efcec43bf117f592488fae18f096</div><div id='time'> Time: 2021-12-13</div><div id='author'> Author: office@enlite.ai</div><div id='file'> File Name: maze/train/trainers/imitation/bc_trainer.py</div><div id='m_class'> M Class Name: BCTrainer</div><div id='n_method'> N Class Name: BCTrainer</div><div id='m_method'> M Method Name: _run_iteration(2)</div><div id='n_method'> N Method Name: _run_iteration(4)</div><div id='m_parent_class'> M Parent Class: Trainer</div><div id='n_parent_class'> N Parent Class: Trainer</div><div id='m_file'> M File Name: maze/train/trainers/imitation/bc_trainer.py</div><div id='n_file'> N File Name: maze/train/trainers/imitation/bc_trainer.py</div><div id='m_start'> M Start Line: 125</div><div id='m_end'> M End Line: 149</div><div id='n_start'> N Start Line: 120</div><div id='n_end'> N End Line: 135</div><BR>