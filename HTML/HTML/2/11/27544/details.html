<html><h3>Pattern ID :27544
</h3><img src='81866369.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        split_dims = [ctx.dims.depth]
    prefix_name = prefixed_name(ctx, name)
    depth_indexing &= not ctx.model.weight_sharing
    if <a id="change">depth_indexing</a>:
        <a id="change">assert </a>idx is not None, "idx has to be set when depth indexing is true"
        str_shape = [ctx.dims.depth]<a id="change"> + </a>str_shape
    shape = dims_to_shape(ctx, str_shape)
    if prefix_name not in ctx.parameters:
        ctx.parameter_dims[prefix_name] = str_shape
        if std is None and mean is None:
            param, var = stacked_orthogonal_init(ctx, str_shape, column_axes, split_dims)
            param *= scale * post_variance_scale
        else:
            if scale != 1:
                print(f"Warning: get_param creates normal distribution with scale=1 even though it should be {scale}")
            param = normal(ctx, shape)
            if std is not None:
                param *= std
            if mean is not None:
                param += mean
        ctx.parameter_variance[prefix_name] = lr_scale * scale
        param = param.astype(
            ctx.model.storage_dtype if dtype is None else jnp.promote_types(ctx.model.storage_dtype, dtype))
        assign(ctx, name, param)
    param = ctx.parameters[prefix_name]
    if <a id="change">depth_indexing</a>:
        param<a id="change"> = </a>param[idx].reshape(param.shape[1:])
    return param.astype(
        ctx.model.computation_dtype if dtype is None else jnp.promote_types(ctx.model.computation_dtype, dtype))
</code></pre><h3>After Change</h3><pre><code class='java'>
        computation_dtype = jnp.promote_types(ctx.model.computation_dtype, dtype)
        storage_dtype = jnp.promote_types(ctx.model.storage_dtype, dtype)

    if <a id="change">weight_sharing</a>:
        split_name<a id="change"> = </a>[n.split(&quot:&quot)[0] + &quot:&quot for n in name.split(&quot/&quot)]
        for pname, <a id="change">param</a> in ctx.parameters.items():
            <a id="change">if ctx.parameter_dims[pname] != str_shape</a>:
                <a id="change">continue</a>
            pname<a id="change"> = </a>pname.split(&quot/&quot)
            if all(p.startswith(s) for p, s in zip(pname, split_name)):
                return param.astype(computation_dtype)
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/homebrewnlp/olmax/commit/154574249d2a1aa4a4c5a3a0e8d11fd80780d828#diff-d771f0af7a90980d8b63fd1f0f9b1b9b4d1a9d227ac1152acc032a5af1246ad6L113' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 81866369</div><div id='project'> Project Name: homebrewnlp/olmax</div><div id='commit'> Commit Name: 154574249d2a1aa4a4c5a3a0e8d11fd80780d828</div><div id='time'> Time: 2022-04-13</div><div id='author'> Author: 39779310+ClashLuke@users.noreply.github.com</div><div id='file'> File Name: src/backend.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: get_param(13)</div><div id='n_method'> N Method Name: get_param(13)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/backend.py</div><div id='n_file'> N File Name: src/backend.py</div><div id='m_start'> M Start Line: 113</div><div id='m_end'> M End Line: 139</div><div id='n_start'> N Start Line: 113</div><div id='n_end'> N End Line: 148</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        for path in tqdm(self.raw_paths, desc="Files"):
            molecules = list(h5py.File(path).values())

            for <a id="change">mol</a> in tqdm(molecules, desc="Molecules", leave=False):
                z = pt.tensor(mol["atomic_numbers"], dtype=pt.long)
                fq = pt.tensor(mol["formal_charges"], dtype=pt.long)
                q = fq.sum()

                for conf in mol["conformations"].values():

                    &#47&#47 Skip failed calculations
                    if "formation_energy" not in conf:
                        continue

                    assert conf["positions"].attrs["units"] == "Å"
                    pos = pt.tensor(conf["positions"], dtype=pt.float32)
                    assert pos.shape == (z.shape[0], 3)

                    assert conf["formation_energy"].attrs["units"] == "eV"
                    y = pt.tensor(conf["formation_energy"][()], dtype=pt.float64)
                    assert y.shape == ()

                    assert conf["forces"].attrs["units"] == "eV/Å"
                    dy = <a id="change">-pt.tensor(conf["forces"], dtype=pt.float32)</a>
                    assert dy.shape == pos.shape

                    assert conf["partial_charges"].attrs["units"] == "e"
                    pq = pt.tensor(conf["partial_charges"], dtype=pt.float32)
                    assert pq.shape == z.shape

                    assert conf["dipole_moment"].attrs["units"] == "e*Å"
                    dp<a id="change"> = </a>pt.tensor(conf["dipole_moment"], dtype=pt.float32)
                    <a id="change">assert </a>dp.shape == (3,)

                    &#47&#47 Skip samples with large forces
                    if self.max_gradient:</code></pre><h3>After Change</h3><pre><code class='java'>

        assert self.subsample_molecules &gt; 0

        for <a id="change">path</a> in tqdm(self.raw_paths, desc="Files"):

            h5 = h5py.File(path)
            assert h5.attrs["layout"] == "Ace"
            version = h5.attrs["layout_version"]

            mols = None
            load_confs = None
            if version == "1.0":
                assert "name" in h5.attrs
                mols = h5.items()
                load_confs<a id="change"> = </a>self._load_confs_1_0
            elif version == "2.0":
                assert len(h5.keys()) == 1
                mols = list(h5.values())[0].items()
                load_confs = self._load_confs_2_0
            else:
                raise RuntimeError(f"Unsuported layout verions: {version}")

            &#47&#47 Iterate over the molecules
            for <a id="change">i_mol</a>, (mol_id, mol) in tqdm(
                enumerate(mols),
                desc="Molecules",
                total=len(mols),
                leave=False,
            ):

                &#47&#47 Subsample molecules
                <a id="change">if i_mol % self.subsample_molecules != 0</a>:
                    <a id="change">continue</a>

                z = pt.tensor(mol["atomic_numbers"], dtype=pt.long)
                fq = pt.tensor(mol["formal_charges"], dtype=pt.long)
                q = fq.sum()

                for pos, y, neg_dy, pq, dp in load_confs(mol, n_atoms=len(z)):

                    &#47&#47 Skip samples with large forces
                    if self.max_gradient:
                        if neg_dy.norm(dim=1).max() &gt; float(self.max_gradient):
                            continue

                    &#47&#47 Create a sample
                    args = dict(
                        z=z, pos=pos, y=y.view(1, 1), neg_dy=neg_dy, q=q, pq=pq, dp=dp
                    )
                    if mol_ids:
                        args["mol_id"] = mol_id
                    data<a id="change"> = </a>Data(**args)

                    if self.pre_filter is not None and not self.pre_filter(data):
                        continue</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/torchmd/torchmd-net/commit/d23e6500f2cef1fa56d6c99ce5fdb983f1379bca#diff-56de3e28dd2b438171f4af7de59cc66305304d98e21b24faf4d744698d8c1aa3L72' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 81866370</div><div id='project'> Project Name: torchmd/torchmd-net</div><div id='commit'> Commit Name: d23e6500f2cef1fa56d6c99ce5fdb983f1379bca</div><div id='time'> Time: 2022-10-28</div><div id='author'> Author: peastman@stanford.edu</div><div id='file'> File Name: torchmdnet/datasets/ace.py</div><div id='m_class'> M Class Name: Ace</div><div id='n_method'> N Class Name: Ace</div><div id='m_method'> M Method Name: sample_iter(2)</div><div id='n_method'> N Method Name: sample_iter(1)</div><div id='m_parent_class'> M Parent Class: Dataset</div><div id='n_parent_class'> N Parent Class: Dataset</div><div id='m_file'> M File Name: torchmdnet/datasets/ace.py</div><div id='n_file'> N File Name: torchmdnet/datasets/ace.py</div><div id='m_start'> M Start Line: 72</div><div id='m_end'> M End Line: 123</div><div id='n_start'> N Start Line: 144</div><div id='n_end'> N End Line: 206</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    prefix_name = prefixed_name(ctx, name)
    depth_indexing &= not ctx.model.weight_sharing
    if depth_indexing:
        <a id="change">assert </a>idx is not None, "idx has to be set when depth indexing is true"
        str_shape = [ctx.dims.depth]<a id="change"> + </a>str_shape
    shape = dims_to_shape(ctx, str_shape)
    if prefix_name not in ctx.parameters:
        ctx.parameter_dims[prefix_name] = str_shape
        if std is None and mean is None:
            param, var = stacked_orthogonal_init(ctx, str_shape, column_axes, split_dims)
            param *= scale * post_variance_scale
        else:
            if scale != 1:
                print(f"Warning: get_param creates normal distribution with scale=1 even though it should be {scale}")
            param = normal(ctx, shape)
            if std is not None:
                param *= std
            if mean is not None:
                param += mean
        ctx.parameter_variance[prefix_name] = lr_scale * scale
        param = param.astype(
            ctx.model.storage_dtype if dtype is None else jnp.promote_types(ctx.model.storage_dtype, dtype))
        assign(ctx, name, param)
    param = ctx.parameters[prefix_name]
    if depth_indexing:
        param<a id="change"> = </a>param[idx].reshape(param.shape[1:])
    return param.astype(
        ctx.model.computation_dtype if dtype is None else jnp.promote_types(ctx.model.computation_dtype, dtype))
</code></pre><h3>After Change</h3><pre><code class='java'>
        computation_dtype = jnp.promote_types(ctx.model.computation_dtype, dtype)
        storage_dtype = jnp.promote_types(ctx.model.storage_dtype, dtype)

    if <a id="change">weight_sharing</a>:
        split_name<a id="change"> = </a>[n.split(&quot:&quot)[0] + &quot:&quot for n in name.split(&quot/&quot)]
        for pname, <a id="change">param</a> in ctx.parameters.items():
            <a id="change">if ctx.parameter_dims[pname] != str_shape</a>:
                <a id="change">continue</a>
            pname<a id="change"> = </a>pname.split(&quot/&quot)
            if all(p.startswith(s) for p, s in zip(pname, split_name)):
                return param.astype(computation_dtype)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/homebrewnlp/homebrewnlp-jax/commit/154574249d2a1aa4a4c5a3a0e8d11fd80780d828#diff-d771f0af7a90980d8b63fd1f0f9b1b9b4d1a9d227ac1152acc032a5af1246ad6L105' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 81866373</div><div id='project'> Project Name: homebrewnlp/homebrewnlp-jax</div><div id='commit'> Commit Name: 154574249d2a1aa4a4c5a3a0e8d11fd80780d828</div><div id='time'> Time: 2022-04-13</div><div id='author'> Author: 39779310+ClashLuke@users.noreply.github.com</div><div id='file'> File Name: src/backend.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: get_param(13)</div><div id='n_method'> N Method Name: get_param(13)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/backend.py</div><div id='n_file'> N File Name: src/backend.py</div><div id='m_start'> M Start Line: 113</div><div id='m_end'> M End Line: 139</div><div id='n_start'> N Start Line: 113</div><div id='n_end'> N End Line: 148</div><BR>