<html><h3>Pattern ID :2213
</h3><img src='9523696.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

                all_comm_partitions.append(single_comm_all_partitions)

            <a id="change">for </a>p in my_params<a id="change">:
                </a>partitions = param_partition_map[p]
                parts = []
                for part in partitions:
                    params<a id="change">, offsets = partition_param_map[part]</a>
                    found = False
                    for p_idx, _p in enumerate(params):
                        <a id="change">if p.__hash__() == _p.__hash__()</a>:
                            found<a id="change"> = </a>True
                            if offsets[p_idx][0] is not None:
                                my_part<a id="change"> = </a>part.narrow(0,
                                                      offsets[p_idx][0],
                                                      offsets[p_idx][1])
                                parts.append(my_part)
                    assert found
                <a id="change">if </a>p is not None:
                    updated_grad = _unflatten_dense_tensors(torch.cat(parts), [p])
                    p.grad.copy_(updated_grad[0])
</code></pre><h3>After Change</h3><pre><code class='java'>
            flat_all_grads = torch.cat(flat_comm_grads)

            &#47&#47 copy back reduced gradients but only those needed for this local rank
            <a id="change">for </a>param, updated_grad in <a id="change">zip(</a>self.fp16_groups[i], _unflatten_dense_tensors(flat_all_grads, self.fp16_groups[i])<a id="change">):
                </a>if param in my_params:
                    param.grad.copy_(updated_grad)

    def step(self, closure=None):</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 10</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/microsoft/deepspeed/commit/55ed105771d08fbffc0cb6d8cd56a2e61206ad1d#diff-458bf13440cbc0013d248079431500c00e3907e0a747c77fa0c8dda6b8b25f88L536' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 9523696</div><div id='project'> Project Name: microsoft/deepspeed</div><div id='commit'> Commit Name: 55ed105771d08fbffc0cb6d8cd56a2e61206ad1d</div><div id='time'> Time: 2020-09-15</div><div id='author'> Author: jerasley@microsoft.com</div><div id='file'> File Name: deepspeed/runtime/zero/stage1.py</div><div id='m_class'> M Class Name: FP16_DeepSpeedZeroOptimizer_Stage1</div><div id='n_method'> N Class Name: FP16_DeepSpeedZeroOptimizer_Stage1</div><div id='m_method'> M Method Name: reduce_scatter_gradients(4)</div><div id='n_method'> N Method Name: reduce_scatter_gradients(4)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: deepspeed/runtime/zero/stage1.py</div><div id='n_file'> N File Name: deepspeed/runtime/zero/stage1.py</div><div id='m_start'> M Start Line: 539</div><div id='m_end'> M End Line: 613</div><div id='n_start'> N Start Line: 536</div><div id='n_end'> N End Line: 605</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    files = list(data_dir.glob(&quot**/*.mid&quot))
    &#47&#47 files = files[:1000]

    <a id="change">for f</a> in tqdm(files)<a id="change">:
        </a>try:
            mid<a id="change"> = </a>mido.MidiFile(f)
        except Exception:
            continue
        
        g = (Path(dest_path) / f.relative_to(data_path)).with_suffix(&quot.pkl&quot)
        g.parent.mkdir(parents=True, exist_ok=True)

        tempos = {m.tempo for tr in mid.tracks for m in tr if <a id="change">m.type==&quotset_tempo&quot</a>}
        <a id="change">if len(tempos)!=1</a>:
            continue
        micros_per_beat<a id="change"> = tuple(tempos)[0]</a>

        s_per_tick = micros_per_beat / mid.ticks_per_beat / 1e6

        for i,tr in enumerate(mid.tracks):
            seq<a id="change"> = </a>[m for m in tr if m.type==&quotnote_on&quot and m.velocity]
            if len(seq) &lt; min_len:
                continue
            torch.save(dict(</code></pre><h3>After Change</h3><pre><code class='java'>

    with Pool(n_jobs) as pool:
        &#47&#47 p.imap_unordered(ft.partial(process, data_path, dest_path), tqdm(files), 32)
        <a id="change">for </a>_ in tqdm(pool.imap_unordered(process, <a id="change">zip(</a>files, files_out<a id="change">)</a>, 32))<a id="change">:
            </a>pass
       

if __name__=="__main__":</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/intelligent-instruments-lab/iil-python-tools/commit/dbb5cf68b2b6902addf71a598302850beb1473cf#diff-9d738772070de4df243e741743f664299e8ce7ef188e1d5d6f035c83cce242d9L9' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 9523715</div><div id='project'> Project Name: intelligent-instruments-lab/iil-python-tools</div><div id='commit'> Commit Name: dbb5cf68b2b6902addf71a598302850beb1473cf</div><div id='time'> Time: 2022-02-21</div><div id='author'> Author: victor.shepardson@gmail.com</div><div id='file'> File Name: scripts/lakh_prep.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(3)</div><div id='n_method'> N Method Name: main(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: scripts/lakh_prep.py</div><div id='n_file'> N File Name: scripts/lakh_prep.py</div><div id='m_start'> M Start Line: 11</div><div id='m_end'> M End Line: 43</div><div id='n_start'> N Start Line: 41</div><div id='n_end'> N End Line: 56</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            if IS_CUDA:
                if cuda_event:
                    cuda_event.wait()
            <a id="change">for </a>stage, <a id="change">banks</a> in self.memory_bank_per_stage.items()<a id="change">:
                </a>args.insert(0, batch_indices)
                for bank in banks:
                    <a id="change">if bank is not None</a>:
                        if isinstance(bank, tuple):
                            bank<a id="change"> = </a>tuple(x[batch_slot] for x in bank)
                        else:
                            bank<a id="change"> = bank[batch_slot]</a>
                    args.append(bank)
                args.append(self.metadata)
                args.append(self.storage_state)
                code = self.loader.code_per_stage[stage]
                result<a id="change"> = </a>code(*args)
                args = list(result)
                <a id="change">if </a>first_stage:
                    first_stage = False
                    self.memory_context.end_batch(b_ix)
        return tuple(x[:len(batch_indices)] for x in args)</code></pre><h3>After Change</h3><pre><code class='java'>

            for stage_code, define_outputs in code:
                results = stage_code(**args)
                <a id="change">for </a>node_id, result in <a id="change">zip(</a>define_outputs, results<a id="change">):
                    </a>args[f&quotresult_{node_id}&quot] = result
                pass

            result = tuple(args[f&quotresult_{x}&quot] for x in outputs)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/libffcv/ffcv/commit/f8baf227d1243d4207c082d8ea11b89b5a73da32#diff-d1c1d17dd7f942f4223cf450209bcd6a3979e50b67e9c881848b09da0845f7ffL110' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 9523702</div><div id='project'> Project Name: libffcv/ffcv</div><div id='commit'> Commit Name: f8baf227d1243d4207c082d8ea11b89b5a73da32</div><div id='time'> Time: 2022-02-08</div><div id='author'> Author: leclerc@mit.edu</div><div id='file'> File Name: ffcv/loader/epoch_iterator.py</div><div id='m_class'> M Class Name: EpochIterator</div><div id='n_method'> N Class Name: EpochIterator</div><div id='m_method'> M Method Name: run_pipeline(5)</div><div id='n_method'> N Method Name: run_pipeline(5)</div><div id='m_parent_class'> M Parent Class: Thread</div><div id='n_parent_class'> N Parent Class: Thread</div><div id='m_file'> M File Name: ffcv/loader/epoch_iterator.py</div><div id='n_file'> N File Name: ffcv/loader/epoch_iterator.py</div><div id='m_start'> M Start Line: 113</div><div id='m_end'> M End Line: 142</div><div id='n_start'> N Start Line: 111</div><div id='n_end'> N End Line: 140</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

                all_comm_partitions.append(single_comm_all_partitions)

            <a id="change">for p</a> in my_params<a id="change">:
                </a>partitions = param_partition_map[p]
                parts = []
                for part in partitions:
                    params<a id="change">, offsets = partition_param_map[part]</a>
                    found = False
                    for p_idx, _p in enumerate(params):
                        <a id="change">if p.__hash__() == _p.__hash__()</a>:
                            found<a id="change"> = </a>True
                            if offsets[p_idx][0] is not None:
                                my_part<a id="change"> = </a>part.narrow(0,
                                                      offsets[p_idx][0],
                                                      offsets[p_idx][1])
                                parts.append(my_part)
                    assert found
                <a id="change">if </a>p is not None:
                    updated_grad = _unflatten_dense_tensors(torch.cat(parts), [p])
                    p.grad.copy_(updated_grad[0])
</code></pre><h3>After Change</h3><pre><code class='java'>
            flat_all_grads = torch.cat(flat_comm_grads)

            &#47&#47 copy back reduced gradients but only those needed for this local rank
            <a id="change">for </a>param, updated_grad in <a id="change">zip(</a>self.fp16_groups[i], _unflatten_dense_tensors(flat_all_grads, self.fp16_groups[i])<a id="change">):
                </a>if param in my_params:
                    param.grad.copy_(updated_grad)

    def step(self, closure=None):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/microsoft/deepspeed/commit/55ed105771d08fbffc0cb6d8cd56a2e61206ad1d#diff-458bf13440cbc0013d248079431500c00e3907e0a747c77fa0c8dda6b8b25f88L530' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 9523720</div><div id='project'> Project Name: microsoft/deepspeed</div><div id='commit'> Commit Name: 55ed105771d08fbffc0cb6d8cd56a2e61206ad1d</div><div id='time'> Time: 2020-09-15</div><div id='author'> Author: jerasley@microsoft.com</div><div id='file'> File Name: deepspeed/runtime/zero/stage1.py</div><div id='m_class'> M Class Name: FP16_DeepSpeedZeroOptimizer_Stage1</div><div id='n_method'> N Class Name: FP16_DeepSpeedZeroOptimizer_Stage1</div><div id='m_method'> M Method Name: reduce_scatter_gradients(4)</div><div id='n_method'> N Method Name: reduce_scatter_gradients(4)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: deepspeed/runtime/zero/stage1.py</div><div id='n_file'> N File Name: deepspeed/runtime/zero/stage1.py</div><div id='m_start'> M Start Line: 539</div><div id='m_end'> M End Line: 613</div><div id='n_start'> N Start Line: 536</div><div id='n_end'> N End Line: 605</div><BR>