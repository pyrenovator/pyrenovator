<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

                all_comm_partitions.append(single_comm_all_partitions)

            <a id="change">for </a>p in my_params<a id="change">:
                </a>partitions = param_partition_map[p]
                parts = []
                for part in partitions:
                    params<a id="change">, offsets = partition_param_map[part]</a>
                    found = False
                    for p_idx, _p in enumerate(params):
                        <a id="change">if p.__hash__() == _p.__hash__()</a>:
                            found<a id="change"> = </a>True
                            if offsets[p_idx][0] is not None:
                                my_part<a id="change"> = </a>part.narrow(0,
                                                      offsets[p_idx][0],
                                                      offsets[p_idx][1])
                                parts.append(my_part)
                    assert found
                <a id="change">if </a>p is not None:
                    updated_grad = _unflatten_dense_tensors(torch.cat(parts), [p])
                    p.grad.copy_(updated_grad[0])
</code></pre><h3>After Change</h3><pre><code class='java'>
            flat_all_grads = torch.cat(flat_comm_grads)

            &#47&#47 copy back reduced gradients but only those needed for this local rank
            <a id="change">for </a>param, updated_grad in <a id="change">zip(</a>self.fp16_groups[i], _unflatten_dense_tensors(flat_all_grads, self.fp16_groups[i])<a id="change">):
                </a>if param in my_params:
                    param.grad.copy_(updated_grad)

    def step(self, closure=None):</code></pre>