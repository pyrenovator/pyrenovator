<html><h3>Pattern ID :21410
</h3><img src='68223377.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                 cached=False,
                 weight=True,
                 bias=True):
        <a id="change">super(</a>SGConv, self<a id="change">)</a>.__init__()
        self._cached = cached
        self._cached_h = None
        self._k = k</code></pre><h3>After Change</h3><pre><code class='java'>
                 bias=True):
        
        super().__init__()
        <a id="change">if norm not in (&quotnone&quot, &quotboth&quot, &quotright&quot, &quotleft&quot)</a>:
            <a id="change">raise </a>DGLError(<a id="change">&quotInvalid norm value. Must be either "none", "both", "right" or "left".&quot
                           &quot But got "{}".&quot.format(norm</a><a id="change">)</a>)     
        self._in_feats<a id="change"> = </a>in_feats
        self._out_feats<a id="change"> = </a>out_feats            
        self._cached = cached
        self._cached_h = None
        self._k = k
        self._norm = norm
        self._add_self_loop<a id="change"> = </a>add_self_loop

        self.linear<a id="change"> = </a>Linear(in_feats, out_feats, weight=weight, bias=bias)

    def reset_parameters(self):
        Reinitialize learnable parameters.</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 9</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/edisonleeeee/graphwar/commit/c43665fd30401c63acbd50175da1880509a52d21#diff-4e82f29fd5c78eabd4fe1d5867020e786dc0deb6b867e435c11f46885eaadad4L70' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 68223377</div><div id='project'> Project Name: edisonleeeee/graphwar</div><div id='commit'> Commit Name: c43665fd30401c63acbd50175da1880509a52d21</div><div id='time'> Time: 2021-12-06</div><div id='author'> Author: cnljt@outlook.com</div><div id='file'> File Name: graphwar/nn/sgconv.py</div><div id='m_class'> M Class Name: SGConv</div><div id='n_method'> N Class Name: SGConv</div><div id='m_method'> M Method Name: __init__(9)</div><div id='n_method'> N Method Name: __init__(7)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: graphwar/nn/sgconv.py</div><div id='n_file'> N File Name: graphwar/nn/sgconv.py</div><div id='m_start'> M Start Line: 70</div><div id='m_end'> M End Line: 85</div><div id='n_start'> N Start Line: 90</div><div id='n_end'> N End Line: 108</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                 cached=False,
                 weight=True,
                 bias=True):
        <a id="change">super(</a>SGConv, self<a id="change">)</a>.__init__()
        self._cached = cached
        self._cached_h = None
        self._k = k</code></pre><h3>After Change</h3><pre><code class='java'>
                 bias=True):
        
        super().__init__()
        <a id="change">if norm not in (&quotnone&quot, &quotboth&quot, &quotright&quot, &quotleft&quot)</a>:
            <a id="change">raise </a>DGLError(<a id="change">&quotInvalid norm value. Must be either "none", "both", "right" or "left".&quot
                           &quot But got "{}".&quot.format(</a>norm<a id="change">)</a>)     
        self._in_feats<a id="change"> = </a>in_feats
        self._out_feats<a id="change"> = </a>out_feats            
        self._cached = cached
        self._cached_h = None
        self._k = k
        self._norm = norm
        self._add_self_loop<a id="change"> = </a>add_self_loop

        self.linear<a id="change"> = </a>Linear(in_feats, out_feats, weight=weight, bias=bias)

    def reset_parameters(self):
        Reinitialize learnable parameters.</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/edisonleeeee/graphwar/commit/c43665fd30401c63acbd50175da1880509a52d21#diff-4e82f29fd5c78eabd4fe1d5867020e786dc0deb6b867e435c11f46885eaadad4L63' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 68223379</div><div id='project'> Project Name: edisonleeeee/graphwar</div><div id='commit'> Commit Name: c43665fd30401c63acbd50175da1880509a52d21</div><div id='time'> Time: 2021-12-06</div><div id='author'> Author: cnljt@outlook.com</div><div id='file'> File Name: graphwar/nn/sgconv.py</div><div id='m_class'> M Class Name: SGConv</div><div id='n_method'> N Class Name: SGConv</div><div id='m_method'> M Method Name: __init__(9)</div><div id='n_method'> N Method Name: __init__(7)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: graphwar/nn/sgconv.py</div><div id='n_file'> N File Name: graphwar/nn/sgconv.py</div><div id='m_start'> M Start Line: 70</div><div id='m_end'> M End Line: 85</div><div id='n_start'> N Start Line: 90</div><div id='n_end'> N End Line: 108</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    

    def __init__(self, learning_rate, momentum=0.0, *args, **kwargs):
        <a id="change">super()</a>.__init__(learning_rate, momentum, *args, **kwargs)


class Lamb(object):</code></pre><h3>After Change</h3><pre><code class='java'>
    def __init__(self, learning_rate=0.01, momentum=0.0, nesterov=False, weight_decay=0.0, grad_clip=None):
        self.learning_rate = learning_rate
        self.momentum = momentum
        <a id="change">if weight_decay &lt; 0.0</a>:
            <a id="change">raise </a>ValueError(<a id="change">"weight_decay should not smaller than 0.0, but got {}".format(</a>weight_decay<a id="change">)</a>)
        self.weight_decay<a id="change"> = </a>float(weight_decay)
        self.grad_clip<a id="change"> = </a>grad_clip
        self.nesterov<a id="change"> = </a>nesterov
        self.sgd<a id="change"> = </a>tf.optimizers.SGD(learning_rate=self.learning_rate, momentum=self.momentum, nesterov=self.nesterov)

    def apply_gradients(self, grads_and_vars):
        if grads_and_vars is None:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayerx/commit/fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e#diff-797de54c3f4efa1d7823630b977a65b75bc98d859fd0992baa1e575dd43fe85fL313' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 68223411</div><div id='project'> Project Name: tensorlayer/tensorlayerx</div><div id='commit'> Commit Name: fc4e6ad217c1fe6ef9637601309a42fdbd7ac75e</div><div id='time'> Time: 2022-03-25</div><div id='author'> Author: jiaronghan@outlook.com</div><div id='file'> File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_class'> M Class Name: Momentum</div><div id='n_method'> N Class Name: Momentum</div><div id='m_method'> M Method Name: __init__(6)</div><div id='n_method'> N Method Name: __init__(3)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: tf.compat.v1.train.MomentumOptimizer</div><div id='m_file'> M File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='n_file'> N File Name: tensorlayerx/optimizers/tensorflow_optimizers.py</div><div id='m_start'> M Start Line: 314</div><div id='m_end'> M End Line: 314</div><div id='n_start'> N Start Line: 617</div><div id='n_end'> N End Line: 625</div><BR>