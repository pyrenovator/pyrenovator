<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    test_loader = val_loader

    train_iter<a id="change"> = </a><a id="change">ForeverDataIterator(</a>train_loader<a id="change">)</a>

    &#47&#47 create model
    print("=&gt; using pre-trained model &quot{}&quot".format(args.arch))</code></pre><h3>After Change</h3><pre><code class='java'>

    &#47&#47 define optimizer and lr scheduler
    optimizer = SGD(classifier.get_parameters(args.lr), momentum=args.momentum, weight_decay=args.wd, nesterov=True)
    milestones<a id="change"> = </a><a id="change">[]</a>
    <a id="change">for </a><a id="change">milestone</a> in <a id="change">args.lr_decay_epochs.split(&quot,&quot</a><a id="change">):
        </a><a id="change">milestones.append(</a><a id="change">int(milestone</a><a id="change">))</a>

    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(
        optimizer, milestones, gamma=args.lr_gamma)
    &#47&#47 resume from the best checkpoint
    if args.phase != &quottrain&quot:
        checkpoint = torch.load(logger.get_checkpoint_path(&quotbest&quot), map_location=&quotcpu&quot)
        classifier.load_state_dict(checkpoint)

    if args.phase == &quottest&quot:
        acc1 = validate(test_loader, classifier, args)
        print(acc1)
        return

    &#47&#47 start training
    best_acc1 = 0.0
    bss_module = BSS(k=args.k)
    for epoch in range(args.epochs):
        &#47&#47 train for one epoch
        train(train_loader, classifier, bss_module, optimizer,
              epoch, args)
        <a id="change">lr_scheduler.step()</a>
        &#47&#47 evaluate on validation set
        acc1 = validate(val_loader, classifier, args)
</code></pre>