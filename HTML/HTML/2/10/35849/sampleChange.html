<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        item_emb = self.item_embedding(item_seq)
        feature_emb = self.feature_embedding(feature_seq)

        <a id="change">if </a><a id="change">feature_seq.dim() == 3</a>: &#47&#47 pos_features [B Len Num]
            feature_mask = (feature_seq != 0).float()
            &#47&#47 set the padding as zero
            feature_mask<a id="change"> = </a><a id="change">feature_mask.unsqueeze(-1</a><a id="change">)</a>.expand_as(feature_emb)
            feature_emb = (feature_emb * feature_mask).sum(dim=-2) &#47&#47 [B Len H]

</code></pre><h3>After Change</h3><pre><code class='java'>

        sparse_embedding, dense_embedding = self.embed_input_fields(item_seq)
        &#47&#47 concat the sparse embedding and float embedding
        feature_table<a id="change"> = </a><a id="change">[]</a>
        if sparse_embedding is not None:
            feature_table.append(sparse_embedding)
        if dense_embedding is not None:
            <a id="change">feature_table.append(</a>dense_embedding<a id="change">)</a>

        feature_table = torch.cat(feature_table, dim=1)
        &#47&#47 [batch len num_features hidden_size]
        table_shape = feature_table.shape

        feat_num, embedding_size = table_shape[-2], table_shape[-1]
        feature_emb<a id="change"> = </a>feature_table.view(table_shape[:-2] + (feat_num * embedding_size,))
        feature_gru_output, _ = self.feature_gru_layers(feature_emb) &#47&#47 [B Len H]

        output_concat = torch.cat((item_gru_output, feature_gru_output), -1)  &#47&#47 [B Len 2*H]</code></pre>