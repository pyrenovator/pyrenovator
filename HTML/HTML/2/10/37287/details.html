<html><h3>Pattern ID :37287
</h3><img src='107391590.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 setup model
    loss_record, most_recent_encoder_path = model.learn(dataset_dict, pretrain_epochs, pretrain_batches)
    <a id="change">if </a>ppo_finetune and not isinstance(model, algos.RecurrentCPC):
        encoder_checkpoint<a id="change"> = </a>model.encoder_checkpoints_path
        all_checkpoints = glob(os.path.join(encoder_checkpoint, &quot*&quot))
        latest_checkpoint = max(all_checkpoints, key=os.path.getctime)
        encoder_feature_extractor_kwargs<a id="change"> = {</a>&quotfeatures_dim&quot: algo_params["representation_dim"],
                                            &quotencoder_path&quot: os.path.abspath(latest_checkpoint)<a id="change">}</a>

        &#47&#47 TODO figure out how to not have to set `ortho_init` to False for the whole policy
        policy_kwargs<a id="change"> = </a>{&quotfeatures_extractor_class&quot: EncoderFeatureExtractor,
                         &quotfeatures_extractor_kwargs&quot: encoder_feature_extractor_kwargs,
                         &quotortho_init&quot: False}
        ppo_model = PPO(policy=ActorCriticPolicy, env=venv,
                        verbose=1, policy_kwargs=policy_kwargs)
        ppo_model<a id="change"> = </a>initialize_non_features_extractor(ppo_model)
        ppo_model.learn(total_timesteps=ppo_timesteps)

    venv.close()</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 setup model
    loss_record, most_recent_encoder_path = model.learn(
        webdataset, pretrain_epochs, pretrain_batches)
    <a id="change">if </a>ppo_finetune and not isinstance(model, algos.RecurrentCPC):
        <a id="change">raise </a><a id="change">NotImplementedError("PPO fine-tuning is not currently supported"</a><a id="change">)</a>

        &#47&#47 encoder_checkpoint = model.encoder_checkpoints_path
        &#47&#47 all_checkpoints = glob(os.path.join(encoder_checkpoint, &quot*&quot))
        &#47&#47 latest_checkpoint = max(all_checkpoints, key=os.path.getctime)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 9</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/humancompatibleai/eirli/commit/47ccff934463fdfcd9a8887bff27286744d1fe17#diff-85e6dfcecb4f7bb11eda0f3c08b89808cf7f28a3eaf145199d43441864816161L123' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 107391590</div><div id='project'> Project Name: humancompatibleai/eirli</div><div id='commit'> Commit Name: 47ccff934463fdfcd9a8887bff27286744d1fe17</div><div id='time'> Time: 2020-11-16</div><div id='author'> Author: sam@qxcv.net</div><div id='file'> File Name: src/il_representations/scripts/run_rep_learner.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: run(11)</div><div id='n_method'> N Method Name: run(11)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/il_representations/scripts/run_rep_learner.py</div><div id='n_file'> N File Name: src/il_representations/scripts/run_rep_learner.py</div><div id='m_start'> M Start Line: 126</div><div id='m_end'> M End Line: 161</div><div id='n_start'> N Start Line: 123</div><div id='n_end'> N End Line: 165</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    &#47&#47 setup model
    loss_record, most_recent_encoder_path = model.learn(dataset_dict, pretrain_epochs, pretrain_batches)
    <a id="change">if </a>ppo_finetune and not isinstance(model, algos.RecurrentCPC):
        encoder_checkpoint<a id="change"> = </a>model.encoder_checkpoints_path
        all_checkpoints = glob(os.path.join(encoder_checkpoint, &quot*&quot))
        latest_checkpoint = max(all_checkpoints, key=os.path.getctime)
        encoder_feature_extractor_kwargs<a id="change"> = {</a>&quotfeatures_dim&quot: algo_params["representation_dim"],
                                            &quotencoder_path&quot: os.path.abspath(latest_checkpoint)<a id="change">}</a>

        &#47&#47 TODO figure out how to not have to set `ortho_init` to False for the whole policy
        policy_kwargs<a id="change"> = </a>{&quotfeatures_extractor_class&quot: EncoderFeatureExtractor,
                         &quotfeatures_extractor_kwargs&quot: encoder_feature_extractor_kwargs,
                         &quotortho_init&quot: False}
        ppo_model<a id="change"> = </a>PPO(policy=ActorCriticPolicy, env=venv,
                        verbose=1, policy_kwargs=policy_kwargs)
        ppo_model = initialize_non_features_extractor(ppo_model)
        ppo_model.learn(total_timesteps=ppo_timesteps)</code></pre><h3>After Change</h3><pre><code class='java'>
    &#47&#47 setup model
    loss_record, most_recent_encoder_path = model.learn(
        webdataset, pretrain_epochs, pretrain_batches)
    <a id="change">if </a>ppo_finetune and not isinstance(model, algos.RecurrentCPC):
        <a id="change">raise </a><a id="change">NotImplementedError("PPO fine-tuning is not currently supported"</a><a id="change">)</a>

        &#47&#47 encoder_checkpoint = model.encoder_checkpoints_path
        &#47&#47 all_checkpoints = glob(os.path.join(encoder_checkpoint, &quot*&quot))
        &#47&#47 latest_checkpoint = max(all_checkpoints, key=os.path.getctime)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/humancompatibleai/eirli/commit/1086682dee2c96d5c039e3190b1fec8ce4c683bd#diff-85e6dfcecb4f7bb11eda0f3c08b89808cf7f28a3eaf145199d43441864816161L114' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 107391596</div><div id='project'> Project Name: humancompatibleai/eirli</div><div id='commit'> Commit Name: 1086682dee2c96d5c039e3190b1fec8ce4c683bd</div><div id='time'> Time: 2020-11-16</div><div id='author'> Author: sam@qxcv.net</div><div id='file'> File Name: src/il_representations/scripts/run_rep_learner.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: run(11)</div><div id='n_method'> N Method Name: run(11)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/il_representations/scripts/run_rep_learner.py</div><div id='n_file'> N File Name: src/il_representations/scripts/run_rep_learner.py</div><div id='m_start'> M Start Line: 126</div><div id='m_end'> M End Line: 161</div><div id='n_start'> N Start Line: 123</div><div id='n_end'> N End Line: 165</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        assert (
            model_kwargs.get("stopping_criteria", None) is None
        ), "For RemoteGenerationMixin models use BloomConstraints instead of stopping_criteria"
        <a id="change">if </a>inputs is not None:
            assert isinstance(inputs, torch.Tensor) and inputs.ndim == 2, "inputs must be a 2d tensor [batch, length]"
        prefix_length = 0 if inputs is None else inputs.size(1)
        prefix_length += self.config.pre_seq_len</code></pre><h3>After Change</h3><pre><code class='java'>
        elif max_length is None and max_new_tokens is not None:
            max_length = prefix_length + max_new_tokens

        <a id="change">if </a>num_beams &gt; 1 and session is not None:
            <a id="change">raise </a><a id="change">NotImplementedError(
                "Reusing inference session in .generate() along with beam search is not supported yet"</a><a id="change">
            )</a>

        if inputs is not None:
            assert isinstance(inputs, torch.Tensor) and inputs.ndim == 2, "inputs must be a 2d tensor [batch, length]"
            if session is not None and session.last_token_id is not None:
                inputs<a id="change"> = </a>torch.cat(<a id="change">[</a>session.last_token_id, inputs<a id="change"></a>], dim=1)
        else:
            if session is not None and session.last_token_id is not None:
                inputs<a id="change"> = </a>session.last_token_id
            else:
                assert bos_token_id is not None, "You have to provide a bos_token_id if you do not provide inputs"
                inputs = torch.tensor([[bos_token_id]] * num_beams, dtype=torch.long, device=self.device)
        batch_size = inputs.size(0)

        if decoding_algorithm is None:
            if do_sample:
                decoding_algorithm = self._choose_sample_algorithm(temperature, top_k, top_p)
            elif num_beams is not None and num_beams &gt; 1:
                decoding_algorithm = BeamSearchAlgorithm(num_beams, batch_size=batch_size)
            else:
                decoding_algorithm = GreedyAlgorithm()

        if num_beams &gt; 1:
            inputs = torch.cat([inputs] * num_beams, dim=0)
            if batch_size &gt; 1:
                &#47&#47 TODO: resolve padding problem
                logger.warning(
                    f"You set batch_size {batch_size} within beam search generation. "
                    f"Be careful, results on sequences with different length may be padded wrong way"
                )

        if num_return_sequences is None:
            num_return_sequences = 1

        assert num_return_sequences &lt;= num_beams, (
            f"You want more sequences than the beam has."
            " Check num_return_sequences: {num_return_sequences} and num_beams: {num_beams}."
        )

        constraints = self._get_constraints(
            inputs=inputs,
            eos_token_id=eos_token_id,
            pad_token_id=pad_token_id,
            provided_constraints=provided_constraints,
        )

        if session is None:
            context_manager<a id="change"> = </a>self.inference_session(max_length=max_length)
        else:
            context_manager<a id="change"> = </a>contextlib.nullcontext(session)  &#47&#47 Doesn&quott actually enter session or exit from it
        with context_manager as session:
            outputs = []
            &#47&#47 Find samples with padded inputs.</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/bigscience-workshop/distributed-bloom/commit/e8fac92e591ff0d5d080ec0211b22b54ec5553d6#diff-e1500a8c01c4689039d03ea554ddedcc167eb2e8b3d8369d7fdb0047f897224dL30' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 107391613</div><div id='project'> Project Name: bigscience-workshop/distributed-bloom</div><div id='commit'> Commit Name: e8fac92e591ff0d5d080ec0211b22b54ec5553d6</div><div id='time'> Time: 2022-12-05</div><div id='author'> Author: borzunov.alexander@gmail.com</div><div id='file'> File Name: src/petals/client/remote_generation.py</div><div id='m_class'> M Class Name: RemoteGenerationMixin</div><div id='n_method'> N Class Name: RemoteGenerationMixin</div><div id='m_method'> M Method Name: generate(15)</div><div id='n_method'> N Method Name: generate(15)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/petals/client/remote_generation.py</div><div id='n_file'> N File Name: src/petals/client/remote_generation.py</div><div id='m_start'> M Start Line: 77</div><div id='m_end'> M End Line: 130</div><div id='n_start'> N Start Line: 106</div><div id='n_end'> N End Line: 198</div><BR>