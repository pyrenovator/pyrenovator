<html><h3>Pattern ID :17236
</h3><img src='57366023.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        if self._num_training_steps is None:
            if self.trainer.train_dataloader is None:
                <a id="change">try:
                    </a>dataloader<a id="change"> = </a>self.train_dataloader()
                <a id="change">except </a>NotImplementedError:
                    <a id="change">raise RuntimeError(
                        </a>"To use linear warmup cosine annealing lr"
                        "set the dataloader with .set_loaders(...)"<a id="change">
                    )</a>

            dataset_size = getattr(self, "dali_epoch_size", None) or len(dataloader.dataset)

            dataset_size = self.trainer.limit_train_batches * dataset_size</code></pre><h3>After Change</h3><pre><code class='java'>
        Compute the number of training steps for each epoch.

        if self._num_training_steps is None:
            <a id="change">try:
                </a>dataset = self.extra_args.get("dataset", None)
                if dataset not in ["cifar10", "cifar100", "stl10"]:
                    folder = os.path.join(self.extra_args["data_dir"], self.extra_args["train_dir"])
                else:
                    folder = None
                no_labels = self.extra_args.get("no_labels", False)
                data_fraction = self.extra_args.get("data_fraction", -1.0)

                dataset_size = compute_dataset_size(
                    dataset=dataset,
                    folder=folder,
                    train=True,
                    no_labels=no_labels,
                    data_fraction=data_fraction,
                )
            <a id="change">except</a>:
                <a id="change">raise </a><a id="change">RuntimeError(
                    "Please pass &quotdataset&quot or &quotdata_dir &quot"
                    "and &quottrain_dir&quot as parameters to the model."</a><a id="change">
                )</a>

            dataset_size = self.trainer.limit_train_batches * dataset_size

            num_devices = 1</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 9</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/vturrisi/contrastive-learning/commit/eb07a9c7c2872efb1ae83767f59a67fa616a7652#diff-bf0f79abb0e767723150e5fe64a4a5935492d2fcf2eb222cfe5f8c901bcd11b0L362' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 57366023</div><div id='project'> Project Name: vturrisi/contrastive-learning</div><div id='commit'> Commit Name: eb07a9c7c2872efb1ae83767f59a67fa616a7652</div><div id='time'> Time: 2022-05-02</div><div id='author'> Author: vt.turrisi@gmail.com</div><div id='file'> File Name: solo/methods/base.py</div><div id='m_class'> M Class Name: BaseMethod</div><div id='n_method'> N Class Name: BaseMethod</div><div id='m_method'> M Method Name: num_training_steps(1)</div><div id='n_method'> N Method Name: num_training_steps(1)</div><div id='m_parent_class'> M Parent Class: pl.LightningModule</div><div id='n_parent_class'> N Parent Class: pl.LightningModule</div><div id='m_file'> M File Name: solo/methods/base.py</div><div id='n_file'> N File Name: solo/methods/base.py</div><div id='m_start'> M Start Line: 383</div><div id='m_end'> M End Line: 392</div><div id='n_start'> N Start Line: 362</div><div id='n_end'> N End Line: 384</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        self._forward_hook = None
        self._gradient_hook = None

        <a id="change">try:
            </a>self._param<a id="change"> = </a>self._layer.__getattr__(self._param_name)  &#47&#47 type: Parameter
        <a id="change">except </a>Exception as err:
            <a id="change">raise RuntimeError(
                </a>"Error occurred while trying to get param {} in layer {}: {}".format(
                    self._param_name, self._layer, err
                )<a id="change">
            )</a>

        self._param_mask = torch.ones(self._param.shape)  &#47&#47 initialize to all ones
        self._param_init = None  &#47&#47 type: Tensor
        self._param_unmasked = None  &#47&#47 type: Tensor</code></pre><h3>After Change</h3><pre><code class='java'>

        self._params = []  &#47&#47 type: List[Parameter]
        for layer, param_name in zip(self._layers, self._param_names):
            <a id="change">try:
                </a>self._params.append(layer.__getattr__(param_name))
            <a id="change">except </a>Exception as err:
                <a id="change">raise </a><a id="change">RuntimeError(
                    f"Error occurred while trying to get param {param_name} "
                    f"in layer {layer}: {err}"</a><a id="change">
                )</a>

        &#47&#47 initialize masks to all ones
        self._param_masks = [torch.ones(param.shape) for param in self._params]
        self._params_init = [None] * len(self._layers)  &#47&#47 type: List[Tensor]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/neuralmagic/sparseml/commit/fe5523fe7887a86a5909fd72a531f4577fab358a#diff-212a6d478d3d26572d47b2acd34b0b64e29f79f7bc89469e28e8bbf17b50dae7L41' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 57366022</div><div id='project'> Project Name: neuralmagic/sparseml</div><div id='commit'> Commit Name: fe5523fe7887a86a5909fd72a531f4577fab358a</div><div id='time'> Time: 2021-04-09</div><div id='author'> Author: bfineran@users.noreply.github.com</div><div id='file'> File Name: src/sparseml/pytorch/optim/mask_pruning.py</div><div id='m_class'> M Class Name: ModuleParamPruningMask</div><div id='n_method'> N Class Name: ModuleParamPruningMask</div><div id='m_method'> M Method Name: __init__(9)</div><div id='n_method'> N Method Name: __init__(8)</div><div id='m_parent_class'> M Parent Class: object</div><div id='n_parent_class'> N Parent Class: object</div><div id='m_file'> M File Name: src/sparseml/pytorch/optim/mask_pruning.py</div><div id='n_file'> N File Name: src/sparseml/pytorch/optim/mask_pruning.py</div><div id='m_start'> M Start Line: 43</div><div id='m_end'> M End Line: 92</div><div id='n_start'> N Start Line: 63</div><div id='n_end'> N End Line: 107</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        if self._num_training_steps is None:
            if self.trainer.train_dataloader is None:
                <a id="change">try:
                    </a>dataloader<a id="change"> = </a>self.train_dataloader()
                <a id="change">except </a>NotImplementedError:
                    <a id="change">raise RuntimeError(
                        </a>"To use linear warmup cosine annealing lr"
                        "set the dataloader with .set_loaders(...)"<a id="change">
                    )</a>

            dataset_size = getattr(self, "dali_epoch_size", None) or len(dataloader.dataset)

            dataset_size = self.trainer.limit_train_batches * dataset_size</code></pre><h3>After Change</h3><pre><code class='java'>
        Compute the number of training steps for each epoch.

        if self._num_training_steps is None:
            <a id="change">try:
                </a>dataset = self.extra_args.get("dataset", None)
                if dataset not in ["cifar10", "cifar100", "stl10"]:
                    folder = os.path.join(self.extra_args["data_dir"], self.extra_args["train_dir"])
                else:
                    folder = None
                no_labels = self.extra_args.get("no_labels", False)
                data_fraction = self.extra_args.get("data_fraction", -1.0)

                dataset_size = compute_dataset_size(
                    dataset=dataset,
                    folder=folder,
                    train=True,
                    no_labels=no_labels,
                    data_fraction=data_fraction,
                )
            <a id="change">except</a>:
                <a id="change">raise </a><a id="change">RuntimeError(
                    "Please pass &quotdataset&quot or &quotdata_dir &quot"
                    "and &quottrain_dir&quot as parameters to the model."</a><a id="change">
                )</a>

            dataset_size = self.trainer.limit_train_batches * dataset_size

            num_devices = 1</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/vturrisi/contrastive-learning/commit/eb07a9c7c2872efb1ae83767f59a67fa616a7652#diff-306471ca72f10d091f7adabfddc03e702f662348314d589d172c5c700eae27abL195' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 57366021</div><div id='project'> Project Name: vturrisi/contrastive-learning</div><div id='commit'> Commit Name: eb07a9c7c2872efb1ae83767f59a67fa616a7652</div><div id='time'> Time: 2022-05-02</div><div id='author'> Author: vt.turrisi@gmail.com</div><div id='file'> File Name: solo/methods/linear.py</div><div id='m_class'> M Class Name: LinearModel</div><div id='n_method'> N Class Name: LinearModel</div><div id='m_method'> M Method Name: num_training_steps(1)</div><div id='n_method'> N Method Name: num_training_steps(1)</div><div id='m_parent_class'> M Parent Class: pl.LightningModule</div><div id='n_parent_class'> N Parent Class: pl.LightningModule</div><div id='m_file'> M File Name: solo/methods/linear.py</div><div id='n_file'> N File Name: solo/methods/linear.py</div><div id='m_start'> M Start Line: 199</div><div id='m_end'> M End Line: 208</div><div id='n_start'> N Start Line: 184</div><div id='n_end'> N End Line: 206</div><BR>