<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            itertools.accumulate(batch_size_per_feature)
        )
        recat_per_feature = recat
        recat = <a id="change">[]</a>
        <a id="change">for </a>r in recat_per_feature<a id="change">:
            </a><a id="change">recat.extend(
                </a>list(
                    range(
                        batch_size_per_feature_cumsum[r],
                        batch_size_per_feature_cumsum[r + 1],
                    )
                )<a id="change">
            )</a>

    return recat

</code></pre><h3>After Change</h3><pre><code class='java'>
    with record_function("&#47&#47&#47&#47 all2all_data:recat_permute_gen &#47&#47&#47&#47"):
        recat: List[int] = []

        <a id="change">if local_split == 0</a>:
            return torch.tensor(recat, device=device, dtype=torch.int32)

        feature_order: List[int] = [
            x + num_splits // stagger * y
            for x in range(num_splits // stagger)
            for y in range(stagger)
        ]

        for i in range(local_split):
            for j in feature_order:  &#47&#47 range(num_splits):
                recat.append(i + j * local_split)

        &#47&#47 variable batch size
        if batch_size_per_rank is not None:
            batch_size_per_feature = list(
                itertools.chain.from_iterable(
                    itertools.repeat(x, local_split) for x in batch_size_per_rank
                )
            )
            permuted_batch_size_per_feature = [batch_size_per_feature[r] for r in recat]
            input_offset = [0] + list(itertools.accumulate(batch_size_per_feature))
            output_offset = [0]<a id="change"> + </a>list(
                itertools.accumulate(permuted_batch_size_per_feature)
            )
            recat_tensor<a id="change"> = </a>torch.tensor(
                recat,
                device=device,
                dtype=torch.int32,
            )
            input_offset_tensor = torch.tensor(
                input_offset,
                device=device,
                dtype=torch.int32,
            )
            output_offset_tensor<a id="change"> = </a>torch.tensor(
                output_offset,
                device=device,
                dtype=torch.int32,
            )
            recat = torch.ops.fbgemm.expand_into_jagged_permute(
                recat_tensor,
                input_offset_tensor,
                output_offset_tensor,
                output_offset[-1],
            )
            return recat
        else:
            <a id="change">return </a>torch.tensor(recat, device=device, dtype=torch.int32)


def _split_lengths(</code></pre>