<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def named_parameters(
        self, prefix: str = "", recurse: bool = True, remove_duplicate: bool = True
    ) -&gt; Iterator[Tuple[str, nn.Parameter]]:
        <a id="change">yield from </a><a id="change">()</a>

    def flush(self) -&gt; None:
        self._emb_module.flush()
</code></pre><h3>After Change</h3><pre><code class='java'>
    def named_parameters(
        self, prefix: str = "", recurse: bool = True, remove_duplicate: bool = True
    ) -&gt; Iterator[Tuple[str, nn.Parameter]]:
        <a id="change">for </a>name, <a id="change">tensor</a> in <a id="change">self.named_split_embedding_weights(
            </a>prefix, recurse, remove_duplicate<a id="change">
        ):
            &#47&#47 hack before we support optimizer on sharded parameter level
            </a>param<a id="change"> = </a><a id="change">nn.Parameter(tensor</a><a id="change">)</a>
            &#47&#47 pyre-ignore
            param._overlapped_optimizer<a id="change"> = </a>True
            <a id="change">yield </a>name<a id="change">, param</a>

    def flush(self) -&gt; None:
        self._emb_module.flush()
</code></pre>