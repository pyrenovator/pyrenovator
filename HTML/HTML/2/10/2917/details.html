<html><h3>Pattern ID :2917
</h3><img src='11383772.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def named_parameters(
        self, prefix: str = "", recurse: bool = True, remove_duplicate: bool = True
    ) -&gt; Iterator[Tuple[str, nn.Parameter]]:
        <a id="change">yield from </a><a id="change">()</a>

    def flush(self) -&gt; None:
        self._emb_module.flush()
</code></pre><h3>After Change</h3><pre><code class='java'>
    def named_parameters(
        self, prefix: str = "", recurse: bool = True, remove_duplicate: bool = True
    ) -&gt; Iterator[Tuple[str, nn.Parameter]]:
        <a id="change">for </a>name, <a id="change">tensor</a> in <a id="change">self.named_split_embedding_weights(
            </a>prefix, recurse, remove_duplicate<a id="change">
        ):
            &#47&#47 hack before we support optimizer on sharded parameter level
            </a>param<a id="change"> = </a><a id="change">nn.Parameter(tensor</a><a id="change">)</a>
            &#47&#47 pyre-ignore
            param._overlapped_optimizer<a id="change"> = </a>True
            <a id="change">yield </a>name<a id="change">, param</a>

    def flush(self) -&gt; None:
        self._emb_module.flush()
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 9</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/e8ab2de1cfb9e312fea24290fd74e50f71c8acf3#diff-f743d49eaf5a79885b5b70ca6c2790355c1cbadf8aab5605549ab11c68545109L421' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 11383772</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: e8ab2de1cfb9e312fea24290fd74e50f71c8acf3</div><div id='time'> Time: 2022-12-16</div><div id='author'> Author: colin2328@meta.com</div><div id='file'> File Name: torchrec/distributed/batched_embedding_kernel.py</div><div id='m_class'> M Class Name: BatchedFusedEmbedding</div><div id='n_method'> N Class Name: BatchedFusedEmbedding</div><div id='m_method'> M Method Name: named_parameters(4)</div><div id='n_method'> N Method Name: named_parameters(4)</div><div id='m_parent_class'> M Parent Class: FusedOptimizerModule,BaseBatchedEmbedding</div><div id='n_parent_class'> N Parent Class: FusedOptimizerModule,BaseBatchedEmbedding</div><div id='m_file'> M File Name: torchrec/distributed/batched_embedding_kernel.py</div><div id='n_file'> N File Name: torchrec/distributed/batched_embedding_kernel.py</div><div id='m_start'> M Start Line: 421</div><div id='m_end'> M End Line: 421</div><div id='n_start'> N Start Line: 422</div><div id='n_end'> N End Line: 431</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def named_parameters(
        self, prefix: str = "", recurse: bool = True, remove_duplicate: bool = True
    ) -&gt; Iterator[Tuple[str, nn.Parameter]]:
        <a id="change">yield from </a><a id="change">()</a>

    def flush(self) -&gt; None:
        self._emb_module.flush()
</code></pre><h3>After Change</h3><pre><code class='java'>
    def named_parameters(
        self, prefix: str = "", recurse: bool = True, remove_duplicate: bool = True
    ) -&gt; Iterator[Tuple[str, nn.Parameter]]:
        <a id="change">for </a>name, <a id="change">tensor</a> in <a id="change">self.named_split_embedding_weights(
            </a>prefix, recurse, remove_duplicate<a id="change">
        ):
            &#47&#47 hack before we support optimizer on sharded parameter level
            </a>param<a id="change"> = </a><a id="change">nn.Parameter(</a>tensor<a id="change">)</a>
            &#47&#47 pyre-ignore
            param._overlapped_optimizer<a id="change"> = </a>True
            <a id="change">yield </a>name<a id="change">, param</a>

    def flush(self) -&gt; None:
        self._emb_module.flush()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/e8ab2de1cfb9e312fea24290fd74e50f71c8acf3#diff-f743d49eaf5a79885b5b70ca6c2790355c1cbadf8aab5605549ab11c68545109L418' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 11383769</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: e8ab2de1cfb9e312fea24290fd74e50f71c8acf3</div><div id='time'> Time: 2022-12-16</div><div id='author'> Author: colin2328@meta.com</div><div id='file'> File Name: torchrec/distributed/batched_embedding_kernel.py</div><div id='m_class'> M Class Name: BatchedFusedEmbedding</div><div id='n_method'> N Class Name: BatchedFusedEmbedding</div><div id='m_method'> M Method Name: named_parameters(4)</div><div id='n_method'> N Method Name: named_parameters(4)</div><div id='m_parent_class'> M Parent Class: FusedOptimizerModule,BaseBatchedEmbedding</div><div id='n_parent_class'> N Parent Class: FusedOptimizerModule,BaseBatchedEmbedding</div><div id='m_file'> M File Name: torchrec/distributed/batched_embedding_kernel.py</div><div id='n_file'> N File Name: torchrec/distributed/batched_embedding_kernel.py</div><div id='m_start'> M Start Line: 421</div><div id='m_end'> M End Line: 421</div><div id='n_start'> N Start Line: 422</div><div id='n_end'> N End Line: 431</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    def named_parameters(
        self, prefix: str = "", recurse: bool = True, remove_duplicate: bool = True
    ) -&gt; Iterator[Tuple[str, nn.Parameter]]:
        <a id="change">yield from </a><a id="change">()</a>

    def flush(self) -&gt; None:
        self._emb_module.flush()
</code></pre><h3>After Change</h3><pre><code class='java'>
    def named_parameters(
        self, prefix: str = "", recurse: bool = True, remove_duplicate: bool = True
    ) -&gt; Iterator[Tuple[str, nn.Parameter]]:
        <a id="change">for </a>name, <a id="change">tensor</a> in <a id="change">self.named_split_embedding_weights(
            </a>prefix, recurse, remove_duplicate<a id="change">
        ):
            &#47&#47 hack before we support optimizer on sharded parameter level
            </a>param<a id="change"> = </a><a id="change">nn.Parameter(</a>tensor<a id="change">)</a>
            &#47&#47 pyre-ignore
            param._overlapped_optimizer<a id="change"> = </a>True
            <a id="change">yield </a>name<a id="change">, param</a>

    def flush(self) -&gt; None:
        self._emb_module.flush()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/e8ab2de1cfb9e312fea24290fd74e50f71c8acf3#diff-f743d49eaf5a79885b5b70ca6c2790355c1cbadf8aab5605549ab11c68545109L650' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 11383771</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: e8ab2de1cfb9e312fea24290fd74e50f71c8acf3</div><div id='time'> Time: 2022-12-16</div><div id='author'> Author: colin2328@meta.com</div><div id='file'> File Name: torchrec/distributed/batched_embedding_kernel.py</div><div id='m_class'> M Class Name: BatchedFusedEmbeddingBag</div><div id='n_method'> N Class Name: BatchedFusedEmbeddingBag</div><div id='m_method'> M Method Name: named_parameters(4)</div><div id='n_method'> N Method Name: named_parameters(4)</div><div id='m_parent_class'> M Parent Class: FusedOptimizerModule,BaseBatchedEmbeddingBag</div><div id='n_parent_class'> N Parent Class: FusedOptimizerModule,BaseBatchedEmbeddingBag</div><div id='m_file'> M File Name: torchrec/distributed/batched_embedding_kernel.py</div><div id='n_file'> N File Name: torchrec/distributed/batched_embedding_kernel.py</div><div id='m_start'> M Start Line: 653</div><div id='m_end'> M End Line: 653</div><div id='n_start'> N Start Line: 662</div><div id='n_end'> N End Line: 671</div><BR>