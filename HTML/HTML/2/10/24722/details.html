<html><h3>Pattern ID :24722
</h3><img src='76592848.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        r = torch.rand(20, 10, 2, 2, dtype=torch.float) * 4 - 2
        dtype = torch.qint8
        scales = torch.rand(10) * 0.02 + 0.01
        zero_points<a id="change"> = </a>torch.round(torch.rand(10) * 2 - 1).to(torch.long)
        qr = torch.quantize_per_channel(r, scales, zero_points, 1, dtype)

        &#47&#47 we can&quott reorder the axis
        with self.assertRaises(RuntimeError):
            qr.transpose(0, 1)

        &#47&#47 but we can change memory format
        qlast<a id="change"> = </a>qr.contiguous(memory_format=torch.channels_last)
        <a id="change">self.assertEqual(</a>qr.stride(), list(reversed(sorted(qr.stride())))<a id="change">)</a>
        <a id="change">self.assertNotEqual(</a>qlast.stride(), list(reversed(sorted(qlast.stride())))<a id="change">)</a>
        self.assertEqual(qr.int_repr(), qlast.int_repr())
        &#47&#47 TODO(&#47&#4738095): Replace assertEqualIgnoreType. See issue &#47&#4738095
        self.assertEqualIgnoreType(scales, qlast.q_per_channel_scales())
        self.assertEqual(zero_points, qlast.q_per_channel_zero_points())</code></pre><h3>After Change</h3><pre><code class='java'>
                qx.permute([1, 0])

    def test_qtensor_per_channel_permute(self):
        <a id="change">for device</a> in get_supported_device_types()<a id="change">:
            </a>r = torch.rand(20, 10, 2, 2, dtype=torch.float, device=device) * 4 - 2
            dtype = torch.qint8
            scales = torch.rand(10, device=device) * 0.02 + 0.01
            zero_points<a id="change"> = </a>torch.round(torch.rand(10, device=device) * 2 - 1).to(torch.long)
            qr = torch.quantize_per_channel(r, scales, zero_points, 1, dtype)

            &#47&#47 we can&quott reorder the axis
            with self.assertRaises(RuntimeError):
                qr.transpose(0, 1)

            &#47&#47 but we can change memory format
            qlast<a id="change"> = </a>qr.contiguous(memory_format=torch.channels_last)
            <a id="change">self.assertEqual(</a>qr.stride(), list(reversed(sorted(qr.stride())))<a id="change">)</a>
            <a id="change">self.assertNotEqual(</a>qlast.stride(), list(reversed(sorted(qlast.stride())))<a id="change">)</a>
            self.assertEqual(qr.int_repr(), qlast.int_repr())
            &#47&#47 TODO(&#47&#4738095): Replace assertEqualIgnoreType. See issue &#47&#4738095
            self.assertEqualIgnoreType(scales, qlast.q_per_channel_scales())
            self.assertEqual(zero_points, qlast.q_per_channel_zero_points())</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 9</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/d51b437b740444cd4fff32068ab1e3b3be10c9e1#diff-3497fd7b86986c69542de39ddb63f340c63445596a813fab8c7a60036831b65eL570' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 76592848</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: d51b437b740444cd4fff32068ab1e3b3be10c9e1</div><div id='time'> Time: 2021-07-07</div><div id='author'> Author: cdhernandez@fb.com</div><div id='file'> File Name: test/quantization/core/test_quantized_tensor.py</div><div id='m_class'> M Class Name: TestQuantizedTensor</div><div id='n_method'> N Class Name: TestQuantizedTensor</div><div id='m_method'> M Method Name: test_qtensor_per_channel_permute(1)</div><div id='n_method'> N Method Name: test_qtensor_per_channel_permute(1)</div><div id='m_parent_class'> M Parent Class: TestCase</div><div id='n_parent_class'> N Parent Class: TestCase</div><div id='m_file'> M File Name: test/quantization/core/test_quantized_tensor.py</div><div id='n_file'> N File Name: test/quantization/core/test_quantized_tensor.py</div><div id='m_start'> M Start Line: 570</div><div id='m_end'> M End Line: 589</div><div id='n_start'> N Start Line: 629</div><div id='n_end'> N End Line: 651</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def test_qtensor_resize(self):
        scale, zero_point, dtype = 1.0, 2, torch.uint8
        sizes1<a id="change"> = </a>[1, 2, 3, 4]
        sizes2 = [1 * 2, 3 * 4]
        sizes3 = [1, 2 * 3, 4]
        sizes4 = [1 * 2 * 3 * 4]
        sizes5 = [1, 2, 1, 3, 1, 4]

        q1_int = torch.randint(0, 100, sizes1, dtype=dtype)
        q1 = torch._make_per_tensor_quantized_tensor(q1_int, scale=scale, zero_point=zero_point)
        q2 = q1.resize(*sizes2)
        q3 = q2.resize(*sizes3)
        q4 = q3.resize(*sizes4)
        q5 = q4.resize(*sizes5)

        self.assertEqual(q1.numel(), q2.numel())
        self.assertEqual(q1.numel(), q3.numel())
        self.assertEqual(q1.numel(), q4.numel())
        <a id="change">self.assertEqual(</a>q1.numel(), q5.numel()<a id="change">)</a>

        &#47&#47 Compare original and post-transpose
        a_int<a id="change"> = </a>torch.randint(0, 100, sizes1, dtype=dtype)
        a = torch._make_per_tensor_quantized_tensor(a_int, scale=scale, zero_point=zero_point)
        b = a.transpose(1, 2)  &#47&#47 swaps 2nd and 3rd dimension
        c = b.resize(*sizes1)  &#47&#47 Change the sizes back to the original

        self.assertEqual(a.size(), c.size())
        self.assertEqual(b.q_scale(), c.q_scale())
        self.assertEqual(b.q_zero_point(), c.q_zero_point())
        <a id="change">self.assertNotEqual(</a>b.stride(), c.stride()<a id="change">)</a>
        &#47&#47 size is the same but the underlying data is different
        self.assertNotEqual(b.int_repr(), c.int_repr())
        self.assertFalse(torch.equal(b, c))
</code></pre><h3>After Change</h3><pre><code class='java'>

    def test_qtensor_resize(self):
        scale, zero_point, dtype = 1.0, 2, torch.uint8
        <a id="change">for device</a> in get_supported_device_types()<a id="change">:
            </a>sizes1<a id="change"> = </a>[1, 2, 3, 4]
            sizes2 = [1 * 2, 3 * 4]
            sizes3 = [1, 2 * 3, 4]
            sizes4 = [1 * 2 * 3 * 4]
            sizes5 = [1, 2, 1, 3, 1, 4]

            q1_int = torch.randint(0, 100, sizes1, dtype=dtype, device=device)
            q1 = torch._make_per_tensor_quantized_tensor(q1_int, scale=scale, zero_point=zero_point)
            q2 = q1.resize(*sizes2)
            q3 = q2.resize(*sizes3)
            q4 = q3.resize(*sizes4)
            q5 = q4.resize(*sizes5)

            self.assertEqual(q1.numel(), q2.numel())
            self.assertEqual(q1.numel(), q3.numel())
            self.assertEqual(q1.numel(), q4.numel())
            <a id="change">self.assertEqual(</a>q1.numel(), q5.numel()<a id="change">)</a>

            &#47&#47 Compare original and post-transpose
            a_int<a id="change"> = </a>torch.randint(0, 100, sizes1, dtype=dtype, device=device)
            a = torch._make_per_tensor_quantized_tensor(a_int, scale=scale, zero_point=zero_point)
            b = a.transpose(1, 2)  &#47&#47 swaps 2nd and 3rd dimension
            c = b.resize(*sizes1)  &#47&#47 Change the sizes back to the original

            self.assertEqual(a.size(), c.size())
            self.assertEqual(b.q_scale(), c.q_scale())
            self.assertEqual(b.q_zero_point(), c.q_zero_point())
            <a id="change">self.assertNotEqual(</a>b.stride(), c.stride()<a id="change">)</a>
            &#47&#47 size is the same but the underlying data is different
            self.assertNotEqual(b.int_repr(), c.int_repr())
            &#47&#47 torch.equal is not supported for the cuda backend
            if device == &quotcpu&quot:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/994ce7dbd9229b94f913d4616727c2a3e8d172a6#diff-3497fd7b86986c69542de39ddb63f340c63445596a813fab8c7a60036831b65eL721' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 76592849</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 994ce7dbd9229b94f913d4616727c2a3e8d172a6</div><div id='time'> Time: 2021-07-07</div><div id='author'> Author: cdhernandez@fb.com</div><div id='file'> File Name: test/quantization/core/test_quantized_tensor.py</div><div id='m_class'> M Class Name: TestQuantizedTensor</div><div id='n_method'> N Class Name: TestQuantizedTensor</div><div id='m_method'> M Method Name: test_qtensor_resize(1)</div><div id='n_method'> N Method Name: test_qtensor_resize(1)</div><div id='m_parent_class'> M Parent Class: TestCase</div><div id='n_parent_class'> N Parent Class: TestCase</div><div id='m_file'> M File Name: test/quantization/core/test_quantized_tensor.py</div><div id='n_file'> N File Name: test/quantization/core/test_quantized_tensor.py</div><div id='m_start'> M Start Line: 723</div><div id='m_end'> M End Line: 763</div><div id='n_start'> N Start Line: 762</div><div id='n_end'> N End Line: 807</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    def test_fake_quant_control(self):
        torch.manual_seed(42)
        X<a id="change"> = </a>torch.rand(20, 10, dtype=torch.float32)
        fq_module = torch.quantization.default_fake_quant()
        &#47&#47 Output of fake quant is not identical to input
        Y = fq_module(X)
        self.assertNotEqual(Y, X)
        torch.quantization.disable_fake_quant(fq_module)
        X = torch.rand(20, 10, dtype=torch.float32)
        Y = fq_module(X)
        &#47&#47 Fake quant is disabled,output is identical to input
        <a id="change">self.assertEqual(</a>Y, X<a id="change">)</a>

        &#47&#47 Explicit copy at this point in time, because FakeQuant keeps internal
        &#47&#47 state in mutable buffers.
        scale = fq_module.scale.clone().detach()
        zero_point = fq_module.zero_point.clone().detach()

        torch.quantization.disable_observer(fq_module)
        torch.quantization.enable_fake_quant(fq_module)
        X = 10.0 * torch.rand(20, 10, dtype=torch.float32) - 5.0
        Y<a id="change"> = </a>fq_module(X)
        <a id="change">self.assertNotEqual(</a>Y, X<a id="change">)</a>
        &#47&#47 Observer is disabled, scale and zero-point do not change
        self.assertEqual(fq_module.scale, scale)
        self.assertEqual(fq_module.zero_point, zero_point)
        torch.quantization.enable_observer(fq_module)</code></pre><h3>After Change</h3><pre><code class='java'>
            self.assertEqual(loaded_fq_module.calculate_qparams(), fq_module.calculate_qparams())

    def test_fake_quant_control(self):
        <a id="change">for fq_module</a> in [torch.quantization.default_fake_quant(),
                          _LearnableFakeQuantize.with_args(observer=MovingAverageMinMaxObserver, quant_min=0,
                                                           quant_max=255,
                                                           dtype=torch.quint8, qscheme=torch.per_tensor_affine,
                                                           reduce_range=True)()]<a id="change">:
            </a>torch.manual_seed(42)
            X<a id="change"> = </a>torch.rand(20, 10, dtype=torch.float32)
            &#47&#47 Output of fake quant is not identical to input
            Y = fq_module(X)
            self.assertNotEqual(Y, X)
            if type(fq_module) == _LearnableFakeQuantize:
                fq_module.toggle_fake_quant(False)
            else:
                torch.quantization.disable_fake_quant(fq_module)
            X = torch.rand(20, 10, dtype=torch.float32)
            Y = fq_module(X)
            &#47&#47 Fake quant is disabled,output is identical to input
            <a id="change">self.assertEqual(</a>Y, X<a id="change">)</a>

            &#47&#47 Explicit copy at this point in time, because FakeQuant keeps internal
            &#47&#47 state in mutable buffers.
            scale = fq_module.scale.clone().detach()
            zero_point = fq_module.zero_point.clone().detach()

            if type(fq_module) == _LearnableFakeQuantize:
                fq_module.toggle_observer_update(False)
                fq_module.toggle_fake_quant(True)
            else:
                torch.quantization.disable_observer(fq_module)
                torch.quantization.enable_fake_quant(fq_module)
            X = 10.0 * torch.rand(20, 10, dtype=torch.float32) - 5.0
            Y<a id="change"> = </a>fq_module(X)
            <a id="change">self.assertNotEqual(</a>Y, X<a id="change">)</a>
            &#47&#47 Observer is disabled, scale and zero-point do not change
            self.assertEqual(fq_module.scale, scale)
            self.assertEqual(fq_module.zero_point, zero_point)
            if type(fq_module) == _LearnableFakeQuantize:</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/pytorch/pytorch/commit/0c60922fb0614132433779ad45ab8f30783db2ae#diff-a1ad3ccb2c291d8bbc6e0a986f133b7da6bb380e3dd5d2ae6e428c4efd780738L1178' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 76592862</div><div id='project'> Project Name: pytorch/pytorch</div><div id='commit'> Commit Name: 0c60922fb0614132433779ad45ab8f30783db2ae</div><div id='time'> Time: 2021-02-03</div><div id='author'> Author: haichuan@fb.com</div><div id='file'> File Name: test/quantization/test_workflow_module.py</div><div id='m_class'> M Class Name: TestFakeQuantize</div><div id='n_method'> N Class Name: TestFakeQuantize</div><div id='m_method'> M Method Name: test_fake_quant_control(1)</div><div id='n_method'> N Method Name: test_fake_quant_control(1)</div><div id='m_parent_class'> M Parent Class: TestCase</div><div id='n_parent_class'> N Parent Class: TestCase</div><div id='m_file'> M File Name: test/quantization/test_workflow_module.py</div><div id='n_file'> N File Name: test/quantization/test_workflow_module.py</div><div id='m_start'> M Start Line: 1179</div><div id='m_end'> M End Line: 1209</div><div id='n_start'> N Start Line: 1119</div><div id='n_end'> N End Line: 1165</div><BR>