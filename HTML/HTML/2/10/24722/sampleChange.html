<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        r = torch.rand(20, 10, 2, 2, dtype=torch.float) * 4 - 2
        dtype = torch.qint8
        scales = torch.rand(10) * 0.02 + 0.01
        zero_points<a id="change"> = </a>torch.round(torch.rand(10) * 2 - 1).to(torch.long)
        qr = torch.quantize_per_channel(r, scales, zero_points, 1, dtype)

        &#47&#47 we can&quott reorder the axis
        with self.assertRaises(RuntimeError):
            qr.transpose(0, 1)

        &#47&#47 but we can change memory format
        qlast<a id="change"> = </a>qr.contiguous(memory_format=torch.channels_last)
        <a id="change">self.assertEqual(</a>qr.stride(), list(reversed(sorted(qr.stride())))<a id="change">)</a>
        <a id="change">self.assertNotEqual(</a>qlast.stride(), list(reversed(sorted(qlast.stride())))<a id="change">)</a>
        self.assertEqual(qr.int_repr(), qlast.int_repr())
        &#47&#47 TODO(&#47&#4738095): Replace assertEqualIgnoreType. See issue &#47&#4738095
        self.assertEqualIgnoreType(scales, qlast.q_per_channel_scales())
        self.assertEqual(zero_points, qlast.q_per_channel_zero_points())</code></pre><h3>After Change</h3><pre><code class='java'>
                qx.permute([1, 0])

    def test_qtensor_per_channel_permute(self):
        <a id="change">for device</a> in get_supported_device_types()<a id="change">:
            </a>r = torch.rand(20, 10, 2, 2, dtype=torch.float, device=device) * 4 - 2
            dtype = torch.qint8
            scales = torch.rand(10, device=device) * 0.02 + 0.01
            zero_points<a id="change"> = </a>torch.round(torch.rand(10, device=device) * 2 - 1).to(torch.long)
            qr = torch.quantize_per_channel(r, scales, zero_points, 1, dtype)

            &#47&#47 we can&quott reorder the axis
            with self.assertRaises(RuntimeError):
                qr.transpose(0, 1)

            &#47&#47 but we can change memory format
            qlast<a id="change"> = </a>qr.contiguous(memory_format=torch.channels_last)
            <a id="change">self.assertEqual(</a>qr.stride(), list(reversed(sorted(qr.stride())))<a id="change">)</a>
            <a id="change">self.assertNotEqual(</a>qlast.stride(), list(reversed(sorted(qlast.stride())))<a id="change">)</a>
            self.assertEqual(qr.int_repr(), qlast.int_repr())
            &#47&#47 TODO(&#47&#4738095): Replace assertEqualIgnoreType. See issue &#47&#4738095
            self.assertEqualIgnoreType(scales, qlast.q_per_channel_scales())
            self.assertEqual(zero_points, qlast.q_per_channel_zero_points())</code></pre>