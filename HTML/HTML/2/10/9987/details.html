<html><h3>Pattern ID :9987
</h3><img src='35640767.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if small_parameter(param_name, grad):  &#47&#47 Do adam update for small parameters
            grad = adam(inner_ctx, grad, step)
        else:  &#47&#47 Do shampoo-sm3 update for large parameters
            grad = <a id="change">shampoo(inner_ctx</a>, <a id="change">param_name</a>, <a id="change">grad</a>, step<a id="change">)</a> * <a id="change">sm3(inner_ctx</a>, <a id="change">param_name</a>, <a id="change">grad</a><a id="change">)</a>
            grad = ema(inner_ctx, grad, step, 1 - ctx.optimizer.momentum_beta, "momentum", heavyball=True)
            ctx.parameters[param_name] = (1 + ctx.optimizer.weight_decay * parameter_lr) * ctx.parameters[param_name]
        ctx.parameters[param_name] = grad * parameter_lr + ctx.parameters[param_name]
</code></pre><h3>After Change</h3><pre><code class='java'>
    ctx = ctx.add_to_prefix("optimizer")
    lr = -get_current_lr(ctx, step)

    for <a id="change">param_name</a>, grad in grads.items():
        if "optimizer" in param_name:
            continue
        inner_ctx = ctx.add_to_prefix(param_name, count=False)
        parameter_lr = lr * ctx.parameter_variance.get(param_name, 1)
        grad = grad.astype(ctx.model.storage_dtype)
        grad = adaptive_gradient_clipping(ctx, param_name, grad)

        if small_parameter(param_name, grad):  &#47&#47 Do adam update for small parameters
            grad = adam(inner_ctx, grad, step)
        else:  &#47&#47 Do shampoo-sm3 update for large parameters
            grad = graft(<a id="change">sm3(inner_ctx</a>, <a id="change">param_name</a>, <a id="change">grad</a><a id="change">)</a>, <a id="change">shampoo(inner_ctx</a>, <a id="change">param_name</a>, <a id="change">grad</a>, step<a id="change">)</a>)
            grad = ema(inner_ctx, grad, step, 1 - ctx.optimizer.momentum_beta, "momentum", heavyball=True)
            ctx.parameters[param_name] = (1 + ctx.optimizer.weight_decay * parameter_lr) * ctx.parameters[param_name]
        ctx.parameters[param_name] = grad * parameter_lr + ctx.parameters[param_name]</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/homebrewnlp/olmax/commit/b03cf79882fc5c6be7208950c3d21094c80f7f8d#diff-4540d7c8541eb25568b448ea775d64c43be83ce020bc67dd8340da4d4e804846L115' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 35640767</div><div id='project'> Project Name: homebrewnlp/olmax</div><div id='commit'> Commit Name: b03cf79882fc5c6be7208950c3d21094c80f7f8d</div><div id='time'> Time: 2022-05-18</div><div id='author'> Author: 39779310+ClashLuke@users.noreply.github.com</div><div id='file'> File Name: src/optimizer.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: update(3)</div><div id='n_method'> N Method Name: update(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/optimizer.py</div><div id='n_file'> N File Name: src/optimizer.py</div><div id='m_start'> M Start Line: 115</div><div id='m_end'> M End Line: 126</div><div id='n_start'> N Start Line: 120</div><div id='n_end'> N End Line: 131</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    ctx = ctx.add_to_prefix("optimizer")
    lr = -get_current_lr(ctx, step)

    for <a id="change">param_name</a>, grad in grads.items():
        if "optimizer" in param_name:
            continue
        <a id="change">inner_ctx</a> = ctx.add_to_prefix(param_name, count=False)
        parameter_lr = lr * ctx.parameter_variance.get(param_name, 1)
        grad = grad.astype(ctx.model.storage_dtype)
        <a id="change">grad</a> = adaptive_gradient_clipping(ctx, param_name, grad)

        if small_parameter(param_name, grad):  &#47&#47 Do adam update for small parameters
            grad = adam(inner_ctx, grad, step)
        else:  &#47&#47 Do shampoo-sm3 update for large parameters
            grad = graft(<a id="change">sm3(</a>inner_ctx, param_name, grad<a id="change">)</a>, <a id="change">shampoo(</a>inner_ctx, param_name, grad, step<a id="change">)</a>)
            grad = ema(inner_ctx, grad, step, 1 - ctx.optimizer.momentum_beta, "momentum", heavyball=True,
                       quantize=False)
            ctx.parameters[param_name] = (1 + ctx.optimizer.weight_decay * parameter_lr) * ctx.parameters[param_name]</code></pre><h3>After Change</h3><pre><code class='java'>
    ctx = ctx.add_to_prefix("optimizer")
    lr = -get_current_lr(ctx, step)

    for <a id="change">param_name</a>, grad in grads.items():
        if "optimizer" in param_name:
            continue
        <a id="change">inner_ctx</a> = ctx.add_to_prefix(param_name, count=False)
        parameter_lr = lr * ctx.parameter_variance.get(param_name, 1)
        grad = grad.astype(ctx.model.storage_dtype)
        <a id="change">grad</a> = adaptive_gradient_clipping(ctx, param_name, grad)

        if small_parameter(param_name, grad):  &#47&#47 Do adam update for small parameters
            grad = adam(inner_ctx, grad, step)
        else:  &#47&#47 Do shampoo-sm3 update for large parameters
            grad = <a id="change">shampoo(</a>inner_ctx, param_name, grad, step<a id="change">)</a> * <a id="change">sm3(</a>inner_ctx, param_name, grad<a id="change">)</a>
            grad = ema(inner_ctx, grad, step, 1 - ctx.optimizer.momentum_beta, "momentum", heavyball=True)
            ctx.parameters[param_name] = (1 + ctx.optimizer.weight_decay * parameter_lr) * ctx.parameters[param_name]
        ctx.parameters[param_name] = grad * parameter_lr + ctx.parameters[param_name]
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/homebrewnlp/homebrewnlp-jax/commit/897c96699d2be018303036f6839fced5c91fa468#diff-4540d7c8541eb25568b448ea775d64c43be83ce020bc67dd8340da4d4e804846L116' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 35640765</div><div id='project'> Project Name: homebrewnlp/homebrewnlp-jax</div><div id='commit'> Commit Name: 897c96699d2be018303036f6839fced5c91fa468</div><div id='time'> Time: 2022-05-17</div><div id='author'> Author: 39779310+ClashLuke@users.noreply.github.com</div><div id='file'> File Name: src/optimizer.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: update(3)</div><div id='n_method'> N Method Name: update(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/optimizer.py</div><div id='n_file'> N File Name: src/optimizer.py</div><div id='m_start'> M Start Line: 120</div><div id='m_end'> M End Line: 131</div><div id='n_start'> N Start Line: 115</div><div id='n_end'> N End Line: 126</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    ctx = ctx.add_to_prefix("optimizer")
    lr = -get_current_lr(ctx, step)

    for <a id="change">param_name</a>, grad in grads.items():
        if "optimizer" in param_name:
            continue
        <a id="change">inner_ctx</a> = ctx.add_to_prefix(param_name, count=False)
        parameter_lr = lr * ctx.parameter_variance.get(param_name, 1)
        grad = grad.astype(ctx.model.storage_dtype)
        <a id="change">grad</a> = adaptive_gradient_clipping(ctx, param_name, grad)

        if small_parameter(param_name, grad):  &#47&#47 Do adam update for small parameters
            grad = adam(inner_ctx, grad, step)
        else:  &#47&#47 Do shampoo-sm3 update for large parameters
            grad = <a id="change">shampoo(</a>inner_ctx, param_name, grad, step<a id="change">)</a> * <a id="change">sm3(</a>inner_ctx, param_name, grad<a id="change">)</a>
            grad = ema(inner_ctx, grad, step, 1 - ctx.optimizer.momentum_beta, "momentum", heavyball=True)
            ctx.parameters[param_name] = (1 + ctx.optimizer.weight_decay * parameter_lr) * ctx.parameters[param_name]
        ctx.parameters[param_name] = grad * parameter_lr + ctx.parameters[param_name]
</code></pre><h3>After Change</h3><pre><code class='java'>
    ctx = ctx.add_to_prefix("optimizer")
    lr = -get_current_lr(ctx, step)

    for <a id="change">param_name</a>, grad in grads.items():
        if "optimizer" in param_name:
            continue
        <a id="change">inner_ctx</a> = ctx.add_to_prefix(param_name, count=False)
        parameter_lr = lr * ctx.parameter_variance.get(param_name, 1)
        grad = grad.astype(ctx.model.storage_dtype)
        <a id="change">grad</a> = adaptive_gradient_clipping(ctx, param_name, grad)

        if small_parameter(param_name, grad):  &#47&#47 Do adam update for small parameters
            grad = adam(inner_ctx, grad, step)
        else:  &#47&#47 Do shampoo-sm3 update for large parameters
            grad = graft(<a id="change">sm3(</a>inner_ctx, param_name, grad<a id="change">)</a>, <a id="change">shampoo(</a>inner_ctx, param_name, grad, step<a id="change">)</a>)
            grad = ema(inner_ctx, grad, step, 1 - ctx.optimizer.momentum_beta, "momentum", heavyball=True)
            ctx.parameters[param_name] = (1 + ctx.optimizer.weight_decay * parameter_lr) * ctx.parameters[param_name]
        ctx.parameters[param_name] = grad * parameter_lr + ctx.parameters[param_name]</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/homebrewnlp/olmax/commit/b03cf79882fc5c6be7208950c3d21094c80f7f8d#diff-4540d7c8541eb25568b448ea775d64c43be83ce020bc67dd8340da4d4e804846L111' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 35640785</div><div id='project'> Project Name: homebrewnlp/olmax</div><div id='commit'> Commit Name: b03cf79882fc5c6be7208950c3d21094c80f7f8d</div><div id='time'> Time: 2022-05-18</div><div id='author'> Author: 39779310+ClashLuke@users.noreply.github.com</div><div id='file'> File Name: src/optimizer.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: update(3)</div><div id='n_method'> N Method Name: update(3)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/optimizer.py</div><div id='n_file'> N File Name: src/optimizer.py</div><div id='m_start'> M Start Line: 115</div><div id='m_end'> M End Line: 126</div><div id='n_start'> N Start Line: 120</div><div id='n_end'> N End Line: 131</div><BR>