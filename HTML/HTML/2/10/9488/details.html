<html><h3>Pattern ID :9488
</h3><img src='33911991.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        query_flat = query.view(batch, query.shape[1], -1).transpose(1, 2)
        key_flat = key.view(batch, key.shape[1], -1).transpose(1, 2)
        query = reshape(self.query(query_flat))
        key = <a id="change">reshape(self.key(key_flat)).transpose(2</a>, <a id="change">3</a><a id="change">)</a>
        value = reshape(self.value(key_flat))

        attn<a id="change"> = </a>torch.matmul(query, key) / sqrt(self.dim_head)
        mask, start_mask = causal_mask(height * width)
        mask = mask.type_as(query)
        start_mask = start_mask.type_as(query)</code></pre><h3>After Change</h3><pre><code class='java'>
        attn = torch.softmax(attn, dim=3) * start_mask
        attn = self.drop(attn)

        y<a id="change"> = </a><a id="change">(attn @ v).reshape(batch_size, height, width, self.head_dim*self.nb_heads).permute(0</a>, <a id="change">3</a>, <a id="change">1</a>, <a id="change">2</a><a id="change">)</a>
        return y

class PixelBlock(nn.Module):
    def __init__(self, in_channel, channel, kernel_size, nb_res_blocks, dropout=0.1, condition_dim=0):</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/vvvm23/vqvae-2/commit/e15b9e3d7c13f912682873648e5aaaf81b1dfec5#diff-a112c46d500b0c8291c2990801b6878d4693377171a2ede123da1a50dcc72c61L110' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33911991</div><div id='project'> Project Name: vvvm23/vqvae-2</div><div id='commit'> Commit Name: e15b9e3d7c13f912682873648e5aaaf81b1dfec5</div><div id='time'> Time: 2021-06-02</div><div id='author'> Author: alexander.f.mckinney@durham.ac.uk</div><div id='file'> File Name: pixelsnail.py</div><div id='m_class'> M Class Name: CausalAttention</div><div id='n_method'> N Class Name: CausalAttention</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: HelperModule</div><div id='n_parent_class'> N Parent Class: HelperModule</div><div id='m_file'> M File Name: pixelsnail.py</div><div id='n_file'> N File Name: pixelsnail.py</div><div id='m_start'> M Start Line: 184</div><div id='m_end'> M End Line: 204</div><div id='n_start'> N Start Line: 110</div><div id='n_end'> N End Line: 130</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        value_states = value_states.view(*proj_shape)

        src_len = key_states.size(1)
        attn_weights<a id="change"> = </a>torch.bmm(query_states, <a id="change">key_states.transpose(1</a>, <a id="change">2</a><a id="change">)</a>)

        &#47&#47 q_t is [batch, seq_length, n_heads, dim_per_head]
        import ipdb; ipdb.set_trace()</code></pre><h3>After Change</h3><pre><code class='java'>
        attn_output = torch.bmm(attn_probs, value_states.view(*proj_shape))

        &#47&#47 w_t is [batch, seq_length, n_heads, seq_length]
        w_t<a id="change"> = </a><a id="change">attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim).permute(0</a>, <a id="change">2</a>, <a id="change">1</a>, <a id="change">3</a><a id="change">)</a>

        &#47&#47 [batch, seq_length, n_heads, seq_length]
        w_tr_matmul = torch.matmul(w_t, relation_v_embeds)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/joaolages/ratransformers/commit/87d3c27f618c060b396039f71734d515d3343a4b#diff-3bc988b93d619436305f06eadc62abd2469ae4a8ee0a636980fcd5202ebeee6fL18' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33911999</div><div id='project'> Project Name: joaolages/ratransformers</div><div id='commit'> Commit Name: 87d3c27f618c060b396039f71734d515d3343a4b</div><div id='time'> Time: 2022-02-11</div><div id='author'> Author: joaop.glages@gmail.com</div><div id='file'> File Name: src/ratransformers/bart.py</div><div id='m_class'> M Class Name: BartRelationalAttention</div><div id='n_method'> N Class Name: BartRelationalAttention</div><div id='m_method'> M Method Name: forward(7)</div><div id='n_method'> N Method Name: forward(7)</div><div id='m_parent_class'> M Parent Class: BartAttention</div><div id='n_parent_class'> N Parent Class: BartAttention</div><div id='m_file'> M File Name: src/ratransformers/bart.py</div><div id='n_file'> N File Name: src/ratransformers/bart.py</div><div id='m_start'> M Start Line: 43</div><div id='m_end'> M End Line: 138</div><div id='n_start'> N Start Line: 43</div><div id='n_end'> N End Line: 139</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        x = self.proj(x)
        Hp, Wp = x.shape[2], x.shape[3]

        x<a id="change"> = </a><a id="change">x.flatten(2).transpose(1</a>, <a id="change">2</a><a id="change">)</a>
        return x, (Hp, Wp)


class HybridEmbed(nn.Module):</code></pre><h3>After Change</h3><pre><code class='java'>
    def forward(self, x):
        x = self.proj(x)
        &#47&#47 B C H W -&gt; B H W C
        x<a id="change"> = </a><a id="change">x.permute(0</a>, <a id="change">2</a>, <a id="change">3</a>, <a id="change">1</a><a id="change">)</a>
        return x


class Attention(nn.Module):</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/alibaba/easycv/commit/9f01a37ad4df57b30430c41df08459025174e8fd#diff-78f05b57f42dcd5f7f060081ceac6dd4b971872c6edf8842805ae881484d5589L529' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33911993</div><div id='project'> Project Name: alibaba/easycv</div><div id='commit'> Commit Name: 9f01a37ad4df57b30430c41df08459025174e8fd</div><div id='time'> Time: 2022-09-15</div><div id='author'> Author: 38110862+tuofeilunhifi@users.noreply.github.com</div><div id='file'> File Name: easycv/models/backbones/vitdet.py</div><div id='m_class'> M Class Name: PatchEmbed</div><div id='n_method'> N Class Name: PatchEmbed</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(2)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: easycv/models/backbones/vitdet.py</div><div id='n_file'> N File Name: easycv/models/backbones/vitdet.py</div><div id='m_start'> M Start Line: 530</div><div id='m_end'> M End Line: 538</div><div id='n_start'> N Start Line: 188</div><div id='n_end'> N End Line: 191</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        attn_scores = F.softmax(scores, dim=-1)

        out = torch.matmul(attn_scores, v)
        out<a id="change"> = </a><a id="change">out.transpose(1</a>, <a id="change">2</a><a id="change">)</a>.contiguous().view(b, -1, self.heads * self.head_dim)
        out = self.out(out)
        return out
</code></pre><h3>After Change</h3><pre><code class='java'>

        q = q.permute(0, 2, 1, 3)
        k = k.permute(0, 2, 1, 3)
        v<a id="change"> = </a><a id="change">v.permute(0</a>, <a id="change">2</a>, <a id="change">1</a>, <a id="change">3</a><a id="change">)</a>

        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / self.scale
        attn_weights = F.softmax(attn_weights, dim=-1)
        out = torch.matmul(attn_weights, v)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/asyml/vision-transformer-pytorch/commit/49517e96e5eccb618d99e3a8909d689020a1612e#diff-fada037ad086638e65c7ae77e3d223963e9afaa26326aab0ea718f4013176e43L69' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33911992</div><div id='project'> Project Name: asyml/vision-transformer-pytorch</div><div id='commit'> Commit Name: 49517e96e5eccb618d99e3a8909d689020a1612e</div><div id='time'> Time: 2020-11-14</div><div id='author'> Author: haoc3@andrew.cmu.edu</div><div id='file'> File Name: model.py</div><div id='m_class'> M Class Name: SelfAttention</div><div id='n_method'> N Class Name: SelfAttention</div><div id='m_method'> M Method Name: forward(2)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: model.py</div><div id='n_file'> N File Name: model.py</div><div id='m_start'> M Start Line: 69</div><div id='m_end'> M End Line: 83</div><div id='n_start'> N Start Line: 85</div><div id='n_end'> N End Line: 96</div><BR>