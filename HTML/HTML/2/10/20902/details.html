<html><h3>Pattern ID :20902
</h3><img src='67249538.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        baselines = estimated_values

        &#47&#47&#47&#47 Calculate the Advantages, A(s,a) = Q(s,a) - \hat{V}(s).
        <a id="change">advantages</a> = <a id="change">[]</a>
        final_gen_objective = torch.zeros([batch_size, 1]).cuda(self.device)
        for t in range(seq_len):
            log_probability = log_probs[:, t].unsqueeze(dim=1)
            cum_advantage = torch.zeros((batch_size, 1))
            cum_advantage = cum_advantage.cuda(self.device)
            for s in range(t, seq_len):
                cum_advantage_tmp = missing[:, s] * np.power(self.gamma, (s - t)) * rewards[:, s]
                cum_advantage_tmp = cum_advantage_tmp.unsqueeze(dim=1)
                cum_advantage += cum_advantage_tmp
            cum_advantage<a id="change"> -= </a>baselines[:, t].unsqueeze(dim=1)
            &#47&#47 Clip advantages.
            cum_advantage = torch.clamp(cum_advantage, -self.advantage_clipping, self.advantage_clipping)
            advantage = missing[:, t].unsqueeze(dim=1) * cum_advantage
            <a id="change">advantages.append(</a>advantage<a id="change">)</a>
            &#47&#47 cum_advantage.detach()
            final_gen_objective<a id="change"> += </a>torch.mul(log_probability, missing[:, t].unsqueeze(dim=1) * cum_advantage)
        final_gen_objective = -torch.sum(final_gen_objective) / batch_size  &#47&#47 max the reward
        maintain_averages_op = None
        advantages<a id="change"> = </a><a id="change">torch.stack(advantages</a><a id="change">, dim=1)</a>

        &#47&#47 return [
        &#47&#47     final_gen_objective, log_probs, rewards, advantages, baselines,
        &#47&#47     maintain_averages_op, critic_loss, cumulative_rewards</code></pre><h3>After Change</h3><pre><code class='java'>
        rewards = rewards.detach()

        &#47&#47 Unstack Tensors into lists.
        missing = 1. - <a id="change">mask_present.float()</a>

        &#47&#47 Cumulative Discounted Returns.  The true value function V*(s).
        cumulative_rewards = []
        batch_size, seq_len = dis_predictions.size()
        for t in range(seq_len):
            cum_value = torch.zeros((batch_size, 1))
            cum_value = cum_value.cuda(device)
            for s in range(t, seq_len):
                cum_value_tmp = missing[:, s] * np.power(self.gamma, (s - t)) * rewards[:, s]
                cum_value_tmp = cum_value_tmp.unsqueeze(dim=1)
                cum_value += cum_value_tmp
            cumulative_rewards.append(cum_value)  &#47&#47 bs*1
        cumulative_rewards = torch.stack(cumulative_rewards, dim=1).squeeze()

        &#47&#47 REINFORCE with different baselines.
        &#47&#47 We create a separate critic functionality for the Discriminator.  This
        &#47&#47 will need to operate unidirectionally and it may take in the past context.
        &#47&#47 Critic loss calculated from the estimated value function \hat{V}(s)
        &#47&#47 versus the true value function V*(s).
        cumulative_rewards = cumulative_rewards.detach()
        critic_loss = self.create_critic_loss(cumulative_rewards, estimated_values, mask_present)

        &#47&#47 Baselines are coming from the critic&quots estimated state values.
        baselines = estimated_values
        baselines = baselines.detach()

        &#47&#47&#47&#47 Calculate the Advantages, A(s,a) = Q(s,a) - \hat{V}(s).
        final_gen_objective = torch.zeros([batch_size, 1]).cuda(self.device)
        for t in range(seq_len):
            log_probability = log_probs[:, t].unsqueeze(dim=1)  &#47&#47 bs*1
            cum_advantage = torch.zeros((batch_size, 1))  &#47&#47 bs*1
            cum_advantage = cum_advantage.cuda(self.device)
            for s in range(t, seq_len):
                cum_advantage_tmp = missing[:, s] * np.power(self.gamma, (s - t)) * rewards[:, s]
                cum_advantage_tmp = cum_advantage_tmp.unsqueeze(dim=1)
                cum_advantage = cum_advantage<a id="change"> + </a>cum_advantage_tmp
            cum_advantage = cum_advantage - baselines[:, t].unsqueeze(dim=1)
            &#47&#47 Clip advantages.
            cum_advantage = torch.clamp(cum_advantage, -self.advantage_clipping, self.advantage_clipping)</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 8</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/rucaibox/textbox/commit/cc5517ab006f13503389f456551219a634d7ca28#diff-68afacedbfa3668a63caea6bfdbbe7bdc3951d5d959770422411900c650f67c1L236' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 67249538</div><div id='project'> Project Name: rucaibox/textbox</div><div id='commit'> Commit Name: cc5517ab006f13503389f456551219a634d7ca28</div><div id='time'> Time: 2020-12-26</div><div id='author'> Author: 1318829605@qq.com</div><div id='file'> File Name: textbox/module/Generator/MaskGANGenerator.py</div><div id='m_class'> M Class Name: MaskGANGenerator</div><div id='n_method'> N Class Name: MaskGANGenerator</div><div id='m_method'> M Method Name: calculate_reinforce_objective(5)</div><div id='n_method'> N Method Name: calculate_reinforce_objective(5)</div><div id='m_parent_class'> M Parent Class: GenerativeAdversarialNet</div><div id='n_parent_class'> N Parent Class: GenerativeAdversarialNet</div><div id='m_file'> M File Name: textbox/module/Generator/MaskGANGenerator.py</div><div id='n_file'> N File Name: textbox/module/Generator/MaskGANGenerator.py</div><div id='m_start'> M Start Line: 255</div><div id='m_end'> M End Line: 301</div><div id='n_start'> N Start Line: 236</div><div id='n_end'> N End Line: 290</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        baselines = estimated_values

        &#47&#47&#47&#47 Calculate the Advantages, A(s,a) = Q(s,a) - \hat{V}(s).
        <a id="change">advantages</a> = <a id="change">[]</a>
        final_gen_objective = torch.zeros([batch_size, 1]).cuda(self.device)
        for <a id="change">t</a> in range(seq_len):
            log_probability = log_probs[:, t].unsqueeze(dim=1)
            cum_advantage = torch.zeros((batch_size, 1))
            cum_advantage = cum_advantage.cuda(self.device)
            for s in range(t, seq_len):
                cum_advantage_tmp = missing[:, s] * np.power(self.gamma, (s - t)) * rewards[:, s]
                cum_advantage_tmp = cum_advantage_tmp.unsqueeze(dim=1)
                cum_advantage += cum_advantage_tmp
            cum_advantage<a id="change"> -= </a>baselines[:, t].unsqueeze(dim=1)
            &#47&#47 Clip advantages.
            cum_advantage = torch.clamp(cum_advantage, -self.advantage_clipping, self.advantage_clipping)
            advantage = missing[:, t].unsqueeze(dim=1) * cum_advantage
            <a id="change">advantages.append(</a>advantage<a id="change">)</a>
            &#47&#47 cum_advantage.detach()
            final_gen_objective<a id="change"> += </a>torch.mul(log_probability, missing[:, t].unsqueeze(dim=1) * cum_advantage)
        final_gen_objective = -torch.sum(final_gen_objective) / batch_size  &#47&#47 max the reward
        maintain_averages_op = None
        advantages<a id="change"> = </a><a id="change">torch.stack(</a>advantages<a id="change">, dim=1)</a>

        &#47&#47 return [
        &#47&#47     final_gen_objective, log_probs, rewards, advantages, baselines,
        &#47&#47     maintain_averages_op, critic_loss, cumulative_rewards</code></pre><h3>After Change</h3><pre><code class='java'>
        rewards = rewards.detach()

        &#47&#47 Unstack Tensors into lists.
        missing = 1.<a id="change"> - </a><a id="change">mask_present.float()</a>

        &#47&#47 Cumulative Discounted Returns.  The true value function V*(s).
        cumulative_rewards = []
        batch_size, seq_len = dis_predictions.size()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/rucaibox/textbox/commit/cc5517ab006f13503389f456551219a634d7ca28#diff-68afacedbfa3668a63caea6bfdbbe7bdc3951d5d959770422411900c650f67c1L211' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 67249600</div><div id='project'> Project Name: rucaibox/textbox</div><div id='commit'> Commit Name: cc5517ab006f13503389f456551219a634d7ca28</div><div id='time'> Time: 2020-12-26</div><div id='author'> Author: 1318829605@qq.com</div><div id='file'> File Name: textbox/module/Generator/MaskGANGenerator.py</div><div id='m_class'> M Class Name: MaskGANGenerator</div><div id='n_method'> N Class Name: MaskGANGenerator</div><div id='m_method'> M Method Name: calculate_reinforce_objective(5)</div><div id='n_method'> N Method Name: calculate_reinforce_objective(5)</div><div id='m_parent_class'> M Parent Class: GenerativeAdversarialNet</div><div id='n_parent_class'> N Parent Class: GenerativeAdversarialNet</div><div id='m_file'> M File Name: textbox/module/Generator/MaskGANGenerator.py</div><div id='n_file'> N File Name: textbox/module/Generator/MaskGANGenerator.py</div><div id='m_start'> M Start Line: 255</div><div id='m_end'> M End Line: 301</div><div id='n_start'> N Start Line: 236</div><div id='n_end'> N End Line: 290</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        q_loss = (q_delta * sampling_weigths).mean()

        bg_mask = 1 - obs["fg_mask"]
        <a id="change">q_bg_pred</a> = <a id="change">[]</a>
        for <a id="change">b</a> in range(bg_mask.shape[0]):
            y<a id="change">, x = </a>torch.where(bg_mask[b])
            i<a id="change"> = </a>torch.randint(low=0, high=len(y), size=()).item()
            <a id="change">q_bg_pred.append(</a>qs_pred[b, 0, y[i], x[i]]<a id="change">)</a>
        q_bg_pred<a id="change"> = </a><a id="change">torch.stack(</a>q_bg_pred<a id="change">)</a>
        q_bg_delta = torch.nn.functional.smooth_l1_loss(
            q_bg_pred,
            torch.zeros_like(q_bg_pred),
            reduction="none",</code></pre><h3>After Change</h3><pre><code class='java'>
        q_loss = (q_delta * sampling_weigths).mean()

        q_bg_delta = torch.nn.functional.smooth_l1_loss(
            qs_pred[:, 0]<a id="change"> * </a>(1 - <a id="change">obs["fg_mask"].float()</a>),
            torch.zeros_like(qs_pred)[:, 0],
            reduction="none",
        ).mean(dim=(1, 2))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/wkentaro/safepicking/commit/da3dc0dab7349f8d66d960e74177c69c60f594b6#diff-3a019c87efc0a2b3b3b7b4fc42dbe7ba183756b75362d92dbc72eab8953b42c6L173' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 67249563</div><div id='project'> Project Name: wkentaro/safepicking</div><div id='commit'> Commit Name: da3dc0dab7349f8d66d960e74177c69c60f594b6</div><div id='time'> Time: 2021-05-07</div><div id='author'> Author: www.kentaro.wada@gmail.com</div><div id='file'> File Name: examples/grasp_with_intent/agent.py</div><div id='m_class'> M Class Name: DqnAgent</div><div id='n_method'> N Class Name: DqnAgent</div><div id='m_method'> M Method Name: _update_q(2)</div><div id='n_method'> N Method Name: _update_q(2)</div><div id='m_parent_class'> M Parent Class: Agent</div><div id='n_parent_class'> N Parent Class: Agent</div><div id='m_file'> M File Name: examples/grasp_with_intent/agent.py</div><div id='n_file'> N File Name: examples/grasp_with_intent/agent.py</div><div id='m_start'> M Start Line: 182</div><div id='m_end'> M End Line: 232</div><div id='n_start'> N Start Line: 182</div><div id='n_end'> N End Line: 225</div><BR>