<html><h3>Pattern ID :19894
</h3><img src='64659930.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    &#47&#47 Computing the generator and fnet losses
    diff1_mse = s_gen_output - s_targets

    content_loss<a id="change"> = </a>torch.mean(torch.sum(torch.square(diff1_mse), dim=[3]))
    update_list += [content_loss]
    update_list_name += ["l2_content_loss"]
    gen_loss = content_loss
    fnet_loss = content_loss

    diff2_mse = input_frames - s_input_warp

    warp_loss = torch.mean(torch.sum(torch.square(diff2_mse), dim=[3]))
    update_list += [warp_loss]
    update_list_name += ["l2_warp_loss"]

    vgg_loss = None
    vgg_loss_list = []
    if args.vgg_scaling &gt; 0.0:
        vgg_wei_list = [1.0, 1.0, 1.0, 1.0]
        vgg_loss = 0
        vgg_layer_n = len(vgg_layer_labels)

        for layer_i in range(vgg_layer_n):
            curvgg_diff = torch.sum(gen_vgg[vgg_layer_labels[layer_i]] * target_vgg[vgg_layer_labels[layer_i]], dim=[3])
            scaled_layer_loss = vgg_wei_list[layer_i] * curvgg_diff
            vgg_loss_list += [curvgg_diff]
            vgg_loss += scaled_layer_loss

        gen_loss += args.vgg_scaling * vgg_loss
        fnet_loss += args.vgg_scaling * vgg_loss.detach()
        vgg_loss_list += [vgg_loss]

        update_list += vgg_loss_list
        update_list_name += ["vgg_loss_%d" % (_ + 2) for _ in range(len(vgg_loss_list) - 1)]
        update_list_name += ["vgg_all"]

    if args.pingpang:
        gen_out_first = gen_outputs[:, 0:args.RNN_N - 1, :, :, :]
        gen_out_last_rev = torch.flip(gen_outputs, dims=[1])[:, :args.RNN_N - 1:1, :, :, :]

        pploss = torch.mean(torch.abs(gen_out_first - gen_out_last_rev))

        if args.pp_scaling &gt; 0:
            gen_loss += pploss.cpu() * args.pp_scaling
            fnet_loss += pploss.cpu() * args.pp_scaling
        update_list += [pploss]
        update_list_name += ["PingPang"]

    if (GAN_FLAG):
        t_adversarial_loss = torch.mean(-torch.log(tdiscrim_fake_output.detach() + args.EPS))
        d_adversarial_loss = torch.mean(-torch.log(tdiscrim_fake_output + args.EPS))
        dt_ratio = torch.min(torch.tensor(args.Dt_ratio_max),
                             args.Dt_ratio_0 + args.Dt_ratio_add * torch.tensor(Global_step, dtype=torch.float32))

    gen_loss += args.ratio * t_adversarial_loss.cpu()
    fnet_loss += args.ratio * t_adversarial_loss.cpu()
    update_list += [t_adversarial_loss]
    update_list_name += ["t_adversarial_loss"]
    &#47&#47 Computing gradients for Generator and updating weights
    if (args.D_LAYERLOSS):
        gen_loss += sum_layer_loss.cpu() * dt_ratio
    gen_loss<a id="change"> = </a>gen_loss.cuda()
    optimizer_g.zero_grad()
    <a id="change">gen_loss.backward()</a>
    optimizer_g.step()

    &#47&#47 Computing discriminator loss
    if (GAN_FLAG):</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 Computing the generator and fnet losses
        diff1_mse = s_gen_output - s_targets

        content_loss<a id="change"> = </a>torch.mean(torch.sum(torch.square(diff1_mse), dim=[3]))
        update_list += [content_loss]
        update_list_name += ["l2_content_loss"]
        gen_loss = content_loss
        fnet_loss = content_loss

        diff2_mse = input_frames - s_input_warp

        warp_loss = torch.mean(torch.sum(torch.square(diff2_mse), dim=[3]))
        update_list += [warp_loss]
        update_list_name += ["l2_warp_loss"]

        vgg_loss = None
        vgg_loss_list = []
        if args.vgg_scaling &gt; 0.0:
            vgg_wei_list = [1.0, 1.0, 1.0, 1.0]
            vgg_loss = 0
            vgg_layer_n = len(vgg_layer_labels)

            for layer_i in range(vgg_layer_n):
                curvgg_diff = torch.sum(gen_vgg[vgg_layer_labels[layer_i]] * target_vgg[vgg_layer_labels[layer_i]], dim=[3])
                scaled_layer_loss = vgg_wei_list[layer_i] * curvgg_diff
                vgg_loss_list += [curvgg_diff]
                vgg_loss += scaled_layer_loss

            gen_loss += args.vgg_scaling * vgg_loss
            fnet_loss += args.vgg_scaling * vgg_loss.detach()
            vgg_loss_list += [vgg_loss]

            update_list += vgg_loss_list
            update_list_name += ["vgg_loss_%d" % (_ + 2) for _ in range(len(vgg_loss_list) - 1)]
            update_list_name += ["vgg_all"]

        if args.pingpang:
            gen_out_first = gen_outputs[:, 0:args.RNN_N - 1, :, :, :]
            gen_out_last_rev = torch.flip(gen_outputs, dims=[1])[:, :args.RNN_N - 1:1, :, :, :]

            pploss = torch.mean(torch.abs(gen_out_first - gen_out_last_rev))

            if args.pp_scaling &gt; 0:
                gen_loss += pploss.cpu() * args.pp_scaling
                fnet_loss += pploss.cpu() * args.pp_scaling
            update_list += [pploss]
            update_list_name += ["PingPang"]

        if (GAN_FLAG):
            t_adversarial_loss = torch.mean(-torch.log(tdiscrim_fake_output.detach() + args.EPS))
            d_adversarial_loss = torch.mean(-torch.log(tdiscrim_fake_output + args.EPS))
            dt_ratio = torch.min(torch.tensor(args.Dt_ratio_max),
                                 args.Dt_ratio_0 + args.Dt_ratio_add * torch.tensor(Global_step, dtype=torch.float32))

        gen_loss += args.ratio * t_adversarial_loss.cpu()
        fnet_loss += args.ratio * t_adversarial_loss.cpu()
        update_list += [t_adversarial_loss]
        update_list_name += ["t_adversarial_loss"]
        &#47&#47 Computing gradients for Generator and updating weights
        if (args.D_LAYERLOSS):
            gen_loss += sum_layer_loss.cpu() * dt_ratio
        gen_loss<a id="change"> = </a>gen_loss.cuda()


        &#47&#47 Computing discriminator loss
        if (GAN_FLAG):
            t_discrim_fake_loss = torch.log(1 - tdiscrim_fake_output + args.EPS)
            t_discrim_real_loss = torch.log(tdiscrim_real_output + args.EPS)

            t_discrim_loss = torch.mean(-(t_discrim_fake_loss + t_discrim_real_loss))
            t_balance = torch.mean(t_discrim_real_loss) + d_adversarial_loss

            update_list += [t_discrim_loss]
            update_list_name += ["t_discrim_loss"]

            update_list += [torch.mean(tdiscrim_real_output), torch.mean(tdiscrim_fake_output)]
            update_list_name += ["t_discrim_real_output", "t_discrim_fake_output"]

            if (args.D_LAYERLOSS and Fix_margin &gt; 0.0):
                discrim_loss = t_discrim_loss + d_layer_loss * dt_ratio

            &#47&#47 Computing gradients for Discriminator and updating weights
            else:
                discrim_loss = t_discrim_loss
            discrim_loss = discrim_loss.cuda()

            tb_exp_averager = EMA(0.99)
            init_average = torch.zeros_like(t_balance)
            tb_exp_averager.register("TB_average", init_average)
            tb = tb_exp_averager.forward("TB_average", t_balance)

            update_list += [gen_loss]
            update_list_name += ["All_loss_Gen"]

            tb_exp_averager.register("Loss_average", init_average)
            update_list_avg = [tb_exp_averager.forward("Loss_average", _) for _ in update_list]
        &#47&#47 Computing gradients for fnet and updating weights
        optimizer_g.zero_grad()
        <a id="change">scaler.scale(gen_loss).backward()</a>
        <a id="change">scaler.step(</a>optimizer_g<a id="change">)</a>
        scaler.update()
        optimizer_d.zero_grad()
        scaler.scale(discrim_loss).backward()
        scaler.step(optimizer_d)
        <a id="change">scaler.update()</a>
        &#47&#47fnet_loss = fnet_loss.cuda()
        &#47&#47fnet_optimizer.zero_grad()
        &#47&#47fnet_loss.backward()
        &#47&#47fnet_optimizer.step()</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 9</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/dwightfoster/pytorch-tecogan/commit/025e19b3ae985186b2d39607436eda9a3579b8c1#diff-f5a7fdedd2c0ddebef9c8da0be135c2dfddb9e5339087d894cdd812cea93a47eL68' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 64659930</div><div id='project'> Project Name: dwightfoster/pytorch-tecogan</div><div id='commit'> Commit Name: 025e19b3ae985186b2d39607436eda9a3579b8c1</div><div id='time'> Time: 2021-03-17</div><div id='author'> Author: dwightfoster03@gmail.com</div><div id='file'> File Name: code/train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: TecoGAN(11)</div><div id='n_method'> N Method Name: TecoGAN(11)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: code/train.py</div><div id='n_file'> N File Name: code/train.py</div><div id='m_start'> M Start Line: 68</div><div id='m_end'> M End Line: 352</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 357</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

    s_gen_output = torch.reshape(gen_outputs,
                                 (args.batch_size * inputimages, 3, args.crop_size * 4, args.crop_size * 4))
    s_targets<a id="change"> = </a>torch.reshape(r_targets, (args.batch_size * inputimages, 3, args.crop_size * 4, args.crop_size * 4))

    update_list = []
    update_list_name = []

    &#47&#47 Preparing vgg layers
    if args.vgg_scaling &gt; 0.0:
        vgg_layer_labels = [&quotvgg_19/conv2_2&quot, &quotvgg_19/conv3_4&quot, &quotvgg_19/conv4_4&quot]
        gen_vgg = VGG19_slim(s_gen_output, deep_list=vgg_layer_labels)
        target_vgg = VGG19_slim(s_targets, deep_list=vgg_layer_labels)

    if (GAN_FLAG):
        t_size = int(3 * (inputimages // 3))
        t_gen_output = torch.reshape(gen_outputs[:, :t_size, :, :, :],
                                     (args.batch_size * t_size, 3, args.crop_size * 4, args.crop_size * 4))
        t_targets = torch.reshape(r_targets[:, :t_size, :, :, :],
                                  (args.batch_size * t_size, 3, args.crop_size * 4, args.crop_size * 4))
        t_batch = args.batch_size * t_size // 3

        &#47&#47 Preparing inputs for discriminator
        if not args.pingpang:
            fnet_input_back = torch.cat((r_inputs[:, 2:t_size:3, :, :, :], r_inputs[:, 1:t_size:3, :, :, :]), dim=1)
            fnet_input_back = torch.reshape(fnet_input_back,
                                            (t_batch, 2 * output_channel, args.crop_size, args.crop_size))



            gen_flow_back = upscale_four(fnet_input_back[:, 0:2] * 4.0)

            gen_flow_back = torch.reshape(gen_flow_back,
                                          (args.batch_size, t_size // 3, 2, args.crop_size * 4, args.crop_size * 4))

            T_inputs_VPre_batch = identity(gen_flow[:, 0:t_size:3, :, :, :])
            T_inputs_V_batch = torch.zeros_like(T_inputs_VPre_batch)
            T_inputs_VNxt_batch = preprocess(gen_flow_back)

        else:
            T_inputs_VPre_batch = identity(gen_flow[:, 0:t_size:3, :, :, :])
            T_inputs_V_batch = torch.zeros_like(T_inputs_VPre_batch)
            T_inputs_VNxt_batch = torch.flip(gen_flow, dims=[1])[:, 1: t_size:3, :, :, :]

        T_vel = torch.stack([T_inputs_VPre_batch, T_inputs_V_batch, T_inputs_VNxt_batch], axis=2)
        T_vel = torch.reshape(T_vel, (args.batch_size * t_size, args.crop_size * 4, args.crop_size * 4, 2))
        T_vel = T_vel.detach()

        if args.crop_dt &lt; 1.0:
            crop_size_dt = int(args.crop_size * 4 * args.crop_dt)
            offset_dt = (args.crop_size * 4 - crop_size_dt) // 2
            crop_size_dt = args.crop_size * 4 - offset_dt * 2
            paddings = (offset_dt, offset_dt, offset_dt, offset_dt)
        real_warp0 = F.grid_sample(t_targets, T_vel)

        real_warp = torch.reshape(real_warp0, (t_batch, 9, args.crop_size * 4, args.crop_size * 4))
        if (args.crop_dt &lt; 1.0):
            real_warp = functional.resized_crop(real_warp, offset_dt, offset_dt, crop_size_dt, crop_size_dt,
                                                [crop_size_dt, crop_size_dt])
        &#47&#47 Passing real inputs to discriminator
        if (args.Dt_mergeDs):
            if (args.crop_dt &lt; 1.0):
                real_warp = F.pad(real_warp, paddings, "constant")
            before_warp = torch.reshape(t_targets, (t_batch, 9, args.crop_size * 4, args.crop_size * 4))
            t_input = torch.reshape(r_inputs[:, :t_size, :, :, :],
                                    (t_batch, 9, args.crop_size, args.crop_size))
            input_hi = functional.resize(t_input, [args.crop_size * 4, args.crop_size * 4])
            real_warp = torch.cat((before_warp, real_warp, input_hi), dim=1)

            tdiscrim_real_output, real_layers = discriminator_F(real_warp)

        else:
            tdiscrim_real_output = discriminator_F(real_warp.cuda())

        &#47&#47 Reshaping Generator output to pass to Discriminator
        fake_warp0 = F.grid_sample(t_gen_output, T_vel)

        fake_warp = torch.reshape(fake_warp0, (t_batch, 9, args.crop_size * 4, args.crop_size * 4))
        if (args.crop_dt &lt; 1.0):
            fake_warp = functional.resized_crop(fake_warp, offset_dt, offset_dt, crop_size_dt, crop_size_dt,
                                                size=[crop_size_dt, crop_size_dt])
        &#47&#47 Passing generated images to discriminator
        if (args.Dt_mergeDs):
            if (args.crop_dt &lt; 1.0):
                fake_warp = F.pad(fake_warp, paddings, "constant")
            before_warp = torch.reshape(before_warp, (t_batch, 9, args.crop_size * 4, args.crop_size * 4))
            fake_warp = torch.cat((before_warp, fake_warp, input_hi), dim=1)
            tdiscrim_fake_output, fake_layers = discriminator_F(fake_warp.cuda().detach())

        else:
            tdiscrim_fake_output = discriminator_F(fake_warp.cuda().detach())

        &#47&#47 Computing the layer loss using the VGG network and discriminator outputs
        if (args.D_LAYERLOSS):
            Fix_Range = 0.02
            Fix_margin = 0.0

            sum_layer_loss = 0
            d_layer_loss = 0

            layer_loss_list = []
            layer_n = len(real_layers)
            layer_norm = [12.0, 14.0, 24.0, 48.0, 100.0]
            for layer_i in range(layer_n):
                real_layer = real_layers[layer_i]
                false_layer = fake_layers[layer_i]

                layer_diff = real_layer.detach() - false_layer.detach()
                layer_loss = torch.mean(torch.sum(torch.abs(layer_diff), dim=[3]))

                layer_loss_list += [layer_loss]

                scaled_layer_loss = Fix_Range * layer_loss / layer_norm[layer_i]

                sum_layer_loss += scaled_layer_loss
                if Fix_margin &gt; 0.0:
                    d_layer_loss += torch.max(0.0, torch.tensor(Fix_margin - scaled_layer_loss))

            update_list += layer_loss_list
            update_list_name += [("D_layer_%d_loss" % _) for _ in range(layer_n)]
            update_list += [sum_layer_loss]
            update_list_name += ["D_layer_loss_sum"]

            if Fix_margin &gt; 0.0:
                update_list += [d_layer_loss]
                update_list_name += ["D_layer_loss_for_D_sum"]
    &#47&#47 Computing the generator and fnet losses
    diff1_mse = s_gen_output - s_targets

    content_loss = torch.mean(torch.sum(torch.square(diff1_mse), dim=[3]))
    update_list += [content_loss]
    update_list_name += ["l2_content_loss"]
    gen_loss<a id="change"> = </a>content_loss
    fnet_loss = content_loss

    diff2_mse = input_frames - s_input_warp

    warp_loss = torch.mean(torch.sum(torch.square(diff2_mse), dim=[3]))
    update_list += [warp_loss]
    update_list_name += ["l2_warp_loss"]

    vgg_loss = None
    vgg_loss_list = []
    if args.vgg_scaling &gt; 0.0:
        vgg_wei_list = [1.0, 1.0, 1.0, 1.0]
        vgg_loss = 0
        vgg_layer_n = len(vgg_layer_labels)

        for layer_i in range(vgg_layer_n):
            curvgg_diff = torch.sum(gen_vgg[vgg_layer_labels[layer_i]] * target_vgg[vgg_layer_labels[layer_i]], dim=[3])
            scaled_layer_loss = vgg_wei_list[layer_i] * curvgg_diff
            vgg_loss_list += [curvgg_diff]
            vgg_loss += scaled_layer_loss

        gen_loss += args.vgg_scaling * vgg_loss
        fnet_loss += args.vgg_scaling * vgg_loss.detach()
        vgg_loss_list += [vgg_loss]

        update_list += vgg_loss_list
        update_list_name += ["vgg_loss_%d" % (_ + 2) for _ in range(len(vgg_loss_list) - 1)]
        update_list_name += ["vgg_all"]

    if args.pingpang:
        gen_out_first = gen_outputs[:, 0:args.RNN_N - 1, :, :, :]
        gen_out_last_rev = torch.flip(gen_outputs, dims=[1])[:, :args.RNN_N - 1:1, :, :, :]

        pploss = torch.mean(torch.abs(gen_out_first - gen_out_last_rev))

        if args.pp_scaling &gt; 0:
            gen_loss += pploss.cpu() * args.pp_scaling
            fnet_loss += pploss.cpu() * args.pp_scaling
        update_list += [pploss]
        update_list_name += ["PingPang"]

    if (GAN_FLAG):
        t_adversarial_loss = torch.mean(-torch.log(tdiscrim_fake_output.detach() + args.EPS))
        d_adversarial_loss = torch.mean(-torch.log(tdiscrim_fake_output + args.EPS))
        dt_ratio = torch.min(torch.tensor(args.Dt_ratio_max),
                             args.Dt_ratio_0 + args.Dt_ratio_add * torch.tensor(Global_step, dtype=torch.float32))

    gen_loss += args.ratio * t_adversarial_loss.cpu()
    fnet_loss += args.ratio * t_adversarial_loss.cpu()
    update_list += [t_adversarial_loss]
    update_list_name += ["t_adversarial_loss"]
    &#47&#47 Computing gradients for Generator and updating weights
    if (args.D_LAYERLOSS):
        gen_loss += sum_layer_loss.cpu() * dt_ratio
    gen_loss = gen_loss.cuda()
    optimizer_g.zero_grad()
    <a id="change">gen_loss.backward()</a>
    optimizer_g.step()

    &#47&#47 Computing discriminator loss
    if (GAN_FLAG):</code></pre><h3>After Change</h3><pre><code class='java'>

        s_gen_output = torch.reshape(gen_outputs,
                                     (args.batch_size * inputimages, 3, args.crop_size * 4, args.crop_size * 4))
        s_targets<a id="change"> = </a>torch.reshape(r_targets, (args.batch_size * inputimages, 3, args.crop_size * 4, args.crop_size * 4))

        update_list = []
        update_list_name = []

        &#47&#47 Preparing vgg layers
        if args.vgg_scaling &gt; 0.0:
            vgg_layer_labels = [&quotvgg_19/conv2_2&quot, &quotvgg_19/conv3_4&quot, &quotvgg_19/conv4_4&quot]
            gen_vgg = VGG19_slim(s_gen_output, deep_list=vgg_layer_labels)
            target_vgg = VGG19_slim(s_targets, deep_list=vgg_layer_labels)

        if (GAN_FLAG):
            t_size = int(3 * (inputimages // 3))
            t_gen_output = torch.reshape(gen_outputs[:, :t_size, :, :, :],
                                         (args.batch_size * t_size, 3, args.crop_size * 4, args.crop_size * 4))
            t_targets = torch.reshape(r_targets[:, :t_size, :, :, :],
                                      (args.batch_size * t_size, 3, args.crop_size * 4, args.crop_size * 4))
            t_batch = args.batch_size * t_size // 3

            &#47&#47 Preparing inputs for discriminator
            if not args.pingpang:
                fnet_input_back = torch.cat((r_inputs[:, 2:t_size:3, :, :, :], r_inputs[:, 1:t_size:3, :, :, :]), dim=1)
                fnet_input_back = torch.reshape(fnet_input_back,
                                                (t_batch, 2 * output_channel, args.crop_size, args.crop_size))



                gen_flow_back = upscale_four(fnet_input_back[:, 0:2] * 4.0)

                gen_flow_back = torch.reshape(gen_flow_back,
                                              (args.batch_size, t_size // 3, 2, args.crop_size * 4, args.crop_size * 4))

                T_inputs_VPre_batch = identity(gen_flow[:, 0:t_size:3, :, :, :])
                T_inputs_V_batch = torch.zeros_like(T_inputs_VPre_batch)
                T_inputs_VNxt_batch = preprocess(gen_flow_back)

            else:
                T_inputs_VPre_batch = identity(gen_flow[:, 0:t_size:3, :, :, :])
                T_inputs_V_batch = torch.zeros_like(T_inputs_VPre_batch)
                T_inputs_VNxt_batch = torch.flip(gen_flow, dims=[1])[:, 1: t_size:3, :, :, :]

            T_vel = torch.stack([T_inputs_VPre_batch, T_inputs_V_batch, T_inputs_VNxt_batch], axis=2)
            T_vel = torch.reshape(T_vel, (args.batch_size * t_size, args.crop_size * 4, args.crop_size * 4, 2))
            T_vel = T_vel.detach()

            if args.crop_dt &lt; 1.0:
                crop_size_dt = int(args.crop_size * 4 * args.crop_dt)
                offset_dt = (args.crop_size * 4 - crop_size_dt) // 2
                crop_size_dt = args.crop_size * 4 - offset_dt * 2
                paddings = (offset_dt, offset_dt, offset_dt, offset_dt)
            real_warp0 = F.grid_sample(t_targets, T_vel)

            real_warp = torch.reshape(real_warp0, (t_batch, 9, args.crop_size * 4, args.crop_size * 4))
            if (args.crop_dt &lt; 1.0):
                real_warp = functional.resized_crop(real_warp, offset_dt, offset_dt, crop_size_dt, crop_size_dt,
                                                    [crop_size_dt, crop_size_dt])
            &#47&#47 Passing real inputs to discriminator
            if (args.Dt_mergeDs):
                if (args.crop_dt &lt; 1.0):
                    real_warp = F.pad(real_warp, paddings, "constant")
                before_warp = torch.reshape(t_targets, (t_batch, 9, args.crop_size * 4, args.crop_size * 4))
                t_input = torch.reshape(r_inputs[:, :t_size, :, :, :],
                                        (t_batch, 9, args.crop_size, args.crop_size))
                input_hi = functional.resize(t_input, [args.crop_size * 4, args.crop_size * 4])
                real_warp = torch.cat((before_warp, real_warp, input_hi), dim=1)

                tdiscrim_real_output, real_layers = discriminator_F(real_warp)

            else:
                tdiscrim_real_output = discriminator_F(real_warp.cuda())

            &#47&#47 Reshaping Generator output to pass to Discriminator
            fake_warp0 = F.grid_sample(t_gen_output, T_vel.half())

            fake_warp = torch.reshape(fake_warp0, (t_batch, 9, args.crop_size * 4, args.crop_size * 4))
            if (args.crop_dt &lt; 1.0):
                fake_warp = functional.resized_crop(fake_warp, offset_dt, offset_dt, crop_size_dt, crop_size_dt,
                                                    size=[crop_size_dt, crop_size_dt])
            &#47&#47 Passing generated images to discriminator
            if (args.Dt_mergeDs):
                if (args.crop_dt &lt; 1.0):
                    fake_warp = F.pad(fake_warp, paddings, "constant")
                before_warp = torch.reshape(before_warp, (t_batch, 9, args.crop_size * 4, args.crop_size * 4))
                fake_warp = torch.cat((before_warp, fake_warp, input_hi), dim=1)
                tdiscrim_fake_output, fake_layers = discriminator_F(fake_warp.cuda().detach())

            else:
                tdiscrim_fake_output = discriminator_F(fake_warp.cuda().detach())

            &#47&#47 Computing the layer loss using the VGG network and discriminator outputs
            if (args.D_LAYERLOSS):
                Fix_Range = 0.02
                Fix_margin = 0.0

                sum_layer_loss = 0
                d_layer_loss = 0

                layer_loss_list = []
                layer_n = len(real_layers)
                layer_norm = [12.0, 14.0, 24.0, 48.0, 100.0]
                for layer_i in range(layer_n):
                    real_layer = real_layers[layer_i]
                    false_layer = fake_layers[layer_i]

                    layer_diff = real_layer.detach() - false_layer.detach()
                    layer_loss = torch.mean(torch.sum(torch.abs(layer_diff), dim=[3]))

                    layer_loss_list += [layer_loss]

                    scaled_layer_loss = Fix_Range * layer_loss / layer_norm[layer_i]

                    sum_layer_loss += scaled_layer_loss
                    if Fix_margin &gt; 0.0:
                        d_layer_loss += torch.max(0.0, torch.tensor(Fix_margin - scaled_layer_loss))

                update_list += layer_loss_list
                update_list_name += [("D_layer_%d_loss" % _) for _ in range(layer_n)]
                update_list += [sum_layer_loss]
                update_list_name += ["D_layer_loss_sum"]

                if Fix_margin &gt; 0.0:
                    update_list += [d_layer_loss]
                    update_list_name += ["D_layer_loss_for_D_sum"]
        &#47&#47 Computing the generator and fnet losses
        diff1_mse = s_gen_output - s_targets

        content_loss = torch.mean(torch.sum(torch.square(diff1_mse), dim=[3]))
        update_list += [content_loss]
        update_list_name += ["l2_content_loss"]
        gen_loss<a id="change"> = </a>content_loss
        fnet_loss = content_loss

        diff2_mse = input_frames - s_input_warp

        warp_loss = torch.mean(torch.sum(torch.square(diff2_mse), dim=[3]))
        update_list += [warp_loss]
        update_list_name += ["l2_warp_loss"]

        vgg_loss = None
        vgg_loss_list = []
        if args.vgg_scaling &gt; 0.0:
            vgg_wei_list = [1.0, 1.0, 1.0, 1.0]
            vgg_loss = 0
            vgg_layer_n = len(vgg_layer_labels)

            for layer_i in range(vgg_layer_n):
                curvgg_diff = torch.sum(gen_vgg[vgg_layer_labels[layer_i]] * target_vgg[vgg_layer_labels[layer_i]], dim=[3])
                scaled_layer_loss = vgg_wei_list[layer_i] * curvgg_diff
                vgg_loss_list += [curvgg_diff]
                vgg_loss += scaled_layer_loss

            gen_loss += args.vgg_scaling * vgg_loss
            fnet_loss += args.vgg_scaling * vgg_loss.detach()
            vgg_loss_list += [vgg_loss]

            update_list += vgg_loss_list
            update_list_name += ["vgg_loss_%d" % (_ + 2) for _ in range(len(vgg_loss_list) - 1)]
            update_list_name += ["vgg_all"]

        if args.pingpang:
            gen_out_first = gen_outputs[:, 0:args.RNN_N - 1, :, :, :]
            gen_out_last_rev = torch.flip(gen_outputs, dims=[1])[:, :args.RNN_N - 1:1, :, :, :]

            pploss = torch.mean(torch.abs(gen_out_first - gen_out_last_rev))

            if args.pp_scaling &gt; 0:
                gen_loss += pploss.cpu() * args.pp_scaling
                fnet_loss += pploss.cpu() * args.pp_scaling
            update_list += [pploss]
            update_list_name += ["PingPang"]

        if (GAN_FLAG):
            t_adversarial_loss = torch.mean(-torch.log(tdiscrim_fake_output.detach() + args.EPS))
            d_adversarial_loss = torch.mean(-torch.log(tdiscrim_fake_output + args.EPS))
            dt_ratio = torch.min(torch.tensor(args.Dt_ratio_max),
                                 args.Dt_ratio_0 + args.Dt_ratio_add * torch.tensor(Global_step, dtype=torch.float32))

        gen_loss += args.ratio * t_adversarial_loss.cpu()
        fnet_loss += args.ratio * t_adversarial_loss.cpu()
        update_list += [t_adversarial_loss]
        update_list_name += ["t_adversarial_loss"]
        &#47&#47 Computing gradients for Generator and updating weights
        if (args.D_LAYERLOSS):
            gen_loss += sum_layer_loss.cpu() * dt_ratio
        gen_loss = gen_loss.cuda()


        &#47&#47 Computing discriminator loss
        if (GAN_FLAG):
            t_discrim_fake_loss = torch.log(1 - tdiscrim_fake_output + args.EPS)
            t_discrim_real_loss = torch.log(tdiscrim_real_output + args.EPS)

            t_discrim_loss = torch.mean(-(t_discrim_fake_loss + t_discrim_real_loss))
            t_balance = torch.mean(t_discrim_real_loss) + d_adversarial_loss

            update_list += [t_discrim_loss]
            update_list_name += ["t_discrim_loss"]

            update_list += [torch.mean(tdiscrim_real_output), torch.mean(tdiscrim_fake_output)]
            update_list_name += ["t_discrim_real_output", "t_discrim_fake_output"]

            if (args.D_LAYERLOSS and Fix_margin &gt; 0.0):
                discrim_loss = t_discrim_loss + d_layer_loss * dt_ratio

            &#47&#47 Computing gradients for Discriminator and updating weights
            else:
                discrim_loss = t_discrim_loss
            discrim_loss = discrim_loss.cuda()

            tb_exp_averager = EMA(0.99)
            init_average = torch.zeros_like(t_balance)
            tb_exp_averager.register("TB_average", init_average)
            tb = tb_exp_averager.forward("TB_average", t_balance)

            update_list += [gen_loss]
            update_list_name += ["All_loss_Gen"]

            tb_exp_averager.register("Loss_average", init_average)
            update_list_avg = [tb_exp_averager.forward("Loss_average", _) for _ in update_list]
        &#47&#47 Computing gradients for fnet and updating weights
        optimizer_g.zero_grad()
        <a id="change">scaler.scale(gen_loss).backward()</a>
        scaler.step(optimizer_g)
        <a id="change">scaler.update()</a>
        optimizer_d.zero_grad()
        scaler.scale(discrim_loss).backward()
        <a id="change">scaler.step(</a>optimizer_d<a id="change">)</a>
        scaler.update()
        &#47&#47fnet_loss = fnet_loss.cuda()
        &#47&#47fnet_optimizer.zero_grad()
        &#47&#47fnet_loss.backward()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/dwightfoster/pytorch-tecogan/commit/025e19b3ae985186b2d39607436eda9a3579b8c1#diff-f5a7fdedd2c0ddebef9c8da0be135c2dfddb9e5339087d894cdd812cea93a47eL47' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 64659674</div><div id='project'> Project Name: dwightfoster/pytorch-tecogan</div><div id='commit'> Commit Name: 025e19b3ae985186b2d39607436eda9a3579b8c1</div><div id='time'> Time: 2021-03-17</div><div id='author'> Author: dwightfoster03@gmail.com</div><div id='file'> File Name: code/train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: TecoGAN(11)</div><div id='n_method'> N Method Name: TecoGAN(11)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: code/train.py</div><div id='n_file'> N File Name: code/train.py</div><div id='m_start'> M Start Line: 68</div><div id='m_end'> M End Line: 352</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 357</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
                args, optimizer, i_iter, args.num_epochs * len(trainloader.dataset))

            images = images.cuda()
            labels<a id="change"> = </a>labels.cuda()

            aux, pred = model(images)
            pred = interp(pred)
            aux = interp(aux)

            loss<a id="change"> = </a>seg_loss(pred, labels) + 0.4 * seg_loss(aux, labels)
            <a id="change">loss.backward()</a>
            optimizer.step()

            print(</code></pre><h3>After Change</h3><pre><code class='java'>
    interp = nn.Upsample(size=(input_size, input_size),
                         mode=&quotbilinear&quot, align_corners=True)

    <a id="change">scaler</a> = torch.cuda.amp.GradScaler()

    i_iter = 0
    print(len(trainloader))
    for epoch in range(args.num_epochs):
        for images, labels, _, _, _ in trainloader:

            optimizer.zero_grad()

            i_iter += images.shape[0]
            lr = adjust_learning_rate(
                args, optimizer, i_iter, args.num_epochs * len(trainloader.dataset))

            with torch.cuda.amp.autocast():
                images = images.cuda()
                labels<a id="change"> = </a>labels.cuda()

                aux, pred = model(images)
                pred = interp(pred)
                aux = interp(aux)

                loss<a id="change"> = </a>seg_loss(pred, labels) + 0.4 * seg_loss(aux, labels)

                <a id="change">scaler.scale(loss).backward()</a>
                <a id="change">scaler.step(</a>optimizer<a id="change">)</a>

                <a id="change">scaler.update()</a>

            print(
                f"epoch = {epoch:2d}, iter = {i_iter:6d}/{args.num_epochs * len(trainloader.dataset):6d}, {i_iter/(args.num_epochs * len(trainloader.dataset)):2.2%}, loss_seg = {loss:.3f}, lr = {lr:.6f}")</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/smhassanerfani/atlantis/commit/35a88733c2d4ab90e1ab4a38c2b07faa89eb1d50#diff-ed183d67207df065a11e1289f19d34cc2abbc5448dea952683cfe9728c342b95L65' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 64660156</div><div id='project'> Project Name: smhassanerfani/atlantis</div><div id='commit'> Commit Name: 35a88733c2d4ab90e1ab4a38c2b07faa89eb1d50</div><div id='time'> Time: 2021-09-13</div><div id='author'> Author: serfani@email.sc.edu</div><div id='file'> File Name: train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(0)</div><div id='n_method'> N Method Name: main(0)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: train.py</div><div id='n_file'> N File Name: train.py</div><div id='m_start'> M Start Line: 111</div><div id='m_end'> M End Line: 138</div><div id='n_start'> N Start Line: 111</div><div id='n_end'> N End Line: 146</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            scheduler = CosineAnnealingLR(optimizer, train_iteration, 1e-3, last_epoch=i-1)
        
        start = time.time()
        output<a id="change"> = </a>model(images)
        forward_time = time.time() - start
        
        loss<a id="change"> = </a>criterion(output, target)
        optimizer.zero_grad()
        
        start = time.time()
        <a id="change">loss.backward()</a>
        backward_time = time.time() - start
        
        &#47&#47 Set to 0 the gradient of pruned neurons
        with torch.no_grad():</code></pre><h3>After Change</h3><pre><code class='java'>
    
    train_loader, test_loader = get_data_loaders(os.path.join(config.root, "ImageNet"), 256, 256, 0, True, 8, False)
    optimizer = SGD(model.parameters(), lr=0.1, weight_decay=1e-4)
    <a id="change">scaler</a> = torch.cuda.amp.GradScaler()

    scheduler = CosineAnnealingLR(optimizer, train_iteration, 1e-3)
    criterion = CrossEntropyLoss()
    
    total_neurons = 0
    remaining_neurons = 0
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d):
            total_neurons += module.weight.shape[0]
            remaining_neurons += module.weight.shape[0]
    
    wandb.init()
    wandb.watch(model)
    
    &#47&#47 Train
    for i, (images, target) in enumerate(tqdm(train_loader)):
        
        model.train()
        images, target = images.to(device), target.to(device)

        &#47&#47 Prune the network by 5% at each pass
        if i + 1 % prune_iteration == 0:
            print("Pruning")
            
            remaining_neurons = 0
            for name, module in model.named_modules():
                if isinstance(module, nn.Conv2d):
                    prune.ln_structured(module, &quotweight&quot, amount=0.05, n=2, dim=0)
                    ch_sum = module.weight.sum(dim=(1, 2, 3))
                    remaining_neurons += ch_sum[ch_sum != 0].shape[0]
            
            print(f"The current model has {(remaining_neurons / total_neurons) * 100} % of the original neurons")
            
            optimizer = SGD(model.parameters(), lr=0.1, weight_decay=1e-4)
            scheduler = CosineAnnealingLR(optimizer, train_iteration, 1e-3, last_epoch=i-1)
        
        with torch.cuda.amp.autocast():
            start = time.time()
            output<a id="change"> = </a>model(images)
            forward_time = time.time() - start
            
            loss<a id="change"> = </a>criterion(output, target)
            optimizer.zero_grad()
            
            start = time.time()
            <a id="change">scaler.scale(loss).backward()</a>
            backward_time = time.time() - start
        
        &#47&#47 Set to 0 the gradient of pruned neurons
        with torch.no_grad():
            for name, module in model.named_modules():
                if isinstance(module, nn.Conv2d):
                    for n, p in module.named_parameters():
                        if n == "weight_orig":
                            p.grad.mul_(module.weight_mask)
                        if n == "bias":
                            p.grad.mul_(module.weight_mask[:, 0, 0, 0])
        
        <a id="change">scaler.step(</a>optimizer<a id="change">)</a>
        <a id="change">scaler.update()</a>
        scheduler.step()
        
        &#47&#47 Test
        model.eval()</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/eidoslab/simplify/commit/8c1cf93f9a2545c52c01379b934939040a76cc77#diff-41474220006d3f313aca51ff34722bbf4cb675023f4106ffd78a318ae920faccL39' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 64659925</div><div id='project'> Project Name: eidoslab/simplify</div><div id='commit'> Commit Name: 8c1cf93f9a2545c52c01379b934939040a76cc77</div><div id='time'> Time: 2021-07-01</div><div id='author'> Author: carlo.alberto.barbano@outlook.com</div><div id='file'> File Name: training/train.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: main(1)</div><div id='n_method'> N Method Name: main(1)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: training/train.py</div><div id='n_file'> N File Name: training/train.py</div><div id='m_start'> M Start Line: 55</div><div id='m_end'> M End Line: 112</div><div id='n_start'> N Start Line: 55</div><div id='n_end'> N End Line: 116</div><BR>