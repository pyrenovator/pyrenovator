<html><h3>Pattern ID :9196
</h3><img src='33293784.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        for key, db_offset, ann in zip(keys, self.db_offsets, self.anns):
            ann.add(key, ids = ann_insert_ids + db_offset)

        add_indices = (<a id="change">np.arange(num_memories)[None, :]</a> + self.db_offsets[:, None]) % self.max_num_entries
        self.db[<a id="change">np.arange(self.num_indices)[:, None]</a>, add_indices] = memories
        self.db.flush()

        self.db_offsets += num_memories</code></pre><h3>After Change</h3><pre><code class='java'>
        for key, db_offset, ann in zip(keys, self.db_offsets, self.anns):
            ann.add(key, ids = ann_insert_ids + db_offset)

        add_indices = (<a id="change">rearrange(</a>np.arange(num_memories), <a id="change">&quotj -&gt; 1 j&quot</a><a id="change">) + rearrange(</a>self.db_offsets, <a id="change">&quoti -&gt; i 1&quot</a><a id="change">)</a>) % self.max_num_entries
        self.db[rearrange(np.arange(self.num_indices), &quoti -&gt; i 1&quot), add_indices] = memories
        self.db.flush()
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/lucidrains/memorizing-transformers-pytorch/commit/7553651625c462aa74ebb936dbb588af2f58907b#diff-312d9e6119d058dcad54d06d612544895f07d11f8c3c563ae3ada4163e7f8698L103' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33293784</div><div id='project'> Project Name: lucidrains/memorizing-transformers-pytorch</div><div id='commit'> Commit Name: 7553651625c462aa74ebb936dbb588af2f58907b</div><div id='time'> Time: 2022-03-25</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memorizing_transformers_pytorch/ann_memory.py</div><div id='m_class'> M Class Name: ANNMemory</div><div id='n_method'> N Class Name: ANNMemory</div><div id='m_method'> M Method Name: add(2)</div><div id='n_method'> N Method Name: add(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: memorizing_transformers_pytorch/ann_memory.py</div><div id='n_file'> N File Name: memorizing_transformers_pytorch/ann_memory.py</div><div id='m_start'> M Start Line: 103</div><div id='m_end'> M End Line: 112</div><div id='n_start'> N Start Line: 104</div><div id='n_end'> N End Line: 113</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        &#47&#47 shaw&quots relative positional embedding
        seq = torch.arange(n, device = device)
        dist = seq[:, None] - <a id="change">seq[None, :]</a>
        dist = dist.clip(-max_pos_emb, max_pos_emb) + max_pos_emb
        rel_pos_emb = self.rel_pos_emb(dist).to(q)
        pos_attn = einsum(&quotb h n d, n r d -&gt; b h n r&quot, q, rel_pos_emb) * self.scale
        dots = dots + pos_attn

        if exists(mask) or exists(context_mask):
            mask = default(mask, lambda: torch.ones(*x.shape[:2], device = device))
            context_mask = default(context_mask, mask) if not has_context else default(context_mask, lambda: torch.ones(*context.shape[:2], device = device))
            mask_value = -torch.finfo(dots.dtype).max
            mask = mask[:, None, :, None] * <a id="change">context_mask[:, None, None, :]</a>
            dots.masked_fill_(~mask, mask_value)

        attn = dots.softmax(dim = -1)
</code></pre><h3>After Change</h3><pre><code class='java'>

        &#47&#47 shaw&quots relative positional embedding
        seq = torch.arange(n, device = device)
        dist = <a id="change">rearrange(</a>seq, <a id="change">&quoti -&gt; i ()&quot</a><a id="change">)</a><a id="change"> - </a><a id="change">rearrange(</a>seq, <a id="change">&quotj -&gt; () j&quot</a><a id="change">)</a>
        dist = dist.clip(-max_pos_emb, max_pos_emb) + max_pos_emb
        rel_pos_emb = self.rel_pos_emb(dist).to(q)
        pos_attn = einsum(&quotb h n d, n r d -&gt; b h n r&quot, q, rel_pos_emb) * self.scale
        dots = dots + pos_attn</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/conformer/commit/0f8fbd31dc55d0dd6b6082fa74bad5f7da15fa1e#diff-10353d68441422b150f1ff9647eb5c4a5e0bea5b44830f4cf4ec45d403aee60dL96' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33293785</div><div id='project'> Project Name: lucidrains/conformer</div><div id='commit'> Commit Name: 0f8fbd31dc55d0dd6b6082fa74bad5f7da15fa1e</div><div id='time'> Time: 2021-01-03</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: conformer/conformer.py</div><div id='m_class'> M Class Name: Attention</div><div id='n_method'> N Class Name: Attention</div><div id='m_method'> M Method Name: forward(5)</div><div id='n_method'> N Method Name: forward(5)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: conformer/conformer.py</div><div id='n_file'> N File Name: conformer/conformer.py</div><div id='m_start'> M Start Line: 106</div><div id='m_end'> M End Line: 117</div><div id='n_start'> N Start Line: 98</div><div id='n_end'> N End Line: 109</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        if mask is not None:
            mask = F.pad(mask.flatten(1), (1, 0), value = True)
            assert mask.shape[-1] == dots.shape[-1], &quotmask has incorrect dimensions&quot
            mask = <a id="change">mask[:, None, :]</a> * <a id="change">mask[:, :, None]</a>
            dots.masked_fill_(~mask, mask_value)
            del mask

        attn = dots.softmax(dim=-1)</code></pre><h3>After Change</h3><pre><code class='java'>
        if mask is not None:
            mask = F.pad(mask.flatten(1), (1, 0), value = True)
            assert mask.shape[-1] == dots.shape[-1], &quotmask has incorrect dimensions&quot
            mask = <a id="change">rearrange(</a>mask, <a id="change">&quotb i -&gt; b () i ()&quot</a><a id="change">)</a><a id="change"> * </a><a id="change">rearrange(</a>mask, <a id="change">&quotb j -&gt; b () () j&quot</a><a id="change">)</a>
            dots.masked_fill_(~mask, mask_value)
            del mask

        attn = dots.softmax(dim=-1)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/vit-pytorch/commit/3f2cbc6e23b935224b57915a2b0e1a21eb60c5a6#diff-fce7f92e0b957764a240f657482514430a2fcdd8a0fcfc1d23372f119a92e6ccL49' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33293780</div><div id='project'> Project Name: lucidrains/vit-pytorch</div><div id='commit'> Commit Name: 3f2cbc6e23b935224b57915a2b0e1a21eb60c5a6</div><div id='time'> Time: 2021-02-17</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: vit_pytorch/vit_pytorch.py</div><div id='m_class'> M Class Name: Attention</div><div id='n_method'> N Class Name: Attention</div><div id='m_method'> M Method Name: forward(3)</div><div id='n_method'> N Method Name: forward(3)</div><div id='m_parent_class'> M Parent Class: nn.Module</div><div id='n_parent_class'> N Parent Class: nn.Module</div><div id='m_file'> M File Name: vit_pytorch/vit_pytorch.py</div><div id='n_file'> N File Name: vit_pytorch/vit_pytorch.py</div><div id='m_start'> M Start Line: 58</div><div id='m_end'> M End Line: 60</div><div id='n_start'> N Start Line: 58</div><div id='n_end'> N End Line: 60</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
        for key, db_offset, ann in zip(keys, self.db_offsets, self.anns):
            ann.add(key, ids = ann_insert_ids + db_offset)

        add_indices = (np.arange(num_memories)[None, :] + <a id="change">self.db_offsets[:, None]</a>) % self.max_num_entries
        self.db[<a id="change">np.arange(self.num_indices)[:, None]</a>, add_indices] = memories
        self.db.flush()

        self.db_offsets += num_memories</code></pre><h3>After Change</h3><pre><code class='java'>
        for key, db_offset, ann in zip(keys, self.db_offsets, self.anns):
            ann.add(key, ids = ann_insert_ids + db_offset)

        add_indices = (<a id="change">rearrange(</a>np.arange(num_memories), <a id="change">&quotj -&gt; 1 j&quot</a><a id="change">)</a><a id="change"> + </a><a id="change">rearrange(</a>self.db_offsets, <a id="change">&quoti -&gt; i 1&quot</a><a id="change">)</a>) % self.max_num_entries
        self.db[rearrange(np.arange(self.num_indices), &quoti -&gt; i 1&quot), add_indices] = memories
        self.db.flush()
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/memorizing-transformers-pytorch/commit/7553651625c462aa74ebb936dbb588af2f58907b#diff-312d9e6119d058dcad54d06d612544895f07d11f8c3c563ae3ada4163e7f8698L100' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 33293779</div><div id='project'> Project Name: lucidrains/memorizing-transformers-pytorch</div><div id='commit'> Commit Name: 7553651625c462aa74ebb936dbb588af2f58907b</div><div id='time'> Time: 2022-03-25</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: memorizing_transformers_pytorch/ann_memory.py</div><div id='m_class'> M Class Name: ANNMemory</div><div id='n_method'> N Class Name: ANNMemory</div><div id='m_method'> M Method Name: add(2)</div><div id='n_method'> N Method Name: add(2)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: memorizing_transformers_pytorch/ann_memory.py</div><div id='n_file'> N File Name: memorizing_transformers_pytorch/ann_memory.py</div><div id='m_start'> M Start Line: 103</div><div id='m_end'> M End Line: 112</div><div id='n_start'> N Start Line: 104</div><div id='n_end'> N End Line: 113</div><BR>