<html><h3>Pattern ID :10428
</h3><img src='36430474.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if is_train:
            model.zero_grad()
            loss.backward()
            <a id="change">optimizer.step()</a>

        total_loss += loss.item()

        tqdm_bar.set_description(&quot{} Epoch: [{}] Loss: {:.4f}&quot.format(desc, epoch, loss.item()))</code></pre><h3>After Change</h3><pre><code class='java'>
        _, z_j = model(x_j)

        loss = loss_func(z_i, z_j)
        loss<a id="change"> /= </a>args.accumulation_steps

        if is_train:
            loss.backward()

        <a id="change">if (i + 1) % args.accumulation_steps == 0</a> and is_train:
            <a id="change">optimizer.step()</a>
            model.zero_grad()

        total_loss += loss.item()
</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 7</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/mdiephuis/simclr/commit/903e8cf101c5589645c09840b36dab214ce5e1a1#diff-d27ba13ec888fc8b5237821fabbb1f3d6fde41243c432625a72b14a610f217ccL90' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 36430474</div><div id='project'> Project Name: mdiephuis/simclr</div><div id='commit'> Commit Name: 903e8cf101c5589645c09840b36dab214ce5e1a1</div><div id='time'> Time: 2020-03-09</div><div id='author'> Author: Maurits.Diephuis@unige.ch</div><div id='file'> File Name: train_features.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: train_validate(6)</div><div id='n_method'> N Method Name: train_validate(6)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: train_features.py</div><div id='n_file'> N File Name: train_features.py</div><div id='m_start'> M Start Line: 90</div><div id='m_end'> M End Line: 111</div><div id='n_start'> N Start Line: 94</div><div id='n_end'> N End Line: 123</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

            loss = (1 - lambda_st) * loss_output_st + lambda_st * t_st * t_st * loss_KD
            loss.backward()
            <a id="change">self.optimizer.step()</a>

            self.writer.set_step((epoch - 1) * self.len_epoch + batch_idx)
            self.train_metrics.update(&quotloss&quot, loss.item())
            for met in self.metric_ftns:</code></pre><h3>After Change</h3><pre><code class='java'>
        self.student.train()
        self.train_metrics.reset()
        
        for <a id="change">batch_idx</a>, (data, target) in enumerate(self.data_loader):
            data, target = data.to(self.device), target.to(self.device)

            output_tc = self.teacher(data)
            &#47&#47 TODO: Find an elegant way to free the feature map and computation graph
            output_tc = torch.tensor(output_tc.detach().cpu().numpy()).cuda()
            
            output_st = self.student(data)
            supervised_loss = self.criterion(output_st, target)
            kd_loss<a id="change"> = </a>self.kd_criterion(output_st, output_tc)

            loss = (1 - self.lamb) * supervised_loss + self.lamb* kd_loss
            loss = loss / self.accumulation_steps
            loss.backward()

            <a id="change">if (batch_idx+1) % self.accumulation_steps == 0</a>:
                <a id="change">self.optimizer.step()</a>
                self.optimizer.zero_grad()

            self.writer.set_step((epoch - 1) * self.len_epoch + batch_idx)
            self.train_metrics.update(&quotloss&quot, loss.item())</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lehduong/knowledge-distillation-by-replacing-cheap-conv/commit/6a2fac62789427919e4a6d91296bdb0801eecb79#diff-0564ece5aadc13d5923043addc422a8058c6d96026493e84652cedc2f1bdca99L214' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 36430473</div><div id='project'> Project Name: lehduong/knowledge-distillation-by-replacing-cheap-conv</div><div id='commit'> Commit Name: 6a2fac62789427919e4a6d91296bdb0801eecb79</div><div id='time'> Time: 2020-01-20</div><div id='author'> Author: oopsxilitol@gmail.com</div><div id='file'> File Name: trainer/trainer.py</div><div id='m_class'> M Class Name: TrainerTeacherAssistant</div><div id='n_method'> N Class Name: TrainerTeacherAssistant</div><div id='m_method'> M Method Name: _train_epoch(2)</div><div id='n_method'> N Method Name: _train_epoch(2)</div><div id='m_parent_class'> M Parent Class: BaseTrainer,BaseKnowledgeDistillationTrainer</div><div id='n_parent_class'> N Parent Class: BaseTrainer,BaseKnowledgeDistillationTrainer</div><div id='m_file'> M File Name: trainer/trainer.py</div><div id='n_file'> N File Name: trainer/trainer.py</div><div id='m_start'> M Start Line: 215</div><div id='m_end'> M End Line: 234</div><div id='n_start'> N Start Line: 174</div><div id='n_end'> N End Line: 193</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
            output = self.model(data)
            loss = self.criterion(output, target)
            loss.backward()
            <a id="change">self.optimizer.step()</a>

            self._logging(batch_idx, epoch, data, output, target, loss)

            if batch_idx == self.len_epoch:</code></pre><h3>After Change</h3><pre><code class='java'>
        
        self.model.train()
        self.train_metrics.reset()
        for <a id="change">batch_idx</a>, (data, target) in enumerate(self.data_loader):
            data, target = data.to(self.device), target.to(self.device)

            output = self.model(data)
            loss = self.criterion(output, target)
            loss<a id="change"> = </a>loss / self.accumulation_steps
            loss.backward()
            <a id="change">if (batch_idx+1) % self.accumulation_steps == 0</a>:
                <a id="change">self.optimizer.step()</a>
                self.optimizer.zero_grad()

            self._logging(batch_idx, epoch, data, output, target, loss)
</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lehduong/knowledge-distillation-by-replacing-cheap-conv/commit/6a2fac62789427919e4a6d91296bdb0801eecb79#diff-0564ece5aadc13d5923043addc422a8058c6d96026493e84652cedc2f1bdca99L105' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 36430471</div><div id='project'> Project Name: lehduong/knowledge-distillation-by-replacing-cheap-conv</div><div id='commit'> Commit Name: 6a2fac62789427919e4a6d91296bdb0801eecb79</div><div id='time'> Time: 2020-01-20</div><div id='author'> Author: oopsxilitol@gmail.com</div><div id='file'> File Name: trainer/trainer.py</div><div id='m_class'> M Class Name: Trainer</div><div id='n_method'> N Class Name: Trainer</div><div id='m_method'> M Method Name: _train_epoch(2)</div><div id='n_method'> N Method Name: _train_epoch(2)</div><div id='m_parent_class'> M Parent Class: BaseTrainer</div><div id='n_parent_class'> N Parent Class: BaseTrainer</div><div id='m_file'> M File Name: trainer/trainer.py</div><div id='n_file'> N File Name: trainer/trainer.py</div><div id='m_start'> M Start Line: 117</div><div id='m_end'> M End Line: 121</div><div id='n_start'> N Start Line: 68</div><div id='n_end'> N End Line: 79</div><BR>