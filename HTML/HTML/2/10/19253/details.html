<html><h3>Pattern ID :19253
</h3><img src='62676105.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        _check_response_errors(response)
        task = response.parsed.task

        <a id="change">if task == TaskType.TEXTCLASSIFICATION</a>:
            from rubrix.sdk.api.text_classification import _get_dataset_data

            map_fn<a id="change"> = </a>self._text_classification_sdk_to_record
            request_class<a id="change"> = </a>TextClassificationQuery
        elif task == TaskType.TOKENCLASSIFICATION:
            from rubrix.sdk.api.token_classification import _get_dataset_data

            map_fn<a id="change"> = </a>self._token_classification_sdk_to_record
            request_class<a id="change"> = </a>TokenClassificationQuery
        else:
            <a id="change">raise ValueError(
                f"Sorry, load method is only allowed with token and text classification"</a><a id="change">
            )</a>
        response = _get_dataset_data.sync_detailed(
            client=self._client,
            name=name,
            request=request_class(ids=ids or []),</code></pre><h3>After Change</h3><pre><code class='java'>

        task_config = {
            TaskType.TEXTCLASSIFICATION: (
                text_classification_get_dataset_data<a id="change">,
                self._text_classification_sdk_to_client,
                TextClassificationQuery</a>,
            ),
            TaskType.TOKENCLASSIFICATION: (
                token_classification_get_dataset_data,</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 9</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/recognai/rubrix/commit/2eb78d55e689aa09ee41118efd77eaaaceccbb9b#diff-d20401640b99530f47a19433d5b6491c1a612c9c449ae2661b3d42683a5e564aL192' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 62676105</div><div id='project'> Project Name: recognai/rubrix</div><div id='commit'> Commit Name: 2eb78d55e689aa09ee41118efd77eaaaceccbb9b</div><div id='time'> Time: 2021-09-07</div><div id='author'> Author: francisco@recogn.ai</div><div id='file'> File Name: src/rubrix/client/__init__.py</div><div id='m_class'> M Class Name: RubrixClient</div><div id='n_method'> N Class Name: RubrixClient</div><div id='m_method'> M Method Name: load(4)</div><div id='n_method'> N Method Name: load(4)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: src/rubrix/client/__init__.py</div><div id='n_file'> N File Name: src/rubrix/client/__init__.py</div><div id='m_start'> M Start Line: 192</div><div id='m_end'> M End Line: 208</div><div id='n_start'> N Start Line: 224</div><div id='n_end'> N End Line: 251</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
class GeneralGroupedDataLoader(NegSampleBasedDataLoader):
    def __init__(self, config, dataset, sampler, phase, neg_sample_args,
                 batch_size=1, dl_format=InputType.POINTWISE, shuffle=False):
        <a id="change">if neg_sample_args[&quotstrategy&quot] != &quotto&quot</a>:
            <a id="change">raise ValueError(&quotneg_sample strategy in GeneralGroupedDataLoader() should be `to`&quot</a><a id="change">)</a>
        if dl_format == InputType.PAIRWISE:
            raise ValueError(&quotpairwise dataloader cannot neg sample to&quot)

        self.uid2items<a id="change"> = </a>dataset.uid2items
        self.full = (neg_sample_args[&quotto&quot] == -1)

        super(GeneralGroupedDataLoader, self).__init__(config, dataset, sampler, phase, neg_sample_args,
                                                       batch_size, dl_format, shuffle)

        label_field<a id="change"> = </a>self.config[&quotLABEL_FIELD&quot]
        self.dataset.field2type[label_field] = &quotfloat&quot
        self.dataset.field2source[label_field]<a id="change"> = </a>&quotinter&quot
        self.dataset.field2seqlen[label_field]<a id="change"> = </a>1

    def _batch_size_adaptation(self):
        if self.neg_sample_args[&quotto&quot] == -1:</code></pre><h3>After Change</h3><pre><code class='java'>
class GeneralGroupedDataLoader(GeneralInteractionBasedDataLoader):
    def __init__(self, config, dataset, sampler, phase, neg_sample_args,
                 batch_size=1, dl_format=InputType.POINTWISE, shuffle=False):
        self.uid2index<a id="change">, self.uid2items_num</a> = dataset.uid2index

        super(GeneralGroupedDataLoader, self).__init__(config, dataset, sampler, phase, neg_sample_args,
                                                       batch_size, dl_format, shuffle)</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/rucaibox/recbole/commit/f60d378cf81fae5001fcb3c8ddba9ef8d4553aac#diff-f41d5d93e76efa5638a691f8947c424c58ec3f51b884fc0d235ddd324dd574f2L208' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 62676100</div><div id='project'> Project Name: rucaibox/recbole</div><div id='commit'> Commit Name: f60d378cf81fae5001fcb3c8ddba9ef8d4553aac</div><div id='time'> Time: 2020-08-11</div><div id='author'> Author: 297086016@qq.com</div><div id='file'> File Name: data/dataloader.py</div><div id='m_class'> M Class Name: GeneralGroupedDataLoader</div><div id='n_method'> N Class Name: GeneralGroupedDataLoader</div><div id='m_method'> M Method Name: __init__(9)</div><div id='n_method'> N Method Name: __init__(9)</div><div id='m_parent_class'> M Parent Class: GeneralInteractionBasedDataLoader</div><div id='n_parent_class'> N Parent Class: NegSampleBasedDataLoader</div><div id='m_file'> M File Name: data/dataloader.py</div><div id='n_file'> N File Name: data/dataloader.py</div><div id='m_start'> M Start Line: 210</div><div id='m_end'> M End Line: 224</div><div id='n_start'> N Start Line: 213</div><div id='n_end'> N End Line: 213</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>

        should_backwards = any([*map(lambda t: t.requires_grad, (q, k, v, attn_bias))])

        <a id="change">if v_dim == 64</a>:
            forward<a id="change"> = </a>forward_value_64
            backward<a id="change"> = </a>backward_value_64
        elif v_dim == 32:
            forward<a id="change"> = </a>forward_value_32
            backward<a id="change"> = </a>backward_value_32
        else:
            <a id="change">raise ValueError(&quotinvalid value dimension&quot</a><a id="change">)</a>

        o, l = forward(
            q, k, v,
            mask,</code></pre><h3>After Change</h3><pre><code class='java'>
        scale,
        causal
    ):
        qk_dim, v_dim = q.shape[-1]<a id="change">, v.shape[-1]</a>

        batch, heads, seq, _, dim, device, dtype = *q.shape, v.shape[-1], q.device, q.dtype

        mask = default(mask, lambda: torch.ones(q.shape[0], 0, device = q.device, dtype = torch.bool))</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/lucidrains/flash-cosine-sim-attention/commit/2537a7c7f24af6caa566ba4fe1487516e0ed9780#diff-9682bb6c6f80a5ac1b8cf3f60ac58f5e21edb3235f4a36b9caed7ba4823b0931L82' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 62676102</div><div id='project'> Project Name: lucidrains/flash-cosine-sim-attention</div><div id='commit'> Commit Name: 2537a7c7f24af6caa566ba4fe1487516e0ed9780</div><div id='time'> Time: 2022-09-24</div><div id='author'> Author: lucidrains@gmail.com</div><div id='file'> File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='m_class'> M Class Name: FlashCosineSimAttention</div><div id='n_method'> N Class Name: FlashCosineSimAttention</div><div id='m_method'> M Method Name: forward(8)</div><div id='n_method'> N Method Name: forward(8)</div><div id='m_parent_class'> M Parent Class: Function</div><div id='n_parent_class'> N Parent Class: Function</div><div id='m_file'> M File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='n_file'> N File Name: flash_cosine_sim_attention/flash_cosine_sim_attention.py</div><div id='m_start'> M Start Line: 90</div><div id='m_end'> M End Line: 128</div><div id='n_start'> N Start Line: 88</div><div id='n_end'> N End Line: 88</div><BR>