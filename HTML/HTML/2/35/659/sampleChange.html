<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        torch.tensor: values of each orbital radial part at each position
    

    if <a id="change">derivative == 0</a>:

        rn = fast_power(R, bas_n)
        expr = torch.exp(-bas_exp * R*R)

        <a id="change">return </a>rn * expr

    elif <a id="change">derivative &gt; 0</a>:

        rn<a id="change"> = </a>fast_power(R, bas_n)
        nabla_rn = (bas_n * R**(bas_n - 2)).unsqueeze(-1) * xyz

        er = torch.exp(-bas_exp * R**2)
        nabla_er = -2 * (bas_exp * er).unsqueeze(-1) * xyz

        if derivative == 1:
            <a id="change">if </a>jacobian:
                nabla_rn = nabla_rn.sum(3)
                nabla_er = nabla_er.sum(3)
                return nabla_rn * er + rn * nabla_er
            else:
                return nabla_rn * \
                    er.unsqueeze(-1) + rn.unsqueeze(-1) * nabla_er

        elif <a id="change">derivative == 2</a>:

            lap_rn<a id="change"> = </a>bas_n * (3 * R**(bas_n - 2)
                              + (xyz**2).sum(3) * (bas_n - 2) * R**(bas_n - 4))

            lap_er<a id="change"> = </a>4 * bas_exp**2 * (xyz**2).sum(3) * er \
                - 6 * bas_exp * er

            return lap_rn * er + 2 * \</code></pre><h3>After Change</h3><pre><code class='java'>
        torch.tensor: values of each orbital radial part at each position
    

    if <a id="change">not isinstance(derivative, list)</a>:
        derivative = <a id="change">[derivative</a>]

    def _kernel():
        return rn * er

    def _first_derivative_kernel():
        if jacobian:
            nabla_rn_sum = nabla_rn.sum(3)
            nabla_er_sum = nabla_er.sum(3)
            return nabla_rn_sum * er + rn * nabla_er_sum
        else:
            return nabla_rn * \
                er.unsqueeze(-1) + rn.unsqueeze(-1) * nabla_er

    def _second_derivative_kernel():
        lap_rn = bas_n * (3 * R**(bas_n - 2)
                          + (xyz**2).sum(3) * (bas_n - 2) * R**(bas_n - 4))

        lap_er = 4 * bas_exp**2 * (xyz**2).sum(3) * er \
            - 6 * bas_exp * er

        return lap_rn * er + 2 * \
            (nabla_rn * nabla_er).sum(3) + rn * lap_er

    &#47&#47 computes the basic  quantities
    rn = fast_power(R, bas_n)
    er = torch.exp(-bas_exp * R*R)

    &#47&#47 computes the grads
    if any(x in derivative for x in [1, 2]):
        nabla_rn = (bas_n * R**(bas_n - 2)).unsqueeze(-1) * xyz
        nabla_er = -2 * (bas_exp * er).unsqueeze(-1) * xyz

    &#47&#47 prepare the output/function calls
    <a id="change">output = </a><a id="change">[]</a>
    fns<a id="change"> = </a><a id="change">[</a>_kernel,
           _first_derivative_kernel,
           _second_derivative_kernel<a id="change"></a>]

    &#47&#47 compute the requested derivatives
    <a id="change">for d</a> in derivative<a id="change">:
        output.append(</a><a id="change">fns[d]())</a>

    <a id="change">if len(derivative) == 1</a>:
        <a id="change">return output</a><a id="change">[0]</a>
    else:
        <a id="change">return output</a>
</code></pre>