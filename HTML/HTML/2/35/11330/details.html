<html><h3>Pattern ID :11330
</h3><img src='38531035.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    has_feature_processor: bool = False,
) -&gt; float:
    input_read_size = math.ceil(
        <a id="change">global_batch_size</a><a id="change"> * sum(input_lengths</a><a id="change">)</a> / local_world_size * input_data_type_size
    )
    if is_weighted or has_feature_processor:
        input_read_size *= 2
    embedding_lookup_size<a id="change"> = </a>(
        <a id="change">global_batch_size</a><a id="change">
        * sum(input_lengths)
        / local_world_size
        * emb_dim
        * </a>output_data_type_size
    )
    num_outputs = (
        <a id="change">len(input_lengths</a><a id="change">)</a><a id="change">
        if is_pooled</a><a id="change">
        else </a><a id="change">float(sum(input_lengths</a><a id="change">) / local_world_size</a><a id="change">)</a>
    )
    output_write_size = (
        global_batch_size * emb_dim * num_outputs * output_data_type_size
    )</code></pre><h3>After Change</h3><pre><code class='java'>
    has_feature_processor: bool = False,
) -&gt; float:

    num_inputs<a id="change"> = </a>(
        <a id="change">sum([x * y for x, y in zip(input_lengths, num_objects)]</a><a id="change">) / </a>local_world_size
    )
    num_outputs = <a id="change">sum(num_objects</a><a id="change">) if is_pooled</a><a id="change"> else </a>num_inputs

    input_read_size = math.ceil(num_inputs * global_batch_size * input_data_type_size)
    if is_weighted or has_feature_processor:
        input_read_size *= 2
    embedding_lookup_size<a id="change"> = </a>(
        num_inputs<a id="change"> * global_batch_size * emb_dim * </a>output_data_type_size
    )
    output_write_size = (
        global_batch_size * emb_dim * num_outputs * output_data_type_size</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 4</div><BR><div id='size'>Non-data size: 26</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/pytorch/torchrec/commit/c5a46f1ad1734736e9f05708940336413df74f26#diff-03f056051f1fe77d422eadb7df762af8b375e6dbb1e7f2be7cbe786266aae090L381' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 38531035</div><div id='project'> Project Name: pytorch/torchrec</div><div id='commit'> Commit Name: c5a46f1ad1734736e9f05708940336413df74f26</div><div id='time'> Time: 2022-05-12</div><div id='author'> Author: dstaay@fb.com</div><div id='file'> File Name: torchrec/distributed/planner/shard_estimators.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _get_twrw_sharding_perf(14)</div><div id='n_method'> N Method Name: _get_twrw_sharding_perf(13)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: torchrec/distributed/planner/shard_estimators.py</div><div id='n_file'> N File Name: torchrec/distributed/planner/shard_estimators.py</div><div id='m_start'> M Start Line: 381</div><div id='m_end'> M End Line: 396</div><div id='n_start'> N Start Line: 387</div><div id='n_end'> N End Line: 406</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    has_feature_processor: bool = False,
) -&gt; float:
    input_read_size = math.ceil(
        global_batch_size<a id="change"> * sum(</a>input_lengths<a id="change">)</a> / world_size * input_data_type_size
    )
    if is_weighted or has_feature_processor:
        input_read_size *= 2
    embedding_lookup_size<a id="change"> = </a>(
        global_batch_size<a id="change">
        * sum(input_lengths)
        / world_size
        * </a><a id="change">emb_dim
        * </a>output_data_type_size
    )
    num_outputs = (
        <a id="change">len(</a>input_lengths<a id="change">)</a><a id="change"> if is_pooled</a><a id="change"> else </a><a id="change">float(sum(</a>input_lengths<a id="change">) / </a>world_size<a id="change">)</a>
    )
    output_write_size = (
        global_batch_size * emb_dim * num_outputs * output_data_type_size
    )</code></pre><h3>After Change</h3><pre><code class='java'>
    has_feature_processor: bool = False,
) -&gt; float:

    num_inputs<a id="change"> = </a><a id="change">sum([x * y for x, y in zip(input_lengths, num_objects)]</a><a id="change">) / </a>world_size
    num_outputs = <a id="change">sum(</a>num_objects<a id="change">) if is_pooled</a><a id="change"> else </a>num_inputs

    input_read_size = math.ceil(num_inputs * global_batch_size * input_data_type_size)
    if is_weighted or has_feature_processor:
        input_read_size *= 2
    embedding_lookup_size<a id="change"> = </a>(
        num_inputs<a id="change"> * global_batch_size * emb_dim * </a>output_data_type_size
    )

    output_write_size = (</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/c5a46f1ad1734736e9f05708940336413df74f26#diff-03f056051f1fe77d422eadb7df762af8b375e6dbb1e7f2be7cbe786266aae090L302' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 38531034</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: c5a46f1ad1734736e9f05708940336413df74f26</div><div id='time'> Time: 2022-05-12</div><div id='author'> Author: dstaay@fb.com</div><div id='file'> File Name: torchrec/distributed/planner/shard_estimators.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _get_rw_sharding_perf(14)</div><div id='n_method'> N Method Name: _get_rw_sharding_perf(13)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: torchrec/distributed/planner/shard_estimators.py</div><div id='n_file'> N File Name: torchrec/distributed/planner/shard_estimators.py</div><div id='m_start'> M Start Line: 318</div><div id='m_end'> M End Line: 331</div><div id='n_start'> N Start Line: 327</div><div id='n_end'> N End Line: 344</div><BR>'><BR><BR><BR><h3>Before Change</h3><pre><code class='java'>
    has_feature_processor: bool = False,
) -&gt; float:
    input_read_size = math.ceil(
        global_batch_size<a id="change"> * sum(</a>input_lengths<a id="change">)</a> / local_world_size * input_data_type_size
    )
    if is_weighted or has_feature_processor:
        input_read_size *= 2
    embedding_lookup_size<a id="change"> = </a>(
        global_batch_size<a id="change">
        * sum(input_lengths)
        / local_world_size
        * </a><a id="change">emb_dim
        * </a>output_data_type_size
    )
    num_outputs = (
        <a id="change">len(</a>input_lengths<a id="change">)</a><a id="change">
        if is_pooled</a><a id="change">
        else </a><a id="change">float(sum(</a>input_lengths<a id="change">) / </a>local_world_size<a id="change">)</a>
    )
    output_write_size = (
        global_batch_size * emb_dim * num_outputs * output_data_type_size
    )</code></pre><h3>After Change</h3><pre><code class='java'>
    has_feature_processor: bool = False,
) -&gt; float:

    num_inputs<a id="change"> = </a>(
        <a id="change">sum([x * y for x, y in zip(input_lengths, num_objects)]</a><a id="change">) / </a>local_world_size
    )
    num_outputs = <a id="change">sum(</a>num_objects<a id="change">) if is_pooled</a><a id="change"> else </a>num_inputs

    input_read_size = math.ceil(num_inputs * global_batch_size * input_data_type_size)
    if is_weighted or has_feature_processor:
        input_read_size *= 2
    embedding_lookup_size<a id="change"> = </a>(
        num_inputs<a id="change"> * global_batch_size * </a><a id="change">emb_dim * </a>output_data_type_size
    )
    output_write_size = (
        global_batch_size * emb_dim * num_outputs * output_data_type_size</code></pre>'><BR><BR><BR><BR><div id='link'><a href='https://github.com/facebookresearch/torchrec/commit/c5a46f1ad1734736e9f05708940336413df74f26#diff-03f056051f1fe77d422eadb7df762af8b375e6dbb1e7f2be7cbe786266aae090L365' target='_blank'>Link</a></div><div id='fragmentid'> Fragment ID: 38531038</div><div id='project'> Project Name: facebookresearch/torchrec</div><div id='commit'> Commit Name: c5a46f1ad1734736e9f05708940336413df74f26</div><div id='time'> Time: 2022-05-12</div><div id='author'> Author: dstaay@fb.com</div><div id='file'> File Name: torchrec/distributed/planner/shard_estimators.py</div><div id='m_class'> M Class Name: AnonimousClass</div><div id='n_method'> N Class Name: AnonimousClass</div><div id='m_method'> M Method Name: _get_twrw_sharding_perf(14)</div><div id='n_method'> N Method Name: _get_twrw_sharding_perf(13)</div><div id='m_parent_class'> M Parent Class: </div><div id='n_parent_class'> N Parent Class: </div><div id='m_file'> M File Name: torchrec/distributed/planner/shard_estimators.py</div><div id='n_file'> N File Name: torchrec/distributed/planner/shard_estimators.py</div><div id='m_start'> M Start Line: 381</div><div id='m_end'> M End Line: 396</div><div id='n_start'> N Start Line: 387</div><div id='n_end'> N End Line: 406</div><BR>